                                                                      
                                                                      
                                                                      
                                                                      
          deep net with over a million layers, and we would have a single (very long) instance
          to train it. Instead, we will use the dataset’s window() method to convert this long
          sequence of characters into many smaller windows of text. Every instance in the data‐
          set will be a fairly short substring of the whole text, and the RNN will be unrolled
          only over the length of these substrings. This is called truncated backpropagation
          through time. Let’s call the window() method to create a dataset of short text windows:
                                                                      
            n_steps = 100                                             
            window_length = n_steps + 1 # target = input shifted 1 character ahead
            dataset = dataset.window(window_length, shift=1, drop_remainder=True)
                   You can try tuning n_steps: it is easier to train RNNs on shorter
                   input sequences, but of course the RNN will not be able to learn
                   any pattern longer than n_steps, so don’t make it too small.
                                                                      
                                                                      
          By default, the window() method creates nonoverlapping windows, but to get the
          largest possible training set we use shift=1 so that the first window contains charac‐
          ters 0 to 100, the second contains characters 1 to 101, and so on. To ensure that all
          windows are exactly 101 characters long (which will allow us to create batches
          without having to do any padding), we set drop_remainder=True (otherwise the last
          100 windows will contain 100 characters, 99 characters, and so on down to 1
          character).                                                 
                                                                      
          The window() method creates a dataset that contains windows, each of which is also
          represented as a dataset. It’s a nested dataset, analogous to a list of lists. This is useful
          when you want to transform each window by calling its dataset methods (e.g., to
          shuffle them or batch them). However, we cannot use a nested dataset directly for
          training, as our model will expect tensors as input, not datasets. So, we must call the
          flat_map() method: it converts a nested dataset into a flat dataset (one that does not
          contain datasets). For example, suppose {1, 2, 3} represents a dataset containing the
          sequence of tensors 1, 2, and 3. If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}},
          you get back the flat dataset {1, 2, 3, 4, 5, 6}. Moreover, the flat_map() method takes
          a function as an argument, which allows you to transform each dataset in the nested
          dataset before flattening. For example, if you pass the function lambda ds:
          ds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5,
          6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset of tensors of size 2. With that
          in mind, we are ready to flatten our dataset:               
            dataset = dataset.flat_map(lambda window: window.batch(window_length))
          Notice that we call batch(window_length) on each window: since all windows have
          exactly that length, we will get a single tensor for each of them. Now the dataset con‐
          tains consecutive windows of 101 characters each. Since Gradient Descent works best
                                                                      