                                                                      
                                                                      
                                                                      
                                                                      
          when the instances in the training set are independent and identically distributed (see
          Chapter 4), we need to shuffle these windows. Then we can batch the windows and
          separate the inputs (the first 100 characters) from the target (the last character):
                                                                      
            batch_size = 32                                           
            dataset = dataset.shuffle(10000).batch(batch_size)        
            dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
          Figure 16-1 summarizes the dataset preparation steps discussed so far (showing win‐
          dows of length 11 rather than 101, and a batch size of 3 instead of 32).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-1. Preparing a dataset of shuffled windows        
                                                                      
          As discussed in Chapter 13, categorical input features should generally be encoded,
          usually as one-hot vectors or as embeddings. Here, we will encode each character
          using a one-hot vector because there are fairly few distinct characters (only 39):
            dataset = dataset.map(                                    
               lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))
                                                                      
          Finally, we just need to add prefetching:                   
            dataset = dataset.prefetch(1)                             
          That’s it! Preparing the dataset was the hardest part. Now let’s create the model.
                                                                      
          Building and Training the Char-RNN Model                    
                                                                      
          To predict the next character based on the previous 100 characters, we can use an
          RNN with 2 GRU layers of 128 units each and 20% dropout on both the inputs (drop
          out) and the hidden states (recurrent_dropout). We can tweak these hyperparame‐
          ters later, if needed. The output layer is a time-distributed Dense layer like we saw in
          Chapter 15. This time this layer must have 39 units (max_id) because there are 39 dis‐
          tinct characters in the text, and we want to output a probability for each possible
          character (at each time step). The output probabilities should sum up to 1 at each
          time step, so we apply the softmax activation function to the outputs of the Dense