                                                                      
                                                                      
                                                                      
                                                                      
          method estimates the log of the probability density function (PDF) at that location.
          The greater the score, the higher the density:              
                                                                      
            >>> gm.score_samples(X)                                   
            array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,
                -4.39802535, -3.80743859])                            
          If you compute the exponential of these scores, you get the value of the PDF at the
          location of the given instances. These are not probabilities, but probability densities:
          they can take on any positive value, not just a value between 0 and 1. To estimate the
          probability that an instance will fall within a particular region, you would have to
          integrate the PDF over that region (if you do so over the entire space of possible
          instance locations, the result will be 1).                  
          Figure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the
          density contours of this model.                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-17. Cluster means, decision boundaries, and density contours of a trained
          Gaussian mixture model                                      
                                                                      
          Nice! The algorithm clearly found an excellent solution. Of course, we made its task
          easy by generating the data using a set of 2D Gaussian distributions (unfortunately,
          real-life data is not always so Gaussian and low-dimensional). We also gave the algo‚Äê
          rithm the correct number of clusters. When there are many dimensions, or many
          clusters, or few instances, EM can struggle to converge to the optimal solution. You
          might need to reduce the difficulty of the task by limiting the number of parameters
          that the algorithm has to learn. One way to do this is to limit the range of shapes and
          orientations that the clusters can have. This can be achieved by imposing constraints
          on the covariance matrices. To do this, set the covariance_type hyperparameter to
          one of the following values:                                
                                                                      