                                                                      
                                                                      
                                                                      
                                                                      
          A SequenceExample contains a Features object for the contextual data and a Fea
          tureLists object that contains one or more named FeatureList objects (e.g., a Fea
          tureList named "content" and another named "comments"). Each FeatureList
          contains a list of Feature objects, each of which may be a list of byte strings, a list of
          64-bit integers, or a list of floats (in this example, each Feature would represent a
          sentence or a comment, perhaps in the form of a list of word identifiers). Building a
          SequenceExample, serializing it, and parsing it is similar to building, serializing, and
          parsing an Example, but you must use tf.io.parse_single_sequence_example() to
          parse a single SequenceExample or tf.io.parse_sequence_example() to parse a
          batch. Both functions return a tuple containing the context features (as a dictionary)
          and the feature lists (also as a dictionary). If the feature lists contain sequences of
          varying sizes (as in the preceding example), you may want to convert them to ragged
          tensors, using tf.RaggedTensor.from_sparse() (see the notebook for the full code):
                                                                      
            parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(
               serialized_sequence_example, context_feature_descriptions,
               sequence_feature_descriptions)                         
            parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists["content"])
          Now that you know how to efficiently store, load, and parse data, the next step is to
          prepare it so that it can be fed to a neural network.       
          Preprocessing the Input Features                            
                                                                      
          Preparing your data for a neural network requires converting all features into numer‐
          ical features, generally normalizing them, and more. In particular, if your data con‐
          tains categorical features or text features, they need to be converted to numbers. This
          can be done ahead of time when preparing your data files, using any tool you like
          (e.g., NumPy, pandas, or Scikit-Learn). Alternatively, you can preprocess your data on
          the fly when loading it with the Data API (e.g., using the dataset’s map() method, as
          we saw earlier), or you can include a preprocessing layer directly in your model. Let’s
          look at this last option now.                               
          For example, here is how you can implement a standardization layer using a Lambda
          layer. For each feature, it subtracts the mean and divides by its standard deviation
          (plus a tiny smoothing term to avoid division by zero):     
                                                                      
            means = np.mean(X_train, axis=0, keepdims=True)           
            stds = np.std(X_train, axis=0, keepdims=True)             
            eps = keras.backend.epsilon()                             
            model = keras.models.Sequential([                         
               keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),
               [...] # other layers                                   
            ])                                                        
                                                                      