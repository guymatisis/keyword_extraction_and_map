                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-1. TF Serving can serve multiple models and automatically deploy the latest
          version of each model                                       
                                                                      
          So let’s suppose you have trained an MNIST model using tf.keras, and you want to
          deploy it to TF Serving. The first thing you have to do is export this model to Tensor‐
          Flow’s SavedModel format.                                   
                                                                      
          Exporting SavedModels                                       
          TensorFlow provides a simple tf.saved_model.save() function to export models to
          the SavedModel format. All you need to do is give it the model, specifying its name
          and version number, and the function will save the model’s computation graph and its
          weights:                                                    
            model = keras.models.Sequential([...])                    
            model.compile([...])                                      
            history = model.fit([...])                                
            model_version = "0001"                                    
            model_name = "my_mnist_model"                             
            model_path = os.path.join(model_name, model_version)      
            tf.saved_model.save(model, model_path)                    
          Alternatively, you can just use the model’s save() method (model.save(model_
          path)): as long as the file’s extension is not .h5, the model will be saved using the
          SavedModel format instead of the HDF5 format.               
                                                                      
          It’s usually a good idea to include all the preprocessing layers in the final model you
          export so that it can ingest data in its natural form once it is deployed to production.
          This avoids having to take care of preprocessing separately within the application that
          uses the model. Bundling the preprocessing steps within the model also makes it sim‐
          pler to update them later on and limits the risk of mismatch between a model and the
          preprocessing steps it requires.                            
                                                                      
                                                                      