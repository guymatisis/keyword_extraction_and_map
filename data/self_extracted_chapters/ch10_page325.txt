                                                                      
                                                                      
                                                                      
                                                                      
          tures can coalesce into far fewer high-level features. A typical neural network for
          MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200,
          and the third with 100. However, this practice has been largely abandoned because it
          seems that using the same number of neurons in all hidden layers performs just as
          well in most cases, or even better; plus, there is only one hyperparameter to tune,
          instead of one per layer. That said, depending on the dataset, it can sometimes help to
          make the first hidden layer bigger than the others.         
                                                                      
          Just like the number of layers, you can try increasing the number of neurons gradu‐
          ally until the network starts overfitting. But in practice, it’s often simpler and more
          efficient to pick a model with more layers and neurons than you actually need, then
          use early stopping and other regularization techniques to prevent it from overfitting.
          Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants”
          approach: instead of wasting time looking for pants that perfectly match your size,
          just use large stretch pants that will shrink down to the right size. With this approach,
          you avoid bottleneck layers that could ruin your model. On the flip side, if a layer has
          too few neurons, it will not have enough representational power to preserve all the
          useful information from the inputs (e.g., a layer with two neurons can only output 2D
          data, so if it processes 3D data, some information will be lost). No matter how big and
          powerful the rest of the network is, that information will never be recovered.
                   In general you will get more bang for your buck by increasing the
                   number of layers instead of the number of neurons per layer.
                                                                      
                                                                      
                                                                      
          Learning Rate, Batch Size, and Other Hyperparameters        
                                                                      
          The numbers of hidden layers and neurons are not the only hyperparameters you can
          tweak in an MLP. Here are some of the most important ones, as well as tips on how to
          set them:                                                   
          Learning rate                                               
            The learning rate is arguably the most important hyperparameter. In general, the
            optimal learning rate is about half of the maximum learning rate (i.e., the learn‐
            ing rate above which the training algorithm diverges, as we saw in Chapter 4).
            One way to find a good learning rate is to train the model for a few hundred iter‐
            ations, starting with a very low learning rate (e.g., 10-5) and gradually increasing
            it up to a very large value (e.g., 10). This is done by multiplying the learning rate
            by a constant factor at each iteration (e.g., by exp(log(106)/500) to go from 10-5 to
            10 in 500 iterations). If you plot the loss as a function of the learning rate (using a
            log scale for the learning rate), you should see it dropping at first. But after a
            while, the learning rate will be too large, so the loss will shoot back up: the opti‐
                                                                      