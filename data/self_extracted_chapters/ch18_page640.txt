                                                                      
                                                                      
                                                                      
                                                                      
          Double DQN                                                  
                                                                      
          In a 2015 paper,14 DeepMind researchers tweaked their DQN algorithm, increasing
          its performance and somewhat stabilizing training. They called this variant Double
          DQN. The update was based on the observation that the target network is prone to
          overestimating Q-Values. Indeed, suppose all actions are equally good: the Q-Values
          estimated by the target model should be identical, but since they are approximations,
          some may be slightly greater than others, by pure chance. The target model will
          always select the largest Q-Value, which will be slightly greater than the mean Q-
          Value, most likely overestimating the true Q-Value (a bit like counting the height of
          the tallest random wave when measuring the depth of a pool). To fix this, they pro‐
          posed using the online model instead of the target model when selecting the best
          actions for the next states, and using the target model only to estimate the Q-Values
          for these best actions. Here is the updated training_step() function:
            def training_step(batch_size):                            
               experiences = sample_experiences(batch_size)           
               states, actions, rewards, next_states, dones = experiences
               next_Q_values = model.predict(next_states)             
               best_next_actions = np.argmax(next_Q_values, axis=1)   
               next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()
               next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1)
               target_Q_values = (rewards +                           
                          (1 - dones) * discount_factor * next_best_Q_values)
               mask = tf.one_hot(actions, n_outputs)                  
               [...] # the rest is the same as earlier                
          Just a few months later, another improvement to the DQN algorithm was proposed.
          Prioritized Experience Replay                               
          Instead of sampling experiences uniformly from the replay buffer, why not sample
          important experiences more frequently? This idea is called importance sampling (IS)
          or prioritized experience replay (PER), and it was introduced in a 2015 paper15 by
          DeepMind researchers (once again!).                         
          More specifically, experiences are considered “important” if they are likely to lead to
          fast learning progress. But how can we estimate this? One reasonable approach is to
          measure the magnitude of the TD error δ = r + γ·V(s′) – V(s). A large TD error indi‐
          cates that a transition (s, r, s′) is very surprising, and thus probably worth learning
                                                                      
                                                                      
                                                                      
                                                                      
          14 Hado van Hasselt et al., “Deep Reinforcement Learning with Double Q-Learning,” Proceedings of the 30th
           AAAI Conference on Artificial Intelligence (2015): 2094–2100.
          15 Tom Schaul et al., “Prioritized Experience Replay,” arXiv preprint arXiv:1511.05952 (2015).