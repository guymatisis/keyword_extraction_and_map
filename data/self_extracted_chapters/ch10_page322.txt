                                                                      
                                                                      
                                                                      
                                                                      
            >>> rnd_search_cv.best_params_                            
            {'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}
            >>> rnd_search_cv.best_score_                             
            -0.3189529188278931                                       
            >>> model = rnd_search_cv.best_estimator_.model           
          You can now save this model, evaluate it on the test set, and, if you are satisfied with
          its performance, deploy it to production. Using randomized search is not too hard,
          and it works well for many fairly simple problems. When training is slow, however
          (e.g., for more complex problems with larger datasets), this approach will only
          explore a tiny portion of the hyperparameter space. You can partially alleviate this
          problem by assisting the search process manually: first run a quick random search
          using wide ranges of hyperparameter values, then run another search using smaller
          ranges of values centered on the best ones found during the first run, and so on. This
          approach will hopefully zoom in on a good set of hyperparameters. However, it’s very
          time consuming, and probably not the best use of your time. 
          Fortunately, there are many techniques to explore a search space much more effi‐
          ciently than randomly. Their core idea is simple: when a region of the space turns out
          to be good, it should be explored more. Such techniques take care of the “zooming”
          process for you and lead to much better solutions in much less time. Here are some
          Python libraries you can use to optimize hyperparameters:   
          Hyperopt                                                    
            A popular library for optimizing over all sorts of complex search spaces (includ‐
            ing real values, such as the learning rate, and discrete values, such as the number
            of layers).                                               
                                                                      
          Hyperas, kopt, or Talos                                     
            Useful libraries for optimizing hyperparameters for Keras models (the first two
            are based on Hyperopt).                                   
          Keras Tuner                                                 
            An easy-to-use hyperparameter optimization library by Google for Keras models,
            with a hosted service for visualization and analysis.     
                                                                      
          Scikit-Optimize (skopt)                                     
            A general-purpose optimization library. The BayesSearchCV class performs
            Bayesian optimization using an interface similar to GridSearchCV.
          Spearmint                                                   
            A Bayesian optimization library.                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      