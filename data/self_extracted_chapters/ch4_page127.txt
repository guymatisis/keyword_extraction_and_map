                                                                      
                                                                      
                                                                      
                                                                      
            from sklearn.linear_model import SGDRegressor             
            sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)
            sgd_reg.fit(X, y.ravel())                                 
          Once again, you find a solution quite close to the one returned by the Normal
          Equation:                                                   
            >>> sgd_reg.intercept_, sgd_reg.coef_                     
            (array([4.24365286]), array([2.8250878]))                 
                                                                      
          Mini-batch Gradient Descent                                 
                                                                      
          The last Gradient Descent algorithm we will look at is called Mini-batch Gradient
          Descent. It is simple to understand once you know Batch and Stochastic Gradient
          Descent: at each step, instead of computing the gradients based on the full training set
          (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD
          computes the gradients on small random sets of instances called mini-batches. The
          main advantage of Mini-batch GD over Stochastic GD is that you can get a perfor‐
          mance boost from hardware optimization of matrix operations, especially when using
          GPUs.                                                       
          The algorithm’s progress in parameter space is less erratic than with Stochastic GD,
          especially with fairly large mini-batches. As a result, Mini-batch GD will end up walk‐
          ing around a bit closer to the minimum than Stochastic GD—but it may be harder for
          it to escape from local minima (in the case of problems that suffer from local minima,
          unlike Linear Regression). Figure 4-11 shows the paths taken by the three Gradient
          Descent algorithms in parameter space during training. They all end up near the
          minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic
          GD and Mini-batch GD continue to walk around. However, don’t forget that Batch
          GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD
          would also reach the minimum if you used a good learning schedule.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-11. Gradient Descent paths in parameter space      
                                                                      