                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          On the left is the noisy input image, and on the right is the clean target image. Now
          let’s train the classifier and make it clean this image:    
                                                                      
            knn_clf.fit(X_train_mod, y_train_mod)                     
            clean_digit = knn_clf.predict([X_test_mod[some_index]])   
            plot_digit(clean_digit)                                   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Looks close enough to the target! This concludes our tour of classification. You
          should now know how to select good metrics for classification tasks, pick the appro‐
          priate precision/recall trade-off, compare classifiers, and more generally build good
          classification systems for a variety of tasks.              
                                                                      
          Exercises                                                   
                                                                      
           1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy
            on the test set. Hint: the KNeighborsClassifier works quite well for this task;
            you just need to find good hyperparameter values (try a grid search on the
            weights and n_neighbors hyperparameters).                 
           2. Write a function that can shift an MNIST image in any direction (left, right, up,
            or down) by one pixel.5 Then, for each image in the training set, create four shif‐
            ted copies (one per direction) and add them to the training set. Finally, train your
            best model on this expanded training set and measure its accuracy on the test set.
            You should observe that your model performs even better now! This technique of
            artificially growing the training set is called data augmentation or training set
            expansion.                                                
                                                                      
                                                                      
                                                                      
          5 You can use the shift() function from the scipy.ndimage.interpolation module. For example,
           shift(image, [2, 1], cval=0) shifts the image two pixels down and one pixel to the right.