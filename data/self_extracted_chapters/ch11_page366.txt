                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-9. With dropout regularization, at each training iteration a random subset of
          all neurons in one or more layers—except the output layer—are “dropped out”; these
          neurons output 0 at this iteration (represented by the dashed arrows)
                                                                      
          It’s surprising at first that this destructive technique works at all. Would a company
          perform better if its employees were told to toss a coin every morning to decide
          whether or not to go to work? Well, who knows; perhaps it would! The company
          would be forced to adapt its organization; it could not rely on any single person to
          work the coffee machine or perform any other critical tasks, so this expertise would
          have to be spread across several people. Employees would have to learn to cooperate
          with many of their coworkers, not just a handful of them. The company would
          become much more resilient. If one person quit, it wouldn’t make much of a differ‐
          ence. It’s unclear whether this idea would actually work for companies, but it certainly
          does for neural networks. Neurons trained with dropout cannot co-adapt with their
          neighboring neurons; they have to be as useful as possible on their own. They also
          cannot rely excessively on just a few input neurons; they must pay attention to each of
          their input neurons. They end up being less sensitive to slight changes in the inputs.
          In the end, you get a more robust network that generalizes better.
          Another way to understand the power of dropout is to realize that a unique neural
          network is generated at each training step. Since each neuron can be either present or
          absent, there are a total of 2N possible networks (where N is the total number of drop‐
          pable neurons). This is such a huge number that it is virtually impossible for the same
          neural network to be sampled twice. Once you have run 10,000 training steps, you
          have essentially trained 10,000 different neural networks (each with just one training
          instance). These neural networks are obviously not independent because they share
          many of their weights, but they are nevertheless all different. The resulting neural
          network can be seen as an averaging ensemble of all these smaller neural networks.
                                                                      
                                                                      
                                                                      