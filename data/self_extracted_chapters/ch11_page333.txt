                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-1. Logistic activation function saturation        
                                                                      
          Glorot and He Initialization                                
          In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable
          gradients problem. They point out that we need the signal to flow properly in both
          directions: in the forward direction when making predictions, and in the reverse
          direction when backpropagating gradients. We don’t want the signal to die out, nor
          do we want it to explode and saturate. For the signal to flow properly, the authors
          argue that we need the variance of the outputs of each layer to be equal to the var‐
          iance of its inputs,2 and we need the gradients to have equal variance before and after
          flowing through a layer in the reverse direction (please check out the paper if you are
          interested in the mathematical details). It is actually not possible to guarantee both
          unless the layer has an equal number of inputs and neurons (these numbers are called
          the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compro‐
          mise that has proven to work very well in practice: the connection weights of each
          layer must be initialized randomly as described in Equation 11-1, where fan = (fan
                                                      avg  in         
          + fan )/2. This initialization strategy is called Xavier initialization or Glorot initiali‐
             out                                                      
          zation, after the paper’s first author.                     
                                                                      
                                                                      
                                                                      
                                                                      
          2 Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but
           if you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐
           ing. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come
           out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude
           as it came in.                                             