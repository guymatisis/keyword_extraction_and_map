                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-10. The first 20 steps of Stochastic Gradient Descent
                                                                      
          Note that since instances are picked randomly, some instances may be picked several
          times per epoch, while others may not be picked at all. If you want to be sure that the
          algorithm goes through every instance at each epoch, another approach is to shuffle
          the training set (making sure to shuffle the input features and the labels jointly), then
          go through it instance by instance, then shuffle it again, and so on. However, this
          approach generally converges more slowly.                   
                                                                      
                   When using Stochastic Gradient Descent, the training instances
                   must be independent and identically distributed (IID) to ensure
                   that the parameters get pulled toward the global optimum, on aver‐
                   age. A simple way to ensure this is to shuffle the instances during
                   training (e.g., pick each instance randomly, or shuffle the training
                   set at the beginning of each epoch). If you do not shuffle the
                   instances—for example, if the instances are sorted by label—then
                   SGD will start by optimizing for one label, then the next, and so on,
                   and it will not settle close to the global minimum.
          To perform Linear Regression using Stochastic GD with Scikit-Learn, you can use the
          SGDRegressor class, which defaults to optimizing the squared error cost function.
          The following code runs for maximum 1,000 epochs or until the loss drops by less
          than 0.001 during one epoch (max_iter=1000, tol=1e-3). It starts with a learning rate
          of 0.1 (eta0=0.1), using the default learning schedule (different from the preceding
          one). Lastly, it does not use any regularization (penalty=None; more details on this
          shortly):                                                   
                                                                      
                                                                      