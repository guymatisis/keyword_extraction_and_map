                                                                      
                                                                      
                                                                      
                                                                      
          By default, all variables and all operations will be placed on the first GPU
          (named /gpu:0), except for variables and operations that don’t have a GPU kernel:14
          these are placed on the CPU (named /cpu:0). A tensor or variable’s device attribute
          tells you which device it was placed on:15                  
                                                                      
            >>> a = tf.Variable(42.0)                                 
            >>> a.device                                              
            '/job:localhost/replica:0/task:0/device:GPU:0'            
            >>> b = tf.Variable(42)                                   
            >>> b.device                                              
            '/job:localhost/replica:0/task:0/device:CPU:0'            
          You can safely ignore the prefix /job:localhost/replica:0/task:0 for now (it
          allows you to place operations on other machines when using a TensorFlow cluster;
          we will talk about jobs, replicas, and tasks later in this chapter). As you can see, the
          first variable was placed on GPU 0, which is the default device. However, the second
          variable was placed on the CPU: this is because there are no GPU kernels for integer
          variables (or for operations involving integer tensors), so TensorFlow fell back to the
          CPU.                                                        
          If you want to place an operation on a different device than the default one, use a
          tf.device() context:                                        
            >>> with tf.device("/cpu:0"):                             
            ...  c = tf.Variable(42.0)                                
            ...                                                       
            >>> c.device                                              
            '/job:localhost/replica:0/task:0/device:CPU:0'            
                   The CPU is always treated as a single device (/cpu:0), even if your
                   machine has multiple CPU cores. Any operation placed on the
                   CPU may run in parallel across multiple cores if it has a multi‐
                   threaded kernel.                                   
                                                                      
          If you explicitly try to place an operation or variable on a device that does not exist or
          for which there is no kernel, then you will get an exception. However, in some cases
          you may prefer to fall back to the CPU; for example, if your program may run both
          on CPU-only machines and on GPU machines, you may want TensorFlow to ignore
          your tf.device("/gpu:*") on CPU-only machines. To do this, you can call tf.con
          fig.set_soft_device_placement(True) just after importing TensorFlow: when a
                                                                      
                                                                      
          14 As we saw in Chapter 12, a kernel is a variable or operation’s implementation for a specific data type and
           device type. For example, there is a GPU kernel for the float32 tf.matmul() operation, but there is no GPU
           kernel for int32 tf.matmul() (only a CPU kernel).          
          15 You can also use tf.debugging.set_log_device_placement(True) to log all device placements.