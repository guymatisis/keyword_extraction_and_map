                                                                      
                                                                      
                                                                      
                                                                      
                   With Stochastic and Mini-batch Gradient Descent, the curves are
                   not so smooth, and it may be hard to know whether you have
                   reached the minimum or not. One solution is to stop only after the
                   validation error has been above the minimum for some time (when
                   you are confident that the model will not do any better), then roll
                   back the model parameters to the point where the validation error
                   was at a minimum.                                  
                                                                      
          Here is a basic implementation of early stopping:           
            from sklearn.base import clone                            
                                                                      
            # prepare the data                                        
            poly_scaler = Pipeline([                                  
                 ("poly_features", PolynomialFeatures(degree=90, include_bias=False)),
                 ("std_scaler", StandardScaler())                     
               ])                                                     
            X_train_poly_scaled = poly_scaler.fit_transform(X_train)  
            X_val_poly_scaled = poly_scaler.transform(X_val)          
            sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
                          penalty=None, learning_rate="constant", eta0=0.0005)
            minimum_val_error = float("inf")                          
            best_epoch = None                                         
            best_model = None                                         
            for epoch in range(1000):                                 
               sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off
               y_val_predict = sgd_reg.predict(X_val_poly_scaled)     
               val_error = mean_squared_error(y_val, y_val_predict)   
               if val_error < minimum_val_error:                      
                 minimum_val_error = val_error                        
                 best_epoch = epoch                                   
                 best_model = clone(sgd_reg)                          
          Note that with warm_start=True, when the fit() method is called it continues train‐
          ing where it left off, instead of restarting from scratch.  
          Logistic Regression                                         
          As we discussed in Chapter 1, some regression algorithms can be used for classifica‐
          tion (and vice versa). Logistic Regression (also called Logit Regression) is commonly
          used to estimate the probability that an instance belongs to a particular class (e.g.,
          what is the probability that this email is spam?). If the estimated probability is greater
          than 50%, then the model predicts that the instance belongs to that class (called the
          positive class, labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to
          the negative class, labeled “0”). This makes it a binary classifier.
                                                                      