                                                                      
                                                                      
                                                                      
                                                                      
          The simple policy gradients algorithm we just trained solved the CartPole task, but it
          would not scale well to larger and more complex tasks. Indeed, it is highly sample
          inefficient, meaning it needs to explore the game for a very long time before it can
          make significant progress. This is due to the fact that it must run multiple episodes to
          estimate the advantage of each action, as we have seen. However, it is the foundation
          of more powerful algorithms, such as Actor-Critic algorithms (which we will discuss
          briefly at the end of this chapter).                        
                                                                      
          We will now look at another popular family of algorithms. Whereas PG algorithms
          directly try to optimize the policy to increase rewards, the algorithms we will look at
          now are less direct: the agent learns to estimate the expected return for each state, or
          for each action in each state, then it uses this knowledge to decide how to act. To
          understand these algorithms, we must first introduce Markov decision processes.
          Markov Decision Processes                                   
                                                                      
          In the early 20th century, the mathematician Andrey Markov studied stochastic pro‐
          cesses with no memory, called Markov chains. Such a process has a fixed number of
          states, and it randomly evolves from one state to another at each step. The probability
          for it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s, s
          ′), not on past states (this is why we say that the system has no memory).
                                                                      
          Figure 18-7 shows an example of a Markov chain with four states.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-7. Example of a Markov chain                      
                                                                      
          Suppose that the process starts in state s , and there is a 70% chance that it will
                                   0                                  
          remain in that state at the next step. Eventually it is bound to leave that state and
          never come back because no other state points back to s . If it goes to state s , it will
                                           0           1              
          then most likely go to state s (90% probability), then immediately back to state s
                           2                               1          