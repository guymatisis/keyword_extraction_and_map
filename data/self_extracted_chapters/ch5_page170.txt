                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Equation 5-8. Second-degree polynomial mapping            
                                                                      
                        2                                             
                       x                                              
                        1                                             
                  x                                                   
                  1                                                   
            ϕ x =ϕ  =  2x x                                           
                        1 2                                           
                  x                                                   
                  2                                                   
                        2                                             
                       x                                              
                        2                                             
          Notice that the transformed vector is 3D instead of 2D. Now let’s look at what hap‐
          pens to a couple of 2D vectors, a and b, if we apply this second-degree polynomial
          mapping and then compute the dot product7 of the transformed vectors (See Equa‐
          tion 5-9).                                                  
            Equation 5-9. Kernel trick for a second-degree polynomial mapping
                        2 ⊺   2                                       
                       a    b                                         
                       1     1                                        
            ϕ a ⊺ ϕ b = 2a a 2b b =a 2 b 2 +2a b a b +a 2 b 2         
                        1 2   1 2 1 1   1 1 2 2 2 2                   
                        2     2                                       
                       a    b                                         
                       2     2                                        
                               a ⊺ b 2                                
                            2   1  1   ⊺ 2                            
                    = a b +a b =     = a b                            
                      1 1 2 2  a  b                                   
                                2  2                                  
          How about that? The dot product of the transformed vectors is equal to the square of
          the dot product of the original vectors: ϕ(a)⊺ ϕ(b) = (a⊺ b)2.
          Here is the key insight: if you apply the transformation ϕ to all training instances,
          then the dual problem (see Equation 5-6) will contain the dot product ϕ(x(i))⊺ ϕ(x(j)).
          But if ϕ is the second-degree polynomial transformation defined in Equation 5-8,
          then you can replace this dot product of transformed vectors simply by xi⊺ x j 2 . So,
          you don’t need to transform the training instances at all; just replace the dot product
          by its square in Equation 5-6. The result will be strictly the same as if you had gone
          through the trouble of transforming the training set then fitting a linear SVM algo‐
          rithm, but this trick makes the whole process much more computationally efficient.
          The function K(a, b) = (a⊺ b)2 is a second-degree polynomial kernel. In Machine
          Learning, a kernel is a function capable of computing the dot product ϕ(a)⊺ ϕ(b),
          7 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a · b. However, in
           Machine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the
           dot product is achieved by computing a⊺b. To remain consistent with the rest of the book, we will use this
           notation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.