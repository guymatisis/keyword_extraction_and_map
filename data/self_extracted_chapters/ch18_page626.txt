                                                                      
                                                                      
                                                                      
                                                                      
          (with 100% probability). It may alternate a number of times between these two states,
          but eventually it will fall into state s and remain there forever (this is a terminal
                                3                                     
          state). Markov chains can have very different dynamics, and they are heavily used in
          thermodynamics, chemistry, statistics, and much more.       
          Markov decision processes were first described in the 1950s by Richard Bellman.12
          They resemble Markov chains but with a twist: at each step, an agent can choose one
          of several possible actions, and the transition probabilities depend on the chosen
          action. Moreover, some state transitions return some reward (positive or negative),
          and the agent’s goal is to find a policy that will maximize reward over time.
          For example, the MDP represented in Figure 18-8 has three states (represented by cir‐
          cles) and up to three possible discrete actions at each step (represented by diamonds).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-8. Example of a Markov decision process           
                                                                      
          If it starts in state s , the agent can choose between actions a , a , or a . If it chooses
                     0                        0 1  2                  
          action a , it just remains in state s with certainty, and without any reward. It can thus
               1             0                                        
          decide to stay there forever if it wants to. But if it chooses action a , it has a 70% prob‐
                                                0                     
          ability of gaining a reward of +10 and remaining in state s . It can then try again and
                                            0                         
          again to gain as much reward as possible, but at one point it is going to end up
          instead in state s . In state s it has only two possible actions: a or a . It can choose to
                    1     1                   0  2                    
          stay put by repeatedly choosing action a , or it can choose to move on to state s and
                                 0                       2            
          get a negative reward of –50 (ouch). In state s it has no other choice than to take
                                      2                               
          action a , which will most likely lead it back to state s , gaining a reward of +40 on the
               1                         0                            
          12 Richard Bellman, “A Markovian Decision Process,” Journal of Mathematics and Mechanics 6, no. 5 (1957):
           679–684.                                                   