                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-5. Architecture of a Perceptron with two input neurons, one bias neuron, and
          three output neurons                                        
                                                                      
          Thanks to the magic of linear algebra, Equation 10-2 makes it possible to efficiently
          compute the outputs of a layer of artificial neurons for several instances at once.
                                                                      
            Equation 10-2. Computing the outputs of a fully connected layer
            h  X =ϕ XW+b                                              
             W,b                                                      
          In this equation:                                           
                                                                      
           • As always, X represents the matrix of input features. It has one row per instance
            and one column per feature.                               
           • The weight matrix W contains all the connection weights except for the ones
            from the bias neuron. It has one row per input neuron and one column per artifi‐
            cial neuron in the layer.                                 
                                                                      
           • The bias vector b contains all the connection weights between the bias neuron
            and the artificial neurons. It has one bias term per artificial neuron.
           • The function ϕ is called the activation function: when the artificial neurons are
            TLUs, it is a step function (but we will discuss other activation functions shortly).
                                                                      
          So, how is a Perceptron trained? The Perceptron training algorithm proposed by
          Rosenblatt was largely inspired by Hebb’s rule. In his 1949 book The Organization of
          Behavior (Wiley), Donald Hebb suggested that when a biological neuron triggers
          another neuron often, the connection between these two neurons grows stronger. Sie‐
          grid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire
          together, wire together”; that is, the connection weight between two neurons tends to
          increase when they fire simultaneously. This rule later became known as Hebb’s rule
          (or Hebbian learning). Perceptrons are trained using a variant of this rule that takes
          into account the error made by the network when it makes a prediction; the