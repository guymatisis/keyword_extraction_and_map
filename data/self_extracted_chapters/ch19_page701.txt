                                                                      
                                                                      
                                                                      
                                                                      
          With that, you have all you need to run any operation on any device, and exploit the
          power of your GPUs! Here are some of the things you could do:
                                                                      
           • You could train several models in parallel, each on its own GPU: just write a
            training script for each model and run them in parallel, setting
            CUDA_DEVICE_ORDER and CUDA_VISIBLE_DEVICES so that each script only sees a
            single GPU device. This is great for hyperparameter tuning, as you can train in
            parallel multiple models with different hyperparameters. If you have a single
            machine with two GPUs, and it takes one hour to train one model on one GPU,
            then training two models in parallel, each on its own dedicated GPU, will take
            just one hour. Simple!                                    
           • You could train a model on a single GPU and perform all the preprocessing in
            parallel on the CPU, using the dataset’s prefetch() method17 to prepare the next
            few batches in advance so that they are ready when the GPU needs them (see
            Chapter 13).                                              
           • If your model takes two images as input and processes them using two CNNs
            before joining their outputs, then it will probably run much faster if you place
            each CNN on a different GPU.                              
                                                                      
           • You can create an efficient ensemble: just place a different trained model on each
            GPU so that you can get all the predictions much faster to produce the ensem‐
            ble’s final prediction.                                   
          But what if you want to train a single model across multiple GPUs?
                                                                      
          Training Models Across Multiple Devices                     
                                                                      
          There are two main approaches to training a single model across multiple devices:
          model parallelism, where the model is split across the devices, and data parallelism,
          where the model is replicated across every device, and each replica is trained on a
          subset of the data. Let’s look at these two options closely before we train a model on
          multiple GPUs.                                              
                                                                      
          Model Parallelism                                           
          So far we have trained each neural network on a single device. What if we want to
          train a single neural network across multiple devices? This requires chopping the
          model into separate chunks and running each chunk on a different device.
                                                                      
                                                                      
                                                                      
          17 At the time of this writing it only prefetches the data to the CPU RAM, but you can use tf.data.experimen
           tal.prefetch_to_device() to make it prefetch the data and push it to the device of your choice so that the
           GPU does not waste time waiting for the data to be transferred.