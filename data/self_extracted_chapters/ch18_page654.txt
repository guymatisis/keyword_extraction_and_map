                                                                      
                                                                      
                                                                      
                                                                      
          Creating the Replay Buffer and the Corresponding Observer   
                                                                      
          The TF-Agents library provides various replay buffer implementations in the
          tf_agents.replay_buffers package. Some are purely written in Python (their mod‐
          ule names start with py_), and others are written based on TensorFlow (their module
          names start with tf_). We will use the TFUniformReplayBuffer class in the
          tf_agents.replay_buffers.tf_uniform_replay_buffer package. It provides a
          high-performance implementation of a replay buffer with uniform sampling:21
            from tf_agents.replay_buffers import tf_uniform_replay_buffer
                                                                      
            replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
               data_spec=agent.collect_data_spec,                     
               batch_size=tf_env.batch_size,                          
               max_length=1000000)                                    
          Let’s look at each of these arguments:                      
          data_spec                                                   
            The specification of the data that will be saved in the replay buffer. The DQN
            agent knowns what the collected data will look like, and it makes the data spec
            available via its collect_data_spec attribute, so that’s what we give the replay
            buffer.                                                   
          batch_size                                                  
                                                                      
            The number of trajectories that will be added at each step. In our case, it will be
            one, since the driver will just execute one action per step and collect one trajec‐
            tory. If the environment were a batched environment, meaning an environment
            that takes a batch of actions at each step and returns a batch of observations, then
            the driver would have to save a batch of trajectories at each step. Since we are
            using a TensorFlow replay buffer, it needs to know the size of the batches it will
            handle (to build the computation graph). An example of a batched environment
            is the ParallelPyEnvironment (from the tf_agents.environments.paral
            lel_py_environment package): it runs multiple environments in parallel in sepa‐
            rate processes (they can be different as long as they have the same action and
            observation specs), and at each step it takes a batch of actions and executes them
            in the environments (one action per environment), then it returns all the result‐
            ing observations.                                         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          21 At the time of this writing, there is no prioritized experience replay buffer yet, but one will likely be open
           sourced soon.                                              