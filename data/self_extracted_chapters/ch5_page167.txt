                                                                      
                                                                      
                                                                      
                                                                      
                   We are minimizing ½ w⊺ w, which is equal to ½∥ w ∥2, rather than
                   minimizing ∥ w ∥. Indeed, ½∥ w ∥2 has a nice, simple derivative (it
                   is just w), while ∥ w ∥ is not differentiable at w = 0. Optimization
                   algorithms work much better on differentiable functions.
                                                                      
          To get the soft margin objective, we need to introduce a slack variable ζ(i) ≥ 0 for each
          instance:4 ζ(i) measures how much the ith instance is allowed to violate the margin. We
          now have two conflicting objectives: make the slack variables as small as possible to
          reduce the margin violations, and make ½ w⊺ w as small as possible to increase the
          margin. This is where the C hyperparameter comes in: it allows us to define the trade‐
          off between these two objectives. This gives us the constrained optimization problem
          in Equation 5-4.                                            
                                                                      
            Equation 5-4. Soft margin linear SVM classifier objective 
                                                                      
                          m                                           
             minimize 1 w ⊺ w+C ∑ ζ i                                 
              w,b,ζ 2    i=1                                          
             subject to t i w ⊺ x i +b ≥1−ζ i and ζ i ≥0 for i=1,2,⋯,m
          Quadratic Programming                                       
                                                                      
          The hard margin and soft margin problems are both convex quadratic optimization
          problems with linear constraints. Such problems are known as Quadratic Program‐
          ming (QP) problems. Many off-the-shelf solvers are available to solve QP problems
          by using a variety of techniques that are outside the scope of this book.5
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          4 Zeta (ζ) is the sixth letter of the Greek alphabet.       
          5 To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vandenber‐
           ghe’s book Convex Optimization (Cambridge University Press, 2004) or watch Richard Brown’s series of video
           lectures.                                                  