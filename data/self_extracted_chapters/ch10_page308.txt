                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            housing = fetch_california_housing()                      
                                                                      
            X_train_full, X_test, y_train_full, y_test = train_test_split(
               housing.data, housing.target)                          
            X_train, X_valid, y_train, y_valid = train_test_split(    
               X_train_full, y_train_full)                            
            scaler = StandardScaler()                                 
            X_train = scaler.fit_transform(X_train)                   
            X_valid = scaler.transform(X_valid)                       
            X_test = scaler.transform(X_test)                         
          Using the Sequential API to build, train, evaluate, and use a regression MLP to make
          predictions is quite similar to what we did for classification. The main differences are
          the fact that the output layer has a single neuron (since we only want to predict a sin‐
          gle value) and uses no activation function, and the loss function is the mean squared
          error. Since the dataset is quite noisy, we just use a single hidden layer with fewer
          neurons than before, to avoid overfitting:                  
            model = keras.models.Sequential([                         
               keras.layers.Dense(30, activation="relu", input_shape=X_train.shape[1:]),
               keras.layers.Dense(1)                                  
            ])                                                        
            model.compile(loss="mean_squared_error", optimizer="sgd") 
            history = model.fit(X_train, y_train, epochs=20,          
                        validation_data=(X_valid, y_valid))           
            mse_test = model.evaluate(X_test, y_test)                 
            X_new = X_test[:3] # pretend these are new instances      
            y_pred = model.predict(X_new)                             
          As you can see, the Sequential API is quite easy to use. However, although Sequen
          tial models are extremely common, it is sometimes useful to build neural networks
          with more complex topologies, or with multiple inputs or outputs. For this purpose,
          Keras offers the Functional API.                            
          Building Complex Models Using the Functional API            
          One example of a nonsequential neural network is a Wide & Deep neural network.
          This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng
          et al.16 It connects all or part of the inputs directly to the output layer, as shown in
          Figure 10-14. This architecture makes it possible for the neural network to learn both
          deep patterns (using the deep path) and simple rules (through the short path).17 In
          contrast, a regular MLP forces all the data to flow through the full stack of layers;
                                                                      
          16 Heng-Tze Cheng et al., “Wide & Deep Learning for Recommender Systems,” Proceedings of the First Workshop
           on Deep Learning for Recommender Systems (2016): 7–10.     
          17 The short path can also be used to provide manually engineered features to the neural network.