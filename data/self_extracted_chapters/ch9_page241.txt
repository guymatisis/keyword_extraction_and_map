                                                                      
                                                                      
                                                                      
                                                                      
          The K-Means algorithm                                       
                                                                      
          So, how does the algorithm work? Well, suppose you were given the centroids. You
          could easily label all the instances in the dataset by assigning each of them to the clus‐
          ter whose centroid is closest. Conversely, if you were given all the instance labels, you
          could easily locate all the centroids by computing the mean of the instances for each
          cluster. But you are given neither the labels nor the centroids, so how can you pro‐
          ceed? Well, just start by placing the centroids randomly (e.g., by picking k instances at
          random and using their locations as centroids). Then label the instances, update the
          centroids, label the instances, update the centroids, and so on until the centroids stop
          moving. The algorithm is guaranteed to converge in a finite number of steps (usually
          quite small); it will not oscillate forever.2               
          You can see the algorithm in action in Figure 9-4: the centroids are initialized ran‐
          domly (top left), then the instances are labeled (top right), then the centroids are
          updated (center left), the instances are relabeled (center right), and so on. As you can
          see, in just three iterations, the algorithm has reached a clustering that seems close to
          optimal.                                                    
                                                                      
                   The computational complexity of the algorithm is generally linear
                   with regard to the number of instances m, the number of clusters k,
                   and the number of dimensions n. However, this is only true when
                   the data has a clustering structure. If it does not, then in the worst-
                   case scenario the complexity can increase exponentially with the
                   number of instances. In practice, this rarely happens, and K-Means
                   is generally one of the fastest clustering algorithms.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          2 That’s because the mean squared distance between the instances and their closest centroid can only go down
           at each step.                                              