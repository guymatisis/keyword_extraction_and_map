                                                                      
                                                                      
                                                                      
                                                                      
          will get the documentation for this module. By default, TF Hub will cache the down‐
          loaded files into the local system’s temporary directory. You may prefer to download
          them into a more permanent directory to avoid having to download them again after
          every system cleanup. To do that, set the TFHUB_CACHE_DIR environment variable to
          the directory of your choice (e.g., os.environ["TFHUB_CACHE_DIR"] = "./
          my_tfhub_cache").                                           
                                                                      
          So far, we have looked at time series, text generation using Char-RNN, and sentiment
          analysis using word-level RNN models, training our own word embeddings or reus‐
          ing pretrained embeddings. Let’s now look at another important NLP task: neural
          machine translation (NMT), first using a pure Encoder–Decoder model, then improv‐
          ing it with attention mechanisms, and finally looking the extraordinary Transformer
          architecture.                                               
          An Encoder–Decoder Network for Neural Machine               
                                                                      
          Translation                                                 
                                                                      
          Let’s take a look at a simple neural machine translation model10 that will translate
          English sentences to French (see Figure 16-3).              
          In short, the English sentences are fed to the encoder, and the decoder outputs the
          French translations. Note that the French translations are also used as inputs to the
          decoder, but shifted back by one step. In other words, the decoder is given as input
          the word that it should have output at the previous step (regardless of what it actually
          output). For the very first word, it is given the start-of-sequence (SOS) token. The
          decoder is expected to end the sentence with an end-of-sequence (EOS) token.
          Note that the English sentences are reversed before they are fed to the encoder. For
          example, “I drink milk” is reversed to “milk drink I.” This ensures that the beginning
          of the English sentence will be fed last to the encoder, which is useful because that’s
          generally the first thing that the decoder needs to translate.
                                                                      
          Each word is initially represented by its ID (e.g., 288 for the word “milk”). Next, an
          embedding layer returns the word embedding. These word embeddings are what is
          actually fed to the encoder and the decoder.                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          10 Ilya Sutskever et al., “Sequence to Sequence Learning with Neural Networks,” arXiv preprint arXiv:1409.3215
           (2014).                                                    