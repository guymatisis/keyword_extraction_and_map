                                                                      
                                                                      
                                                                      
                                                                      
                   Separable convolutional layers use fewer parameters, less memory,
                   and fewer computations than regular convolutional layers, and in
                   general they even perform better, so you should consider using
                   them by default (except after layers with few channels).
                                                                      
          The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐
          versity of Hong Kong. They used an ensemble of many different techniques, includ‐
          ing a sophisticated object-detection system called GBD-Net,21 to achieve a top-five
          error rate below 3%. Although this result is unquestionably impressive, the complex‐
          ity of the solution contrasted with the simplicity of ResNets. Moreover, one year later
          another fairly simple architecture performed even better, as we will see now.
                                                                      
          SENet                                                       
                                                                      
          The winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-
          Excitation Network (SENet).22 This architecture extends existing architectures such as
          inception networks and ResNets, and boosts their performance. This allowed SENet
          to win the competition with an astonishing 2.25% top-five error rate! The extended
          versions of inception networks and ResNets are called SE-Inception and SE-ResNet,
          respectively. The boost comes from the fact that a SENet adds a small neural network,
          called an SE block, to every unit in the original architecture (i.e., every inception
          module or every residual unit), as shown in Figure 14-20.   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-20. SE-Inception module (left) and SE-ResNet unit (right)
                                                                      
                                                                      
                                                                      
          21 Xingyu Zeng et al., “Crafting GBD-Net for Object Detection,” IEEE Transactions on Pattern Analysis and
           Machine Intelligence 40, no. 9 (2018): 2109–2123.          
          22 Jie Hu et al., “Squeeze-and-Excitation Networks,” Proceedings of the IEEE Conference on Computer Vision and
           Pattern Recognition (2018): 7132–7141.                     