                                                                      
                                                                      
                                                                      
                                                                      
                   This algorithm is an example of Dynamic Programming, which
                   breaks down a complex problem into tractable subproblems that
                   can be tackled iteratively.                        
                                                                      
                                                                      
          Knowing the optimal state values can be useful, in particular to evaluate a policy, but
          it does not give us the optimal policy for the agent. Luckily, Bellman found a very
          similar algorithm to estimate the optimal state-action values, generally called Q-
          Values (Quality Values). The optimal Q-Value of the state-action pair (s, a), noted
          Q*(s, a), is the sum of discounted future rewards the agent can expect on average
          after it reaches the state s and chooses action a, but before it sees the outcome of this
          action, assuming it acts optimally after that action.       
                                                                      
          Here is how it works: once again, you start by initializing all the Q-Value estimates to
          zero, then you update them using the Q-Value Iteration algorithm (see Equation
          18-3).                                                      
                                                                      
            Equation 18-3. Q-Value Iteration algorithm                
            Q   s,a ∑T s,a,s′ R s,a,s′ +γ· max Q s′,a′ for all s′a    
             k+1    s′             a′  k                              
                                                                      
          Once you have the optimal Q-Values, defining the optimal policy, noted π*(s), is triv‐
          ial: when the agent is in state s, it should choose the action with the highest Q-Value
          for that state: π* s = argmax Q* s,a .                      
                         a                                            
          Let’s apply this algorithm to the MDP represented in Figure 18-8. First, we need to
          define the MDP:                                             
            transition_probabilities = [ # shape=[s, a, s']           
                 [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], 
                 [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],            
                 [None, [0.8, 0.1, 0.1], None]]                       
            rewards = [ # shape=[s, a, s']                            
                 [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],                 
                 [[0, 0, 0], [0, 0, 0], [0, 0, -50]],                 
                 [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]                 
            possible_actions = [[0, 1, 2], [0, 2], [1]]               
          For example, to know the transition probability from s to s after playing action a ,
                                          2  0             1          
          we will look up transition_probabilities[2][1][0] (which is 0.8). Similarly, to
          get the corresponding reward, we will look up rewards[2][1][0] (which is +40).
          And to get the list of possible actions in s , we will look up possible_actions[2] (in
                                  2                                   
          this case, only action a is possible). Next, we must initialize all the Q-Values to 0
                        1                                             
          (except for the the impossible actions, for which we set the Q-Values to –∞):