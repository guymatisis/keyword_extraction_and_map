                                                                      
                                                                      
                                                                      
                                                                      
          OK, that’s our baseline: 96.9% accuracy. Let’s see if we can do better by using K-Means
          as a preprocessing step. We will create a pipeline that will first cluster the training set
          into 50 clusters and replace the images with their distances to these 50 clusters, then
          apply a Logistic Regression model:                          
                                                                      
            from sklearn.pipeline import Pipeline                     
            pipeline = Pipeline([                                     
               ("kmeans", KMeans(n_clusters=50)),                     
               ("log_reg", LogisticRegression()),                     
            ])                                                        
            pipeline.fit(X_train, y_train)                            
                   Since there are 10 different digits, it is tempting to set the number
                   of clusters to 10. However, each digit can be written several differ‐
                   ent ways, so it is preferable to use a larger number of clusters, such
                   as 50.                                             
                                                                      
          Now let’s evaluate this classification pipeline:            
                                                                      
            >>> pipeline.score(X_test, y_test)                        
            0.9777777777777777                                        
          How about that? We reduced the error rate by almost 30% (from about 3.1% to about
          2.2%)!                                                      
          But we chose the number of clusters k arbitrarily; we can surely do better. Since K-
          Means is just a preprocessing step in a classification pipeline, finding a good value for
          k is much simpler than earlier. There’s no need to perform silhouette analysis or mini‐
          mize the inertia; the best value of k is simply the one that results in the best classifica‐
          tion performance during cross-validation. We can use GridSearchCV to find the
          optimal number of clusters:                                 
            from sklearn.model_selection import GridSearchCV          
                                                                      
            param_grid = dict(kmeans__n_clusters=range(2, 100))       
            grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
            grid_clf.fit(X_train, y_train)                            
          Let’s look at the best value for k and the performance of the resulting pipeline:
            >>> grid_clf.best_params_                                 
            {'kmeans__n_clusters': 99}                                
            >>> grid_clf.score(X_test, y_test)                        
            0.9822222222222222                                        
          With k = 99 clusters, we get a significant accuracy boost, reaching 98.22% accuracy
          on the test set. Cool! You may want to keep exploring higher values for k, since 99
          was the largest value in the range we explored.             
                                                                      