                                                                      
                                                                      
                                                                      
                                                                      
          ping input sequences (rather than the shuffled and overlapping sequences we used to
          train stateless RNNs). When creating the Dataset, we must therefore use
          shift=n_steps (instead of shift=1) when calling the window() method. Moreover,
          we must obviously not call the shuffle() method. Unfortunately, batching is much
          harder when preparing a dataset for a stateful RNN than it is for a stateless RNN.
          Indeed, if we were to call batch(32), then 32 consecutive windows would be put in
          the same batch, and the following batch would not continue each of these window
          where it left off. The first batch would contain windows 1 to 32 and the second batch
          would contain windows 33 to 64, so if you consider, say, the first window of each
          batch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest
          solution to this problem is to just use “batches” containing a single window:
                                                                      
            dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
            dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)
            dataset = dataset.flat_map(lambda window: window.batch(window_length))
            dataset = dataset.batch(1)                                
            dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
            dataset = dataset.map(                                    
               lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))
            dataset = dataset.prefetch(1)                             
          Figure 16-2 summarizes the first steps.                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-2. Preparing a dataset of consecutive sequence fragments for a stateful RNN
                                                                      
          Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s
          text into 32 texts of equal length, create one dataset of consecutive input sequences
          for each of them, and finally use tf.train.Dataset.zip(datasets).map(lambda
          *windows: tf.stack(windows)) to create proper consecutive batches, where the nth
          input sequence in a batch starts off exactly where the nth input sequence ended in the
          previous batch (see the notebook for the full code).        
                                                                      
                                                                      