                                                                      
                                                                      
                                                                      
                                                                      
                 else:                                                
                   return np.c_[X, rooms_per_household, population_per_household]
                                                                      
            attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
            housing_extra_attribs = attr_adder.transform(housing.values)
          In this example the transformer has one hyperparameter, add_bedrooms_per_room,
          set to True by default (it is often helpful to provide sensible defaults). This hyperpara‐
          meter will allow you to easily find out whether adding this attribute helps the
          Machine Learning algorithms or not. More generally, you can add a hyperparameter
          to gate any data preparation step that you are not 100% sure about. The more you
          automate these data preparation steps, the more combinations you can automatically
          try out, making it much more likely that you will find a great combination (and sav‐
          ing you a lot of time).                                     
          Feature Scaling                                             
                                                                      
          One of the most important transformations you need to apply to your data is feature
          scaling. With few exceptions, Machine Learning algorithms don’t perform well when
          the input numerical attributes have very different scales. This is the case for the hous‐
          ing data: the total number of rooms ranges from about 6 to 39,320, while the median
          incomes only range from 0 to 15. Note that scaling the target values is generally not
          required.                                                   
          There are two common ways to get all attributes to have the same scale: min-max
          scaling and standardization.                                
                                                                      
          Min-max scaling (many people call this normalization) is the simplest: values are shif‐
          ted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting
          the min value and dividing by the max minus the min. Scikit-Learn provides a trans‐
          former called MinMaxScaler for this. It has a feature_range hyperparameter that lets
          you change the range if, for some reason, you don’t want 0–1.
          Standardization is different: first it subtracts the mean value (so standardized values
          always have a zero mean), and then it divides by the standard deviation so that the
          resulting distribution has unit variance. Unlike min-max scaling, standardization
          does not bound values to a specific range, which may be a problem for some algo‐
          rithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐
          ever, standardization is much less affected by outliers. For example, suppose a district
          had a median income equal to 100 (by mistake). Min-max scaling would then crush
          all the other values from 0–15 down to 0–0.15, whereas standardization would not be
          much affected. Scikit-Learn provides a transformer called StandardScaler for
          standardization.                                            
                                                                      
                                                                      
                                                                      