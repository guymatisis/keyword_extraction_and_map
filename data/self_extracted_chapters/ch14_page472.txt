                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-16. Regular deep neural network (left) and deep residual network (right)
                                                                      
          Now let’s look at ResNet’s architecture (see Figure 14-17). It is surprisingly simple. It
          starts and ends exactly like GoogLeNet (except without a dropout layer), and in
          between is just a very deep stack of simple residual units. Each residual unit is com‐
          posed of two convolutional layers (and no pooling layer!), with Batch Normalization
          (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions
          (stride 1, "same" padding).                                 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      