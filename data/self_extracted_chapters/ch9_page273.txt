                                                                      
                                                                      
                                                                      
                                                                      
          In practice, there are different techniques to maximize the ELBO. In mean field varia‐
          tional inference, it is necessary to pick the family of distributions q(z; λ) and the prior
          p(z) very carefully to ensure that the equation for the ELBO simplifies to a form that
          can be computed. Unfortunately, there is no general way to do this. Picking the right
          family of distributions and the right prior depends on the task and requires some
          mathematical skills. For example, the distributions and lower-bound equations used
          in Scikit-Learn’s BayesianGaussianMixture class are presented in the documenta‐
          tion. From these equations it is possible to derive update equations for the cluster
          parameters and assignment variables: these are then used very much like in the
          Expectation-Maximization algorithm. In fact, the computational complexity of the
          BayesianGaussianMixture class is similar to that of the GaussianMixture class (but
          generally significantly slower). A simpler approach to maximizing the ELBO is called
          black box stochastic variational inference (BBSVI): at each iteration, a few samples are
          drawn from q, and they are used to estimate the gradients of the ELBO with regard to
          the variational parameters λ, which are then used in a gradient ascent step. This
          approach makes it possible to use Bayesian inference with any kind of model (pro‐
          vided it is differentiable), even deep neural networks; using Bayesian inference with
          deep neural networks is called Bayesian Deep Learning.      
                                                                      
                   If you want to dive deeper into Bayesian statistics, check out the
                   book Bayesian Data Analysis by Andrew Gelman et al. (Chapman
                   & Hall).                                           
                                                                      
                                                                      
          Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try
          to fit a dataset with different shapes, you may have bad surprises. For example, let’s
          see what happens if we use a Bayesian Gaussian mixture model to cluster the moons
          dataset (see Figure 9-24).                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-24. Fitting a Gaussian mixture to nonellipsoidal clusters
                                                                      
                                                                      
                                                                      