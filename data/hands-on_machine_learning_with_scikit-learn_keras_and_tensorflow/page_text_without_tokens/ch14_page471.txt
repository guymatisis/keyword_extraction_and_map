                                                                      
                                                                      
                                                                      
                                                                      
          ResNet                                                      
                                                                      
          Kaiming He et al. won the ILSVRC 2015 challenge using a Residual Network (or
          ResNet),16 that delivered an astounding top-five error rate under 3.6%. The winning
          variant used an extremely deep CNN composed of 152 layers (other variants had 34,
          50, and 101 layers). It confirmed the general trend: models are getting deeper and
          deeper, with fewer and fewer parameters. The key to being able to train such a deep
          network is to use skip connections (also called shortcut connections): the signal feeding
          into a layer is also added to the output of a layer located a bit higher up the stack. Let’s
          see why this is useful.                                     
          When training a neural network, the goal is to make it model a target function h(x).
          If you add the input x to the output of the network (i.e., you add a skip connection),
          then the network will be forced to model f(x) = h(x) – x rather than h(x). This is
          called residual learning (see Figure 14-15).                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-15. Residual learning                             
                                                                      
          When you initialize a regular neural network, its weights are close to zero, so the net‐
          work just outputs values close to zero. If you add a skip connection, the resulting net‐
          work just outputs a copy of its inputs; in other words, it initially models the identity
          function. If the target function is fairly close to the identity function (which is often
          the case), this will speed up training considerably.        
          Moreover, if you add many skip connections, the network can start making progress
          even if several layers have not started learning yet (see Figure 14-16). Thanks to skip
          connections, the signal can easily make its way across the whole network. The deep
          residual network can be seen as a stack of residual units (RUs), where each residual
          unit is a small neural network with a skip connection.      
                                                                      
                                                                      
                                                                      
          16 Kaiming He et al., “Deep Residual Learning for Image Recognition,” arXiv preprint arXiv:1512:03385 (2015).