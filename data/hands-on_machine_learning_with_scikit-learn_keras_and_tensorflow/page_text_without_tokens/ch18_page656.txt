                                                                      
                                                                      
                                                                      
                                                                      
          to count the number of episodes, the number of steps taken, and most importantly
          the average return per episode and the average episode length:
                                                                      
            from tf_agents.metrics import tf_metrics                  
            train_metrics = [                                         
               tf_metrics.NumberOfEpisodes(),                         
               tf_metrics.EnvironmentSteps(),                         
               tf_metrics.AverageReturnMetric(),                      
               tf_metrics.AverageEpisodeLengthMetric(),               
            ]                                                         
                   Discounting the rewards makes sense for training or to implement
                   a policy, as it makes it possible to balance the importance of imme‐
                   diate rewards with future rewards. However, once an episode is
                   over, we can evaluate how good it was overalls by summing the
                   undiscounted rewards. For this reason, the AverageReturnMetric
                   computes the sum of undiscounted rewards for each episode, and it
                   keeps track of the streaming mean of these sums over all the epi‐
                   sodes it encounters.                               
          At any time, you can get the value of each of these metrics by calling its result()
          method (e.g., train_metrics[0].result()). Alternatively, you can log all metrics by
          calling log_metrics(train_metrics) (this function is located in the
          tf_agents.eval.metric_utils package):                       
                                                                      
            >>> from tf_agents.eval.metric_utils import log_metrics   
            >>> import logging                                        
            >>> logging.get_logger().set_level(logging.INFO)          
            >>> log_metrics(train_metrics)                            
            [...]                                                     
            NumberOfEpisodes = 0                                      
            EnvironmentSteps = 0                                      
            AverageReturn = 0.0                                       
            AverageEpisodeLength = 0.0                                
          Next, let’s create the collect driver.                      
          Creating the Collect Driver                                 
          As we explored in Figure 18-13, a driver is an object that explores an environment
          using a given policy, collects experiences, and broadcasts them to some observers. At
          each step, the following things happen:                     
                                                                      
           • The driver passes the current time step to the collect policy, which uses this time
            step to choose an action and returns an action step object containing the action.
                                                                      
                                                                      