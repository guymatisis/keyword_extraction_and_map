                                                                      
                                                                      
                                                                      
                                                                      
            tional cost and the number of parameters, speeding up training and improving
            generalization.                                           
                                                                      
           • Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like a
            single powerful convolutional layer, capable of capturing more complex patterns.
            Indeed, instead of sweeping a simple linear classifier across the image (as a single
            convolutional layer does), this pair of convolutional layers sweeps a two-layer
            neural network across the image.                          
          In short, you can think of the whole inception module as a convolutional layer on
          steroids, able to output feature maps that capture complex patterns at various scales.
                                                                      
                   The number of convolutional kernels for each convolutional layer
                   is a hyperparameter. Unfortunately, this means that you have six
                   more hyperparameters to tweak for every inception layer you add.
                                                                      
                                                                      
          Now let’s look at the architecture of the GoogLeNet CNN (see Figure 14-14). The
          number of feature maps output by each convolutional layer and each pooling layer is
          shown before the kernel size. The architecture is so deep that it has to be represented
          in three columns, but GoogLeNet is actually one tall stack, including nine inception
          modules (the boxes with the spinning tops). The six numbers in the inception mod‐
          ules represent the number of feature maps output by each convolutional layer in the
          module (in the same order as in Figure 14-13). Note that all the convolutional layers
          use the ReLU activation function.                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      