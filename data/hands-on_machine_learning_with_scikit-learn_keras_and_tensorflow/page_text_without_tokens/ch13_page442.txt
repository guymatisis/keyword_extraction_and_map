                                                                      
                                                                      
                                                                      
                                                                      
            mnist_train = mnist_train.shuffle(10000).batch(32)        
            mnist_train = mnist_train.map(lambda items: (items["image"], items["label"]))
            mnist_train = mnist_train.prefetch(1)                     
          But it’s simpler to ask the load() function to do this for you by setting as_super
          vised=True (obviously this works only for labeled datasets). You can also specify the
          batch size if you want. Then you can pass the dataset directly to your tf.keras model:
            dataset = tfds.load(name="mnist", batch_size=32, as_supervised=True)
            mnist_train = dataset["train"].prefetch(1)                
            model = keras.models.Sequential([...])                    
            model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd")
            model.fit(mnist_train, epochs=5)                          
          This was quite a technical chapter, and you may feel that it is a bit far from the
          abstract beauty of neural networks, but the fact is Deep Learning often involves large
          amounts of data, and knowing how to load, parse, and preprocess it efficiently is a
          crucial skill to have. In the next chapter, we will look at convolutional neural net‐
          works, which are among the most successful neural net architectures for image pro‐
          cessing and many other applications.                        
                                                                      
          Exercises                                                   
                                                                      
           1. Why would you want to use the Data API?                 
           2. What are the benefits of splitting a large dataset into multiple files?
                                                                      
           3. During training, how can you tell that your input pipeline is the bottleneck?
            What can you do to fix it?                                
           4. Can you save any binary data to a TFRecord file, or only serialized protocol
            buffers?                                                  
           5. Why would you go through the hassle of converting all your data to the Example
            protobuf format? Why not use your own protobuf definition?
           6. When using TFRecords, when would you want to activate compression? Why
            not do it systematically?                                 
                                                                      
           7. Data can be preprocessed directly when writing the data files, or within the
            tf.data pipeline, or in preprocessing layers within your model, or using TF Trans‐
            form. Can you list a few pros and cons of each option?    
           8. Name a few common techniques you can use to encode categorical features.
            What about text?                                          
           9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a train‐
            ing set, a validation set, and a test set; shuffle the training set; and save each
            dataset to multiple TFRecord files. Each record should be a serialized Example
            protobuf with two features: the serialized image (use tf.io.serialize_tensor()
                                                                      