                                                                      
                                                                      
                                                                      
                                                                      
            from sklearn.tree import DecisionTreeRegressor            
                                                                      
            tree_reg1 = DecisionTreeRegressor(max_depth=2)            
            tree_reg1.fit(X, y)                                       
          Next, we’ll train a second DecisionTreeRegressor on the residual errors made by the
          first predictor:                                            
            y2 = y - tree_reg1.predict(X)                             
            tree_reg2 = DecisionTreeRegressor(max_depth=2)            
            tree_reg2.fit(X, y2)                                      
          Then we train a third regressor on the residual errors made by the second predictor:
                                                                      
            y3 = y2 - tree_reg2.predict(X)                            
            tree_reg3 = DecisionTreeRegressor(max_depth=2)            
            tree_reg3.fit(X, y3)                                      
          Now we have an ensemble containing three trees. It can make predictions on a new
          instance simply by adding up the predictions of all the trees:
            y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
          Figure 7-9 represents the predictions of these three trees in the left column, and the
          ensemble’s predictions in the right column. In the first row, the ensemble has just one
          tree, so its predictions are exactly the same as the first tree’s predictions. In the second
          row, a new tree is trained on the residual errors of the first tree. On the right you can
          see that the ensemble’s predictions are equal to the sum of the predictions of the first
          two trees. Similarly, in the third row another tree is trained on the residual errors of
          the second tree. You can see that the ensemble’s predictions gradually get better as
          trees are added to the ensemble.                            
          A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe
          gressor class. Much like the RandomForestRegressor class, it has hyperparameters to
          control the growth of Decision Trees (e.g., max_depth, min_samples_leaf), as well as
          hyperparameters to control the ensemble training, such as the number of trees
          (n_estimators). The following code creates the same ensemble as the previous one:
                                                                      
            from sklearn.ensemble import GradientBoostingRegressor    
            gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)
            gbrt.fit(X, y)                                            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      