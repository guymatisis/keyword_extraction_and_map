                                                                      
                                                                      
                                                                      
                                                                      
          datasets if you want (but of course they need to have been loaded and preprocessed
          first).                                                     
                                                                      
          If you want to build your own custom training loop (as in Chapter 12), you can just
          iterate over the training set, very naturally:              
            for X_batch, y_batch in train_set:                        
               [...] # perform one Gradient Descent step              
          In fact, it is even possible to create a TF Function (see Chapter 12) that performs the
          whole training loop:                                        
            @tf.function                                              
            def train(model, optimizer, loss_fn, n_epochs, [...]):    
               train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])
               for X_batch, y_batch in train_set:                     
                 with tf.GradientTape() as tape:                      
                   y_pred = model(X_batch)                            
                   main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
                   loss = tf.add_n([main_loss] + model.losses)        
                 grads = tape.gradient(loss, model.trainable_variables)
                 optimizer.apply_gradients(zip(grads, model.trainable_variables))
          Congratulations, you now know how to build powerful input pipelines using the Data
          API! However, so far we have used CSV files, which are common, simple, and conve‐
          nient but not really efficient, and do not support large or complex data structures
          (such as images or audio) very well. So let’s see how to use TFRecords instead.
                   If you are happy with CSV files (or whatever other format you are
                   using), you do not have to use TFRecords. As the saying goes, if it
                   ain’t broke, don’t fix it! TFRecords are useful when the bottleneck
                   during training is loading and parsing the data.   
                                                                      
          The TFRecord Format                                         
                                                                      
          The TFRecord format is TensorFlow’s preferred format for storing large amounts of
          data and reading it efficiently. It is a very simple binary format that just contains a
          sequence of binary records of varying sizes (each record is comprised of a length, a
          CRC checksum to check that the length was not corrupted, then the actual data, and
          finally a CRC checksum for the data). You can easily create a TFRecord file using the
          tf.io.TFRecordWriter class:                                 
            with tf.io.TFRecordWriter("my_data.tfrecord") as f:       
               f.write(b"This is the first record")                   
               f.write(b"And this is the second record")              
                                                                      
                                                                      
                                                                      