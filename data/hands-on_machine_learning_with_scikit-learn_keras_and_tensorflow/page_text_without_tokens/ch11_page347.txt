                                                                      
                                                                      
                                                                      
                                                                      
                   The more similar the tasks are, the more layers you want to reuse
                   (starting with the lower layers). For very similar tasks, try keeping
                   all the hidden layers and just replacing the output layer.
                                                                      
                                                                      
          Try freezing all the reused layers first (i.e., make their weights non-trainable so that
          Gradient Descent won’t modify them), then train your model and see how it per‐
          forms. Then try unfreezing one or two of the top hidden layers to let backpropaga‐
          tion tweak them and see if performance improves. The more training data you have,
          the more layers you can unfreeze. It is also useful to reduce the learning rate when
          you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.
                                                                      
          If you still cannot get good performance, and you have little training data, try drop‐
          ping the top hidden layer(s) and freezing all the remaining hidden layers again. You
          can iterate until you find the right number of layers to reuse. If you have plenty of
          training data, you may try replacing the top hidden layers instead of dropping them,
          and even adding more hidden layers.                         
          Transfer Learning with Keras                                
                                                                      
          Let’s look at an example. Suppose the Fashion MNIST dataset only contained eight
          classes—for example, all the classes except for sandal and shirt. Someone built and
          trained a Keras model on that set and got reasonably good performance (>90% accu‐
          racy). Let’s call this model A. You now want to tackle a different task: you have images
          of sandals and shirts, and you want to train a binary classifier (positive=shirt,
          negative=sandal). Your dataset is quite small; you only have 200 labeled images.
          When you train a new model for this task (let’s call it model B) with the same archi‐
          tecture as model A, it performs reasonably well (97.2% accuracy). But since it’s a
          much easier task (there are just two classes), you were hoping for more. While drink‐
          ing your morning coffee, you realize that your task is quite similar to task A, so per‐
          haps transfer learning can help? Let’s find out!            
          First, you need to load model A and create a new model based on that model’s layers.
          Let’s reuse all the layers except for the output layer:     
            model_A = keras.models.load_model("my_model_A.h5")        
            model_B_on_A = keras.models.Sequential(model_A.layers[:-1])
            model_B_on_A.add(keras.layers.Dense(1, activation="sigmoid"))
          Note that model_A and model_B_on_A now share some layers. When you train
          model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone
          model_A before you reuse its layers. To do this, you clone model A’s architecture with
          clone_model(), then copy its weights (since clone_model() does not clone the
          weights):                                                   
                                                                      
                                                                      