                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                              Cross Entropy                           
                                                                      
           Cross entropy originated from information theory. Suppose you want to efficiently
           transmit information about the weather every day. If there are eight options (sunny,
           rainy, etc.), you could encode each option using three bits because 23 = 8. However, if
           you think it will be sunny almost every day, it would be much more efficient to code
           “sunny” on just one bit (0) and the other seven options on four bits (starting with a
           1). Cross entropy measures the average number of bits you actually send per option.
           If your assumption about the weather is perfect, cross entropy will be equal to the
           entropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump‐
           tions are wrong (e.g., if it rains often), cross entropy will be greater by an amount
           called the Kullback–Leibler (KL) divergence.               
           The cross entropy between two probability distributions p and q is defined as H(p,q)
           = —Σx p(x) log q(x) (at least when the distributions are discrete). For more details,
           check out my video on the subject.                         
                                                                      
          The gradient vector of this cost function with regard to θ(k) is given by Equation 4-23.
                                                                      
            Equation 4-23. Cross entropy gradient vector for class k  
                     m                                                
            ∇  J Θ = 1 ∑ p i −y i x i                                 
             θ k   m i=1 k k                                          
          Now you can compute the gradient vector for every class, then use Gradient Descent
          (or any other optimization algorithm) to find the parameter matrix Θ that minimizes
          the cost function.                                          
                                                                      
          Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-
          Learn’s LogisticRegression uses one-versus-the-rest by default when you train it on
          more than two classes, but you can set the multi_class hyperparameter to "multino
          mial" to switch it to Softmax Regression. You must also specify a solver that supports
          Softmax Regression, such as the "lbfgs" solver (see Scikit-Learn’s documentation for
          more details). It also applies ℓ regularization by default, which you can control using
                           2                                          
          the hyperparameter C:                                       
            X = iris["data"][:, (2, 3)] # petal length, petal width   
            y = iris["target"]                                        
            softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10)
            softmax_reg.fit(X, y)                                     
          So the next time you find an iris with petals that are 5 cm long and 2 cm wide, you
          can ask your model to tell you what type of iris it is, and it will answer Iris virginica
          (class 2) with 94.2% probability (or Iris versicolor with 5.8% probability):