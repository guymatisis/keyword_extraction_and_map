                                                                      
                                                                      
                                                                      
                                                                      
          since the beginning of training). It does so by using exponential decay in the first step
          (see Equation 11-7).                                        
                                                                      
            Equation 11-7. RMSProp algorithm                          
                                                                      
            1. s  βs+ 1−β ∇ J θ ⊗∇ J θ                                
                         θ     θ                                      
            2. θ  θ−η∇ J θ ⊘ s+ε                                      
                      θ                                               
          The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but
          this default value often works well, so you may not need to tune it at all.
          As you might expect, Keras has an RMSprop optimizer:        
            optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)   
          Note that the rho argument corresponds to β in Equation 11-7. Except on very simple
          problems, this optimizer almost always performs much better than AdaGrad. In fact,
          it was the preferred optimization algorithm of many researchers until Adam optimi‐
          zation came around.                                         
                                                                      
          Adam and Nadam Optimization                                 
                                                                      
          Adam,17 which stands for adaptive moment estimation, combines the ideas of momen‐
          tum optimization and RMSProp: just like momentum optimization, it keeps track of
          an exponentially decaying average of past gradients; and just like RMSProp, it keeps
          track of an exponentially decaying average of past squared gradients (see Equation
          11-8).18                                                    
                                                                      
            Equation 11-8. Adam algorithm                             
            1. m   β m− 1−β ∇ J θ                                     
                   1      1 θ                                         
            2. s  β s+ 1−β ∇ J θ ⊗∇ J θ                               
                   2    2  θ    θ                                     
                    m                                                 
            3. m                                                      
                      t                                               
                   1−β                                                
                      1                                               
                    s                                                 
            4.  s                                                     
                      t                                               
                  1−β                                                 
                     2                                                
            5. θ  θ+ηm⊘  s +ε                                         
          17 Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,” arXiv preprint arXiv:
           1412.6980 (2014).                                          
          18 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the
           first moment while the variance is often called the second moment, hence the name of the algorithm.