                                                                      
                                                                      
                                                                      
                                                                      
          accurate Q-Value estimates (or close enough), then the optimal policy is choosing the
          action that has the highest Q-Value (i.e., the greedy policy).
                                                                      
            Equation 18-5. Q-Learning algorithm                       
                                                                      
            Q s,a r+γ·max Q s′,a′                                     
                α     a′                                              
                                                                      
          For each state-action pair (s, a), this algorithm keeps track of a running average of the
          rewards r the agent gets upon leaving the state s with action a, plus the sum of dis‐
          counted future rewards it expects to get. To estimate this sum, we take the maximum
          of the Q-Value estimates for the next state s′, since we assume that the target policy
          would act optimally from then on.                           
          Let’s implement the Q-Learning algorithm. First, we will need to make an agent
          explore the environment. For this, we need a step function so that the agent can exe‐
          cute one action and get the resulting state and reward:     
                                                                      
            def step(state, action):                                  
               probas = transition_probabilities[state][action]       
               next_state = np.random.choice([0, 1, 2], p=probas)     
               reward = rewards[state][action][next_state]            
               return next_state, reward                              
          Now let’s implement the agent’s exploration policy. Since the state space is pretty
          small, a simple random policy will be sufficient. If we run the algorithm for long
          enough, the agent will visit every state many times, and it will also try every possible
          action many times:                                          
            def exploration_policy(state):                            
               return np.random.choice(possible_actions[state])       
          Next, after we initialize the Q-Values just like earlier, we are ready to run the Q-
          Learning algorithm with learning rate decay (using power scheduling, introduced in
          Chapter 11):                                                
            alpha0 = 0.05 # initial learning rate                     
            decay = 0.005 # learning rate decay                       
            gamma = 0.90 # discount factor                            
            state = 0 # initial state                                 
            for iteration in range(10000):                            
               action = exploration_policy(state)                     
               next_state, reward = step(state, action)               
               next_value = np.max(Q_values[next_state])              
               alpha = alpha0 / (1 + iteration * decay)               
               Q_values[state, action] *= 1 - alpha                   
               Q_values[state, action] += alpha * (reward + gamma * next_value)
               state = next_state                                     