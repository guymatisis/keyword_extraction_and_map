                                                                      
                                                                      
                                                                      
                                                                      
            and in general Adam performs better. So, this is just one more optimizer you can
            try if you experience problems with Adam on some task.    
                                                                      
          Nadam                                                       
            Nadam optimization is Adam optimization plus the Nesterov trick, so it will
            often converge slightly faster than Adam. In his report introducing this techni‐
            que,19 the researcher Timothy Dozat compares many different optimizers on vari‐
            ous tasks and finds that Nadam generally outperforms Adam but is sometimes
            outperformed by RMSProp.                                  
                                                                      
                   Adaptive optimization methods (including RMSProp, Adam, and
                   Nadam optimization) are often great, converging fast to a good sol‐
                   ution. However, a 2017 paper20 by Ashia C. Wilson et al. showed
                   that they can lead to solutions that generalize poorly on some data‐
                   sets. So when you are disappointed by your model’s performance,
                   try using plain Nesterov Accelerated Gradient instead: your dataset
                   may just be allergic to adaptive gradients. Also check out the latest
                   research, because it’s moving fast.                
          All the optimization techniques discussed so far only rely on the first-order partial
          derivatives (Jacobians). The optimization literature also contains amazing algorithms
          based on the second-order partial derivatives (the Hessians, which are the partial
          derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply
          to deep neural networks because there are n2 Hessians per output (where n is the
          number of parameters), as opposed to just n Jacobians per output. Since DNNs typi‐
          cally have tens of thousands of parameters, the second-order optimization algorithms
          often don’t even fit in memory, and even when they do, computing the Hessians is
          just too slow.                                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          19 Timothy Dozat, “Incorporating Nesterov Momentum into Adam” (2016).
          20 Ashia C. Wilson et al., “The Marginal Value of Adaptive Gradient Methods in Machine Learning,” Advances in
           Neural Information Processing Systems 30 (2017): 4148–4158.