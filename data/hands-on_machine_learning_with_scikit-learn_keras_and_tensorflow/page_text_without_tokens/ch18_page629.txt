                                                                      
                                                                      
                                                                      
                                                                      
            Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions
            for state, actions in enumerate(possible_actions):        
               Q_values[state, actions] = 0.0 # for all possible actions
          Now let’s run the Q-Value Iteration algorithm. It applies Equation 18-3 repeatedly, to
          all Q-Values, for every state and every possible action:    
            gamma = 0.90 # the discount factor                        
                                                                      
            for iteration in range(50):                               
               Q_prev = Q_values.copy()                               
               for s in range(3):                                     
                 for a in possible_actions[s]:                        
                   Q_values[s, a] = np.sum([                          
                        transition_probabilities[s][a][sp]            
                        * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))
                      for sp in range(3)])                            
          That’s it! The resulting Q-Values look like this:           
            >>> Q_values                                              
            array([[18.91891892, 17.02702702, 13.62162162],           
                [ 0.   ,     -inf, -4.87971488],                      
                [    -inf, 50.13365013, -inf]])                       
          For example, when the agent is in state s and it chooses action a , the expected sum
                                  0             1                     
          of discounted future rewards is approximately 17.0.         
          For each state, let’s look at the action that has the highest Q-Value:
            >>> np.argmax(Q_values, axis=1) # optimal action for each state
            array([0, 0, 1])                                          
          This gives us the optimal policy for this MDP, when using a discount factor of 0.90: in
          state s choose action a ; in state s choose action a (i.e., stay put); and in state s
              0         0      1         0                 2          
          choose action a (the only possible action). Interestingly, if we increase the discount
                   1                                                  
          factor to 0.95, the optimal policy changes: in state s the best action becomes a (go
                                         1               2            
          through the fire!). This makes sense because the more you value future rewards, the
          more you are willing to put up with some pain now for the promise of future bliss.
          Temporal Difference Learning                                
          Reinforcement Learning problems with discrete actions can often be modeled as
          Markov decision processes, but the agent initially has no idea what the transition
          probabilities are (it does not know T(s, a, s′)), and it does not know what the rewards
          are going to be either (it does not know R(s, a, s′)). It must experience each state and
          each transition at least once to know the rewards, and it must experience them multi‐
          ple times if it is to have a reasonable estimate of the transition probabilities.
          The Temporal Difference Learning (TD Learning) algorithm is very similar to the
          Value Iteration algorithm, but tweaked to take into account the fact that the agent has