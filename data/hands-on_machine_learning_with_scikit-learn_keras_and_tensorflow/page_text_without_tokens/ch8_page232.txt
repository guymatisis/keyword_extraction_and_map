                                                                      
                                                                      
                                                                      
                                                                      
          space, then we want the squared distance between z(i) and ∑m w z j to be as small
                                             j=1 i,j                  
          as possible. This idea leads to the unconstrained optimization problem described in
          Equation 8-5. It looks very similar to the first step, but instead of keeping the instan‐
          ces fixed and finding the optimal weights, we are doing the reverse: keeping the
          weights fixed and finding the optimal position of the instances’ images in the low-
          dimensional space. Note that Z is the matrix containing all z(i).
            Equation 8-5. LLE step two: reducing dimensionality while preserving relationships
                                                                      
                    m     m     2                                     
            Z= argmin ∑ z i − ∑ w z j                                 
                            i,j                                       
                Z  i=1   j=1                                          
          Scikit-Learn’s LLE implementation has the following computational complexity:
          O(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the
          weights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐
          nately, the m2 in the last term makes this algorithm scale poorly to very large datasets.
          Other Dimensionality Reduction Techniques                   
                                                                      
          There are many other dimensionality reduction techniques, several of which are
          available in Scikit-Learn. Here are some of the most popular ones:
                                                                      
          Random Projections                                          
            As its name suggests, projects the data to a lower-dimensional space using a ran‐
            dom linear projection. This may sound crazy, but it turns out that such a random
            projection is actually very likely to preserve distances well, as was demonstrated
            mathematically by William B. Johnson and Joram Lindenstrauss in a famous
            lemma. The quality of the dimensionality reduction depends on the number of
            instances and the target dimensionality, but surprisingly not on the initial dimen‐
            sionality. Check out the documentation for the sklearn.random_projection
            package for more details.                                 
          Multidimensional Scaling (MDS)                              
            Reduces dimensionality while trying to preserve the distances between the
            instances.                                                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      