                                                                      
                                                                      
                                                                      
                                                                      
          It will not only speed up training considerably, but also require significantly less
          training data.                                              
                                                                      
          Suppose you have access to a DNN that was trained to classify pictures into 100 dif‐
          ferent categories, including animals, plants, vehicles, and everyday objects. You now
          want to train a DNN to classify specific types of vehicles. These tasks are very similar,
          even partly overlapping, so you should try to reuse parts of the first network (see
          Figure 11-4).                                               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-4. Reusing pretrained layers                      
                                                                      
                   If the input pictures of your new task don’t have the same size as
                   the ones used in the original task, you will usually have to add a
                   preprocessing step to resize them to the size expected by the origi‐
                   nal model. More generally, transfer learning will work best when
                   the inputs have similar low-level features.        
                                                                      
          The output layer of the original model should usually be replaced because it is most
          likely not useful at all for the new task, and it may not even have the right number of
          outputs for the new task.                                   
          Similarly, the upper hidden layers of the original model are less likely to be as useful
          as the lower layers, since the high-level features that are most useful for the new task
          may differ significantly from the ones that were most useful for the original task. You
          want to find the right number of layers to reuse.           
                                                                      
                                                                      
                                                                      
                                                                      