                                                                      
                                                                      
                                                                      
                                                                      
          set on a graph and often gain some important insights by visually detecting patterns,
          such as clusters. Moreover, DataViz is essential to communicate your conclusions to
          people who are not data scientists—in particular, decision makers who will use your
          results.                                                    
                                                                      
          In this chapter we will discuss the curse of dimensionality and get a sense of what
          goes on in high-dimensional space. Then, we will consider the two main approaches
          to dimensionality reduction (projection and Manifold Learning), and we will go
          through three of the most popular dimensionality reduction techniques: PCA, Kernel
          PCA, and LLE.                                               
          The Curse of Dimensionality                                 
                                                                      
          We are so used to living in three dimensions1 that our intuition fails us when we try
          to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to
          picture in our minds (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a
          1,000-dimensional space.                                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2
                                                                      
          It turns out that many things behave very differently in high-dimensional space. For
          example, if you pick a random point in a unit square (a 1 × 1 square), it will have only
          about a 0.4% chance of being located less than 0.001 from a border (in other words, it
          is very unlikely that a random point will be “extreme” along any dimension). But in a
          10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most
          points in a high-dimensional hypercube are very close to the border.3
                                                                      
                                                                      
          1 Well, four dimensions if you count time, and a few more if you are a string theorist.
          2 Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia user Nerd‐
           Boy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract.
          3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put
           in their coffee), if you consider enough dimensions.       