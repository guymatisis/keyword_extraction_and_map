                                                                      
                                                                      
                                                                      
                                                                      
            >>> np.mean(y_train_partially_propagated == y_train[partially_propagated])
            0.9896907216494846                                        
                                                                      
                             Active Learning                          
                                                                      
           To continue improving your model and your training set, the next step could be to do
           a few rounds of active learning, which is when a human expert interacts with the
           learning algorithm, providing labels for specific instances when the algorithm
           requests them. There are many different strategies for active learning, but one of the
           most common ones is called uncertainty sampling. Here is how it works:
            1. The model is trained on the labeled instances gathered so far, and this model is
              used to make predictions on all the unlabeled instances.
            2. The instances for which the model is most uncertain (i.e., when its estimated
              probability is lowest) are given to the expert to be labeled.
                                                                      
            3. You iterate this process until the performance improvement stops being worth
              the labeling effort.                                    
           Other strategies include labeling the instances that would result in the largest model
           change, or the largest drop in the model’s validation error, or the instances that differ‐
           ent models disagree on (e.g., an SVM or a Random Forest).  
                                                                      
                                                                      
          Before we move on to Gaussian mixture models, let’s take a look at DBSCAN,
          another popular clustering algorithm that illustrates a very different approach based
          on local density estimation. This approach allows the algorithm to identify clusters of
          arbitrary shapes.                                           
          DBSCAN                                                      
                                                                      
          This algorithm defines clusters as continuous regions of high density. Here is how it
          works:                                                      
                                                                      
           • For each instance, the algorithm counts how many instances are located within a
            small distance ε (epsilon) from it. This region is called the instance’s ε-
            neighborhood.                                             
           • If an instance has at least min_samples instances in its ε-neighborhood (includ‐
            ing itself), then it is considered a core instance. In other words, core instances are
            those that are located in dense regions.                  
           • All instances in the neighborhood of a core instance belong to the same cluster.
            This neighborhood may include other core instances; therefore, a long sequence
            of neighboring core instances forms a single cluster.     
                                                                      
                                                                      