                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models with
          Lasso models and uses smaller α values.                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-18. A linear model (left) and a polynomial model (right), both using various
          levels of Lasso regularization                              
                                                                      
          An important characteristic of Lasso Regression is that it tends to eliminate the
          weights of the least important features (i.e., set them to zero). For example, the
          dashed line in the righthand plot in Figure 4-18 (with α = 10-7) looks quadratic,
          almost linear: all the weights for the high-degree polynomial features are equal to
          zero. In other words, Lasso Regression automatically performs feature selection and
          outputs a sparse model (i.e., with few nonzero feature weights).
          You can get a sense of why this is the case by looking at Figure 4-19: the axes repre‐
          sent two model parameters, and the background contours represent different loss
          functions. In the top-left plot, the contours represent the ℓ loss (|θ | + |θ |), which
                                             1    1   2               
          drops linearly as you get closer to any axis. For example, if you initialize the model
          parameters to θ = 2 and θ = 0.5, running Gradient Descent will decrement both
                   1      2                                           
          parameters equally (as represented by the dashed yellow line); therefore θ will reach
                                                     2                
          0 first (since it was closer to 0 to begin with). After that, Gradient Descent will roll
          down the gutter until it reaches θ = 0 (with a bit of bouncing around, since the gradi‐
                             1                                        
          ents of ℓ never get close to 0: they are either –1 or 1 for each parameter). In the top-
               1                                                      
          right plot, the contours represent Lasso’s cost function (i.e., an MSE cost function plus
          an ℓ loss). The small white circles show the path that Gradient Descent takes to opti‐
            1                                                         
          mize some model parameters that were initialized around θ = 0.25 and θ = –1:
                                              1        2              
          notice once again how the path quickly reaches θ = 0, then rolls down the gutter and
                                      2                               
          ends up bouncing around the global optimum (represented by the red square). If we
          increased α, the global optimum would move left along the dashed yellow line, while