                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-11. Tuning the number of trees using early stopping
                                                                      
          It is also possible to implement early stopping by actually stopping training early
          (instead of training a large number of trees first and then looking back to find the
          optimal number). You can do so by setting warm_start=True, which makes Scikit-
          Learn keep existing trees when the fit() method is called, allowing incremental
          training. The following code stops training when the validation error does not
          improve for five iterations in a row:                       
            gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)
                                                                      
            min_val_error = float("inf")                              
            error_going_up = 0                                        
            for n_estimators in range(1, 120):                        
               gbrt.n_estimators = n_estimators                       
               gbrt.fit(X_train, y_train)                             
               y_pred = gbrt.predict(X_val)                           
               val_error = mean_squared_error(y_val, y_pred)          
               if val_error < min_val_error:                          
                 min_val_error = val_error                            
                 error_going_up = 0                                   
               else:                                                  
                 error_going_up += 1                                  
                 if error_going_up == 5:                              
                   break # early stopping                             
          The GradientBoostingRegressor class also supports a subsample hyperparameter,
          which specifies the fraction of training instances to be used for training each tree. For
          example, if subsample=0.25, then each tree is trained on 25% of the training instanâ€
          ces, selected randomly. As you can probably guess by now, this technique trades a
          higher bias for a lower variance. It also speeds up training considerably. This is called
          Stochastic Gradient Boosting.                               