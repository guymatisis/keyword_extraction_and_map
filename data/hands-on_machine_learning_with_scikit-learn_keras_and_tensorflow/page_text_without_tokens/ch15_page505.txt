                                                                      
                                                                      
                                                                      
                                                                      
          Baseline Metrics                                            
                                                                      
          Before we start using RNNs, it is often a good idea to have a few baseline metrics, or
          else we may end up thinking our model works great when in fact it is doing worse
          than basic models. For example, the simplest approach is to predict the last value in
          each series. This is called naive forecasting, and it is sometimes surprisingly difficult to
          outperform. In this case, it gives us a mean squared error of about 0.020:
            >>> y_pred = X_valid[:, -1]                               
            >>> np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
            0.020211367                                               
          Another simple approach is to use a fully connected network. Since it expects a flat
          list of features for each input, we need to add a Flatten layer. Let’s just use a simple
          Linear Regression model so that each prediction will be a linear combination of the
          values in the time series:                                  
            model = keras.models.Sequential([                         
               keras.layers.Flatten(input_shape=[50, 1]),             
               keras.layers.Dense(1)                                  
            ])                                                        
          If we compile this model using the MSE loss and the default Adam optimizer, then fit
          it on the training set for 20 epochs and evaluate it on the validation set, we get an
          MSE of about 0.004. That’s much better than the naive approach!
                                                                      
          Implementing a Simple RNN                                   
          Let’s see if we can beat that with a simple RNN:            
                                                                      
            model = keras.models.Sequential([                         
             keras.layers.SimpleRNN(1, input_shape=[None, 1])         
            ])                                                        
          That’s really the simplest RNN you can build. It just contains a single layer, with a sin‐
          gle neuron, as we saw in Figure 15-1. We do not need to specify the length of the
          input sequences (unlike in the previous model), since a recurrent neural network can
          process any number of time steps (this is why we set the first input dimension to
          None). By default, the SimpleRNN layer uses the hyperbolic tangent activation func‐
          tion. It works exactly as we saw earlier: the initial state h is set to 0, and it is passed
                                          (init)                      
          to a single recurrent neuron, along with the value of the first time step, x . The neu‐
                                                    (0)               
          ron computes a weighted sum of these values and applies the hyperbolic tangent acti‐
          vation function to the result, and this gives the first output, y . In a simple RNN, this
                                              0                       
          output is also the new state h . This new state is passed to the same recurrent neuron
                           0                                          
          along with the next input value, x , and the process is repeated until the last time
                              (1)                                     
          step. Then the layer just outputs the last value, y . All of this is performed simultane‐
                                      49                              
          ously for every time series.                                