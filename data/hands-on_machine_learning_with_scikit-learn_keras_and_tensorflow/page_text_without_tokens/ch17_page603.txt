                                                                      
                                                                      
                                                                      
                                                                      
          The paper also introduced several other techniques aimed at increasing the diversity
          of the outputs (to avoid mode collapse) and making training more stable:
                                                                      
          Minibatch standard deviation layer                          
            Added near the end of the discriminator. For each position in the inputs, it com‐
            putes the standard deviation across all channels and all instances in the batch
            (S = tf.math.reduce_std(inputs, axis=[0, -1])). These standard deviations
            are then averaged across all points to get a single value (v = tf.reduce_
            mean(S)). Finally, an extra feature map is added to each instance in the batch and
            filled with the computed value (tf.concat([inputs, tf.fill([batch_size,
            height, width, 1], v)], axis=-1)). How does this help? Well, if the genera‐
            tor produces images with little variety, then there will be a small standard devia‐
            tion across feature maps in the discriminator. Thanks to this layer, the
            discriminator will have easy access to this statistic, making it less likely to be
            fooled by a generator that produces too little diversity. This will encourage the
            generator to produce more diverse outputs, reducing the risk of mode collapse.
          Equalized learning rate                                     
            Initializes all weights using a simple Gaussian distribution with mean 0 and stan‐
            dard deviation 1 rather than using He initialization. However, the weights are
            scaled down at runtime (i.e., every time the layer is executed) by the same factor
            as in He initialization: they are divided by 2/n , where n is the number
                                         inputs  inputs               
            of inputs to the layer. The paper demonstrated that this technique significantly
            improved the GAN’s performance when using RMSProp, Adam, or other adap‐
            tive gradient optimizers. Indeed, these optimizers normalize the gradient updates
            by their estimated standard deviation (see Chapter 11), so parameters that have a
            larger dynamic range17 will take longer to train, while parameters with a small
            dynamic range may be updated too quickly, leading to instabilities. By rescaling
            the weights as part of the model itself rather than just rescaling them upon initi‐
            alization, this approach ensures that the dynamic range is the same for all param‐
            eters, throughout training, so they all learn at the same speed. This both speeds
            up and stabilizes training.                               
          Pixelwise normalization layer                               
            Added after each convolutional layer in the generator. It normalizes each activa‐
            tion based on all the activations in the same image and at the same location, but
            across all channels (dividing by the square root of the mean squared activation).
            In TensorFlow code, this is inputs / tf.sqrt(tf.reduce_mean(tf.square(X),
            axis=-1, keepdims=True) + 1e-8) (the smoothing term 1e-8 is needed to
                                                                      
                                                                      
                                                                      
          17 The dynamic range of a variable is the ratio between the highest and the lowest value it may take.