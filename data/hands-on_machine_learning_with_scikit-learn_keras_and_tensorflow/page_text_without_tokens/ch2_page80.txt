                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            final_predictions = final_model.predict(X_test_prepared)  
                                                                      
            final_mse = mean_squared_error(y_test, final_predictions) 
            final_rmse = np.sqrt(final_mse) # => evaluates to 47,730.2
          In some cases, such a point estimate of the generalization error will not be quite
          enough to convince you to launch: what if it is just 0.1% better than the model cur‐
          rently in production? You might want to have an idea of how precise this estimate is.
          For this, you can compute a 95% confidence interval for the generalization error using
          scipy.stats.t.interval():                                   
            >>> from scipy import stats                               
            >>> confidence = 0.95                                     
            >>> squared_errors = (final_predictions - y_test) ** 2    
            >>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
            ...              loc=squared_errors.mean(),               
            ...              scale=stats.sem(squared_errors)))        
            ...                                                       
            array([45685.10470776, 49691.25001878])                   
          If you did a lot of hyperparameter tuning, the performance will usually be slightly
          worse than what you measured using cross-validation (because your system ends up
          fine-tuned to perform well on the validation data and will likely not perform as well
          on unknown datasets). It is not the case in this example, but when this happens you
          must resist the temptation to tweak the hyperparameters to make the numbers look
          good on the test set; the improvements would be unlikely to generalize to new data.
          Now comes the project prelaunch phase: you need to present your solution (high‐
          lighting what you have learned, what worked and what did not, what assumptions
          were made, and what your system’s limitations are), document everything, and create
          nice presentations with clear visualizations and easy-to-remember statements (e.g.,
          “the median income is the number one predictor of housing prices”). In this Califor‐
          nia housing example, the final performance of the system is not better than the
          experts’ price estimates, which were often off by about 20%, but it may still be a good
          idea to launch it, especially if this frees up some time for the experts so they can work
          on more interesting and productive tasks.                   
          Launch, Monitor, and Maintain Your System                   
                                                                      
          Perfect, you got approval to launch! You now need to get your solution ready for pro‐
          duction (e.g., polish the code, write documentation and tests, and so on). Then you
          can deploy your model to your production environment. One way to do this is to save
          the trained Scikit-Learn model (e.g., using joblib), including the full preprocessing
          and prediction pipeline, then load this trained model within your production envi‐
          ronment and use it to make predictions by calling its predict() method. For exam‐
          ple, perhaps the model will be used within a website: the user will type in some data