                                                                      
                                                                      
                                                                      
                                                                      
            from tf_agents.networks.q_network import QNetwork         
                                                                      
            preprocessing_layer = keras.layers.Lambda(                
                            lambda obs: tf.cast(obs, np.float32) / 255.)
            conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]
            fc_layer_params=[512]                                     
            q_net = QNetwork(                                         
               tf_env.observation_spec(),                             
               tf_env.action_spec(),                                  
               preprocessing_layers=preprocessing_layer,              
               conv_layer_params=conv_layer_params,                   
               fc_layer_params=fc_layer_params)                       
          This QNetwork takes an observation as input and outputs one Q-Value per action, so
          we must give it the specifications of the observations and the actions. It starts with a
          preprocessing layer: a simple Lambda layer that casts the observations to 32-bit floats
          and normalizes them (the values will range from 0.0 to 1.0). The observations contain
          unsigned bytes, which use 4 times less space than 32-bit floats, which is why we did
          not cast the observations to 32-bit floats earlier; we want to save RAM in the replay
          buffer. Next, the network applies three convolutional layers: the first has 32 8 × 8 fil‐
          ters and uses a stride of 4, the second has 64 4 × 4 filters and a stride of 2, and the
          third has 64 3 × 3 filters and a stride of 1. Lastly, it applies a dense layer with 512
          units, followed by a dense output layer with 4 units, one per Q-Value to output (i.e.,
          one per action). All convolutional layers and all dense layers except the output layer
          use the ReLU activation function by default (you can change this by setting the acti
          vation_fn argument). The output layer does not use any activation function.
          Under the hood, a QNetwork is composed of two parts: an encoding network that pro‐
          cesses the observations, followed by a dense output layer that outputs one Q-Value
          per action. TF-Agent’s EncodingNetwork class implements a neural network architec‐
          ture found in various agents (see Figure 18-14).            
          It may have one or more inputs. For example, if each observation is composed of
          some sensor data plus an image from a camera, you will have two inputs. Each input
          may require some preprocessing steps, in which case you can specify a list of Keras
          layers via the preprocessing_layers argument, with one preprocessing layer per
          input, and the network will apply each layer to the corresponding input (if an input
          requires multiple layers of preprocessing, you can pass a whole model, since a Keras
          model can always be used as a layer). If there are two inputs or more, you must also
          pass an extra layer via the preprocessing_combiner argument, to combine the out‐
          puts from the preprocessing layers into a single output.    
          Next, the encoding network will optionally apply a list of convolutions sequentially,
          provided you specify their parameters via the conv_layer_params argument. This
          must be a list composed of 3-tuples (one per convolutional layer) indicating the
                                                                      