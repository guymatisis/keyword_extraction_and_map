                                                                      
                                                                      
                                                                      
                                                                      
          Piecewise constant scheduling                               
            Use a constant learning rate for a number of epochs (e.g., η = 0.1 for 5 epochs),
                                               0                      
            then a smaller learning rate for another number of epochs (e.g., η = 0.001 for 50
                                                   1                  
            epochs), and so on. Although this solution can work very well, it requires fid‐
            dling around to figure out the right sequence of learning rates and how long to
            use each of them.                                         
          Performance scheduling                                      
            Measure the validation error every N steps (just like for early stopping), and
            reduce the learning rate by a factor of λ when the error stops dropping.
          1cycle scheduling                                           
            Contrary to the other approaches, 1cycle (introduced in a 2018 paper21 by Leslie
            Smith) starts by increasing the initial learning rate η , growing linearly up to η
                                            0              1          
            halfway through training. Then it decreases the learning rate linearly down to η
                                                           0          
            again during the second half of training, finishing the last few epochs by drop‐
            ping the rate down by several orders of magnitude (still linearly). The maximum
            learning rate η is chosen using the same approach we used to find the optimal
                     1                                                
            learning rate, and the initial learning rate η is chosen to be roughly 10 times
                                       0                              
            lower. When using a momentum, we start with a high momentum first (e.g.,
            0.95), then drop it down to a lower momentum during the first half of training
            (e.g., down to 0.85, linearly), and then bring it back up to the maximum value
            (e.g., 0.95) during the second half of training, finishing the last few epochs with
            that maximum value. Smith did many experiments showing that this approach
            was often able to speed up training considerably and reach better performance.
            For example, on the popular CIFAR10 image dataset, this approach reached
            91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800
            epochs through a standard approach (with the same neural network
            architecture).                                            
          A 2013 paper22 by Andrew Senior et al. compared the performance of some of the
          most popular learning schedules when using momentum optimization to train deep
          neural networks for speech recognition. The authors concluded that, in this setting,
          both performance scheduling and exponential scheduling performed well. They
          favored exponential scheduling because it was easy to tune and it converged slightly
          faster to the optimal solution (they also mentioned that it was easier to implement
          21 Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch
           Size, Momentum, and Weight Decay,” arXiv preprint arXiv:1803.09820 (2018).
          22 Andrew Senior et al., “An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recogni‐
           tion,” Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (2013):
           6724–6728.                                                 