                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-15. Predictions in a multilayer stacking ensemble  
                                                                      
          Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard
          to roll out your own implementation (see the following exercises). Alternatively, you
          can use an open source implementation such as DESlib.       
          Exercises                                                   
                                                                      
                                                                      
           1. If you have trained five different models on the exact same training data, and
            they all achieve 95% precision, is there any chance that you can combine these
            models to get better results? If so, how? If not, why?    
           2. What is the difference between hard and soft voting classifiers?
           3. Is it possible to speed up training of a bagging ensemble by distributing it across
            multiple servers? What about pasting ensembles, boosting ensembles, Random
            Forests, or stacking ensembles?                           
           4. What is the benefit of out-of-bag evaluation?           
                                                                      
           5. What makes Extra-Trees more random than regular Random Forests? How can
            this extra randomness help? Are Extra-Trees slower or faster than regular Ran‚Äê
            dom Forests?                                              
           6. If your AdaBoost ensemble underfits the training data, which hyperparameters
            should you tweak and how?                                 
                                                                      