                                                                      
                                                                      
                                                                      
                                                                      
          strategy, create a MirroredStrategy object, call its scope() method to get a distribu‐
          tion context, and wrap the creation and compilation of your model inside that con‐
          text. Then call the model’s fit() method normally:          
                                                                      
            distribution = tf.distribute.MirroredStrategy()           
            with distribution.scope():                                
               mirrored_model = keras.models.Sequential([...])        
               mirrored_model.compile([...])                          
            batch_size = 100 # must be divisible by the number of replicas
            history = mirrored_model.fit(X_train, y_train, epochs=10) 
                                                                      
          Under the hood, tf.keras is distribution-aware, so in this MirroredStrategy context it
          knows that it must replicate all variables and operations across all available GPU
          devices. Note that the fit() method will automatically split each training batch
          across all the replicas, so it’s important that the batch size be divisible by the number
          of replicas. And that’s all! Training will generally be significantly faster than using a
          single device, and the code change was really minimal.      
          Once you have finished training your model, you can use it to make predictions effi‐
          ciently: call the predict() method, and it will automatically split the batch across all
          replicas, making predictions in parallel (again, the batch size must be divisible by the
          number of replicas). If you call the model’s save() method, it will be saved as a regu‐
          lar model, not as a mirrored model with multiple replicas. So when you load it, it will
          run like a regular model, on a single device (by default GPU 0, or the CPU if there are
          no GPUs). If you want to load a model and run it on all available devices, you must
          call keras.models.load_model() within a distribution context:
            with distribution.scope():                                
               mirrored_model = keras.models.load_model("my_mnist_model.h5")
          If you only want to use a subset of all the available GPU devices, you can pass the list
          to the MirroredStrategy’s constructor:                      
                                                                      
            distribution = tf.distribute.MirroredStrategy(["/gpu:0", "/gpu:1"])
          By default, the MirroredStrategy class uses the NVIDIA Collective Communications
          Library (NCCL) for the AllReduce mean operation, but you can change it by setting
          the cross_device_ops argument to an instance of the tf.distribute.Hierarchical
          CopyAllReduce class, or an instance of the tf.distribute.ReductionToOneDevice
          class. The default NCCL option is based on the tf.distribute.NcclAllReduce class,
          which is usually faster, but this depends on the number and types of GPUs, so you
          may want to give the alternatives a try.21                  
                                                                      
                                                                      
          21 For more details on AllReduce algorithms, read this great post by Yuichiro Ueno, and this page on scaling
           with NCCL.                                                 