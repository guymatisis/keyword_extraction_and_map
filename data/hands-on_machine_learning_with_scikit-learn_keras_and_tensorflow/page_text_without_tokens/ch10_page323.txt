                                                                      
                                                                      
                                                                      
                                                                      
          Hyperband                                                   
            A fast hyperparameter tuning library based on the recent Hyperband paper22 by
            Lisha Li et al.                                           
                                                                      
          Sklearn-Deap                                                
            A hyperparameter optimization library based on evolutionary algorithms, with a
            GridSearchCV-like interface.                              
          Moreover, many companies offer services for hyperparameter optimization. We’ll dis‐
          cuss Google Cloud AI Platform’s hyperparameter tuning service in Chapter 19. Other
          options include services by Arimo and SigOpt, and CallDesk’s Oscar.
                                                                      
          Hyperparameter tuning is still an active area of research, and evolutionary algorithms
          are making a comeback. For example, check out DeepMind’s excellent 2017 paper,23
          where the authors jointly optimize a population of models and their hyperparame‐
          ters. Google has also used an evolutionary approach, not just to search for hyperpara‐
          meters but also to look for the best neural network architecture for the problem; their
          AutoML suite is already available as a cloud service. Perhaps the days of building neu‐
          ral networks manually will soon be over? Check out Google’s post on this topic. In
          fact, evolutionary algorithms have been used successfully to train individual neural
          networks, replacing the ubiquitous Gradient Descent! For an example, see the 2017
          post by Uber where the authors introduce their Deep Neuroevolution technique.
          But despite all this exciting progress and all these tools and services, it still helps to
          have an idea of what values are reasonable for each hyperparameter so that you can
          build a quick prototype and restrict the search space. The following sections provide
          guidelines for choosing the number of hidden layers and neurons in an MLP and for
          selecting good values for some of the main hyperparameters. 
          Number of Hidden Layers                                     
                                                                      
          For many problems, you can begin with a single hidden layer and get reasonable
          results. An MLP with just one hidden layer can theoretically model even the most
          complex functions, provided it has enough neurons. But for complex problems, deep
          networks have a much higher parameter efficiency than shallow ones: they can model
          complex functions using exponentially fewer neurons than shallow nets, allowing
          them to reach much better performance with the same amount of training data.
          To understand why, suppose you are asked to draw a forest using some drawing soft‐
          ware, but you are forbidden to copy and paste anything. It would take an enormous
                                                                      
                                                                      
          22 Lisha Li et al., “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,” Journal of
           Machine Learning Research 18 (April 2018): 1–52.           
          23 Max Jaderberg et al., “Population Based Training of Neural Networks,” arXiv preprint arXiv:1711.09846
           (2017).                                                    