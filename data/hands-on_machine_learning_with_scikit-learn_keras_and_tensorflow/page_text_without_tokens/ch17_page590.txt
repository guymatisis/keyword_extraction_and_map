                                                                      
                                                                      
                                                                      
                                                                      
          rather than the sum. So, the reconstruction loss is 784 times smaller than we need it
          to be. We could define a custom loss to compute the sum rather than the mean, but it
          is simpler to divide the latent loss by 784 (the final loss will be 784 times smaller than
          it should be, but this just means that we should use a larger learning rate).
                                                                      
          Note that we use the RMSprop optimizer, which works well in this case. And finally we
          can train the autoencoder!                                  
            history = variational_ae.fit(X_train, X_train, epochs=50, batch_size=128,
                             validation_data=[X_valid, X_valid])      
          Generating Fashion MNIST Images                             
                                                                      
          Now let’s use this variational autoencoder to generate images that look like fashion
          items. All we need to do is sample random codings from a Gaussian distribution and
          decode them:                                                
            codings = tf.random.normal(shape=[12, codings_size])      
            images = variational_decoder(codings).numpy()             
                                                                      
          Figure 17-13 shows the 12 generated images.                 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-13. Fashion MNIST images generated by the variational autoencoder
                                                                      
          The majority of these images look fairly convincing, if a bit too fuzzy. The rest are not
          great, but don’t be too harsh on the autoencoder—it only had a few minutes to learn!
          Give it a bit more fine-tuning and training time, and those images should look better.
                                                                      
          Variational autoencoders make it possible to perform semantic interpolation: instead
          of interpolating two images at the pixel level (which would look as if the two images
          were overlaid), we can interpolate at the codings level. We first run both images
          through the encoder, then we interpolate the two codings we get, and finally we
          decode the interpolated codings to get the final image. It will look like a regular Fash‐
          ion MNIST image, but it will be an intermediate between the original images. In the
          following code example, we take the 12 codings we just generated, we organize them