                                                                      
                                                                      
                                                                      
                                                                      
           • The driver then passes the action to the environment, which returns the next
            time step.                                                
                                                                      
           • Finally, the driver creates a trajectory object to represent this transition and
            broadcasts it to all the observers.                       
          Some policies, such as RNN policies, are stateful: they choose an action based on both
          the given time step and their own internal state. Stateful policies return their own
          state in the action step, along with the chosen action. The driver will then pass this
          state back to the policy at the next time step. Moreover, the driver saves the policy
          state to the trajectory (in the policy_info field), so it ends up in the replay buffer.
          This is essential when training a stateful policy: when the agent samples a trajectory, it
          must set the policy’s state to the state it was in at the time of the sampled time step.
                                                                      
          Also, as discussed earlier, the environment may be a batched environment, in which
          case the driver passes a batched time step to the policy (i.e., a time step object contain‐
          ing a batch of observations, a batch of step types, a batch of rewards, and a batch of
          discounts, all four batches of the same size). The driver also passes a batch of previous
          policy states. The policy then returns a batched action step containing a batch of
          actions and a batch of policy states. Finally, the driver creates a batched trajectory (i.e.,
          a trajectory containing a batch of step types, a batch of observations, a batch of
          actions, a batch of rewards, and more generally a batch for each trajectory attribute,
          with all batches of the same size).                         
          There are two main driver classes: DynamicStepDriver and DynamicEpisodeDriver.
          The first one collects experiences for a given number of steps, while the second col‐
          lects experiences for a given number of episodes. We want to collect experiences for
          four steps for each training iteration (as was done in the 2015 DQN paper), so let’s
          create a DynamicStepDriver:                                 
            from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver
                                                                      
            collect_driver = DynamicStepDriver(                       
               tf_env,                                                
               agent.collect_policy,                                  
               observers=[replay_buffer_observer] + training_metrics, 
               num_steps=update_period) # collect 4 steps for each training iteration
          We give it the environment to play with, the agent’s collect policy, a list of observers
          (including the replay buffer observer and the training metrics), and finally the num‐
          ber of steps to run (in this case, four). We could now run it by calling its run()
          method, but it’s best to warm up the replay buffer with experiences collected using a
          purely random policy. For this, we can use the RandomTFPolicy class and create a sec‐
          ond driver that will run this policy for 20,000 steps (which is equivalent to 80,000
          simulator frames, as was done in the 2015 DQN paper). We can use our ShowPro
          gress observer to display the progress:                     