                                                                      
                                                                      
                                                                      
                                                                      
          In this equation:                                           
                                                                      
           • Y is an m × n matrix containing the layer’s outputs at time step t for each
             (t)     neurons                                          
            instance in the mini-batch (m is the number of instances in the mini-batch and
            n   is the number of neurons).                            
             neurons                                                  
           • X is an m × n matrix containing the inputs for all instances (n is the
             (t)      inputs                          inputs          
            number of input features).                                
           • W is an n × n matrix containing the connection weights for the inputs
              x    inputs neurons                                     
            of the current time step.                                 
           • W is an n × n matrix containing the connection weights for the out‐
              y    neurons neurons                                    
            puts of the previous time step.                           
           • b is a vector of size n containing each neuron’s bias term.
                        neurons                                       
           • The weight matrices W and W are often concatenated vertically into a single
                          x    y                                      
            weight matrix W of shape (n + n ) × n (see the second line of Equa‐
                             inputs neurons neurons                   
            tion 15-2).                                               
           • The notation [X Y ] represents the horizontal concatenation of the matrices
                      (t) (t–1)                                       
            X  and Y .                                                
             (t)  (t–1)                                               
          Notice that Y is a function of X and Y , which is a function of X and Y ,
                  (t)         (t)  (t–1)            (t–1) (t–2)       
          which is a function of X and Y , and so on. This makes Y a function of all the
                        (t–2) (t–3)            (t)                    
          inputs since time t = 0 (that is, X , X , …, X ). At the first time step, t = 0, there are
                             (0) (1) (t)                              
          no previous outputs, so they are typically assumed to be all zeros.
          Memory Cells                                                
          Since the output of a recurrent neuron at time step t is a function of all the inputs
          from previous time steps, you could say it has a form of memory. A part of a neural
          network that preserves some state across time steps is called a memory cell (or simply
          a cell). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell,
          capable of learning only short patterns (typically about 10 steps long, but this varies
          depending on the task). Later in this chapter, we will look at some more complex and
          powerful types of cells capable of learning longer patterns (roughly 10 times longer,
          but again, this depends on the task).                       
          In general a cell’s state at time step t, denoted h (the “h” stands for “hidden”), is a
                                      (t)                             
          function of some inputs at that time step and its state at the previous time step: h =
                                                         (t)          
          f(h , x ). Its output at time step t, denoted y , is also a function of the previous
            (t–1) (t)                 (t)                             
          state and the current inputs. In the case of the basic cells we have discussed so far, the
          output is simply equal to the state, but in more complex cells this is not always the
          case, as shown in Figure 15-3.                              