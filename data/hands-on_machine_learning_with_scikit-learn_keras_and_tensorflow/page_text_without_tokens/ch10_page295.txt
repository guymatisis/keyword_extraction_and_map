                                                                      
                                                                      
                                                                      
                                                                      
          Regarding the loss function, since we are predicting probability distributions, the
          cross-entropy loss (also called the log loss, see Chapter 4) is generally a good choice.
                                                                      
          Table 10-2 summarizes the typical architecture of a classification MLP.
                                                                      
          Table 10-2. Typical classification MLP architecture         
          Hyperparameter Binary classification Multilabel binary classification Multiclass classification
          Input and hidden layers Same as regression Same as regression Same as regression
          # output neurons 1  1 per label  1 per class                
          Output layer activation Logistic Logistic Softmax           
          Loss function Cross entropy Cross entropy Cross entropy     
                                                                      
                   Before we go on, I recommend you go through exercise 1 at the
                   end of this chapter. You will play with various neural network
                   architectures and visualize their outputs using the TensorFlow Play‐
                   ground. This will be very useful to better understand MLPs, includ‐
                   ing the effects of all the hyperparameters (number of layers and
                   neurons, activation functions, and more).          
                                                                      
          Now you have all the concepts you need to start implementing MLPs with Keras!
                                                                      
          Implementing MLPs with Keras                                
                                                                      
          Keras is a high-level Deep Learning API that allows you to easily build, train, evalu‐
          ate, and execute all sorts of neural networks. Its documentation (or specification) is
          available at https://keras.io/. The reference implementation, also called Keras, was
          developed by François Chollet as part of a research project13 and was released as an
          open source project in March 2015. It quickly gained popularity, owing to its ease of
          use, flexibility, and beautiful design. To perform the heavy computations required by
          neural networks, this reference implementation relies on a computation backend. At
          present, you can choose from three popular open source Deep Learning libraries:
          TensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano. Therefore, to avoid
          any confusion, we will refer to this reference implementation as multibackend Keras.
          Since late 2016, other implementations have been released. You can now run Keras on
          Apache MXNet, Apple’s Core ML, JavaScript or TypeScript (to run Keras code in a
          web browser), and PlaidML (which can run on all sorts of GPU devices, not just Nvi‐
          dia). Moreover, TensorFlow itself now comes bundled with its own Keras implemen‐
          tation, tf.keras. It only supports TensorFlow as the backend, but it has the advantage
          of offering some very useful extra features (see Figure 10-10): for example, it supports
                                                                      
                                                                      
          13 Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).