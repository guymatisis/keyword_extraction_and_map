                                                                      
                                                                      
                                                                      
                                                                      
          For our main training loop, instead of calling the get_next() method, we will use a
          tf.data.Dataset. This way, we can benefit from the power of the Data API (e.g., par‐
          allelism and prefetching). For this, we call the replay buffer’s as_dataset() method:
                                                                      
            dataset = replay_buffer.as_dataset(                       
               sample_batch_size=64,                                  
               num_steps=2,                                           
               num_parallel_calls=3).prefetch(3)                      
          We will sample batches of 64 trajectories at each training step (as in the 2015 DQN
          paper), each with 2 steps (i.e., 2 steps = 1 full transition, including the next step’s
          observation). This dataset will process three elements in parallel, and prefetch three
          batches.                                                    
                   For on-policy algorithms such as Policy Gradients, each experience
                   should be sampled once, used from training, and then discarded. In
                   this case, you can still use a replay buffer, but instead of using a
                   Dataset, you would call the replay buffer’s gather_all() method
                   at each training iteration to get a tensor containing all the trajecto‐
                   ries recorded so far, then use them to perform a training step, and
                   finally clear the replay buffer by calling its clear() method.
                                                                      
          Now that we have all the components in place, we are ready to train the model!
                                                                      
          Creating the Training Loop                                  
          To speed up training, we will convert the main functions to TensorFlow Functions.
          For this we will use the tf_agents.utils.common.function() function, which wraps
          tf.function(), with some extra experimental options:        
                                                                      
            from tf_agents.utils.common import function               
            collect_driver.run = function(collect_driver.run)         
            agent.train = function(agent.train)                       
          Let’s create a small function that will run the main training loop for n_iterations:
                                                                      
            def train_agent(n_iterations):                            
               time_step = None                                       
               policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)
               iterator = iter(dataset)                               
               for iteration in range(n_iterations):                  
                 time_step, policy_state = collect_driver.run(time_step, policy_state)
                 trajectories, buffer_info = next(iterator)           
                 train_loss = agent.train(trajectories)               
                 print("\r{} loss:{:.5f}".format(                     
                   iteration, train_loss.loss.numpy()), end="")       
                 if iteration % 1000 == 0:                            
                   log_metrics(train_metrics)                         