                                                                      
                                                                      
                                                                      
                                                                      
          That’s not too hard! However, you may prefer to use a nice self-contained custom
          layer (much like Scikit-Learn’s StandardScaler), rather than having global variables
          like means and stds dangling around:                        
                                                                      
            class Standardization(keras.layers.Layer):                
               def adapt(self, data_sample):                          
                 self.means_ = np.mean(data_sample, axis=0, keepdims=True)
                 self.stds_ = np.std(data_sample, axis=0, keepdims=True)
               def call(self, inputs):                                
                 return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())
          Before you can use this standardization layer, you will need to adapt it to your dataset
          by calling the adapt() method and passing it a data sample. This will allow it to use
          the appropriate mean and standard deviation for each feature:
            std_layer = Standardization()                             
            std_layer.adapt(data_sample)                              
          This sample must be large enough to be representative of your dataset, but it does not
          have to be the full training set: in general, a few hundred randomly selected instances
          will suffice (however, this depends on your task). Next, you can use this preprocess‐
          ing layer like a normal layer:                              
            model = keras.Sequential()                                
            model.add(std_layer)                                      
            [...] # create the rest of the model                      
            model.compile([...])                                      
            model.fit([...])                                          
          If you are thinking that Keras should contain a standardization layer like this one,
          here’s some good news for you: by the time you read this, the keras.layers.Normal
          ization layer will probably be available. It will work very much like our custom
          Standardization layer: first, create the layer, then adapt it to your dataset by passing
          a data sample to the adapt() method, and finally use the layer normally.
          Now let’s look at categorical features. We will start by encoding them as one-hot
          vectors.                                                    
          Encoding Categorical Features Using One-Hot Vectors         
                                                                      
          Consider the ocean_proximity feature in the California housing dataset we explored
          in Chapter 2: it is a categorical feature with five possible values: "<1H OCEAN",
          "INLAND", "NEAR OCEAN", "NEAR BAY", and "ISLAND". We need to encode this feature
          before we feed it to a neural network. Since there are very few categories, we can use
          one-hot encoding. For this, we first need to map each category to its index (0 to 4),
          which can be done using a lookup table:                     
            vocab = ["<1H OCEAN", "INLAND", "NEAR OCEAN", "NEAR BAY", "ISLAND"]
            indices = tf.range(len(vocab), dtype=tf.int64)            
                                                                      