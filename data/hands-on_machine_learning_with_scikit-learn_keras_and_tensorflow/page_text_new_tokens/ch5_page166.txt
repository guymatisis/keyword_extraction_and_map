The dashed lines represent the points where the decision function is equal to 1 or –1:
they are parallel and at equal distance to the decision boundary, and they form a mar‐
gin around it. Training a linear SVM classifier means finding the values of <b>w</b> and <i>b</i>
that make this margin as wide as possible while avoiding margin violations (hard
margin) or limiting them (soft margin).
<header><largefont><b>Training</b></largefont> <largefont><b>Objective</b></largefont></header>
Consider the slope of the decision function: it is equal to the norm of the weight vec‐
∥ ∥
tor, <b>w</b> . If we divide this slope by 2, the points where the decision function is equal
to ±1 are going to be twice as far away from the decision boundary. In other words,
dividing the slope by 2 will multiply the margin by 2. This may be easier to visualize
in 2D, as shown in Figure 5-13. The smaller the weight vector <b>w,</b> the larger the
margin.
<i>Figure</i> <i>5-13.</i> <i>A</i> <i>smaller</i> <i>weight</i> <i>vector</i> <i>results</i> <i>in</i> <i>a</i> <i>larger</i> <i>margin</i>
∥ ∥
So we want to minimize <b>w</b> to get a large margin. If we also want to avoid any
margin violations (hard margin), then we need the decision function to be greater
than 1 for all positive training instances and lower than –1 for negative training
instances. If we define <i>t</i> (i) = –1 for negative instances (if <i>y</i> (i) = 0) and <i>t</i> (i) = 1 for positive
instances (if <i>y(i)</i> = 1), then we can express this constraint as <i>t(i)(w</i> ⊺ <b>x(i)</b> + <i>b)</i> ≥ 1 for all
instances.
We can therefore express the hard margin linear SVM classifier objective as the con‐
strained optimization problem in Equation 5-3.
<i>Equation</i> <i>5-3.</i> <i>Hard</i> <i>margin</i> <i>linear</i> <i>SVM</i> <i>classifier</i> <i>objective</i>
1 ⊺
minimize <b>w</b> <b>w</b>
<b>w,b</b> 2
<i>i</i> ⊺ <i>i</i>
⋯
subject to <i>t</i> <b>w</b> <b>x</b> + <i>b</i> ≥ 1 for <i>i</i> = 1,2, ,m