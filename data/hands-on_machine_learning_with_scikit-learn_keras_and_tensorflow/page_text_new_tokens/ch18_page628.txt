This algorithm is an example of <i>Dynamic</i> <i>Programming,</i> which
breaks down a complex problem into tractable subproblems that
can be tackled iteratively.
Knowing the optimal state values can be useful, in particular to evaluate a policy, but
it does not give us the optimal policy for the agent. Luckily, Bellman found a very
similar algorithm to estimate the optimal <i>state-action</i> <i>values,</i> generally called <i>Q-</i>
<i>Values</i> (Quality Values). The optimal Q-Value of the state-action pair (s, <i>a),</i> noted
<i>Q*(s,</i> <i>a),</i> is the sum of discounted future rewards the agent can expect on average
after it reaches the state <i>s</i> and chooses action <i>a,</i> but before it sees the outcome of this
action, assuming it acts optimally after that action.
Here is how it works: once again, you start by initializing all the Q-Value estimates to
zero, then you update them using the <i>Q-Value</i> <i>Iteration</i> algorithm (see Equation
18-3).
<i>Equation</i> <i>18-3.</i> <i>Q-Value</i> <i>Iteration</i> <i>algorithm</i>
<largefont>∑</largefont> ′ ′ ′ ′ ′
<i>Q</i> <i>s,a</i> <i>T</i> <i>s,a,s</i> <i>R</i> <i>s,a,s</i> + <i>γ</i> · max <i>Q</i> <i>s</i> ,a for all <i>s</i> <i>a</i>
<i>k+1</i> <i>k</i>
<i>a</i> ′
<i>s</i> ′
Once you have the optimal Q-Values, defining the optimal policy, noted <i>π*(s),</i> is triv‐
ial: when the agent is in state <i>s,</i> it should choose the action with the highest Q-Value
for that state: <i>π*</i> <i>s</i> = argmax <i>Q*</i> <i>s,a</i> .
<i>a</i>
Let’s apply this algorithm to the MDP represented in Figure 18-8. First, we need to
define the MDP:
transition_probabilities = [ <i>#</i> <i>shape=[s,</i> <i>a,</i> <i>s']</i>
[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],
[[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],
[None, [0.8, 0.1, 0.1], None]]
rewards = [ <i>#</i> <i>shape=[s,</i> <i>a,</i> <i>s']</i>
[[+10, 0, 0], [0, 0, 0], [0, 0, 0]],
[[0, 0, 0], [0, 0, 0], [0, 0, -50]],
[[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]
possible_actions = [[0, 1, 2], [0, 2], [1]]
For example, to know the transition probability from <i>s</i> to <i>s</i> after playing action <i>a</i> ,
2 0 1
we will look up transition_probabilities[2][1][0] (which is 0.8). Similarly, to
rewards[2][1][0]
get the corresponding reward, we will look up (which is +40).
And to get the list of possible actions in <i>s</i> , we will look up possible_actions[2] (in
2
this case, only action <i>a</i> is possible). Next, we must initialize all the Q-Values to 0
1
(except for the the impossible actions, for which we set the Q-Values to –∞):