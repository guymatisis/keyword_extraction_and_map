<i>Isomap</i>
Creates a graph by connecting each instance to its nearest neighbors, then
reduces dimensionality while trying to preserve the <i>geodesic</i> <i>distances</i> 9 between
the instances.
<i>t-Distributed</i> <i>Stochastic</i> <i>Neighbor</i> <i>Embedding</i> <i>(t-SNE)</i>
Reduces dimensionality while trying to keep similar instances close and dissimi‐
lar instances apart. It is mostly used for visualization, in particular to visualize
clusters of instances in high-dimensional space (e.g., to visualize the MNIST
images in 2D).
<i>Linear</i> <i>Discriminant</i> <i>Analysis</i> <i>(LDA)</i>
Is a classification algorithm, but during training it learns the most discriminative
axes between the classes, and these axes can then be used to define a hyperplane
onto which to project the data. The benefit of this approach is that the projection
will keep classes as far apart as possible, so LDA is a good technique to reduce
dimensionality before running another classification algorithm such as an SVM
classifier.
Figure 8-13 shows the results of a few of these techniques.
<i>Figure</i> <i>8-13.</i> <i>Using</i> <i>various</i> <i>techniques</i> <i>to</i> <i>reduce</i> <i>the</i> <i>Swill</i> <i>roll</i> <i>to</i> <i>2D</i>
<header><largefont><b>Exercises</b></largefont></header>
1. What are the main motivations for reducing a dataset’s dimensionality? What are
the main drawbacks?
2. What is the curse of dimensionality?
9 Thegeodesicdistancebetweentwonodesinagraphisthenumberofnodesontheshortestpathbetween
thesenodes.