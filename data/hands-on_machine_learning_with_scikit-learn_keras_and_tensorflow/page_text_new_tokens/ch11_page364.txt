As for the 1cycle approach, the implementation poses no particular difficulty: just
create a custom callback that modifies the learning rate at each iteration (you can
update the optimizer’s learning rate by changing self.model.optimizer.lr ). See the
“1Cycle scheduling” section of the notebook for an example.
To sum up, exponential decay, performance scheduling, and 1cycle can considerably
speed up convergence, so give them a try!
<header><largefont><b>Avoiding</b></largefont> <largefont><b>Overfitting</b></largefont> <largefont><b>Through</b></largefont> <largefont><b>Regularization</b></largefont></header>
With four parameters I can fit an elephant and with five I can make him wiggle his
trunk.
—John von Neumann, cited by Enrico Fermi in <i>Nature</i> 427
With thousands of parameters, you can fit the whole zoo. Deep neural networks typi‐
cally have tens of thousands of parameters, sometimes even millions. This gives them
an incredible amount of freedom and means they can fit a huge variety of complex
datasets. But this great flexibility also makes the network prone to overfitting the
training set. We need regularization.
We already implemented one of the best regularization techniques in Chapter 10:
early stopping. Moreover, even though Batch Normalization was designed to solve
the unstable gradients problems, it also acts like a pretty good regularizer. In this sec‐
tion we will examine other popular regularization techniques for neural networks: ℓ
1
and ℓ regularization, dropout, and max-norm regularization.
2
<header><largefont><b>ℓ</b></largefont> <largefont><b>and</b></largefont> <largefont><b>ℓ</b></largefont> <largefont><b>Regularization</b></largefont></header>
<b>1</b> <b>2</b>
Just like you did in Chapter 4 for simple linear models, you can use ℓ regularization
2
to constrain a neural network’s connection weights, and/or ℓ regularization if you
1
want a sparse model (with many weights equal to 0). Here is how to apply ℓ regulari‐
2
zation to a Keras layer’s connection weights, using a regularization factor of 0.01:
layer = keras.layers.Dense(100, activation="elu",
kernel_initializer="he_normal",
kernel_regularizer=keras.regularizers.l2(0.01))
l2()
The function returns a regularizer that will be called at each step during training
to compute the regularization loss. This is then added to the final loss. As you might
expect, you can just use keras.regularizers.l1() if you want ℓ regularization; if
1
keras.regularizers.l1_l2()
you want both ℓ and ℓ regularization, use (specifying
1 2
both regularization factors).
Since you will typically want to apply the same regularizer to all layers in your net‐
work, as well as using the same activation function and the same initialization strat‐
egy in all hidden layers, you may find yourself repeating the same arguments. This