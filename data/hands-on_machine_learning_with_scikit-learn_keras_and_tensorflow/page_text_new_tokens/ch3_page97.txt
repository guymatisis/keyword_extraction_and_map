If someone says, “Let’s reach 99% precision,” you should ask, “At
what recall?”
<header><largefont><b>The</b></largefont> <largefont><b>ROC</b></largefont> <largefont><b>Curve</b></largefont></header>
The <i>receiver</i> <i>operating</i> <i>characteristic</i> (ROC) curve is another common tool used with
binary classifiers. It is very similar to the precision/recall curve, but instead of plot‐
ting precision versus recall, the ROC curve plots the <i>true</i> <i>positive</i> <i>rate</i> (another name
for recall) against the <i>false</i> <i>positive</i> <i>rate</i> (FPR). The FPR is the ratio of negative instan‐
ces that are incorrectly classified as positive. It is equal to 1 – the <i>true</i> <i>negative</i> <i>rate</i>
(TNR), which is the ratio of negative instances that are correctly classified as negative.
The TNR is also called <i>specificity.</i> Hence, the ROC curve plots <i>sensitivity</i> (recall) ver‐
sus 1 – <i>specificity.</i>
To plot the ROC curve, you first use the roc_curve() function to compute the TPR
and FPR for various threshold values:
<b>from</b> <b>sklearn.metrics</b> <b>import</b> roc_curve
fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)
Then you can plot the FPR against the TPR using Matplotlib. This code produces the
plot in Figure 3-6:
<b>def</b> plot_roc_curve(fpr, tpr, label=None):
plt.plot(fpr, tpr, linewidth=2, label=label)
plt.plot([0, 1], [0, 1], 'k--') <i>#</i> <i>Dashed</i> <i>diagonal</i>
[...] <i>#</i> <i>Add</i> <i>axis</i> <i>labels</i> <i>and</i> <i>grid</i>
plot_roc_curve(fpr, tpr)
plt.show()
Once again there is a trade-off: the higher the recall (TPR), the more false positives
(FPR) the classifier produces. The dotted line represents the ROC curve of a purely
random classifier; a good classifier stays as far away from that line as possible (toward
the top-left corner).