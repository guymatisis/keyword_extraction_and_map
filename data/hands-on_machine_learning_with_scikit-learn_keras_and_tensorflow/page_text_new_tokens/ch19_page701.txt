With that, you have all you need to run any operation on any device, and exploit the
power of your GPUs! Here are some of the things you could do:
• You could train several models in parallel, each on its own GPU: just write a
training script for each model and run them in parallel, setting
CUDA_DEVICE_ORDER CUDA_VISIBLE_DEVICES
and so that each script only sees a
single GPU device. This is great for hyperparameter tuning, as you can train in
parallel multiple models with different hyperparameters. If you have a single
machine with two GPUs, and it takes one hour to train one model on one GPU,
then training two models in parallel, each on its own dedicated GPU, will take
just one hour. Simple!
• You could train a model on a single GPU and perform all the preprocessing in
prefetch()
parallel on the CPU, using the dataset’s method17 to prepare the next
few batches in advance so that they are ready when the GPU needs them (see
Chapter 13).
• If your model takes two images as input and processes them using two CNNs
before joining their outputs, then it will probably run much faster if you place
each CNN on a different GPU.
• You can create an efficient ensemble: just place a different trained model on each
GPU so that you can get all the predictions much faster to produce the ensem‐
ble’s final prediction.
But what if you want to <i>train</i> a single model across multiple GPUs?
<header><largefont><b>Training</b></largefont> <largefont><b>Models</b></largefont> <largefont><b>Across</b></largefont> <largefont><b>Multiple</b></largefont> <largefont><b>Devices</b></largefont></header>
There are two main approaches to training a single model across multiple devices:
<i>model</i> <i>parallelism,</i> where the model is split across the devices, and <i>data</i> <i>parallelism,</i>
where the model is replicated across every device, and each replica is trained on a
subset of the data. Let’s look at these two options closely before we train a model on
multiple GPUs.
<header><largefont><b>Model</b></largefont> <largefont><b>Parallelism</b></largefont></header>
So far we have trained each neural network on a single device. What if we want to
train a single neural network across multiple devices? This requires chopping the
model into separate chunks and running each chunk on a different device.
17 AtthetimeofthiswritingitonlyprefetchesthedatatotheCPURAM,butyoucanusetf.data.experimen
tal.prefetch_to_device()
tomakeitprefetchthedataandpushittothedeviceofyourchoicesothatthe
GPUdoesnotwastetimewaitingforthedatatobetransferred.