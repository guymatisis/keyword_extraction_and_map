will learn to play an Atari game, you will need an Atari game simulator. If you want to
program a walking robot, then the environment is the real world, and you can
directly train your robot in that environment, but this has its limits: if the robot falls
off a cliff, you can’t just click Undo. You can’t speed up time either; adding more com‐
puting power won’t make the robot move any faster. And it’s generally too expensive
to train 1,000 robots in parallel. In short, training is hard and slow in the real world,
so you generally need a <i>simulated</i> <i>environment</i> at least for bootstrap training. For
example, you may use a library like PyBullet or MuJoCo for 3D physics simulation.
<i>OpenAI</i> <i>Gym10</i> is a toolkit that provides a wide variety of simulated environments
(Atari games, board games, 2D and 3D physical simulations, and so on), so you can
train agents, compare them, or develop new RL algorithms.
Before installing the toolkit, if you created an isolated environment using virtualenv,
you first need to activate it:
$ <b>cd</b> <b>$ML_PATH</b> # Your ML working directory (e.g., $HOME/ml)
$ <b>source</b> <b>my_env/bin/activate</b> # on Linux or MacOS
$ <b>.\my_env\Scripts\activate</b> # on Windows
Next, install OpenAI Gym (if you are not using a virtual environment, you will need
--user
to add the option, or have administrator rights):
$ <b>python3</b> <b>-m</b> <b>pip</b> <b>install</b> <b>-U</b> <b>gym</b>
Depending on your system, you may also need to install the Mesa OpenGL Utility
apt install libglu1-mesa
(GLU) library (e.g., on Ubuntu 18.04 you need to run ).
This library will be needed to render the first environment. Next, open up a Python
shell or a Jupyter notebook and create an environment with make() :
<b>>>></b> <b>import</b> <b>gym</b>
<b>>>></b> env = gym.make("CartPole-v1")
<b>>>></b> obs = env.reset()
<b>>>></b> obs
array([-0.01258566, -0.00156614, 0.04207708, -0.00180545])
Here, we’ve created a CartPole environment. This is a 2D simulation in which a cart
can be accelerated left or right in order to balance a pole placed on top of it (see
Figure 18-4). You can get the list of all available environments by running
gym.envs.registry.all() . After the environment is created, you must initialize it
reset()
using the method. This returns the first observation. Observations depend
on the type of environment. For the CartPole environment, each observation is a 1D
NumPy array containing four floats: these floats represent the cart’s horizontal
10 OpenAIisanartificialintelligenceresearchcompany,fundedinpartbyElonMusk.Itsstatedgoalistopro‐
moteanddevelopfriendlyAIsthatwillbenefithumanity(ratherthanexterminateit).