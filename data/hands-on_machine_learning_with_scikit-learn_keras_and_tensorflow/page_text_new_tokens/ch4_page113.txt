This can be written much more concisely using a vectorized form, as shown in Equa‐
tion 4-2.
<i>Equation</i> <i>4-2.</i> <i>Linear</i> <i>Regression</i> <i>model</i> <i>prediction</i> <i>(vectorized</i> <i>form)</i>
<i>y</i> = <i>h</i> <b>x</b> = <b>θ</b> · <b>x</b>
<b>θ</b>
In this equation:
• <b>θ</b> is the model’s <i>parameter</i> <i>vector,</i> containing the bias term <i>θ</i> and the feature
0
weights <i>θ</i> to <i>θ</i> .
1 <i>n</i>
• <b>x</b> is the instance’s <i>feature</i> <i>vector,</i> containing <i>x</i> to <i>x</i> , with <i>x</i> always equal to 1.
0 <i>n</i> 0
• <b>θ</b> · <b>x</b> is the dot product of the vectors <b>θ</b> and <b>x,</b> which is of course equal to <i>θ</i> <i>x</i> +
0 0
<i>θ</i> <i>x</i> + <i>θ</i> <i>x</i> + ... + <i>θ</i> <i>x</i> .
1 1 2 2 <i>n</i> <i>n</i>
• <i>h</i> is the hypothesis function, using the model parameters <b>θ.</b>
<b>θ</b>
In Machine Learning, vectors are often represented as <i>column</i> <i>vec‐</i>
<i>tors,</i> which are 2D arrays with a single column. If <b>θ</b> and <b>x</b> are col‐
⊺ ⊺
umn vectors, then the prediction is <i>y</i> = <b>θ</b> <b>x,</b> where <b>θ</b> is the
⊺
<i>transpose</i> of <b>θ</b> (a row vector instead of a column vector) and <b>θ</b> <b>x</b> is
⊺
the matrix multiplication of <b>θ</b> and <b>x.</b> It is of course the same pre‐
diction, except that it is now represented as a single-cell matrix
rather than a scalar value. In this book I will use this notation to
avoid switching between dot products and matrix multiplications.
OK, that’s the Linear Regression model—but how do we train it? Well, recall that
training a model means setting its parameters so that the model best fits the training
set. For this purpose, we first need a measure of how well (or poorly) the model fits
the training data. In Chapter 2 we saw that the most common performance measure
of a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There‐
fore, to train a Linear Regression model, we need to find the value of <b>θ</b> that minimi‐
zes the RMSE. In practice, it is simpler to minimize the mean squared error (MSE)
than the RMSE, and it leads to the same result (because the value that minimizes a
function also minimizes its square root). 1
1 Itisoftenthecasethatalearningalgorithmwilltrytooptimizeadifferentfunctionthantheperformance
measureusedtoevaluatethefinalmodel.Thisisgenerallybecausethatfunctioniseasiertocompute,because
ithasusefuldifferentiationpropertiesthattheperformancemeasurelacks,orbecausewewanttoconstrain
themodelduringtraining,asyouwillseewhenwediscussregularization.