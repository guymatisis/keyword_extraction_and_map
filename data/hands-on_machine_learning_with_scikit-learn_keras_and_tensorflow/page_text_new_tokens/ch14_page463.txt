<header><largefont><b>LeNet-5</b></largefont></header>
architecture10
The LeNet-5 is perhaps the most widely known CNN architecture. As
mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used
for handwritten digit recognition (MNIST). It is composed of the layers shown in
Table 14-1.
<i>Table</i> <i>14-1.</i> <i>LeNet-5</i> <i>architecture</i>
<b>Layer</b> <b>Type</b> <b>Maps</b> <b>Size</b> <b>Kernelsize</b> <b>Stride</b> <b>Activation</b>
Out Fullyconnected – 10 – – RBF
F6 Fullyconnected – 84 – – tanh
C5 Convolution 120 1×1 5×5 1 tanh
S4 Avgpooling 16 5×5 2×2 2 tanh
C3 Convolution 16 10×10 5×5 1 tanh
S2 Avgpooling 6 14×14 2×2 2 tanh
C1 Convolution 6 28×28 5×5 1 tanh
In Input 1 32×32 – – –
There are a few extra details to be noted:
• MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and
normalized before being fed to the network. The rest of the network does not use
any padding, which is why the size keeps shrinking as the image progresses
through the network.
• The average pooling layers are slightly more complex than usual: each neuron
computes the mean of its inputs, then multiplies the result by a learnable coeffi‐
cient (one per map) and adds a learnable bias term (again, one per map), then
finally applies the activation function.
• Most neurons in C3 maps are connected to neurons in only three or four S2
maps (instead of all six S2 maps). See table 1 (page 8) in the original paper 10 for
details.
• The output layer is a bit special: instead of computing the matrix multiplication
of the inputs and the weight vector, each neuron outputs the square of the Eucli‐
dian distance between its input vector and its weight vector. Each output meas‐
ures how much the image belongs to a particular digit class. The cross-entropy
10 YannLeCunetal.,“Gradient-BasedLearningAppliedtoDocumentRecognition,”ProceedingsoftheIEEE86,
no.11(1998):2278–2324.