This output tells you that 84.2% of the dataset’s variance lies along the first PC, and
14.6% lies along the second PC. This leaves less than 1.2% for the third PC, so it is
reasonable to assume that the third PC probably carries little information.
<header><largefont><b>Choosing</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Right</b></largefont> <largefont><b>Number</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Dimensions</b></largefont></header>
Instead of arbitrarily choosing the number of dimensions to reduce down to, it is
simpler to choose the number of dimensions that add up to a sufficiently large por‐
tion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for
data visualization—in that case you will want to reduce the dimensionality down to 2
or 3.
The following code performs PCA without reducing dimensionality, then computes
the minimum number of dimensions required to preserve 95% of the training set’s
variance:
pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
n_components=d
You could then set and run PCA again. But there is a much better
option: instead of specifying the number of principal components you want to pre‐
serve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio
of variance you wish to preserve:
pca = PCA(n_components=0.95)
X_reduced = pca.fit_transform(X_train)
Yet another option is to plot the explained variance as a function of the number of
cumsum
dimensions (simply plot ; see Figure 8-8). There will usually be an elbow in the
curve, where the explained variance stops growing fast. In this case, you can see that
reducing the dimensionality down to about 100 dimensions wouldn’t lose too much
explained variance.