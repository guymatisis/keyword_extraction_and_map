embedding of the token that was actually output. During training, it should be the
embedding of the previous target token: this is why we used the TrainingSampler . In
practice, it is often a good idea to start training with the embedding of the target of
the previous time step and gradually transition to using the embedding of the actual
paper12
token that was output at the previous step. This idea was introduced in a 2015
by Samy Bengio et al. The ScheduledEmbeddingTrainingSampler will randomly
choose between the target or the actual output, with a probability that you can gradu‐
ally change during training.
<header><largefont><b>Bidirectional</b></largefont> <largefont><b>RNNs</b></largefont></header>
A each time step, a regular recurrent layer only looks at past and present inputs
before generating its output. In other words, it is “causal,” meaning it cannot look into
the future. This type of RNN makes sense when forecasting time series, but for many
NLP tasks, such as Neural Machine Translation, it is often preferable to look ahead at
the next words before encoding a given word. For example, consider the phrases “the
Queen of the United Kingdom,” “the queen of hearts,” and “the queen bee”: to prop‐
erly encode the word “queen,” you need to look ahead. To implement this, run two
recurrent layers on the same inputs, one reading the words from left to right and the
other reading them from right to left. Then simply combine their outputs at each
time step, typically by concatenating them. This is called a <i>bidirectional</i> <i>recurrent</i> <i>layer</i>
(see Figure 16-5).
To implement a bidirectional recurrent layer in Keras, wrap a recurrent layer in a
keras.layers.Bidirectional layer. For example, the following code creates a bidir‐
GRU
ectional layer:
keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))
The Bidirectional layer will create a clone of the GRU layer (but in
the reverse direction), and it will run both and concatenate their
outputs. So although the GRU layer has 10 units, the Bidirectional
layer will output 20 values per time step.
12 SamyBengioetal.,“ScheduledSamplingforSequencePredictionwithRecurrentNeuralNetworks,”arXiv
preprintarXiv:1506.03099(2015).