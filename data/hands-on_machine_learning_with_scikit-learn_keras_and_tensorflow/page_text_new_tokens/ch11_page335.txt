he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',
distribution='uniform')
keras.layers.Dense(10, activation="sigmoid", kernel_initializer=he_avg_init)
<header><largefont><b>Nonsaturating</b></largefont> <largefont><b>Activation</b></largefont> <largefont><b>Functions</b></largefont></header>
One of the insights in the 2010 paper by Glorot and Bengio was that the problems
with unstable gradients were in part due to a poor choice of activation function. Until
then most people had assumed that if Mother Nature had chosen to use roughly sig‐
moid activation functions in biological neurons, they must be an excellent choice. But
it turns out that other activation functions behave much better in deep neural net‐
works—in particular, the ReLU activation function, mostly because it does not satu‐
rate for positive values (and because it is fast to compute).
Unfortunately, the ReLU activation function is not perfect. It suffers from a problem
known as the <i>dying</i> <i>ReLUs:</i> during training, some neurons effectively “die,” meaning
they stop outputting anything other than 0. In some cases, you may find that half of
your network’s neurons are dead, especially if you used a large learning rate. A neu‐
ron dies when its weights get tweaked in such a way that the weighted sum of its
inputs are negative for all instances in the training set. When this happens, it just
keeps outputting zeros, and Gradient Descent does not affect it anymore because the
negative.4
gradient of the ReLU function is zero when its input is
To solve this problem, you may want to use a variant of the ReLU function, such as
the <i>leaky</i> <i>ReLU.</i> This function is defined as LeakyReLU (z) = max(αz, <i>z)</i> (see
<i>α</i>
Figure 11-2). The hyperparameter <i>α</i> defines how much the function “leaks”: it is the
slope of the function for <i>z</i> < 0 and is typically set to 0.01. This small slope ensures that
leaky ReLUs never die; they can go into a long coma, but they have a chance to even‐
tually wake up. A 2015 paper 5 compared several variants of the ReLU activation func‐
tion, and one of its conclusions was that the leaky variants always outperformed the
strict ReLU activation function. In fact, setting <i>α</i> = 0.2 (a huge leak) seemed to result
in better performance than <i>α</i> = 0.01 (a small leak). The paper also evaluated the
<i>randomized</i> <i>leaky</i> <i>ReLU</i> (RReLU), where <i>α</i> is picked randomly in a given range during
training and is fixed to an average value during testing. RReLU also performed fairly
well and seemed to act as a regularizer (reducing the risk of overfitting the training
set). Finally, the paper evaluated the <i>parametric</i> <i>leaky</i> <i>ReLU</i> (PReLU), where <i>α</i> is
authorized to be learned during training (instead of being a hyperparameter, it
becomes a parameter that can be modified by backpropagation like any other param‐
4 Unlessitispartofthefirsthiddenlayer,adeadneuronmaysometimescomebacktolife:GradientDescent
mayindeedtweakneuronsinthelayersbelowinsuchawaythattheweightedsumofthedeadneuron’s
inputsispositiveagain.
5 BingXuetal.,“EmpiricalEvaluationofRectifiedActivationsinConvolutionalNetwork,”arXivpreprint
arXiv:1505.00853(2015).