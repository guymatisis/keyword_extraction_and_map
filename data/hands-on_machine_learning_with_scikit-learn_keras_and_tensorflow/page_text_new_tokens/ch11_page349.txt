paper just looks too positive, you should be suspicious: perhaps the flashy new tech‐
nique does not actually help much (in fact, it may even degrade performance), but the
authors tried many variants and reported only the best results (which may be due to
sheer luck), without mentioning how many failures they encountered on the way.
Most of the time, this is not malicious at all, but it is part of the reason so many
results in science can never be reproduced.
Why did I cheat? It turns out that transfer learning does not work very well with
small dense networks, presumably because small networks learn few patterns, and
dense networks learn very specific patterns, which are unlikely to be useful in other
tasks. Transfer learning works best with deep convolutional neural networks, which
tend to learn feature detectors that are much more general (especially in the lower
layers). We will revisit transfer learning in Chapter 14, using the techniques we just
discussed (and this time there will be no cheating, I promise!).
<header><largefont><b>Unsupervised</b></largefont> <largefont><b>Pretraining</b></largefont></header>
Suppose you want to tackle a complex task for which you don’t have much labeled
training data, but unfortunately you cannot find a model trained on a similar task.
Don’t lose hope! First, you should try to gather more labeled training data, but if you
can’t, you may still be able to perform <i>unsupervised</i> <i>pretraining</i> (see Figure 11-5).
Indeed, it is often cheap to gather unlabeled training examples, but expensive to label
them. If you can gather plenty of unlabeled training data, you can try to use it to train
an unsupervised model, such as an autoencoder or a generative adversarial network
(see Chapter 17). Then you can reuse the lower layers of the autoencoder or the lower
layers of the GAN’s discriminator, add the output layer for your task on top, and fine-
tune the final network using supervised learning (i.e., with the labeled training
examples).
It is this technique that Geoffrey Hinton and his team used in 2006 and which led to
the revival of neural networks and the success of Deep Learning. Until 2010, unsuper‐
vised pretraining—typically with restricted Boltzmann machines (RBMs; see Appen‐
dix E)—was the norm for deep nets, and only after the vanishing gradients problem
was alleviated did it become much more common to train DNNs purely using super‐
vised learning. Unsupervised pretraining (today typically using autoencoders or
GANs rather than RBMs) is still a good option when you have a complex task to
solve, no similar model you can reuse, and little labeled training data but plenty of
unlabeled training data.
Note that in the early days of Deep Learning it was difficult to train deep models, so
people would use a technique called <i>greedy</i> <i>layer-wise</i> <i>pretraining</i> (depicted in
Figure 11-5). They would first train an unsupervised model with a single layer, typi‐
cally an RBM, then they would freeze that layer and add another one on top of it,
then train the model again (effectively just training the new layer), then freeze the