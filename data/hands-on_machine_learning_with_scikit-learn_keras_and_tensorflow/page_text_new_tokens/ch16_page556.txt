• The righthand part is the decoder. During training, it takes the target sentence as
input (also represented as a sequence of word IDs), shifted one time step to the
right (i.e., a start-of-sequence token is inserted at the beginning). It also receives
the outputs of the encoder (i.e., the arrows coming from the left side). Note that
the top part of the decoder is also stacked <i>N</i> times, and the encoder stack’s final
outputs are fed to the decoder at each of these <i>N</i> levels. Just like earlier, the
decoder outputs a probability for each possible next word, at each time step (its
output shape is [batch <i>size,</i> <i>max</i> <i>output</i> <i>sentence</i> <i>length,</i> <i>vocabulary</i> <i>length]).</i>
• During inference, the decoder cannot be fed targets, so we feed it the previously
output words (starting with a start-of-sequence token). So the model needs to be
called repeatedly, predicting one more word at every round (which is fed to the
decoder at the next round, until the end-of-sequence token is output).
• Looking more closely, you can see that you are already familiar with most com‐
ponents: there are two embedding layers, 5 × <i>N</i> skip connections, each of them
followed by a layer normalization layer, 2 × <i>N</i> “Feed Forward” modules that are
composed of two dense layers each (the first one using the ReLU activation func‐
tion, the second with no activation function), and finally the output layer is a
dense layer using the softmax activation function. All of these layers are time-
distributed, so each word is treated independently of all the others. But how can
we translate a sentence by only looking at one word at a time? Well, that’s where
the new components come in:
— The encoder’s <i>Multi-Head</i> <i>Attention</i> layer encodes each word’s relationship
with every other word in the same sentence, paying more attention to the
most relevant ones. For example, the output of this layer for the word “Queen”
in the sentence “They welcomed the Queen of the United Kingdom” will
depend on all the words in the sentence, but it will probably pay more atten‐
tion to the words “United” and “Kingdom” than to the words “They” or “wel‐
comed.” This attention mechanism is called <i>self-attention</i> (the sentence is
paying attention to itself). We will discuss exactly how it works shortly. The
decoder’s <i>Masked</i> <i>Multi-Head</i> <i>Attention</i> layer does the same thing, but each
word is only allowed to attend to words located before it. Finally, the decoder’s
upper Multi-Head Attention layer is where the decoder pays attention to the
words in the input sentence. For example, the decoder will probably pay close
attention to the word “Queen” in the input sentence when it is about to output
this word’s translation.
— The <i>positional</i> <i>embeddings</i> are simply dense vectors (much like word embed‐
dings) that represent the position of a word in the sentence. The <i>n</i> th positional
embedding is added to the word embedding of the <i>nth</i> word in each sentence.
This gives the model access to each word’s position, which is needed because
the Multi-Head Attention layers do not consider the order or the position of
the words; they only look at their relationships. Since all the other layers are