<i>Figure</i> <i>8-12.</i> <i>Unrolled</i> <i>Swiss</i> <i>roll</i> <i>using</i> <i>LLE</i>
Here’s how LLE works: for each training instance <b>x(i),</b> the algorithm identifies its <i>k</i>
<b>x(i)</b>
closest neighbors (in the preceding code <i>k</i> = 10), then tries to reconstruct as a lin‐
ear function of these neighbors. More specifically, it finds the weights <i>w</i> such that
<i>i,j</i>
(i) <i>m</i> <i>j</i>
the squared distance between <b>x</b> and ∑ <i>w</i> <b>x</b> is as small as possible, assuming <i>w</i>
<i>j</i> = 1 <i>i,</i> <i>j</i> <i>i,j</i>
= 0 if <b>x(j)</b> is not one of the <i>k</i> closest neighbors of <b>x(i).</b> Thus the first step of LLE is the
constrained optimization problem described in Equation 8-4, where <b>W</b> is the weight
matrix containing all the weights <i>w</i> . The second constraint simply normalizes the
<i>i,j</i>
weights for each training instance <b>x</b> (i) .
<i>Equation</i> <i>8-4.</i> <i>LLE</i> <i>step</i> <i>one:</i> <i>linearly</i> <i>modeling</i> <i>local</i> <i>relationships</i>
<i>m</i> <i>m</i> 2
<i>i</i> <i>j</i>
<header><b>W</b> = argmin <largefont>∑</largefont> <b>x</b> − <largefont>∑</largefont> <i>w</i> <b>x</b></header>
<i>i,</i> <i>j</i>
<b>W</b> <i>i</i> = 1 <i>j</i> = 1
<i>j</i> <i>i</i>
<i>w</i> = 0 if <b>x</b> is not one of the <i>k</i> c.n. of <b>x</b>
<i>i,</i> <i>j</i>
subject to
<i>m</i>
<largefont>∑</largefont> ⋯
<i>w</i> = 1 for <i>i</i> = 1,2, ,m
<i>i,</i> <i>j</i>
<i>j</i> = 1
After this step, the weight matrix <b>W</b> (containing the weights <i>w</i> ) encodes the local
<i>i,</i> <i>j</i>
linear relationships between the training instances. The second step is to map the
training instances into a <i>d-dimensional</i> space (where <i>d</i> < <i>n)</i> while preserving these
local relationships as much as possible. If <b>z</b> (i) is the image of <b>x</b> (i) in this <i>d-dimensional</i>