dicted advantages. Here is a simple Dueling DQN model, implemented using the
Functional API:
K = keras.backend
input_states = keras.layers.Input(shape=[4])
hidden1 = keras.layers.Dense(32, activation="elu")(input_states)
hidden2 = keras.layers.Dense(32, activation="elu")(hidden1)
state_values = keras.layers.Dense(1)(hidden2)
raw_advantages = keras.layers.Dense(n_outputs)(hidden2)
advantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)
Q_values = state_values + advantages
model = keras.Model(inputs=[input_states], outputs=[Q_values])
The rest of the algorithm is just the same as earlier. In fact, you can build a Double
Dueling DQN and combine it with prioritized experience replay! More generally,
many RL techniques can be combined, as DeepMind demonstrated in a 2017 paper.18
The paper’s authors combined six different techniques into an agent called <i>Rainbow,</i>
which largely outperformed the state of the art.
Unfortunately, implementing all of these techniques, debugging them, fine-tuning
them, and of course training the models can require a huge amount of work. So
instead of reinventing the wheel, it is often best to reuse scalable and well-tested libra‐
ries, such as TF-Agents.
<header><largefont><b>The</b></largefont> <largefont><b>TF-Agents</b></largefont> <largefont><b>Library</b></largefont></header>
The TF-Agents library is a Reinforcement Learning library based on TensorFlow,
developed at Google and open sourced in 2018. Just like OpenAI Gym, it provides
many off-the-shelf environments (including wrappers for all OpenAI Gym environ‐
ments), plus it supports the PyBullet library (for 3D physics simulation), DeepMind’s
DM Control library (based on MuJoCo’s physics engine), and Unity’s ML-Agents
library (simulating many 3D environments). It also implements many RL algorithms,
including REINFORCE, DQN, and DDQN, as well as various RL components such
as efficient replay buffers and metrics. It is fast, scalable, easy to use, and customiza‐
ble: you can create your own environments and neural nets, and you can customize
pretty much any component. In this section we will use TF-Agents to train an agent
to play <i>Breakout,</i> the famous Atari game (see Figure 18-1119), using the DQN algo‐
rithm (you can easily switch to another algorithm if you prefer).
18 MatteoHesseletal.,“Rainbow:CombiningImprovementsinDeepReinforcementLearning,”arXivpreprint
arXiv:1710.02298(2017):3215–3222.
19 Ifyoudon’tknowthisgame,it’ssimple:aballbouncesaroundandbreaksbrickswhenittouchesthem.You
controlapaddlenearthebottomofthescreen.Thepaddlecangoleftorright,andyoumustgettheballto
breakeverybrick,whilepreventingitfromtouchingthebottomofthescreen.