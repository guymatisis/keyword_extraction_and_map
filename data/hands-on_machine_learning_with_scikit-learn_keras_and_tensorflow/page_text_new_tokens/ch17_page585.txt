We now have all we need to implement a sparse autoencoder based on the KL diver‐
gence. First, let’s create a custom regularizer to apply KL divergence regularization:
K = keras.backend
kl_divergence = keras.losses.kullback_leibler_divergence
<b>class</b> <b>KLDivergenceRegularizer(keras.regularizers.Regularizer):</b>
<b>def</b> <b>__init__(self,</b> weight, target=0.1):
self.weight = weight
self.target = target
<b>def</b> <b>__call__(self,</b> inputs):
mean_activities = K.mean(inputs, axis=0)
<b>return</b> self.weight * (
kl_divergence(self.target, mean_activities) +
kl_divergence(1. - self.target, 1. - mean_activities))
Now we can build the sparse autoencoder, using the KLDivergenceRegularizer for
the coding layer’s activations:
kld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)
sparse_kl_encoder = keras.models.Sequential([
keras.layers.Flatten(input_shape=[28, 28]),
keras.layers.Dense(100, activation="selu"),
keras.layers.Dense(300, activation="sigmoid", activity_regularizer=kld_reg)
])
sparse_kl_decoder = keras.models.Sequential([
keras.layers.Dense(100, activation="selu", input_shape=[300]),
keras.layers.Dense(28 * 28, activation="sigmoid"),
keras.layers.Reshape([28, 28])
])
sparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])
After training this sparse autoencoder on Fashion MNIST, the activations of the neu‐
rons in the coding layer are mostly close to 0 (about 70% of all activations are lower
than 0.1), and all neurons have a mean activation around 0.1 (about 90% of all neu‐
rons have a mean activation between 0.1 and 0.2), as shown in Figure 17-11.
<i>Figure</i> <i>17-11.</i> <i>Distribution</i> <i>of</i> <i>all</i> <i>the</i> <i>activations</i> <i>in</i> <i>the</i> <i>coding</i> <i>layer</i> <i>(left)</i> <i>and</i> <i>distribution</i>
<i>of</i> <i>the</i> <i>mean</i> <i>activation</i> <i>per</i> <i>neuron</i> <i>(right)</i>