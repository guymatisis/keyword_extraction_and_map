impurity measure: a set’s entropy is zero when it contains instances of only one class.
Equation 6-3 shows the definition of the entropy of the <i>ith</i> node. For example, the
depth-2 left node in Figure 6-1 has an entropy equal to –(49/54) log (49/54) – (5/54)
2
log (5/54) ≈ 0.445.
2
<i>Equation</i> <i>6-3.</i> <i>Entropy</i>
<i>n</i>
<i>H</i> = − <largefont>∑</largefont> <i>p</i> log <i>p</i>
<i>i</i> <i>i,k</i> 2 <i>i,k</i>
<i>k</i> = 1
<i>p</i> ≠ 0
<i>i,k</i>
So, should you use Gini impurity or entropy? The truth is, most of the time it does
not make a big difference: they lead to similar trees. Gini impurity is slightly faster to
compute, so it is a good default. However, when they differ, Gini impurity tends to
isolate the most frequent class in its own branch of the tree, while entropy tends to
produce slightly more balanced trees. 5
<header><largefont><b>Regularization</b></largefont> <largefont><b>Hyperparameters</b></largefont></header>
Decision Trees make very few assumptions about the training data (as opposed to lin‐
ear models, which assume that the data is linear, for example). If left unconstrained,
the tree structure will adapt itself to the training data, fitting it very closely—indeed,
most likely overfitting it. Such a model is often called a <i>nonparametric</i> <i>model,</i> not
because it does not have any parameters (it often has a lot) but because the number of
parameters is not determined prior to training, so the model structure is free to stick
closely to the data. In contrast, a <i>parametric</i> <i>model,</i> such as a linear model, has a pre‐
determined number of parameters, so its degree of freedom is limited, reducing the
risk of overfitting (but increasing the risk of underfitting).
To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom
during training. As you know by now, this is called regularization. The regularization
hyperparameters depend on the algorithm used, but generally you can at least restrict
the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the
max_depth hyperparameter (the default value is None , which means unlimited).
max_depth
Reducing will regularize the model and thus reduce the risk of overfitting.
The DecisionTreeClassifier class has a few other parameters that similarly restrict
min_samples_split
the shape of the Decision Tree: (the minimum number of sam‐
ples a node must have before it can be split), min_samples_leaf (the minimum num‐
ber of samples a leaf node must have), min_weight_fraction_leaf (same as
5 SeeSebastianRaschka’sinterestinganalysisformoredetails.