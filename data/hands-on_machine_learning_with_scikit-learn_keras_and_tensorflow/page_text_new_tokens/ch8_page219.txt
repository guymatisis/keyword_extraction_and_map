<i>Figure</i> <i>8-6.</i> <i>The</i> <i>decision</i> <i>boundary</i> <i>may</i> <i>not</i> <i>always</i> <i>be</i> <i>simpler</i> <i>with</i> <i>lower</i> <i>dimensions</i>
<header><largefont><b>PCA</b></largefont></header>
<i>Principal</i> <i>Component</i> <i>Analysis</i> (PCA) is by far the most popular dimensionality reduc‐
tion algorithm. First it identifies the hyperplane that lies closest to the data, and then
it projects the data onto it, just like in Figure 8-2.
<header><largefont><b>Preserving</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Variance</b></largefont></header>
Before you can project the training set onto a lower-dimensional hyperplane, you
first need to choose the right hyperplane. For example, a simple 2D dataset is repre‐
sented on the left in Figure 8-7, along with three different axes (i.e., 1D hyperplanes).
On the right is the result of the projection of the dataset onto each of these axes. As
you can see, the projection onto the solid line preserves the maximum variance, while
the projection onto the dotted line preserves very little variance and the projection
onto the dashed line preserves an intermediate amount of variance.