<i>Figure</i> <i>17-1.</i> <i>The</i> <i>chess</i> <i>memory</i> <i>experiment</i> <i>(left)</i> <i>and</i> <i>a</i> <i>simple</i> <i>autoencoder</i> <i>(right)</i>
As you can see, an autoencoder typically has the same architecture as a Multi-Layer
Perceptron (MLP; see Chapter 10), except that the number of neurons in the output
layer must be equal to the number of inputs. In this example, there is just one hidden
layer composed of two neurons (the encoder), and one output layer composed of
three neurons (the decoder). The outputs are often called the <i>reconstructions</i> because
the autoencoder tries to reconstruct the inputs, and the cost function contains a
<i>reconstruction</i> <i>loss</i> that penalizes the model when the reconstructions are different
from the inputs.
Because the internal representation has a lower dimensionality than the input data (it
is 2D instead of 3D), the autoencoder is said to be <i>undercomplete.</i> An undercomplete
autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to
output a copy of its inputs. It is forced to learn the most important features in the
input data (and drop the unimportant ones).
Let’s see how to implement a very simple undercomplete autoencoder for dimension‐
ality reduction.
<header><largefont><b>Performing</b></largefont> <largefont><b>PCA</b></largefont> <largefont><b>with</b></largefont> <largefont><b>an</b></largefont> <largefont><b>Undercomplete</b></largefont> <largefont><b>Linear</b></largefont></header>
<header><largefont><b>Autoencoder</b></largefont></header>
If the autoencoder uses only linear activations and the cost function is the mean
squared error (MSE), then it ends up performing Principal Component Analysis
(PCA; see Chapter 8).
The following code builds a simple linear autoencoder to perform PCA on a 3D data‐
set, projecting it to 2D: