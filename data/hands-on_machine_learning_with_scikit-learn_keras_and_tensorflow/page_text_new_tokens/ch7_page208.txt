It is possible to use Gradient Boosting with other cost functions.
loss
This is controlled by the hyperparameter (see Scikit-Learn’s
documentation for more details).
It is worth noting that an optimized implementation of Gradient Boosting is available
in the popular Python library XGBoost, which stands for Extreme Gradient Boosting.
This package was initially developed by Tianqi Chen as part of the Distributed (Deep)
Machine Learning Community (DMLC), and it aims to be extremely fast, scalable,
and portable. In fact, XGBoost is often an important component of the winning
entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:
<b>import</b> <b>xgboost</b>
xgb_reg = xgboost.XGBRegressor()
xgb_reg.fit(X_train, y_train)
y_pred = xgb_reg.predict(X_val)
XGBoost also offers several nice features, such as automatically taking care of early
stopping:
xgb_reg.fit(X_train, y_train,
eval_set=[(X_val, y_val)], early_stopping_rounds=2)
y_pred = xgb_reg.predict(X_val)
You should definitely check it out!
<header><largefont><b>Stacking</b></largefont></header>
The last Ensemble method we will discuss in this chapter is called <i>stacking</i> (short for
<i>stacked</i> <i>generalization).18</i> It is based on a simple idea: instead of using trivial functions
(such as hard voting) to aggregate the predictions of all predictors in an ensemble,
why don’t we train a model to perform this aggregation? Figure 7-12 shows such an
ensemble performing a regression task on a new instance. Each of the bottom three
predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor
(called a <i>blender,</i> or a <i>meta</i> <i>learner)</i> takes these predictions as inputs and makes the
final prediction (3.0).
18 DavidH.Wolpert,“StackedGeneralization,”NeuralNetworks5,no.2(1992):241–259.