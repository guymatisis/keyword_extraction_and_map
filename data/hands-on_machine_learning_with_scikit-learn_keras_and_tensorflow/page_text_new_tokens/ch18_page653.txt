<b>from</b> <b>tf_agents.agents.dqn.dqn_agent</b> <b>import</b> DqnAgent
train_step = tf.Variable(0)
update_period = 4 <i>#</i> <i>train</i> <i>the</i> <i>model</i> <i>every</i> <i>4</i> <i>steps</i>
optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,
epsilon=0.00001, centered=True)
epsilon_fn = keras.optimizers.schedules.PolynomialDecay(
initial_learning_rate=1.0, <i>#</i> <i>initial</i> <i>ε</i>
decay_steps=250000 // update_period, <i>#</i> <i><=></i> <i>1,000,000</i> <i>ALE</i> <i>frames</i>
end_learning_rate=0.01) <i>#</i> <i>final</i> <i>ε</i>
agent = DqnAgent(tf_env.time_step_spec(),
tf_env.action_spec(),
q_network=q_net,
optimizer=optimizer,
target_update_period=2000, <i>#</i> <i><=></i> <i>32,000</i> <i>ALE</i> <i>frames</i>
td_errors_loss_fn=keras.losses.Huber(reduction="none"),
gamma=0.99, <i>#</i> <i>discount</i> <i>factor</i>
train_step_counter=train_step,
epsilon_greedy=lambda: epsilon_fn(train_step))
agent.initialize()
Let’s walk through this code:
• We first create a variable that will count the number of training steps.
• Then we build the optimizer, using the same hyperparameters as in the 2015
DQN paper.
PolynomialDecay
• Next, we create a object that will compute the <i>ε</i> value for the <i>ε-</i>
greedy collect policy, given the current training step (it is normally used to decay
the learning rate, hence the names of the arguments, but it will work just fine to
decay any other value). It will go from 1.0 down to 0.01 (the value used during in
the 2015 DQN paper) in 1 million ALE frames, which corresponds to 250,000
steps, since we use frame skipping with a period of 4. Moreover, we will train the
agent every 4 steps (i.e., 16 ALE frames), so <i>ε</i> will actually decay over 62,500
<i>training</i> steps.
DQNAgent QNet
• We then build the , passing it the time step and action specs, the
work to train, the optimizer, the number of training steps between target model
train_step
updates, the loss function to use, the discount factor, the variable,
and a function that returns the <i>ε</i> value (it must take no argument, which is why
we need a lambda to pass the train_step ).
Note that the loss function must return an error per instance, not the mean error,
which is why we set reduction="none" .
• Lastly, we initialize the agent.
Next, let’s build the replay buffer and the observer that will write to it.