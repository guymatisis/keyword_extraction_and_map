<i>Hyperband</i>
A fast hyperparameter tuning library based on the recent Hyperband paper22 by
Lisha Li et al.
<i>Sklearn-Deap</i>
A hyperparameter optimization library based on evolutionary algorithms, with a
GridSearchCV -like interface.
Moreover, many companies offer services for hyperparameter optimization. We’ll dis‐
cuss Google Cloud AI Platform’s hyperparameter tuning service in Chapter 19. Other
options include services by Arimo and SigOpt, and CallDesk’s Oscar.
Hyperparameter tuning is still an active area of research, and evolutionary algorithms
are making a comeback. For example, check out DeepMind’s excellent 2017 paper, 23
where the authors jointly optimize a population of models and their hyperparame‐
ters. Google has also used an evolutionary approach, not just to search for hyperpara‐
meters but also to look for the best neural network architecture for the problem; their
AutoML suite is already available as a cloud service. Perhaps the days of building neu‐
ral networks manually will soon be over? Check out Google’s post on this topic. In
fact, evolutionary algorithms have been used successfully to train individual neural
networks, replacing the ubiquitous Gradient Descent! For an example, see the 2017
post by Uber where the authors introduce their <i>Deep</i> <i>Neuroevolution</i> technique.
But despite all this exciting progress and all these tools and services, it still helps to
have an idea of what values are reasonable for each hyperparameter so that you can
build a quick prototype and restrict the search space. The following sections provide
guidelines for choosing the number of hidden layers and neurons in an MLP and for
selecting good values for some of the main hyperparameters.
<header><largefont><b>Number</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Hidden</b></largefont> <largefont><b>Layers</b></largefont></header>
For many problems, you can begin with a single hidden layer and get reasonable
results. An MLP with just one hidden layer can theoretically model even the most
complex functions, provided it has enough neurons. But for complex problems, deep
networks have a much higher <i>parameter</i> <i>efficiency</i> than shallow ones: they can model
complex functions using exponentially fewer neurons than shallow nets, allowing
them to reach much better performance with the same amount of training data.
To understand why, suppose you are asked to draw a forest using some drawing soft‐
ware, but you are forbidden to copy and paste anything. It would take an enormous
22 LishaLietal.,“Hyperband:ANovelBandit-BasedApproachtoHyperparameterOptimization,”Journalof
<i>MachineLearningResearch18(April2018):1–52.</i>
23 MaxJaderbergetal.,“PopulationBasedTrainingofNeuralNetworks,”arXivpreprintarXiv:1711.09846
(2017).