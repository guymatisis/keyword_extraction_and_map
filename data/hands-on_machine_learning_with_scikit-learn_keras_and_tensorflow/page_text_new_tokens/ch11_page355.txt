element-wise division, and <i>ε</i> is a smoothing term to avoid division by zero, typically
set to 10–10). This vectorized form is equivalent to simultaneously computing
<i>θ</i> <i>θ</i> − <i>η</i> ∂J <b>θ</b> /∂θ / <i>s</i> + <i>ε</i> for all parameters <i>θ.</i>
<i>i</i> <i>i</i> <i>i</i> <i>i</i> <i>i</i>
In short, this algorithm decays the learning rate, but it does so faster for steep dimen‐
sions than for dimensions with gentler slopes. This is called an <i>adaptive</i> <i>learning</i> <i>rate.</i>
It helps point the resulting updates more directly toward the global optimum (see
Figure 11-7). One additional benefit is that it requires much less tuning of the learn‐
ing rate hyperparameter <i>η.</i>
<i>Figure</i> <i>11-7.</i> <i>AdaGrad</i> <i>versus</i> <i>Gradient</i> <i>Descent:</i> <i>the</i> <i>former</i> <i>can</i> <i>correct</i> <i>its</i> <i>direction</i> <i>ear‐</i>
<i>lier</i> <i>to</i> <i>point</i> <i>to</i> <i>the</i> <i>optimum</i>
AdaGrad frequently performs well for simple quadratic problems, but it often stops
too early when training neural networks. The learning rate gets scaled down so much
that the algorithm ends up stopping entirely before reaching the global optimum. So
even though Keras has an Adagrad optimizer, you should not use it to train deep neu‐
ral networks (it may be efficient for simpler tasks such as Linear Regression, though).
Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate
optimizers.
<header><largefont><b>RMSProp</b></largefont></header>
As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never con‐
verging to the global optimum. The <i>RMSProp</i> algorithm16 fixes this by accumulating
only the gradients from the most recent iterations (as opposed to all the gradients
16 ThisalgorithmwascreatedbyGeoffreyHintonandTijmenTielemanin2012andpresentedbyGeoffreyHin‐
toninhisCourseraclassonneuralnetworks(slides:https://homl.info/57;video:https://homl.info/58).Amus‐
ingly,sincetheauthorsdidnotwriteapapertodescribethealgorithm,researchersoftencite“slide29in
lecture6”intheirpapers.