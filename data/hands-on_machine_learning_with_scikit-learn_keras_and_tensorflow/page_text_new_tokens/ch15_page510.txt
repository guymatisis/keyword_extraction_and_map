To be clear, at time step 0 the model will output a vector containing the forecasts for
time steps 1 to 10, then at time step 1 the model will forecast time steps 2 to 11, and
so on. So each target must be a sequence of the same length as the input sequence,
containing a 10-dimensional vector at each step. Let’s prepare these target sequences:
Y = np.empty((10000, n_steps, 10)) <i>#</i> <i>each</i> <i>target</i> <i>is</i> <i>a</i> <i>sequence</i> <i>of</i> <i>10D</i> <i>vectors</i>
<b>for</b> step_ahead <b>in</b> range(1, 10 + 1):
Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]
Y_train = Y[:7000]
Y_valid = Y[7000:9000]
Y_test = Y[9000:]
It may be surprising that the targets will contain values that appear
X_train
in the inputs (there is a lot of overlap between and
Y_train).
Isn’t that cheating? Fortunately, not at all: at each time
step, the model only knows about past time steps, so it cannot look
ahead. It is said to be a <i>causal</i> model.
return_sequen
To turn the model into a sequence-to-sequence model, we must set
ces=True
in all recurrent layers (even the last one), and we must apply the output
Dense layer at every time step. Keras offers a TimeDistributed layer for this very pur‐
Dense
pose: it wraps any layer (e.g., a layer) and applies it at every time step of its
input sequence. It does this efficiently, by reshaping the inputs so that each time step
is treated as a separate instance (i.e., it reshapes the inputs from [batch <i>size,</i> <i>time</i> <i>steps,</i>
<i>input</i> <i>dimensions]</i> to [batch <i>size</i> × <i>time</i> <i>steps,</i> <i>input</i> <i>dimensions];</i> in this example, the
SimpleRNN
number of input dimensions is 20 because the previous layer has 20 units),
then it runs the Dense layer, and finally it reshapes the outputs back to sequences (i.e.,
it reshapes the outputs from [batch <i>size</i> × <i>time</i> <i>steps,</i> <i>output</i> <i>dimensions]</i> to [batch <i>size,</i>
<i>time</i> <i>steps,</i> <i>output</i> <i>dimensions];</i> in this example the number of output dimensions is
Dense
10, since the layer has 10 units). 2 Here is the updated model:
model = keras.models.Sequential([
keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
keras.layers.SimpleRNN(20, return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
The Dense layer actually supports sequences as inputs (and even higher-dimensional
TimeDistributed(Dense(…))
inputs): it handles them just like , meaning it is applied
to the last input dimension only (independently across all time steps). Thus, we could
replace the last layer with just Dense(10) . For the sake of clarity, however, we will
TimeDistributed(Dense(10)) Dense
keep using because it makes it clear that the
TimeDistributed(Dense(n)) Conv1D(n, filter_size=1)
2 Notethata layerisequivalenttoa layer.