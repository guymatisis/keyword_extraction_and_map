<i>Figure</i> <i>16-8.</i> <i>The</i> <i>Transformer</i> <i>architecture</i> <i>22</i>
Let’s walk through this figure:
• The lefthand part is the encoder. Just like earlier, it takes as input a batch of sen‐
tences represented as sequences of word IDs (the input shape is [batch <i>size,</i> <i>max</i>
<i>input</i> <i>sentence</i> <i>length]),</i> and it encodes each word into a 512-dimensional repre‐
sentation (so the encoder’s output shape is [batch <i>size,</i> <i>max</i> <i>input</i> <i>sentence</i> <i>length,</i>
512]). Note that the top part of the encoder is stacked <i>N</i> times (in the paper,
<i>N</i> = 6).
22 Thisisfigure1fromthepaper,reproducedwiththekindauthorizationoftheauthors.