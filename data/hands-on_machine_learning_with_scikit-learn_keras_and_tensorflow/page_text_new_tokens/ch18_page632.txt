This algorithm will converge to the optimal Q-Values, but it will take many iterations,
and possibly quite a lot of hyperparameter tuning. As you can see in Figure 18-9, the
Q-Value Iteration algorithm (left) converges very quickly, in fewer than 20 iterations,
while the Q-Learning algorithm (right) takes about 8,000 iterations to converge.
Obviously, not knowing the transition probabilities or the rewards makes finding the
optimal policy significantly harder!
<i>Figure</i> <i>18-9.</i> <i>The</i> <i>Q-Value</i> <i>Iteration</i> <i>algorithm</i> <i>(left)</i> <i>versus</i> <i>the</i> <i>Q-Learning</i> <i>algorithm</i>
<i>(right)</i>
The Q-Learning algorithm is called an <i>off-policy</i> algorithm because the policy being
trained is not necessarily the one being executed: in the previous code example, the
policy being executed (the exploration policy) is completely random, while the policy
being trained will always choose the actions with the highest Q-Values. Conversely,
the Policy Gradients algorithm is an <i>on-policy</i> algorithm: it explores the world using
the policy being trained. It is somewhat surprising that Q-Learning is capable of
learning the optimal policy by just watching an agent act randomly (imagine learning
to play golf when your teacher is a drunk monkey). Can we do better?
<header><largefont><b>Exploration</b></largefont> <largefont><b>Policies</b></largefont></header>
Of course, Q-Learning can work only if the exploration policy explores the MDP
thoroughly enough. Although a purely random policy is guaranteed to eventually
visit every state and every transition many times, it may take an extremely long time
to do so. Therefore, a better option is to use the <i>ε-greedy</i> <i>policy</i> (ε is epsilon): at each
step it acts randomly with probability <i>ε,</i> or greedily with probability 1–ε (i.e., choos‐
ing the action with the highest Q-Value). The advantage of the <i>ε-greedy</i> policy (com‐
pared to a completely random policy) is that it will spend more and more time
exploring the interesting parts of the environment, as the Q-Value estimates get better
and better, while still spending some time visiting unknown regions of the MDP. It is
quite common to start with a high value for <i>ε</i> (e.g., 1.0) and then gradually reduce it
(e.g., down to 0.05).