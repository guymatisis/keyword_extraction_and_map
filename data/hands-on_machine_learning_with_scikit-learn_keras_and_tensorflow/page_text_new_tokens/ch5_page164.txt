<i>Figure</i> <i>5-11.</i> <i>SVM</i> <i>Regression</i> <i>using</i> <i>a</i> <i>second-degree</i> <i>polynomial</i> <i>kernel</i>
The following code uses Scikit-Learn’s SVR class (which supports the kernel trick) to
produce the model represented on the left in Figure 5-11:
<b>from</b> <b>sklearn.svm</b> <b>import</b> SVR
svm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1)
svm_poly_reg.fit(X, y)
The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is
LinearSVC LinearSVR
the regression equivalent of the class. The class scales linearly
with the size of the training set (just like the LinearSVC class), while the SVR class gets
SVC
much too slow when the training set grows large (just like the class).
SVMs can also be used for outlier detection; see Scikit-Learn’s doc‐
umentation for more details.
<header><largefont><b>Under</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Hood</b></largefont></header>
This section explains how SVMs make predictions and how their training algorithms
work, starting with linear SVM classifiers. If you are just getting started with Machine
Learning, you can safely skip it and go straight to the exercises at the end of this chap‐
ter, and come back later when you want to get a deeper understanding of SVMs.
First, a word about notations. In Chapter 4 we used the convention of putting all the
model parameters in one vector <b>θ,</b> including the bias term <i>θ</i> and the input feature
0
weights <i>θ</i> to <i>θ</i> , and adding a bias input <i>x</i> = 1 to all instances. In this chapter we will
1 <i>n</i> 0
use a convention that is more convenient (and more common) when dealing with