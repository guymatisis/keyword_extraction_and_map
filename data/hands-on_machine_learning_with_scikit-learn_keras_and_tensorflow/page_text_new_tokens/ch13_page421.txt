cycle_length=n_readers, num_parallel_calls=n_read_threads)
dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)
dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)
<b>return</b> dataset.batch(batch_size).prefetch(1)
Everything should make sense in this code, except the very last line ( prefetch(1) ),
which is important for performance.
<i>Figure</i> <i>13-2.</i> <i>Loading</i> <i>and</i> <i>preprocessing</i> <i>data</i> <i>from</i> <i>multiple</i> <i>CSV</i> <i>files</i>
<header><largefont><b>Prefetching</b></largefont></header>
By calling prefetch(1) at the end, we are creating a dataset that will do its best to
ahead.2
always be one batch In other words, while our training algorithm is working
on one batch, the dataset will already be working in parallel on getting the next batch
ready (e.g., reading the data from disk and preprocessing it). This can improve per‐
formance dramatically, as is illustrated in Figure 13-3. If we also ensure that loading
num_parallel_calls
and preprocessing are multithreaded (by setting when calling
interleave() and map() ), we can exploit multiple cores on the CPU and hopefully
make preparing one batch of data shorter than running a training step on the GPU:
2 Ingeneral,justprefetchingonebatchisfine,butinsomecasesyoumayneedtoprefetchafewmore.Alterna‐
tf.data.experimental.AUTOTUNE
tively,youcanletTensorFlowdecideautomaticallybypassing (thisisan
experimentalfeaturefornow).