Before you can run the training job, you need to write the training code, exactly like
you did earlier for a distributed setup (e.g., using the ParameterServerStrategy ). AI
Platform will take care of setting TF_CONFIG for you on each VM. Once that’s done,
you can deploy it and run it on a TF cluster with a command line like this:
$ <b>gcloud</b> <b>ai-platform</b> <b>jobs</b> <b>submit</b> <b>training</b> <b>my_job_20190531_164700</b> \
<b>--region</b> <b>asia-southeast1</b> \
<b>--scale-tier</b> <b>PREMIUM_1</b> \
<b>--runtime-version</b> <b>2.0</b> \
<b>--python-version</b> <b>3.5</b> \
<b>--package-path</b> <b>/my_project/src/trainer</b> \
<b>--module-name</b> <b>trainer.task</b> \
<b>--staging-bucket</b> <b>gs://my-staging-bucket</b> \
<b>--job-dir</b> <b>gs://my-mnist-model-bucket/trained_model</b> \
<b>--</b>
<b>--my-extra-argument1</b> <b>foo</b> <b>--my-extra-argument2</b> <b>bar</b>
Let’s go through these options. The command will start a training job named
my_job_20190531_164700, asia-southeast1 PREMIUM_1
in the region, using a <i>scale</i>
<i>tier:</i> this corresponds to 20 workers (including a chief) and 11 parameter servers
(check out the other available scale tiers). All these VMs will be based on AI Plat‐
form’s 2.0 runtime (a VM configuration that includes TensorFlow 2.0 and many other
packages) 22 and Python 3.5. The training code is located in the <i>/my_project/src/trainer</i>
gcloud
directory, and the command will automatically bundle it into a pip package
and upload it to GCS at <i>gs://my-staging-bucket.</i> Next, AI Platform will start several
VMs, deploy the package to them, and run the trainer.task module. Lastly, the --
job-dir
argument and the extra arguments (i.e., all the arguments located after the
-- separator) will be passed to the training program: the chief task will usually use the
--job-dir
argument to find out where to save the final model on GCS, in this case at
<i>gs://my-mnist-model-bucket/trained_model.</i> And that’s it! In the GCP console, you can
then open the navigation menu, scroll down to the Artificial Intelligence section, and
open AI Platform → Jobs. You should see your job running, and if you click it you
will see graphs showing the CPU, GPU, and RAM utilization for every task. You can
click View Logs to access the detailed logs using Stackdriver.
If you place the training data on GCS, you can create a
tf.data.TextLineDataset tf.data.TFRecordDataset
or to access
it: just use the GCS paths as the filenames (e.g., <i>gs://my-data-</i>
tf.io.gfile
<i>bucket/my_data_001.csv).</i> These datasets rely on the
package to access files: it supports both local files and GCS files
(but make sure the service account you use has access to GCS).
22 Atthetimeofthiswriting,the2.0runtimeisnotyetavailable,butitshouldbereadybythetimeyouread
this.Checkoutthelistofavailableruntimes.