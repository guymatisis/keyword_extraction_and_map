<i>Figure</i> <i>6-5.</i> <i>Predictions</i> <i>of</i> <i>two</i> <i>Decision</i> <i>Tree</i> <i>regression</i> <i>models</i>
The CART algorithm works mostly the same way as earlier, except that instead of try‐
ing to split the training set in a way that minimizes impurity, it now tries to split the
training set in a way that minimizes the MSE. Equation 6-4 shows the cost function
that the algorithm tries to minimize.
<i>Equation</i> <i>6-4.</i> <i>CART</i> <i>cost</i> <i>function</i> <i>for</i> <i>regression</i>
2
<largefont>∑</largefont> <i>i</i>
MSE = <i>y</i> − <i>y</i>
node node
<i>m</i> <i>m</i> ∈
<i>i</i> node
left right
<i>J</i> <i>k,t</i> = MSE + MSE where
<i>k</i> <i>m</i> left <i>m</i> right 1
<largefont>∑</largefont> <i>i</i>
<i>y</i> = <i>y</i>
node <i>m</i>
<i>i</i> ∈ node
node
Just like for classification tasks, Decision Trees are prone to overfitting when dealing
with regression tasks. Without any regularization (i.e., using the default hyperpara‐
meters), you get the predictions on the left in Figure 6-6. These predictions are obvi‐
ously overfitting the training set very badly. Just setting min_samples_leaf=10 results
in a much more reasonable model, represented on the right in Figure 6-6.
<i>Figure</i> <i>6-6.</i> <i>Regularizing</i> <i>a</i> <i>Decision</i> <i>Tree</i> <i>regressor</i>