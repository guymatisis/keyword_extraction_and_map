<i>Figure</i> <i>16-10.</i> <i>Multi-Head</i> <i>Attention</i> <i>layer</i> <i>architecture23</i>
As you can see, it is just a bunch of Scaled Dot-Product Attention layers, each pre‐
ceded by a linear transformation of the values, keys, and queries (i.e., a time-
Dense
distributed layer with no activation function). All the outputs are simply
concatenated, and they go through a final linear transformation (again, time-
distributed). But why? What is the intuition behind this architecture? Well, consider
the word “played” we discussed earlier (in the sentence “They played chess”). The
encoder was smart enough to encode the fact that it is a verb. But the word represen‐
tation also includes its position in the text, thanks to the positional encodings, and it
probably includes many other features that are useful for its translation, such as the
fact that it is in the past tense. In short, the word representation encodes many differ‐
ent characteristics of the word. If we just used a single Scaled Dot-Product Attention
layer, we would only be able to query all of these characteristics in one shot. This is
why the Multi-Head Attention layer applies multiple different linear transformations
of the values, keys, and queries: this allows the model to apply many different projec‐
tions of the word representation into different subspaces, each focusing on a subset of
the word’s characteristics. Perhaps one of the linear layers will project the word repre‐
sentation into a subspace where all that remains is the information that the word is a
23 Thisistherightpartoffigure2fromthepaper,reproducedwiththekindauthorizationoftheauthors.