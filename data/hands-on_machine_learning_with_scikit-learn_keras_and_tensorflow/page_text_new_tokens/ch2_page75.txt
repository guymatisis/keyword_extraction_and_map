<b>>>></b> <b>from</b> <b>sklearn.ensemble</b> <b>import</b> RandomForestRegressor
<b>>>></b> forest_reg = RandomForestRegressor()
<b>>>></b> forest_reg.fit(housing_prepared, housing_labels)
<b>>>></b> [...]
<b>>>></b> forest_rmse
18603.515021376355
<b>>>></b> display_scores(forest_rmse_scores)
Scores: [49519.80364233 47461.9115823 50029.02762854 52325.28068953
49308.39426421 53446.37892622 48634.8036574 47585.73832311
53490.10699751 50021.5852922 ]
Mean: 50182.303100336096
Standard deviation: 2097.0810550985693
Wow, this is much better: Random Forests look very promising. However, note that
the score on the training set is still much lower than on the validation sets, meaning
that the model is still overfitting the training set. Possible solutions for overfitting are
to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.
Before you dive much deeper into Random Forests, however, you should try out
many other models from various categories of Machine Learning algorithms (e.g.,
several Support Vector Machines with different kernels, and possibly a neural net‐
work), without spending too much time tweaking the hyperparameters. The goal is to
shortlist a few (two to five) promising models.
You should save every model you experiment with so that you can
come back easily to any model you want. Make sure you save both
the hyperparameters and the trained parameters, as well as the
cross-validation scores and perhaps the actual predictions as well.
This will allow you to easily compare scores across model types,
and compare the types of errors they make. You can easily save
Scikit-Learn models by using Python’s pickle module or by using
the joblib library, which is more efficient at serializing large
NumPy arrays (you can install this library using pip):
<b>import</b> <b>joblib</b>
joblib.dump(my_model, "my_model.pkl")
<i>#</i> <i>and</i> <i>later...</i>
my_model_loaded = joblib.load("my_model.pkl")
<header><largefont><b>Fine-Tune</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Model</b></largefont></header>
Let’s assume that you now have a shortlist of promising models. You now need to
fine-tune them. Let’s look at a few ways you can do that.