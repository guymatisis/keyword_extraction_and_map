∥ ∥ )2, ∥ ∥ vector.10
equal to ½( <b>w</b> where <b>w</b> represents the ℓ norm of the weight For
2 2 2
Gradient Descent, just add <i>αw</i> to the MSE gradient vector (Equation 4-6).
It is important to scale the data (e.g., using a StandardScaler)
before performing Ridge Regression, as it is sensitive to the scale of
the input features. This is true of most regularized models.
Figure 4-17 shows several Ridge models trained on some linear data using different <i>α</i>
values. On the left, plain Ridge models are used, leading to linear predictions. On the
right, the data is first expanded using PolynomialFeatures(degree=10) , then it is
StandardScaler
scaled using a , and finally the Ridge models are applied to the result‐
ing features: this is Polynomial Regression with Ridge regularization. Note how
increasing <i>α</i> leads to flatter (i.e., less extreme, more reasonable) predictions, thus
reducing the model’s variance but increasing its bias.
<i>Figure</i> <i>4-17.</i> <i>A</i> <i>linear</i> <i>model</i> <i>(left)</i> <i>and</i> <i>a</i> <i>polynomial</i> <i>model</i> <i>(right),</i> <i>both</i> <i>with</i> <i>various</i> <i>lev‐</i>
<i>els</i> <i>of</i> <i>Ridge</i> <i>regularization</i>
As with Linear Regression, we can perform Ridge Regression either by computing a
closed-form equation or by performing Gradient Descent. The pros and cons are the
10 NormsarediscussedinChapter2.