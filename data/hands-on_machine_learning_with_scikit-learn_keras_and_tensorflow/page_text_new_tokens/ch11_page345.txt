achieving state-of-the-art performance on complex image classification tasks. As this
is bleeding-edge research, however, you may want to wait for additional research to
confirm this finding before you drop Batch Normalization.
<header><largefont><b>Gradient</b></largefont> <largefont><b>Clipping</b></largefont></header>
Another popular technique to mitigate the exploding gradients problem is to clip the
gradients during backpropagation so that they never exceed some threshold. This is
<i>Clipping.12</i>
called <i>Gradient</i> This technique is most often used in recurrent neural net‐
works, as Batch Normalization is tricky to use in RNNs, as we will see in Chapter 15.
For other types of networks, BN is usually sufficient.
In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or
clipnorm
argument when creating an optimizer, like this:
optimizer = keras.optimizers.SGD(clipvalue=1.0)
model.compile(loss="mse", optimizer=optimizer)
This optimizer will clip every component of the gradient vector to a value between
–1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each
and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold is
a hyperparameter you can tune. Note that it may change the orientation of the gradi‐
ent vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly
in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0],
which points roughly in the diagonal between the two axes. In practice, this approach
works well. If you want to ensure that Gradient Clipping does not change the direc‐
clipnorm
tion of the gradient vector, you should clip by norm by setting instead of
clipvalue.
This will clip the whole gradient if its ℓ norm is greater than the thres‐
2
hold you picked. For example, if you set clipnorm=1.0 , then the vector [0.9, 100.0]
will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost elim‐
inating the first component. If you observe that the gradients explode during training
(you can track the size of the gradients using TensorBoard), you may want to try both
clipping by value and clipping by norm, with different thresholds, and see which
option performs best on the validation set.
<header><largefont><b>Reusing</b></largefont> <largefont><b>Pretrained</b></largefont> <largefont><b>Layers</b></largefont></header>
It is generally not a good idea to train a very large DNN from scratch: instead, you
should always try to find an existing neural network that accomplishes a similar task
to the one you are trying to tackle (we will discuss how to find them in Chapter 14),
then reuse the lower layers of this network. This technique is called <i>transfer</i> <i>learning.</i>
12 RazvanPascanuetal.,“OntheDifficultyofTrainingRecurrentNeuralNetworks,”Proceedingsofthe30th
<i>InternationalConferenceonMachineLearning(2013):1310–1318.</i>