<header><largefont><b>Manifold</b></largefont> <largefont><b>Learning</b></largefont></header>
The Swiss roll is an example of a 2D <i>manifold.</i> Put simply, a 2D manifold is a 2D
shape that can be bent and twisted in a higher-dimensional space. More generally, a
<i>d-dimensional</i> manifold is a part of an <i>n-dimensional</i> space (where <i>d</i> < <i>n)</i> that locally
resembles a <i>d-dimensional</i> hyperplane. In the case of the Swiss roll, <i>d</i> = 2 and <i>n</i> = 3: it
locally resembles a 2D plane, but it is rolled in the third dimension.
Many dimensionality reduction algorithms work by modeling the manifold on which
the training instances lie; this is called <i>Manifold</i> <i>Learning.</i> It relies on the <i>manifold</i>
<i>assumption,</i> also called the <i>manifold</i> <i>hypothesis,</i> which holds that most real-world
high-dimensional datasets lie close to a much lower-dimensional manifold. This
assumption is very often empirically observed.
Once again, think about the MNIST dataset: all handwritten digit images have some
similarities. They are made of connected lines, the borders are white, and they are
more or less centered. If you randomly generated images, only a ridiculously tiny
fraction of them would look like handwritten digits. In other words, the degrees of
freedom available to you if you try to create a digit image are dramatically lower than
the degrees of freedom you would have if you were allowed to generate any image
you wanted. These constraints tend to squeeze the dataset into a lower-dimensional
manifold.
The manifold assumption is often accompanied by another implicit assumption: that
the task at hand (e.g., classification or regression) will be simpler if expressed in the
lower-dimensional space of the manifold. For example, in the top row of Figure 8-6
the Swiss roll is split into two classes: in the 3D space (on the left), the decision
boundary would be fairly complex, but in the 2D unrolled manifold space (on the
right), the decision boundary is a straight line.
However, this implicit assumption does not always hold. For example, in the bottom
row of Figure 8-6, the decision boundary is located at <i>x</i> = 5. This decision boundary
1
looks very simple in the original 3D space (a vertical plane), but it looks more com‚Äê
plex in the unrolled manifold (a collection of four independent line segments).
In short, reducing the dimensionality of your training set before training a model will
usually speed up training, but it may not always lead to a better or simpler solution; it
all depends on the dataset.
Hopefully you now have a good sense of what the curse of dimensionality is and how
dimensionality reduction algorithms can fight it, especially when the manifold
assumption holds. The rest of this chapter will go through some of the most popular
algorithms.