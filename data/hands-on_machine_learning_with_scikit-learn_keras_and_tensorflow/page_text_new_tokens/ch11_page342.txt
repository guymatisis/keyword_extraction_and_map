model = keras.models.Sequential([
keras.layers.Flatten(input_shape=[28, 28]),
keras.layers.BatchNormalization(),
keras.layers.Dense(300, activation="elu", kernel_initializer="he_normal"),
keras.layers.BatchNormalization(),
keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal"),
keras.layers.BatchNormalization(),
keras.layers.Dense(10, activation="softmax")
])
That’s all! In this tiny example with just two hidden layers, it’s unlikely that Batch
Normalization will have a very positive impact; but for deeper networks it can make a
tremendous difference.
Let’s display the model summary:
<b>>>></b> model.summary()
Model: "sequential_3"
_________________________________________________________________
Layer (type) Output Shape Param #
=================================================================
flatten_3 (Flatten) (None, 784) 0
_________________________________________________________________
batch_normalization_v2 (Batc (None, 784) 3136
_________________________________________________________________
dense_50 (Dense) (None, 300) 235500
_________________________________________________________________
batch_normalization_v2_1 (Ba (None, 300) 1200
_________________________________________________________________
dense_51 (Dense) (None, 100) 30100
_________________________________________________________________
batch_normalization_v2_2 (Ba (None, 100) 400
_________________________________________________________________
dense_52 (Dense) (None, 10) 1010
=================================================================
Total params: 271,346
Trainable params: 268,978
Non-trainable params: 2,368
As you can see, each BN layer adds four parameters per input: <b>γ,</b> <b>β,</b> <b>μ,</b> and <b>σ</b> (for
example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two
parameters, <b>μ</b> and <b>σ,</b> are the moving averages; they are not affected by backpropaga‐
tion, so Keras calls them “non-trainable”9 (if you count the total number of BN
parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total
number of non-trainable parameters in this model).
9 However,theyareestimatedduringtraining,basedonthetrainingdata,soarguablytheyaretrainable.In
Keras,“non-trainable”reallymeans“untouchedbybackpropagation.”