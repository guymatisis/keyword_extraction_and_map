<i>Figure</i> <i>3-2.</i> <i>An</i> <i>illustrated</i> <i>confusion</i> <i>matrix</i> <i>shows</i> <i>examples</i> <i>of</i> <i>true</i> <i>negatives</i> <i>(top</i> <i>left),</i>
<i>false</i> <i>positives</i> <i>(top</i> <i>right),</i> <i>false</i> <i>negatives</i> <i>(lower</i> <i>left),</i> <i>and</i> <i>true</i> <i>positives</i> <i>(lower</i> <i>right)</i>
<header><largefont><b>Precision</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Recall</b></largefont></header>
Scikit-Learn provides several functions to compute classifier metrics, including preci‐
sion and recall:
<b>>>></b> <b>from</b> <b>sklearn.metrics</b> <b>import</b> precision_score, recall_score
<b>>>></b> precision_score(y_train_5, y_train_pred) <i>#</i> <i>==</i> <i>4096</i> <i>/</i> <i>(4096</i> <i>+</i> <i>1522)</i>
0.7290850836596654
<b>>>></b> recall_score(y_train_5, y_train_pred) <i>#</i> <i>==</i> <i>4096</i> <i>/</i> <i>(4096</i> <i>+</i> <i>1325)</i>
0.7555801512636044
Now your 5-detector does not look as shiny as it did when you looked at its accuracy.
When it claims an image represents a 5, it is correct only 72.9% of the time. More‐
over, it only detects 75.6% of the 5s.
It is often convenient to combine precision and recall into a single metric called the <i>F</i>
<i>1</i>
<i>score,</i> in particular if you need a simple way to compare two classifiers. The F score is
1
the <i>harmonic</i> <i>mean</i> of precision and recall (Equation 3-3). Whereas the regular mean
treats all values equally, the harmonic mean gives much more weight to low values.
As a result, the classifier will only get a high F score if both recall and precision are
1
high.
<i>Equation</i> <i>3-3.</i> <i>F</i>
<i>1</i>
2 precision×recall <i>TP</i>
<i>F</i> = = 2 × =
1 1 1 precision+recall <i>FN</i> +FP
+ <i>TP</i> +
precision recall 2