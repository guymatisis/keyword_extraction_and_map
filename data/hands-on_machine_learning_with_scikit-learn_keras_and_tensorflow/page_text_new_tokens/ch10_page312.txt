<i>Figure</i> <i>10-16.</i> <i>Handling</i> <i>multiple</i> <i>outputs,</i> <i>in</i> <i>this</i> <i>example</i> <i>to</i> <i>add</i> <i>an</i> <i>auxiliary</i> <i>output</i> <i>for</i>
<i>regularization</i>
Adding extra outputs is quite easy: just connect them to the appropriate layers and
add them to your model’s list of outputs. For example, the following code builds the
network represented in Figure 10-16:
[...] <i>#</i> <i>Same</i> <i>as</i> <i>above,</i> <i>up</i> <i>to</i> <i>the</i> <i>main</i> <i>output</i> <i>layer</i>
output = keras.layers.Dense(1, name="main_output")(concat)
aux_output = keras.layers.Dense(1, name="aux_output")(hidden2)
model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])
Each output will need its own loss function. Therefore, when we compile the model,
losses20
we should pass a list of (if we pass a single loss, Keras will assume that the
same loss must be used for all outputs). By default, Keras will compute all these losses
and simply add them up to get the final loss used for training. We care much more
about the main output than about the auxiliary output (as it is just used for regulari‐
zation), so we want to give the main output’s loss a much greater weight. Fortunately,
it is possible to set all the loss weights when compiling the model:
model.compile(loss=["mse", "mse"], loss_weights=[0.9, 0.1], optimizer="sgd")
Now when we train the model, we need to provide labels for each output. In this
example, the main output and the auxiliary output should try to predict the same
y_train
thing, so they should use the same labels. So instead of passing , we need to
pass (y_train, y_train) (and the same goes for y_valid and y_test ):
history = model.fit(
[X_train_A, X_train_B], [y_train, y_train], epochs=20,
validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))
20 Alternatively,youcanpassadictionarythatmapseachoutputnametothecorrespondingloss.Justlikefor
theinputs,thisisusefulwhentherearemultipleoutputs,toavoidgettingtheorderwrong.Thelossweights
andmetrics(discussedshortly)canalsobesetusingdictionaries.