<header><largefont><b>Examples</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Sampling</b></largefont> <largefont><b>Bias</b></largefont></header>
Perhaps the most famous example of sampling bias happened during the US presi‐
dential election in 1936, which pitted Landon against Roosevelt: the <i>Literary</i> <i>Digest</i>
conducted a very large poll, sending mail to about 10 million people. It got 2.4 million
answers, and predicted with high confidence that Landon would get 57% of the votes.
Instead, Roosevelt won with 62% of the votes. The flaw was in the <i>Literary</i> <i>Digest’s</i>
sampling method:
• First, to obtain the addresses to send the polls to, the <i>Literary</i> <i>Digest</i> used tele‐
phone directories, lists of magazine subscribers, club membership lists, and the
like. All of these lists tended to favor wealthier people, who were more likely to
vote Republican (hence Landon).
• Second, less than 25% of the people who were polled answered. Again this intro‐
duced a sampling bias, by potentially ruling out people who didn’t care much
about politics, people who didn’t like the <i>Literary</i> <i>Digest,</i> and other key groups.
This is a special type of sampling bias called <i>nonresponse</i> <i>bias.</i>
Here is another example: say you want to build a system to recognize funk music vid‐
eos. One way to build your training set is to search for “funk music” on YouTube and
use the resulting videos. But this assumes that YouTube’s search engine returns a set of
videos that are representative of all the funk music videos on YouTube. In reality, the
search results are likely to be biased toward popular artists (and if you live in Brazil
you will get a lot of “funk carioca” videos, which sound nothing like James Brown).
On the other hand, how else can you get a large training set?
<header><largefont><b>Poor-Quality</b></largefont> <largefont><b>Data</b></largefont></header>
Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-
quality measurements), it will make it harder for the system to detect the underlying
patterns, so your system is less likely to perform well. It is often well worth the effort
to spend time cleaning up your training data. The truth is, most data scientists spend
a significant part of their time doing just that. The following are a couple of examples
of when you’d want to clean up training data:
• If some instances are clearly outliers, it may help to simply discard them or try to
fix the errors manually.
• If some instances are missing a few features (e.g., 5% of your customers did not
specify their age), you must decide whether you want to ignore this attribute alto‐
gether, ignore these instances, fill in the missing values (e.g., with the median
age), or train one model with the feature and one model without it.