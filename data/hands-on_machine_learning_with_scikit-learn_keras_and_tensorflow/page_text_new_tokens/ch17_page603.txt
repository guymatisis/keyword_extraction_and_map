The paper also introduced several other techniques aimed at increasing the diversity
of the outputs (to avoid mode collapse) and making training more stable:
<i>Minibatch</i> <i>standard</i> <i>deviation</i> <i>layer</i>
Added near the end of the discriminator. For each position in the inputs, it com‐
putes the standard deviation across all channels and all instances in the batch
( S = tf.math.reduce_std(inputs, axis=[0, -1]) ). These standard deviations
v = tf.reduce_
are then averaged across all points to get a single value (
mean(S)).
Finally, an extra feature map is added to each instance in the batch and
filled with the computed value ( tf.concat([inputs, tf.fill([batch_size,
height, width, 1], v)], axis=-1)
). How does this help? Well, if the genera‐
tor produces images with little variety, then there will be a small standard devia‐
tion across feature maps in the discriminator. Thanks to this layer, the
discriminator will have easy access to this statistic, making it less likely to be
fooled by a generator that produces too little diversity. This will encourage the
generator to produce more diverse outputs, reducing the risk of mode collapse.
<i>Equalized</i> <i>learning</i> <i>rate</i>
Initializes all weights using a simple Gaussian distribution with mean 0 and stan‐
dard deviation 1 rather than using He initialization. However, the weights are
scaled down at runtime (i.e., every time the layer is executed) by the same factor
as in He initialization: they are divided by 2/n , where <i>n</i> is the number
inputs inputs
of inputs to the layer. The paper demonstrated that this technique significantly
improved the GAN’s performance when using RMSProp, Adam, or other adap‐
tive gradient optimizers. Indeed, these optimizers normalize the gradient updates
by their estimated standard deviation (see Chapter 11), so parameters that have a
17
larger dynamic range will take longer to train, while parameters with a small
dynamic range may be updated too quickly, leading to instabilities. By rescaling
the weights as part of the model itself rather than just rescaling them upon initi‐
alization, this approach ensures that the dynamic range is the same for all param‐
eters, throughout training, so they all learn at the same speed. This both speeds
up and stabilizes training.
<i>Pixelwise</i> <i>normalization</i> <i>layer</i>
Added after each convolutional layer in the generator. It normalizes each activa‐
tion based on all the activations in the same image and at the same location, but
across all channels (dividing by the square root of the mean squared activation).
In TensorFlow code, this is inputs / tf.sqrt(tf.reduce_mean(tf.square(X),
axis=-1, keepdims=True) + 1e-8) 1e-8
(the smoothing term is needed to
17 Thedynamicrangeofavariableistheratiobetweenthehighestandthelowestvalueitmaytake.