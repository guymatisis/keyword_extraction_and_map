<b>Centroidinitializationmethods</b>
If you happen to know approximately where the centroids should be (e.g., if you ran
init
another clustering algorithm earlier), then you can set the hyperparameter to a
NumPy array containing the list of centroids, and set n_init to 1 :
good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)
Another solution is to run the algorithm multiple times with different random initial‐
izations and keep the best solution. The number of random initializations is con‐
n_init 10
trolled by the hyperparameter: by default, it is equal to , which means that
the whole algorithm described earlier runs 10 times when you call fit() , and Scikit-
Learn keeps the best solution. But how exactly does it know which solution is the
best? It uses a performance metric! That metric is called the model’s <i>inertia,</i> which is
the mean squared distance between each instance and its closest centroid. It is
roughly equal to 223.3 for the model on the left in Figure 9-5, 237.5 for the model on
the right in Figure 9-5, and 211.6 for the model in Figure 9-3. The KMeans class runs
n_init
the algorithm times and keeps the model with the lowest inertia. In this
example, the model in Figure 9-3 will be selected (unless we are very unlucky with
n_init consecutive random initializations). If you are curious, a model’s inertia is
inertia_
accessible via the instance variable:
<b>>>></b> kmeans.inertia_
211.59853725816856
score()
The method returns the negative inertia. Why negative? Because a predic‐
tor’s score() method must always respect Scikit-Learn’s “greater is better” rule: if a
score()
predictor is better than another, its method should return a greater score.
<b>>>></b> kmeans.score(X)
-211.59853725816856
An important improvement to the K-Means algorithm, <i>K-Means++,</i> was proposed in
a 2006 paper by David Arthur and Sergei Vassilvitskii.3 They introduced a smarter
initialization step that tends to select centroids that are distant from one another, and
this improvement makes the K-Means algorithm much less likely to converge to a
suboptimal solution. They showed that the additional computation required for the
smarter initialization step is well worth it because it makes it possible to drastically
reduce the number of times the algorithm needs to be run to find the optimal solu‐
tion. Here is the K-Means++ initialization algorithm:
<b>c(1),</b>
1. Take one centroid chosen uniformly at random from the dataset.
3 DavidArthurandSergeiVassilvitskii,“k-Means++:TheAdvantagesofCarefulSeeding,”Proceedingsofthe
<i>18thAnnualACM-SIAMSymposiumonDiscreteAlgorithms(2007):1027–1035.</i>