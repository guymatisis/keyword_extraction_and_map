The number of Monte Carlo samples you use (100 in this example)
is a hyperparameter you can tweak. The higher it is, the more accu‐
rate the predictions and their uncertainty estimates will be. How‐
ever, if you double it, inference time will also be doubled.
Moreover, above a certain number of samples, you will notice little
improvement. So your job is to find the right trade-off between
latency and accuracy, depending on your application.
If your model contains other layers that behave in a special way during training (such
BatchNormalization
as layers), then you should not force training mode like we just
did. Instead, you should replace the Dropout layers with the following MCDropout
class:27
<b>class</b> <b>MCDropout(keras.layers.Dropout):</b>
<b>def</b> call(self, inputs):
<b>return</b> super().call(inputs, training=True)
Here, we just subclass the Dropout layer and override the call() method to force its
training True MCAlpha
argument to (see Chapter 12). Similarly, you could define an
Dropout AlphaDropout
class by subclassing instead. If you are creating a model from
scratch, it’s just a matter of using MCDropout rather than Dropout . But if you have a
Dropout
model that was already trained using , you need to create a new model that’s
identical to the existing model except that it replaces the Dropout layers with MCDrop
out
, then copy the existing model’s weights to your new model.
In short, MC Dropout is a fantastic technique that boosts dropout models and pro‐
vides better uncertainty estimates. And of course, since it is just regular dropout dur‐
ing training, it also acts like a regularizer.
<header><largefont><b>Max-Norm</b></largefont> <largefont><b>Regularization</b></largefont></header>
Another regularization technique that is popular for neural networks is called <i>max-</i>
<i>norm</i> <i>regularization:</i> for each neuron, it constrains the weights <b>w</b> of the incoming
∥ ∥ ∥ ∥
connections such that <b>w</b> ≤ <i>r,</i> where <i>r</i> is the max-norm hyperparameter and ·
2 2
is the ℓ norm.
2
Max-norm regularization does not add a regularization loss term to the overall loss
function. Instead, it is typically implemented by computing ∥ <b>w</b> ∥ after each training
2
step and rescaling <b>w</b> if needed (w ← <b>w</b> <i>r/</i> ‖ <b>w</b> ‖ ).
2
MCDropout
27 This classwillworkwithallKerasAPIs,includingtheSequentialAPI.Ifyouonlycareaboutthe
FunctionalAPIortheSubclassingAPI,youdonothavetocreateanMCDropoutclass;youcancreatearegular
Dropout training=True
layerandcallitwith .