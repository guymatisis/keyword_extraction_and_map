<header><largefont><b>Cross</b></largefont> <largefont><b>Entropy</b></largefont></header>
Cross entropy originated from information theory. Suppose you want to efficiently
transmit information about the weather every day. If there are eight options (sunny,
rainy, etc.), you could encode each option using three bits because 2 3 = 8. However, if
you think it will be sunny almost every day, it would be much more efficient to code
“sunny” on just one bit (0) and the other seven options on four bits (starting with a
1). Cross entropy measures the average number of bits you actually send per option.
If your assumption about the weather is perfect, cross entropy will be equal to the
entropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump‐
tions are wrong (e.g., if it rains often), cross entropy will be greater by an amount
called the <i>Kullback–Leibler</i> <i>(KL)</i> <i>divergence.</i>
The cross entropy between two probability distributions <i>p</i> and <i>q</i> is defined as <i>H(p,q)</i>
= —Σ <i>p(x)</i> log <i>q(x)</i> (at least when the distributions are discrete). For more details,
<i>x</i>
check out my video on the subject.
<b>θ(k)</b>
The gradient vector of this cost function with regard to is given by Equation 4-23.
<i>Equation</i> <i>4-23.</i> <i>Cross</i> <i>entropy</i> <i>gradient</i> <i>vector</i> <i>for</i> <i>class</i> <i>k</i>
<i>m</i>
1
<i>i</i> <i>i</i> <i>i</i>
<header>∇ <i>J</i> <b>Θ</b> = <largefont>∑</largefont> <i>p</i> − <i>y</i> <b>x</b></header>
<i>k</i> <i>m</i> <i>k</i> <i>k</i>
<b>θ</b> <i>i</i> = 1
Now you can compute the gradient vector for every class, then use Gradient Descent
(or any other optimization algorithm) to find the parameter matrix <b>Θ</b> that minimizes
the cost function.
Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-
LogisticRegression
Learn’s uses one-versus-the-rest by default when you train it on
multi_class "multino
more than two classes, but you can set the hyperparameter to
mial" to switch it to Softmax Regression. You must also specify a solver that supports
"lbfgs"
Softmax Regression, such as the solver (see Scikit-Learn’s documentation for
more details). It also applies ℓ regularization by default, which you can control using
2
the hyperparameter C :
X = iris["data"][:, (2, 3)] <i>#</i> <i>petal</i> <i>length,</i> <i>petal</i> <i>width</i>
y = iris["target"]
softmax_reg = LogisticRegression(multi_class="multinomial",solver="lbfgs", C=10)
softmax_reg.fit(X, y)
So the next time you find an iris with petals that are 5 cm long and 2 cm wide, you
can ask your model to tell you what type of iris it is, and it will answer <i>Iris</i> <i>virginica</i>
(class 2) with 94.2% probability (or <i>Iris</i> <i>versicolor</i> with 5.8% probability):