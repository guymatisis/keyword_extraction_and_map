<i>Figure</i> <i>6-8.</i> <i>Sensitivity</i> <i>to</i> <i>training</i> <i>set</i> <i>details</i>
Random Forests can limit this instability by averaging predictions over many trees, as
we will see in the next chapter.
<header><largefont><b>Exercises</b></largefont></header>
1. What is the approximate depth of a Decision Tree trained (without restrictions)
on a training set with one million instances?
2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it <i>gener‐</i>
<i>ally</i> lower/greater, or <i>always</i> lower/greater?
3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing
max_depth ?
4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling
the input features?
5. If it takes one hour to train a Decision Tree on a training set containing 1 million
instances, roughly how much time will it take to train another Decision Tree on a
training set containing 10 million instances?
6. If your training set contains 100,000 instances, will setting presort=True speed
up training?
7. Train and fine-tune a Decision Tree for the moons dataset by following these
steps:
a. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.
train_test_split()
b. Use to split the dataset into a training set and a test set.