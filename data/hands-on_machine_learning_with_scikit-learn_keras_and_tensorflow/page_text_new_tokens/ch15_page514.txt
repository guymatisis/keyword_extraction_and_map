by the activation function. Finally, it returns the outputs twice (once as the outputs,
and once as the new hidden states). To use this custom cell, all we need to do is create
a keras.layers.RNN layer, passing it a cell instance:
model = keras.models.Sequential([
keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,
input_shape=[None, 1]),
keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),
keras.layers.TimeDistributed(keras.layers.Dense(10))
])
Similarly, you could create a custom cell to apply dropout between each time step. But
there’s a simpler way: all recurrent layers (except for keras.layers.RNN) and all cells
dropout recurrent_dropout
provided by Keras have a hyperparameter and a hyper‐
parameter: the former defines the dropout rate to apply to the inputs (at each time
step), and the latter defines the dropout rate for the hidden states (also at each time
step). No need to create a custom cell to apply dropout at each time step in an RNN.
With these techniques, you can alleviate the unstable gradients problem and train an
RNN much more efficiently. Now let’s look at how to deal with the short-term mem‐
ory problem.
<header><largefont><b>Tackling</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Short-Term</b></largefont> <largefont><b>Memory</b></largefont> <largefont><b>Problem</b></largefont></header>
Due to the transformations that the data goes through when traversing an RNN,
some information is lost at each time step. After a while, the RNN’s state contains vir‐
tually no trace of the first inputs. This can be a showstopper. Imagine Dory the fish6
trying to translate a long sentence; by the time she’s finished reading it, she has no
clue how it started. To tackle this problem, various types of cells with long-term
memory have been introduced. They have proven so successful that the basic cells are
not used much anymore. Let’s first look at the most popular of these long-term mem‐
ory cells: the LSTM cell.
<b>LSTMcells</b>
The <i>Long</i> <i>Short-Term</i> <i>Memory</i> (LSTM) cell was proposed in 1997 7 by Sepp Hochreiter
and Jürgen Schmidhuber and gradually improved over the years by several research‐
ers, such as Alex Graves, Haşim Sak,8 and Wojciech Zaremba.9 If you consider the
6 AcharacterfromtheanimatedmoviesFindingNemoandFindingDorywhohasshort-termmemoryloss.
7 SeppHochreiterandJürgenSchmidhuber,“LongShort-TermMemory,”NeuralComputation9,no.8(1997):
1735–1780.
8 HaşimSaketal.,“LongShort-TermMemoryBasedRecurrentNeuralNetworkArchitecturesforLarge
VocabularySpeechRecognition,”arXivpreprintarXiv:1402.1128(2014).
9 WojciechZarembaetal.,“RecurrentNeuralNetworkRegularization,”arXivpreprintarXiv:1409.2329(2014).