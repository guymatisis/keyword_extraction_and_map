<i>Figure</i> <i>4-10.</i> <i>The</i> <i>first</i> <i>20</i> <i>steps</i> <i>of</i> <i>Stochastic</i> <i>Gradient</i> <i>Descent</i>
Note that since instances are picked randomly, some instances may be picked several
times per epoch, while others may not be picked at all. If you want to be sure that the
algorithm goes through every instance at each epoch, another approach is to shuffle
the training set (making sure to shuffle the input features and the labels jointly), then
go through it instance by instance, then shuffle it again, and so on. However, this
approach generally converges more slowly.
When using Stochastic Gradient Descent, the training instances
must be independent and identically distributed (IID) to ensure
that the parameters get pulled toward the global optimum, on aver‐
age. A simple way to ensure this is to shuffle the instances during
training (e.g., pick each instance randomly, or shuffle the training
set at the beginning of each epoch). If you do not shuffle the
instances—for example, if the instances are sorted by label—then
SGD will start by optimizing for one label, then the next, and so on,
and it will not settle close to the global minimum.
To perform Linear Regression using Stochastic GD with Scikit-Learn, you can use the
SGDRegressor class, which defaults to optimizing the squared error cost function.
The following code runs for maximum 1,000 epochs or until the loss drops by less
max_iter=1000 tol=1e-3
than 0.001 during one epoch ( , ). It starts with a learning rate
of 0.1 (eta0=0.1), using the default learning schedule (different from the preceding
penalty=None
one). Lastly, it does not use any regularization ( ; more details on this
shortly):