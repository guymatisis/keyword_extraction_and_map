On the left is the noisy input image, and on the right is the clean target image. Now
let’s train the classifier and make it clean this image:
knn_clf.fit(X_train_mod, y_train_mod)
clean_digit = knn_clf.predict([X_test_mod[some_index]])
plot_digit(clean_digit)
Looks close enough to the target! This concludes our tour of classification. You
should now know how to select good metrics for classification tasks, pick the appro‐
priate precision/recall trade-off, compare classifiers, and more generally build good
classification systems for a variety of tasks.
<header><largefont><b>Exercises</b></largefont></header>
1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy
on the test set. Hint: the KNeighborsClassifier works quite well for this task;
you just need to find good hyperparameter values (try a grid search on the
weights n_neighbors
and hyperparameters).
2. Write a function that can shift an MNIST image in any direction (left, right, up,
or down) by one pixel. 5 Then, for each image in the training set, create four shif‐
ted copies (one per direction) and add them to the training set. Finally, train your
best model on this expanded training set and measure its accuracy on the test set.
You should observe that your model performs even better now! This technique of
artificially growing the training set is called <i>data</i> <i>augmentation</i> or <i>training</i> <i>set</i>
<i>expansion.</i>
5 Youcanusetheshift()functionfromthescipy.ndimage.interpolationmodule.Forexample,
shift(image, [2, 1], cval=0)
shiftstheimagetwopixelsdownandonepixeltotheright.