<i>Figure</i> <i>16-7.</i> <i>Visual</i> <i>attention:</i> <i>an</i> <i>input</i> <i>image</i> <i>(left)</i> <i>and</i> <i>the</i> <i>model’s</i> <i>focus</i> <i>before</i> <i>produc‐</i>
<i>ing</i> <i>the</i> <i>word</i> <i>“frisbee”</i> <i>(right)18</i>
<header><largefont><b>Explainability</b></largefont></header>
One extra benefit of attention mechanisms is that they make it easier to understand
what led the model to produce its output. This is called <i>explainability.</i> It can be espe‐
cially useful when the model makes a mistake: for example, if an image of a dog walk‐
ing in the snow is labeled as “a wolf walking in the snow,” then you can go back and
check what the model focused on when it output the word “wolf.” You may find that it
was paying attention not only to the dog, but also to the snow, hinting at a possible
explanation: perhaps the way the model learned to distinguish dogs from wolves is by
checking whether or not there’s a lot of snow around. You can then fix this by training
the model with more images of wolves without snow, and dogs with snow. This exam‐
ple comes from a great 2016 paper19 by Marco Tulio Ribeiro et al. that uses a different
approach to explainability: learning an interpretable model locally around a classi‐
fier’s prediction.
In some applications, explainability is not just a tool to debug a model; it can be a
legal requirement (think of a system deciding whether or not it should grant you a
loan).
18 Thisisapartoffigure3fromthepaper.Itisreproducedwiththekindauthorizationoftheauthors.
19 MarcoTulioRibeiroetal.,“‘WhyShouldITrustYou?’:ExplainingthePredictionsofAnyClassifier,”Proceed‐
<i>ingsofthe22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandDataMining(2016):</i>
1135–1144.