class, which can apply different transformers and concatenate their outputs. But you
cannot specify different columns for each transformer; they all apply to the whole
data. It is possible to work around this limitation using a custom transformer for col‐
umn selection (see the Jupyter notebook for an example).
<header><largefont><b>Select</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Train</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Model</b></largefont></header>
At last! You framed the problem, you got the data and explored it, you sampled a
training set and a test set, and you wrote transformation pipelines to clean up and
prepare your data for Machine Learning algorithms automatically. You are now ready
to select and train a Machine Learning model.
<header><largefont><b>Training</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Evaluating</b></largefont> <largefont><b>on</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Set</b></largefont></header>
The good news is that thanks to all these previous steps, things are now going to be
much simpler than you might think. Let’s first train a Linear Regression model, like
we did in the previous chapter:
<b>from</b> <b>sklearn.linear_model</b> <b>import</b> LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)
Done! You now have a working Linear Regression model. Let’s try it out on a few
instances from the training set:
<b>>>></b> some_data = housing.iloc[:5]
<b>>>></b> some_labels = housing_labels.iloc[:5]
<b>>>></b> some_data_prepared = full_pipeline.transform(some_data)
<b>>>></b> <b>print("Predictions:",</b> lin_reg.predict(some_data_prepared))
Predictions: [ 210644.6045 317768.8069 210956.4333 59218.9888 189747.5584]
<b>>>></b> <b>print("Labels:",</b> list(some_labels))
Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]
It works, although the predictions are not exactly accurate (e.g., the first prediction is
off by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐
mean_squared_error()
ing set using Scikit-Learn’s function:
<b>>>></b> <b>from</b> <b>sklearn.metrics</b> <b>import</b> mean_squared_error
<b>>>></b> housing_predictions = lin_reg.predict(housing_prepared)
<b>>>></b> lin_mse = mean_squared_error(housing_labels, housing_predictions)
<b>>>></b> lin_rmse = np.sqrt(lin_mse)
<b>>>></b> lin_rmse
68628.19819848922
This is better than nothing, but clearly not a great score: most districts’ median_hous
ing_values
range between $120,000 and $265,000, so a typical prediction error of
$68,628 is not very satisfying. This is an example of a model underfitting the training
data. When this happens it can mean that the features do not provide enough