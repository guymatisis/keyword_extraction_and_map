get boosted. The second classifier therefore does a better job on these instances, and
so on. The plot on the right represents the same sequence of predictors, except that
the learning rate is halved (i.e., the misclassified instance weights are boosted half as
much at every iteration). As you can see, this sequential learning technique has some
similarities with Gradient Descent, except that instead of tweaking a single predictor’s
parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,
gradually making it better.
<i>Figure</i> <i>7-8.</i> <i>Decision</i> <i>boundaries</i> <i>of</i> <i>consecutive</i> <i>predictors</i>
Once all predictors are trained, the ensemble makes predictions very much like bag‐
ging or pasting, except that predictors have different weights depending on their
overall accuracy on the weighted training set.
There is one important drawback to this sequential learning techni‐
que: it cannot be parallelized (or only partially), since each predic‐
tor can only be trained after the previous predictor has been
trained and evaluated. As a result, it does not scale as well as bag‐
ging or pasting.
Let’s take a closer look at the AdaBoost algorithm. Each instance weight <i>w</i> (i) is initially
set to 1/m. A first predictor is trained, and its weighted error rate <i>r</i> is computed on
1
the training set; see Equation 7-1.
<i>jth</i>
<i>Equation</i> <i>7-1.</i> <i>Weighted</i> <i>error</i> <i>rate</i> <i>of</i> <i>the</i> <i>predictor</i>
<i>m</i>
<largefont>∑</largefont> <i>i</i>
<i>w</i>
<i>i</i> = 1
<i>i</i> <i>i</i>
<i>y</i> ≠ <i>y</i>
<i>j</i> <i>i</i> th th
<i>r</i> = where <i>y</i> is the <i>j</i> predictor’s prediction for the <i>i</i> instance.
<i>j</i> <i>m</i> <i>j</i>
<i>i</i>
<largefont>∑</largefont> <i>w</i>
<i>i</i> = 1