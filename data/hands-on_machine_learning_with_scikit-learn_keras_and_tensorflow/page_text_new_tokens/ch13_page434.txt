the categories. This is called <i>representation</i> <i>learning</i> (we will see other types of repre‐
sentation learning in Chapter 17).
<i>Figure</i> <i>13-4.</i> <i>Embeddings</i> <i>will</i> <i>gradually</i> <i>improve</i> <i>during</i> <i>training</i>
<header><largefont><b>Word</b></largefont> <largefont><b>Embeddings</b></largefont></header>
Not only will embeddings generally be useful representations for the task at hand, but
quite often these same embeddings can be reused successfully for other tasks. The
most common example of this is <i>word</i> <i>embeddings</i> (i.e., embeddings of individual
words): when you are working on a natural language processing task, you are often
better off reusing pretrained word embeddings than training your own.
The idea of using vectors to represent words dates back to the 1960s, and many
sophisticated techniques have been used to generate useful vectors, including using
neural networks. But things really took off in 2013, when Tomáš Mikolov and other
paper9
Google researchers published a describing an efficient technique to learn word
embeddings using neural networks, significantly outperforming previous attempts.
This allowed them to learn embeddings on a very large corpus of text: they trained a
neural network to predict the words near any given word, and obtained astounding
word embeddings. For example, synonyms had very close embeddings, and semanti‐
cally related words such as France, Spain, and Italy ended up clustered together.
It’s not just about proximity, though: word embeddings were also organized along
meaningful axes in the embedding space. Here is a famous example: if you compute
King – Man + Woman (adding and subtracting the embedding vectors of these
words), then the result will be very close to the embedding of the word Queen (see
Figure 13-5). In other words, the word embeddings encode the concept of gender!
9 TomasMikolovetal.,“DistributedRepresentationsofWordsandPhrasesandTheirCompositionality,”Pro‐
<i>ceedingsofthe26thInternationalConferenceonNeuralInformationProcessingSystems2(2013):3111–3119.</i>