<i>Figure</i> <i>4-6.</i> <i>Gradient</i> <i>Descent</i> <i>pitfalls</i>
Fortunately, the MSE cost function for a Linear Regression model happens to be a
<i>convex</i> <i>function,</i> which means that if you pick any two points on the curve, the line
segment joining them never crosses the curve. This implies that there are no local
minima, just one global minimum. It is also a continuous function with a slope that
never changes abruptly.3 These two facts have a great consequence: Gradient Descent
is guaranteed to approach arbitrarily close the global minimum (if you wait long
enough and if the learning rate is not too high).
In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if
the features have very different scales. Figure 4-7 shows Gradient Descent on a train‐
ing set where features 1 and 2 have the same scale (on the left), and on a training set
where feature 1 has much smaller values than feature 2 (on the right).4
<i>Figure</i> <i>4-7.</i> <i>Gradient</i> <i>Descent</i> <i>with</i> <i>(left)</i> <i>and</i> <i>without</i> <i>(right)</i> <i>feature</i> <i>scaling</i>
3 Technicallyspeaking,itsderivativeisLipschitzcontinuous.
4 Sincefeature1issmaller,ittakesalargerchangeinθ 1 toaffectthecostfunction,whichiswhythebowlis
elongatedalongtheθ axis.
1