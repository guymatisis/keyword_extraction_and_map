<i>Figure</i> <i>19-18.</i> <i>Data</i> <i>parallelism</i> <i>using</i> <i>the</i> <i>mirrored</i> <i>strategy</i>
The tricky part when using this approach is to efficiently compute the mean of all the
gradients from all the GPUs and distribute the result across all the GPUs. This can be
done using an <i>AllReduce</i> algorithm, a class of algorithms where multiple nodes col‐
laborate to efficiently perform a reduce operation (such as computing the mean, sum,
and max), while ensuring that all nodes obtain the same final result. Fortunately,
there are off-the-shelf implementations of such algorithms, as we will see.
<b>Dataparallelismwithcentralizedparameters</b>
Another approach is to store the model parameters outside of the GPU devices per‐
forming the computations (called <i>workers),</i> for example on the CPU (see
Figure 19-19). In a distributed setup, you may place all the parameters on one or
more CPU-only servers called <i>parameter</i> <i>servers,</i> whose only role is to host and
update the parameters.