normalize all these discounted rewards (returns) across many episodes by subtracting
the mean and dividing by the standard deviation:
<b>def</b> discount_rewards(rewards, discount_factor):
discounted = np.array(rewards)
<b>for</b> step <b>in</b> range(len(rewards) - 2, -1, -1):
discounted[step] += discounted[step + 1] * discount_factor
<b>return</b> discounted
<b>def</b> discount_and_normalize_rewards(all_rewards, discount_factor):
all_discounted_rewards = [discount_rewards(rewards, discount_factor)
<b>for</b> rewards <b>in</b> all_rewards]
flat_rewards = np.concatenate(all_discounted_rewards)
reward_mean = flat_rewards.mean()
reward_std = flat_rewards.std()
<b>return</b> [(discounted_rewards - reward_mean) / reward_std
<b>for</b> discounted_rewards <b>in</b> all_discounted_rewards]
Let’s check that this works:
<b>>>></b> discount_rewards([10, 0, -50], discount_factor=0.8)
array([-22, -40, -50])
<b>>>></b> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],
<b>...</b> discount_factor=0.8)
<b>...</b>
[array([-0.28435071, -0.86597718, -1.18910299]),
array([1.26665318, 1.0727777 ])]
discount_rewards()
The call to returns exactly what we expect (see Figure 18-6).
You can verify that the function discount_and_normalize_rewards() does indeed
return the normalized action advantages for each action in both episodes. Notice that
the first episode was much worse than the second, so its normalized advantages are
all negative; all actions from the first episode would be considered bad, and con‐
versely all actions from the second episode would be considered good.
We are almost ready to run the algorithm! Now let’s define the hyperparameters. We
will run 150 training iterations, playing 10 episodes per iteration, and each episode
will last at most 200 steps. We will use a discount factor of 0.95:
n_iterations = 150
n_episodes_per_update = 10
n_max_steps = 200
discount_factor = 0.95
We also need an optimizer and the loss function. A regular Adam optimizer with
learning rate 0.01 will do just fine, and we will use the binary cross-entropy loss func‐
tion because we are training a binary classifier (there are two possible actions: left or
right):
optimizer = keras.optimizers.Adam(lr=0.01)
loss_fn = keras.losses.binary_crossentropy