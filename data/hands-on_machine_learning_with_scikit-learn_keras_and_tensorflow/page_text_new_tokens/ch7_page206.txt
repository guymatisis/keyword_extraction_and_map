<i>Figure</i> <i>7-10.</i> <i>GBRT</i> <i>ensembles</i> <i>with</i> <i>not</i> <i>enough</i> <i>predictors</i> <i>(left)</i> <i>and</i> <i>too</i> <i>many</i> <i>(right)</i>
In order to find the optimal number of trees, you can use early stopping (see Chap‐
staged_predict()
ter 4). A simple way to implement this is to use the method: it
returns an iterator over the predictions made by the ensemble at each stage of train‐
ing (with one tree, two trees, etc.). The following code trains a GBRT ensemble with
120 trees, then measures the validation error at each stage of training to find the opti‐
mal number of trees, and finally trains another GBRT ensemble using the optimal
number of trees:
<b>import</b> <b>numpy</b> <b>as</b> <b>np</b>
<b>from</b> <b>sklearn.model_selection</b> <b>import</b> train_test_split
<b>from</b> <b>sklearn.metrics</b> <b>import</b> mean_squared_error
X_train, X_val, y_train, y_val = train_test_split(X, y)
gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)
gbrt.fit(X_train, y_train)
errors = [mean_squared_error(y_val, y_pred)
<b>for</b> y_pred <b>in</b> gbrt.staged_predict(X_val)]
bst_n_estimators = np.argmin(errors) + 1
gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)
gbrt_best.fit(X_train, y_train)
The validation errors are represented on the left of Figure 7-11, and the best model’s
predictions are represented on the right.