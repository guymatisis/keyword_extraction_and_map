<i>m</i> <i>j</i>
space, then we want the squared distance between <b>z</b> (i) and ∑ <i>w</i> <b>z</b> to be as small
<i>j</i> = 1 <i>i,</i> <i>j</i>
as possible. This idea leads to the unconstrained optimization problem described in
Equation 8-5. It looks very similar to the first step, but instead of keeping the instan‐
ces fixed and finding the optimal weights, we are doing the reverse: keeping the
weights fixed and finding the optimal position of the instances’ images in the low-
dimensional space. Note that <b>Z</b> is the matrix containing all <b>z(i).</b>
<i>Equation</i> <i>8-5.</i> <i>LLE</i> <i>step</i> <i>two:</i> <i>reducing</i> <i>dimensionality</i> <i>while</i> <i>preserving</i> <i>relationships</i>
<i>m</i> <i>m</i> 2
<i>i</i> <i>j</i>
<header><b>Z</b> = argmin <largefont>∑</largefont> <b>z</b> − <largefont>∑</largefont> <i>w</i> <b>z</b></header>
<i>i,</i> <i>j</i>
<i>i</i> = 1 <i>j</i> = 1
<b>Z</b>
Scikit-Learn’s LLE implementation has the following computational complexity:
<i>O(m</i> log(m)n log(k)) for finding the <i>k</i> nearest neighbors, <i>O(mnk3)</i> for optimizing the
weights, and <i>O(dm</i> 2 ) for constructing the low-dimensional representations. Unfortu‐
nately, the <i>m</i> 2 in the last term makes this algorithm scale poorly to very large datasets.
<header><largefont><b>Other</b></largefont> <largefont><b>Dimensionality</b></largefont> <largefont><b>Reduction</b></largefont> <largefont><b>Techniques</b></largefont></header>
There are many other dimensionality reduction techniques, several of which are
available in Scikit-Learn. Here are some of the most popular ones:
<i>Random</i> <i>Projections</i>
As its name suggests, projects the data to a lower-dimensional space using a ran‐
dom linear projection. This may sound crazy, but it turns out that such a random
projection is actually very likely to preserve distances well, as was demonstrated
mathematically by William B. Johnson and Joram Lindenstrauss in a famous
lemma. The quality of the dimensionality reduction depends on the number of
instances and the target dimensionality, but surprisingly not on the initial dimen‐
sionality. Check out the documentation for the sklearn.random_projection
package for more details.
<i>Multidimensional</i> <i>Scaling</i> <i>(MDS)</i>
Reduces dimensionality while trying to preserve the distances between the
instances.