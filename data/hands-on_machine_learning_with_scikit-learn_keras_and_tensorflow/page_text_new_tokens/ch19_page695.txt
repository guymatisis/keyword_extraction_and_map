each ID always refers to the same GPU card. For example, if you have four GPU
cards, you could start two programs, assigning two GPUs to each of them, by execut‐
ing commands like the following in two separate terminal windows:
$ <b>CUDA_DEVICE_ORDER=PCI_BUS_ID</b> <b>CUDA_VISIBLE_DEVICES=0,1</b> <b>python3</b> <b>program_1.py</b>
# and in another terminal:
$ <b>CUDA_DEVICE_ORDER=PCI_BUS_ID</b> <b>CUDA_VISIBLE_DEVICES=3,2</b> <b>python3</b> <b>program_2.py</b>
/gpu:0 /gpu:1
Program 1 will then only see GPU cards 0 and 1, named and respec‐
tively, and program 2 will only see GPU cards 2 and 3, named /gpu:1 and /gpu:0
respectively (note the order). Everything will work fine (see Figure 19-12). Of course,
os.envi
you can also define these environment variables in Python by setting
ron["CUDA_DEVICE_ORDER"] and os.environ["CUDA_VISIBLE_DEVICES"] , as long as
you do so before using TensorFlow.
<i>Figure</i> <i>19-12.</i> <i>Each</i> <i>program</i> <i>gets</i> <i>two</i> <i>GPUs</i>
Another option is to tell TensorFlow to grab only a specific amount of GPU RAM.
This must be done immediately after importing TensorFlow. For example, to make
TensorFlow grab only 2 GiB of RAM on each GPU, you must create a <i>virtual</i> <i>GPU</i>
<i>device</i> (also called a <i>logical</i> <i>GPU</i> <i>device)</i> for each physical GPU device and set its
memory limit to 2 GiB (i.e., 2,048 MiB):
<b>for</b> gpu <b>in</b> tf.config.experimental.list_physical_devices("GPU"):
tf.config.experimental.set_virtual_device_configuration(
gpu,
[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])
Now (supposing you have four GPUs, each with at least 4 GiB of RAM) two programs
like this one can run in parallel, each using all four GPU cards (see Figure 19-13).