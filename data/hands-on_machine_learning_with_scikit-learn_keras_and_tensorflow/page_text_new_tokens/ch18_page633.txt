Alternatively, rather than relying only on chance for exploration, another approach is
to encourage the exploration policy to try actions that it has not tried much before.
This can be implemented as a bonus added to the Q-Value estimates, as shown in
Equation 18-6.
<i>Equation</i> <i>18-6.</i> <i>Q-Learning</i> <i>using</i> <i>an</i> <i>exploration</i> <i>function</i>
′ ′ ′ ′
<i>Q</i> <i>s,a</i> <i>r</i> + <i>γ</i> · max <i>f</i> <i>Q</i> <i>s</i> ,a ,N <i>s</i> ,a
<i>α</i>
<i>a</i> ′
In this equation:
<i>N(s′,</i> <i>a′)</i> <i>a′</i> <i>s′.</i>
• counts the number of times the action was chosen in state
• <i>f(Q,</i> <i>N)</i> is an <i>exploration</i> <i>function,</i> such as <i>f(Q,</i> <i>N)</i> = <i>Q</i> + <i>κ/(1</i> + <i>N),</i> where <i>κ</i> is a
curiosity hyperparameter that measures how much the agent is attracted to the
unknown.
<header><largefont><b>Approximate</b></largefont> <largefont><b>Q-Learning</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Q-Learning</b></largefont></header>
The main problem with Q-Learning is that it does not scale well to large (or even
medium) MDPs with many states and actions. For example, suppose you wanted to
use Q-Learning to train an agent to play <i>Ms.</i> <i>Pac-Man</i> (see Figure 18-1). There are
about 150 pellets that Ms. Pac-Man can eat, each of which can be present or absent
(i.e., already eaten). So, the number of possible states is greater than 2150 ≈ 1045. And if
you add all the possible combinations of positions for all the ghosts and Ms. Pac-
Man, the number of possible states becomes larger than the number of atoms in our
planet, so there’s absolutely no way you can keep track of an estimate for every single
Q-Value.
The solution is to find a function <i>Q</i> (s, <i>a)</i> that approximates the Q-Value of any state-
<b>θ</b>
action pair (s, <i>a)</i> using a manageable number of parameters (given by the parameter
vector <b>θ).</b> This is called <i>Approximate</i> <i>Q-Learning.</i> For years it was recommended to
use linear combinations of handcrafted features extracted from the state (e.g., dis‐
tance of the closest ghosts, their directions, and so on) to estimate Q-Values, but in
2013, DeepMind showed that using deep neural networks can work much better,
especially for complex problems, and it does not require any feature engineering. A
DNN used to estimate Q-Values is called a <i>Deep</i> <i>Q-Network</i> (DQN), and using a
DQN for Approximate Q-Learning is called <i>Deep</i> <i>Q-Learning.</i>
Now, how can we train a DQN? Well, consider the approximate Q-Value computed
by the DQN for a given state-action pair (s, <i>a).</i> Thanks to Bellman, we know we want
this approximate Q-Value to be as close as possible to the reward <i>r</i> that we actually
observe after playing action <i>a</i> in state <i>s,</i> plus the discounted value of playing optimally