Figure 6-2 shows this Decision Tree’s decision boundaries. The thick vertical line rep‐
resents the decision boundary of the root node (depth 0): petal length = 2.45 cm.
Since the lefthand area is pure (only <i>Iris</i> <i>setosa),</i> it cannot be split any further. How‐
ever, the righthand area is impure, so the depth-1 right node splits it at petal width =
max_depth
1.75 cm (represented by the dashed line). Since was set to 2, the Decision
Tree stops right there. If you set max_depth to 3, then the two depth-2 nodes would
each add another decision boundary (represented by the dotted lines).
<i>Figure</i> <i>6-2.</i> <i>Decision</i> <i>Tree</i> <i>decision</i> <i>boundaries</i>
<header><largefont><b>Model</b></largefont> <largefont><b>Interpretation:</b></largefont> <largefont><b>White</b></largefont> <largefont><b>Box</b></largefont> <largefont><b>Versus</b></largefont> <largefont><b>Black</b></largefont> <largefont><b>Box</b></largefont></header>
Decision Trees are intuitive, and their decisions are easy to interpret. Such models are
often called <i>white</i> <i>box</i> <i>models.</i> In contrast, as we will see, Random Forests or neural
networks are generally considered <i>black</i> <i>box</i> <i>models.</i> They make great predictions,
and you can easily check the calculations that they performed to make these predic‐
tions; nevertheless, it is usually hard to explain in simple terms why the predictions
were made. For example, if a neural network says that a particular person appears on
a picture, it is hard to know what contributed to this prediction: did the model recog‐
nize that person’s eyes? Their mouth? Their nose? Their shoes? Or even the couch
that they were sitting on? Conversely, Decision Trees provide nice, simple classifica‐
tion rules that can even be applied manually if need be (e.g., for flower classification).
<header><largefont><b>Estimating</b></largefont> <largefont><b>Class</b></largefont> <largefont><b>Probabilities</b></largefont></header>
A Decision Tree can also estimate the probability that an instance belongs to a partic‐
ular class <i>k.</i> First it traverses the tree to find the leaf node for this instance, and then it
returns the ratio of training instances of class <i>k</i> in this node. For example, suppose
you have found a flower whose petals are 5 cm long and 1.5 cm wide. The