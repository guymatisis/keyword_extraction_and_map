get_next()
For our main training loop, instead of calling the method, we will use a
tf.data.Dataset . This way, we can benefit from the power of the Data API (e.g., par‐
as_dataset()
allelism and prefetching). For this, we call the replay buffer’s method:
dataset = replay_buffer.as_dataset(
sample_batch_size=64,
num_steps=2,
num_parallel_calls=3).prefetch(3)
We will sample batches of 64 trajectories at each training step (as in the 2015 DQN
paper), each with 2 steps (i.e., 2 steps = 1 full transition, including the next step’s
observation). This dataset will process three elements in parallel, and prefetch three
batches.
For on-policy algorithms such as Policy Gradients, each experience
should be sampled once, used from training, and then discarded. In
this case, you can still use a replay buffer, but instead of using a
Dataset, gather_all()
you would call the replay buffer’s method
at each training iteration to get a tensor containing all the trajecto‐
ries recorded so far, then use them to perform a training step, and
clear()
finally clear the replay buffer by calling its method.
Now that we have all the components in place, we are ready to train the model!
<header><largefont><b>Creating</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Loop</b></largefont></header>
To speed up training, we will convert the main functions to TensorFlow Functions.
For this we will use the tf_agents.utils.common.function() function, which wraps
tf.function() , with some extra experimental options:
<b>from</b> <b>tf_agents.utils.common</b> <b>import</b> function
collect_driver.run = function(collect_driver.run)
agent.train = function(agent.train)
Let’s create a small function that will run the main training loop for n_iterations :
<b>def</b> train_agent(n_iterations):
time_step = None
policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)
iterator = iter(dataset)
<b>for</b> iteration <b>in</b> range(n_iterations):
time_step, policy_state = collect_driver.run(time_step, policy_state)
trajectories, buffer_info = next(iterator)
train_loss = agent.train(trajectories)
<b>print("\r{}</b> loss:{:.5f}".format(
iteration, train_loss.loss.numpy()), end="")
<b>if</b> iteration % 1000 == 0:
log_metrics(train_metrics)