<i>Figure</i> <i>7-15.</i> <i>Predictions</i> <i>in</i> <i>a</i> <i>multilayer</i> <i>stacking</i> <i>ensemble</i>
Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard
to roll out your own implementation (see the following exercises). Alternatively, you
DESlib
can use an open source implementation such as .
<header><largefont><b>Exercises</b></largefont></header>
1. If you have trained five different models on the exact same training data, and
they all achieve 95% precision, is there any chance that you can combine these
models to get better results? If so, how? If not, why?
2. What is the difference between hard and soft voting classifiers?
3. Is it possible to speed up training of a bagging ensemble by distributing it across
multiple servers? What about pasting ensembles, boosting ensembles, Random
Forests, or stacking ensembles?
4. What is the benefit of out-of-bag evaluation?
5. What makes Extra-Trees more random than regular Random Forests? How can
this extra randomness help? Are Extra-Trees slower or faster than regular Ran‚Äê
dom Forests?
6. If your AdaBoost ensemble underfits the training data, which hyperparameters
should you tweak and how?