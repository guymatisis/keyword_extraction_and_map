<b>def</b> training_step(batch_size):
experiences = sample_experiences(batch_size)
states, actions, rewards, next_states, dones = experiences
next_Q_values = model.predict(next_states)
max_next_Q_values = np.max(next_Q_values, axis=1)
target_Q_values = (rewards +
(1 - dones) * discount_factor * max_next_Q_values)
mask = tf.one_hot(actions, n_outputs)
<b>with</b> tf.GradientTape() <b>as</b> tape:
all_Q_values = model(states)
Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)
loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))
grads = tape.gradient(loss, model.trainable_variables)
optimizer.apply_gradients(zip(grads, model.trainable_variables))
Let’s go through this code:
• First we define some hyperparameters, and we create the optimizer and the loss
function.
training_step()
• Then we create the function. It starts by sampling a batch of
experiences, then it uses the DQN to predict the Q-Value for each possible action
in each experience’s next state. Since we assume that the agent will be playing
optimally, we only keep the maximum Q-Value for each next state. Next, we use
Equation 18-7 to compute the target Q-Value for each experience’s state-action
pair.
• Next, we want to use the DQN to compute the Q-Value for each experienced
state-action pair. However, the DQN will also output the Q-Values for the other
possible actions, not just for the action that was actually chosen by the agent. So
we need to mask out all the Q-Values we do not need. The tf.one_hot() func‐
tion makes it easy to convert an array of action indices into such a mask. For
example, if the first three experiences contain actions 1, 1, 0, respectively, then
[[0, 1], [0, 1], [1, 0],...]
the mask will start with . We can then multiply
the DQN’s output with this mask, and this will zero out all the Q-Values we do
not want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-
Q_values
Values of the experienced state-action pairs. This gives us the tensor,
containing one predicted Q-Value for each experience in the batch.
• Then we compute the loss: it is the mean squared error between the target and
predicted Q-Values for the experienced state-action pairs.
• Finally, we perform a Gradient Descent step to minimize the loss with regard to
the model’s trainable variables.
This was the hardest part. Now training the model is straightforward: