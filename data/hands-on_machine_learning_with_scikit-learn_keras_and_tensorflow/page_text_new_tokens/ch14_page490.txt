• It outputs five bounding boxes for each grid cell (instead of just one), and each
bounding box comes with an objectness score. It also outputs 20 class probabili‐
ties per grid cell, as it was trained on the PASCAL VOC dataset, which contains
20 classes. That’s a total of 45 numbers per grid cell: 5 bounding boxes, each with
4 coordinates, plus 5 objectness scores, plus 20 class probabilities.
• Instead of predicting the absolute coordinates of the bounding box centers,
YOLOv3 predicts an offset relative to the coordinates of the grid cell, where (0, 0)
means the top left of that cell and (1, 1) means the bottom right. For each grid
cell, YOLOv3 is trained to predict only bounding boxes whose center lies in that
cell (but the bounding box itself generally extends well beyond the grid cell).
YOLOv3 applies the logistic activation function to the bounding box coordinates
to ensure they remain in the 0 to 1 range.
• Before training the neural net, YOLOv3 finds five representative bounding box
dimensions, called <i>anchor</i> <i>boxes</i> (or <i>bounding</i> <i>box</i> <i>priors).</i> It does this by applying
the K-Means algorithm (see Chapter 9) to the height and width of the training set
bounding boxes. For example, if the training images contain many pedestrians,
then one of the anchor boxes will likely have the dimensions of a typical pedes‐
trian. Then when the neural net predicts five bounding boxes per grid cell, it
actually predicts how much to rescale each of the anchor boxes. For example,
suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network
predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for
one of the grid cells). This will result in a predicted bounding box of size 150 × 45
pixels. To be more precise, for each grid cell and each anchor box, the network
predicts the log of the vertical and horizontal rescaling factors. Having these pri‐
ors makes the network more likely to predict bounding boxes of the appropriate
dimensions, and it also speeds up training because it will more quickly learn what
reasonable bounding boxes look like.
• The network is trained using images of different scales: every few batches during
training, the network randomly chooses a new image dimension (from 330 × 330
to 608 × 608 pixels). This allows the network to learn to detect objects at different
scales. Moreover, it makes it possible to use YOLOv3 at different scales: the
smaller scale will be less accurate but faster than the larger scale, so you can
choose the right trade-off for your use case.
There are a few more innovations you might be interested in, such as the use of skip
connections to recover some of the spatial resolution that is lost in the CNN (we will
discuss this shortly, when we look at semantic segmentation). In the 2016 paper, the
authors introduce the YOLO9000 model that uses hierarchical classification: the
model predicts a probability for each node in a visual hierarchy called <i>WordTree.</i> This
makes it possible for the network to predict with high confidence that an image rep‐
resents, say, a dog, even though it is unsure what specific type of dog. I encourage you