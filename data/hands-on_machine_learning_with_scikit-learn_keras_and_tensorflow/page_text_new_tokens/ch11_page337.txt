The ELU activation function looks a lot like the ReLU function, with a few major
differences:
• It takes on negative values when <i>z</i> < 0, which allows the unit to have an average
output closer to 0 and helps alleviate the vanishing gradients problem. The
hyperparameter <i>α</i> defines the value that the ELU function approaches when <i>z</i> is a
large negative number. It is usually set to 1, but you can tweak it like any other
hyperparameter.
• It has a nonzero gradient for <i>z</i> < 0, which avoids the dead neurons problem.
• If <i>α</i> is equal to 1 then the function is smooth everywhere, including around <i>z</i> = 0,
which helps speed up Gradient Descent since it does not bounce as much to the
left and right of <i>z</i> = 0.
The main drawback of the ELU activation function is that it is slower to compute
than the ReLU function and its variants (due to the use of the exponential function).
Its faster convergence rate during training compensates for that slow computation,
but still, at test time an ELU network will be slower than a ReLU network.
Then, a 2017 paper7 by Günter Klambauer et al. introduced the Scaled ELU (SELU)
activation function: as its name suggests, it is a scaled variant of the ELU activation
function. The authors showed that if you build a neural network composed exclu‐
sively of a stack of dense layers, and if all hidden layers use the SELU activation func‐
tion, then the network will <i>self-normalize:</i> the output of each layer will tend to
preserve a mean of 0 and standard deviation of 1 during training, which solves the
vanishing/exploding gradients problem. As a result, the SELU activation function
often significantly outperforms other activation functions for such neural nets (espe‐
cially deep ones). There are, however, a few conditions for self-normalization to hap‐
pen (see the paper for the mathematical justification):
• The input features must be standardized (mean 0 and standard deviation 1).
• Every hidden layer’s weights must be initialized with LeCun normal initialization.
In Keras, this means setting kernel_initializer="lecun_normal".
• The network’s architecture must be sequential. Unfortunately, if you try to use
SELU in nonsequential architectures, such as recurrent networks (see Chap‐
ter 15) or networks with <i>skip</i> <i>connections</i> (i.e., connections that skip layers, such
as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will
not necessarily outperform other activation functions.
7 GünterKlambaueretal.,“Self-NormalizingNeuralNetworks,”Proceedingsofthe31stInternationalConference
<i>onNeuralInformationProcessingSystems(2017):972–981.</i>