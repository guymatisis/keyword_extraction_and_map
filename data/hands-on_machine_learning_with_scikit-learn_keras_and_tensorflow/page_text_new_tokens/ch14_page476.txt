Separable convolutional layers use fewer parameters, less memory,
and fewer computations than regular convolutional layers, and in
general they even perform better, so you should consider using
them by default (except after layers with few channels).
The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐
versity of Hong Kong. They used an ensemble of many different techniques, includ‐
ing a sophisticated object-detection system called GBD-Net,21 to achieve a top-five
error rate below 3%. Although this result is unquestionably impressive, the complex‐
ity of the solution contrasted with the simplicity of ResNets. Moreover, one year later
another fairly simple architecture performed even better, as we will see now.
<header><largefont><b>SENet</b></largefont></header>
The winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-
Excitation Network (SENet).22 This architecture extends existing architectures such as
inception networks and ResNets, and boosts their performance. This allowed SENet
to win the competition with an astonishing 2.25% top-five error rate! The extended
versions of inception networks and ResNets are called <i>SE-Inception</i> and <i>SE-ResNet,</i>
respectively. The boost comes from the fact that a SENet adds a small neural network,
called an <i>SE</i> <i>block,</i> to every unit in the original architecture (i.e., every inception
module or every residual unit), as shown in Figure 14-20.
<i>Figure</i> <i>14-20.</i> <i>SE-Inception</i> <i>module</i> <i>(left)</i> <i>and</i> <i>SE-ResNet</i> <i>unit</i> <i>(right)</i>
21 XingyuZengetal.,“CraftingGBD-NetforObjectDetection,”IEEETransactionsonPatternAnalysisand
<i>MachineIntelligence40,no.9(2018):2109–2123.</i>
22 JieHuetal.,“Squeeze-and-ExcitationNetworks,”ProceedingsoftheIEEEConferenceonComputerVisionand
<i>PatternRecognition(2018):7132–7141.</i>