⋯
The Lasso cost function is not differentiable at <i>θ</i> = 0 (for <i>i</i> = 1, 2, , <i>n),</i> but Gradient
<i>i</i>
Descent still works fine if you use a <i>subgradient</i> <i>vector</i> <b>g13</b> instead when any <i>θ</i> = 0.
<i>i</i>
Equation 4-11 shows a subgradient vector equation you can use for Gradient Descent
with the Lasso cost function.
<i>Equation</i> <i>4-11.</i> <i>Lasso</i> <i>Regression</i> <i>subgradient</i> <i>vector</i>
sign <i>θ</i>
1
−1 if <i>θ</i> < 0
<i>i</i>
sign <i>θ</i>
2
<i>g</i> <b>θ,J</b> = ∇ MSE <b>θ</b> + <i>α</i> where sign <i>θ</i> = 0 if <i>θ</i> = 0
<i>i</i>
<b>θ</b> <i>i</i>
⋮
+1 if <i>θ</i> > 0
<i>i</i>
sign <i>θ</i>
<i>n</i>
Here is a small Scikit-Learn example using the Lasso class:
<b>>>></b> <b>from</b> <b>sklearn.linear_model</b> <b>import</b> Lasso
<b>>>></b> lasso_reg = Lasso(alpha=0.1)
<b>>>></b> lasso_reg.fit(X, y)
<b>>>></b> lasso_reg.predict([[1.5]])
array([1.53788174])
SGDRegressor(penalty="l1")
Note that you could instead use .
<header><largefont><b>Elastic</b></largefont> <largefont><b>Net</b></largefont></header>
Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The
regularization term is a simple mix of both Ridge and Lasso’s regularization terms,
and you can control the mix ratio <i>r.</i> When <i>r</i> = 0, Elastic Net is equivalent to Ridge
Regression, and when <i>r</i> = 1, it is equivalent to Lasso Regression (see Equation 4-12).
<i>Equation</i> <i>4-12.</i> <i>Elastic</i> <i>Net</i> <i>cost</i> <i>function</i>
<i>n</i> 1−r <i>n</i> 2
<i>J</i> <b>θ</b> = MSE <b>θ</b> + <i>rα∑</i> <i>θ</i> + <i>α∑</i> <i>θ</i>
<i>i</i> = 1 <i>i</i> 2 <i>i</i> = 1 <i>i</i>
So when should you use plain Linear Regression (i.e., without any regularization),
Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of
regularization, so generally you should avoid plain Linear Regression. Ridge is a good
default, but if you suspect that only a few features are useful, you should prefer Lasso
or Elastic Net because they tend to reduce the useless features’ weights down to zero,
as we have discussed. In general, Elastic Net is preferred over Lasso because Lasso
13 Youcanthinkofasubgradientvectoratanondifferentiablepointasanintermediatevectorbetweenthegra‐
dientvectorsaroundthatpoint.