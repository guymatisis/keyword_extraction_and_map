dropout_encoder = keras.models.Sequential([
keras.layers.Flatten(input_shape=[28, 28]),
keras.layers.Dropout(0.5),
keras.layers.Dense(100, activation="selu"),
keras.layers.Dense(30, activation="selu")
])
dropout_decoder = keras.models.Sequential([
keras.layers.Dense(100, activation="selu", input_shape=[30]),
keras.layers.Dense(28 * 28, activation="sigmoid"),
keras.layers.Reshape([28, 28])
])
dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])
Figure 17-9 shows a few noisy images (with half the pixels turned off), and the
images reconstructed by the dropout-based denoising autoencoder. Notice how the
autoencoder guesses details that are actually not in the input, such as the top of the
white shirt (bottom row, fourth image). As you can see, not only can denoising
autoencoders be used for data visualization or unsupervised pretraining, like the
other autoencoders we’ve discussed so far, but they can also be used quite simply and
efficiently to remove noise from images.
<i>Figure</i> <i>17-9.</i> <i>Noisy</i> <i>images</i> <i>(top)</i> <i>and</i> <i>their</i> <i>reconstructions</i> <i>(bottom)</i>
<header><largefont><b>Sparse</b></largefont> <largefont><b>Autoencoders</b></largefont></header>
Another kind of constraint that often leads to good feature extraction is <i>sparsity:</i> by
adding an appropriate term to the cost function, the autoencoder is pushed to reduce
the number of active neurons in the coding layer. For example, it may be pushed to
have on average only 5% significantly active neurons in the coding layer. This forces
the autoencoder to represent each input as a combination of a small number of acti‐
vations. As a result, each neuron in the coding layer typically ends up representing a
useful feature (if you could speak only a few words per month, you would probably
try to make them worth listening to).
A simple approach is to use the sigmoid activation function in the coding layer (to
constrain the codings to values between 0 and 1), use a large coding layer (e.g., with