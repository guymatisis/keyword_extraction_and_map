As you can see, on the left the Gradient Descent algorithm goes straight toward the
minimum, thereby reaching it quickly, whereas on the right it first goes in a direction
almost orthogonal to the direction of the global minimum, and it ends with a long
march down an almost flat valley. It will eventually reach the minimum, but it will
take a long time.
When using Gradient Descent, you should ensure that all features
StandardScaler
have a similar scale (e.g., using Scikit-Learn’s
class), or else it will take much longer to converge.
This diagram also illustrates the fact that training a model means searching for a
combination of model parameters that minimizes a cost function (over the training
set). It is a search in the model’s <i>parameter</i> <i>space:</i> the more parameters a model has,
the more dimensions this space has, and the harder the search is: searching for a nee‐
dle in a 300-dimensional haystack is much trickier than in 3 dimensions. Fortunately,
since the cost function is convex in the case of Linear Regression, the needle is simply
at the bottom of the bowl.
<header><largefont><b>Batch</b></largefont> <largefont><b>Gradient</b></largefont> <largefont><b>Descent</b></largefont></header>
To implement Gradient Descent, you need to compute the gradient of the cost func‐
tion with regard to each model parameter <i>θ.</i> In other words, you need to calculate
<i>j</i>
how much the cost function will change if you change <i>θ</i> just a little bit. This is called
<i>j</i>
a <i>partial</i> <i>derivative.</i> It is like asking “What is the slope of the mountain under my feet
if I face east?” and then asking the same question facing north (and so on for all other
dimensions, if you can imagine a universe with more than three dimensions). Equa‐
tion 4-5 computes the partial derivative of the cost function with regard to parameter
<i>θ,</i> noted ∂ MSE(θ) / ∂θ.
<i>j</i> <i>j</i>
<i>Equation</i> <i>4-5.</i> <i>Partial</i> <i>derivatives</i> <i>of</i> <i>the</i> <i>cost</i> <i>function</i>
<i>m</i>
∂ 2
<largefont>∑</largefont> ⊺ <i>i</i> <i>i</i> <i>i</i>
MSE <b>θ</b> = <b>θ</b> <b>x</b> − <i>y</i> <i>x</i>
∂θ <i>m</i> <i>j</i>
<i>i</i> = 1
<i>j</i>
Instead of computing these partial derivatives individually, you can use Equation 4-6
∇
to compute them all in one go. The gradient vector, noted MSE(θ), contains all the
<b>θ</b>
partial derivatives of the cost function (one for each model parameter).