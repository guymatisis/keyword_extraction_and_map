OK, that’s our baseline: 96.9% accuracy. Let’s see if we can do better by using K-Means
as a preprocessing step. We will create a pipeline that will first cluster the training set
into 50 clusters and replace the images with their distances to these 50 clusters, then
apply a Logistic Regression model:
<b>from</b> <b>sklearn.pipeline</b> <b>import</b> Pipeline
pipeline = Pipeline([
("kmeans", KMeans(n_clusters=50)),
("log_reg", LogisticRegression()),
])
pipeline.fit(X_train, y_train)
Since there are 10 different digits, it is tempting to set the number
of clusters to 10. However, each digit can be written several differ‐
ent ways, so it is preferable to use a larger number of clusters, such
as 50.
Now let’s evaluate this classification pipeline:
<b>>>></b> pipeline.score(X_test, y_test)
0.9777777777777777
How about that? We reduced the error rate by almost 30% (from about 3.1% to about
2.2%)!
But we chose the number of clusters <i>k</i> arbitrarily; we can surely do better. Since K-
Means is just a preprocessing step in a classification pipeline, finding a good value for
<i>k</i> is much simpler than earlier. There’s no need to perform silhouette analysis or mini‐
mize the inertia; the best value of <i>k</i> is simply the one that results in the best classifica‐
tion performance during cross-validation. We can use GridSearchCV to find the
optimal number of clusters:
<b>from</b> <b>sklearn.model_selection</b> <b>import</b> GridSearchCV
param_grid = dict(kmeans__n_clusters=range(2, 100))
grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)
grid_clf.fit(X_train, y_train)
Let’s look at the best value for <i>k</i> and the performance of the resulting pipeline:
<b>>>></b> grid_clf.best_params_
{'kmeans__n_clusters': 99}
<b>>>></b> grid_clf.score(X_test, y_test)
0.9822222222222222
With <i>k</i> = 99 clusters, we get a significant accuracy boost, reaching 98.22% accuracy
on the test set. Cool! You may want to keep exploring higher values for <i>k,</i> since 99
was the largest value in the range we explored.