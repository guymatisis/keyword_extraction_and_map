The general problem formulation is given by Equation 5-5.
<i>Equation</i> <i>5-5.</i> <i>Quadratic</i> <i>Programming</i> <i>problem</i>
1
⊺ ⊺
Minimize <b>p</b> <b>Hp</b> + <b>f</b> <b>p</b>
2
<b>p</b>
subject to <b>Ap</b> ≤ <b>b</b>
<b>p</b> is an <i>n</i> ‐dimensional vector (n = number of parameters),
<i>p</i> <i>p</i>
<b>H</b> is an <i>n</i> × <i>n</i> matrix,
<i>p</i> <i>p</i>
<b>f</b> is an <i>n</i> ‐dimensional vector,
where
<i>p</i>
<b>A</b> is an <i>n</i> × <i>n</i> matrix (n = number of constraints),
<i>c</i> <i>p</i> <i>c</i>
<b>b</b> is an <i>n</i> ‐dimensional vector.
<i>c</i>
⊺ (i) (i) ⋯
Note that the expression <b>A</b> <b>p</b> ≤ <b>b</b> defines <i>n</i> constraints: <b>p</b> <b>a</b> ≤ <i>b</i> for <i>i</i> = 1, 2, , <i>n</i> ,
<i>c</i> <i>c</i>
<b>a(i)</b> <i>ith</i> <i>b(i)</i> <i>ith</i>
where is the vector containing the elements of the row of <b>A</b> and is the
element of <b>b.</b>
You can easily verify that if you set the QP parameters in the following way, you get
the hard margin linear SVM classifier objective:
• <i>n</i> = <i>n</i> + 1, where <i>n</i> is the number of features (the +1 is for the bias term).
<i>p</i>
• <i>n</i> = <i>m,</i> where <i>m</i> is the number of training instances.
<i>c</i>
• <b>H</b> is the <i>n</i> × <i>n</i> identity matrix, except with a zero in the top-left cell (to ignore
<i>p</i> <i>p</i>
the bias term).
• <b>f</b> = 0, an <i>n</i> -dimensional vector full of 0s.
<i>p</i>
• <b>b</b> = –1, an <i>n</i> -dimensional vector full of –1s.
<i>c</i>
<b>a(i)</b> –t(i) <b>x˙(i),</b> <b>x˙(i)</b> <b>x(i)</b>
• = where is equal to with an extra bias feature <b>x˙</b> = 1.
0
One way to train a hard margin linear SVM classifier is to use an off-the-shelf QP
solver and pass it the preceding parameters. The resulting vector <b>p</b> will contain the
⋯
bias term <i>b</i> = <i>p</i> and the feature weights <i>w</i> = <i>p</i> for <i>i</i> = 1, 2, , <i>n.</i> Similarly, you can
0 <i>i</i> <i>i</i>
use a QP solver to solve the soft margin problem (see the exercises at the end of the
chapter).
To use the kernel trick, we are going to look at a different constrained optimization
problem.
<header><largefont><b>The</b></largefont> <largefont><b>Dual</b></largefont> <largefont><b>Problem</b></largefont></header>
Given a constrained optimization problem, known as the <i>primal</i> <i>problem,</i> it is possi‐
ble to express a different but closely related problem, called its <i>dual</i> <i>problem.</i> The