<i>Figure</i> <i>19-19.</i> <i>Data</i> <i>parallelism</i> <i>with</i> <i>centralized</i> <i>parameters</i>
Whereas the mirrored strategy imposes synchronous weight updates across all GPUs,
this centralized approach allows either synchronous or asynchronous updates. Let’s
see the pros and cons of both options.
<b>Synchronousupdates.</b> With <i>synchronous</i> <i>updates,</i> the aggregator waits until all gradi‐
ents are available before it computes the average gradients and passes them to the
optimizer, which will update the model parameters. Once a replica has finished com‐
puting its gradients, it must wait for the parameters to be updated before it can pro‐
ceed to the next mini-batch. The downside is that some devices may be slower than
others, so all other devices will have to wait for them at every step. Moreover, the
parameters will be copied to every device almost at the same time (immediately after
the gradients are applied), which may saturate the parameter servers’ bandwidth.
To reduce the waiting time at each step, you could ignore the gradi‐
ents from the slowest few replicas (typically ~10%). For example,
you could run 20 replicas, but only aggregate the gradients from
the fastest 18 replicas at each step, and just ignore the gradients
from the last 2. As soon as the parameters are updated, the first 18
replicas can start working again immediately, without having to
wait for the 2 slowest replicas. This setup is generally described as
19
having 18 replicas plus 2 <i>spare</i> <i>replicas.</i>
19 Thisnameisslightlyconfusingbecauseitsoundslikesomereplicasarespecial,doingnothing.Inreality,all
replicasareequivalent:theyallworkhardtobeamongthefastestateachtrainingstep,andthelosersvaryat
everystep(unlesssomedevicesarereallyslowerthanothers).However,itdoesmeanthatifaservercrashes,
trainingwillcontinuejustfine.