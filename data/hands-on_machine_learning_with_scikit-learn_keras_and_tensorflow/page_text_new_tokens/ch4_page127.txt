<b>from</b> <b>sklearn.linear_model</b> <b>import</b> SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)
sgd_reg.fit(X, y.ravel())
Once again, you find a solution quite close to the one returned by the Normal
Equation:
<b>>>></b> sgd_reg.intercept_, sgd_reg.coef_
(array([4.24365286]), array([2.8250878]))
<header><largefont><b>Mini-batch</b></largefont> <largefont><b>Gradient</b></largefont> <largefont><b>Descent</b></largefont></header>
The last Gradient Descent algorithm we will look at is called <i>Mini-batch</i> <i>Gradient</i>
<i>Descent.</i> It is simple to understand once you know Batch and Stochastic Gradient
Descent: at each step, instead of computing the gradients based on the full training set
(as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD
computes the gradients on small random sets of instances called <i>mini-batches.</i> The
main advantage of Mini-batch GD over Stochastic GD is that you can get a perfor‐
mance boost from hardware optimization of matrix operations, especially when using
GPUs.
The algorithm’s progress in parameter space is less erratic than with Stochastic GD,
especially with fairly large mini-batches. As a result, Mini-batch GD will end up walk‐
ing around a bit closer to the minimum than Stochastic GD—but it may be harder for
it to escape from local minima (in the case of problems that suffer from local minima,
unlike Linear Regression). Figure 4-11 shows the paths taken by the three Gradient
Descent algorithms in parameter space during training. They all end up near the
minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic
GD and Mini-batch GD continue to walk around. However, don’t forget that Batch
GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD
would also reach the minimum if you used a good learning schedule.
<i>Figure</i> <i>4-11.</i> <i>Gradient</i> <i>Descent</i> <i>paths</i> <i>in</i> <i>parameter</i> <i>space</i>