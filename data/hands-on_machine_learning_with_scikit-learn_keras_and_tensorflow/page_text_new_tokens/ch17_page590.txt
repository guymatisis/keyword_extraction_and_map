rather than the sum. So, the reconstruction loss is 784 times smaller than we need it
to be. We could define a custom loss to compute the sum rather than the mean, but it
is simpler to divide the latent loss by 784 (the final loss will be 784 times smaller than
it should be, but this just means that we should use a larger learning rate).
RMSprop
Note that we use the optimizer, which works well in this case. And finally we
can train the autoencoder!
history = variational_ae.fit(X_train, X_train, epochs=50, batch_size=128,
validation_data=[X_valid, X_valid])
<header><largefont><b>Generating</b></largefont> <largefont><b>Fashion</b></largefont> <largefont><b>MNIST</b></largefont> <largefont><b>Images</b></largefont></header>
Now let’s use this variational autoencoder to generate images that look like fashion
items. All we need to do is sample random codings from a Gaussian distribution and
decode them:
codings = tf.random.normal(shape=[12, codings_size])
images = variational_decoder(codings).numpy()
Figure 17-13 shows the 12 generated images.
<i>Figure</i> <i>17-13.</i> <i>Fashion</i> <i>MNIST</i> <i>images</i> <i>generated</i> <i>by</i> <i>the</i> <i>variational</i> <i>autoencoder</i>
The majority of these images look fairly convincing, if a bit too fuzzy. The rest are not
great, but don’t be too harsh on the autoencoder—it only had a few minutes to learn!
Give it a bit more fine-tuning and training time, and those images should look better.
Variational autoencoders make it possible to perform <i>semantic</i> <i>interpolation:</i> instead
of interpolating two images at the pixel level (which would look as if the two images
were overlaid), we can interpolate at the codings level. We first run both images
through the encoder, then we interpolate the two codings we get, and finally we
decode the interpolated codings to get the final image. It will look like a regular Fash‐
ion MNIST image, but it will be an intermediate between the original images. In the
following code example, we take the 12 codings we just generated, we organize them