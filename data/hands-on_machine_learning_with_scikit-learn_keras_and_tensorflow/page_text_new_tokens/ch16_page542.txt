will get the documentation for this module. By default, TF Hub will cache the down‐
loaded files into the local system’s temporary directory. You may prefer to download
them into a more permanent directory to avoid having to download them again after
every system cleanup. To do that, set the TFHUB_CACHE_DIR environment variable to
os.environ["TFHUB_CACHE_DIR"] = "./
the directory of your choice (e.g.,
my_tfhub_cache").
So far, we have looked at time series, text generation using Char-RNN, and sentiment
analysis using word-level RNN models, training our own word embeddings or reus‐
ing pretrained embeddings. Let’s now look at another important NLP task: <i>neural</i>
<i>machine</i> <i>translation</i> (NMT), first using a pure Encoder–Decoder model, then improv‐
ing it with attention mechanisms, and finally looking the extraordinary Transformer
architecture.
<header><largefont><b>An</b></largefont> <largefont><b>Encoder–Decoder</b></largefont> <largefont><b>Network</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Machine</b></largefont></header>
<header><largefont><b>Translation</b></largefont></header>
Let’s take a look at a simple neural machine translation model10 that will translate
English sentences to French (see Figure 16-3).
In short, the English sentences are fed to the encoder, and the decoder outputs the
French translations. Note that the French translations are also used as inputs to the
decoder, but shifted back by one step. In other words, the decoder is given as input
the word that it <i>should</i> have output at the previous step (regardless of what it actually
output). For the very first word, it is given the start-of-sequence (SOS) token. The
decoder is expected to end the sentence with an end-of-sequence (EOS) token.
Note that the English sentences are reversed before they are fed to the encoder. For
example, “I drink milk” is reversed to “milk drink I.” This ensures that the beginning
of the English sentence will be fed last to the encoder, which is useful because that’s
generally the first thing that the decoder needs to translate.
Each word is initially represented by its ID (e.g., 288 for the word “milk”). Next, an
embedding
layer returns the word embedding. These word embeddings are what is
actually fed to the encoder and the decoder.
10 IlyaSutskeveretal.,“SequencetoSequenceLearningwithNeuralNetworks,”arXivpreprintarXiv:1409.3215
(2014).