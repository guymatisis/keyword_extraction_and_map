Q_values = np.full((3, 3), -np.inf) <i>#</i> <i>-np.inf</i> <i>for</i> <i>impossible</i> <i>actions</i>
<b>for</b> state, actions <b>in</b> enumerate(possible_actions):
Q_values[state, actions] = 0.0 <i>#</i> <i>for</i> <i>all</i> <i>possible</i> <i>actions</i>
Now let’s run the Q-Value Iteration algorithm. It applies Equation 18-3 repeatedly, to
all Q-Values, for every state and every possible action:
gamma = 0.90 <i>#</i> <i>the</i> <i>discount</i> <i>factor</i>
<b>for</b> iteration <b>in</b> range(50):
Q_prev = Q_values.copy()
<b>for</b> s <b>in</b> range(3):
<b>for</b> a <b>in</b> possible_actions[s]:
Q_values[s, a] = np.sum([
transition_probabilities[s][a][sp]
* (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))
<b>for</b> sp <b>in</b> range(3)])
That’s it! The resulting Q-Values look like this:
<b>>>></b> Q_values
array([[18.91891892, 17.02702702, 13.62162162],
[ 0. , -inf, -4.87971488],
[ -inf, 50.13365013, -inf]])
For example, when the agent is in state <i>s</i> and it chooses action <i>a</i> , the expected sum
0 1
of discounted future rewards is approximately 17.0.
For each state, let’s look at the action that has the highest Q-Value:
<b>>>></b> np.argmax(Q_values, axis=1) <i>#</i> <i>optimal</i> <i>action</i> <i>for</i> <i>each</i> <i>state</i>
array([0, 0, 1])
This gives us the optimal policy for this MDP, when using a discount factor of 0.90: in
state <i>s</i> choose action <i>a</i> ; in state <i>s</i> choose action <i>a</i> (i.e., stay put); and in state <i>s</i>
0 0 1 0 2
choose action <i>a</i> (the only possible action). Interestingly, if we increase the discount
1
factor to 0.95, the optimal policy changes: in state <i>s</i> the best action becomes <i>a</i> (go
1 2
through the fire!). This makes sense because the more you value future rewards, the
more you are willing to put up with some pain now for the promise of future bliss.
<header><largefont><b>Temporal</b></largefont> <largefont><b>Difference</b></largefont> <largefont><b>Learning</b></largefont></header>
Reinforcement Learning problems with discrete actions can often be modeled as
Markov decision processes, but the agent initially has no idea what the transition
′
probabilities are (it does not know <i>T(s,</i> <i>a,</i> <i>s</i> )), and it does not know what the rewards
′
are going to be either (it does not know <i>R(s,</i> <i>a,</i> <i>s</i> )). It must experience each state and
each transition at least once to know the rewards, and it must experience them multi‐
ple times if it is to have a reasonable estimate of the transition probabilities.
The <i>Temporal</i> <i>Difference</i> <i>Learning</i> (TD Learning) algorithm is very similar to the
Value Iteration algorithm, but tweaked to take into account the fact that the agent has