<b>from</b> <b>tensorflow</b> <b>import</b> keras
encoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])
decoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])
autoencoder = keras.models.Sequential([encoder, decoder])
autoencoder.compile(loss="mse", optimizer=keras.optimizers.SGD(lr=0.1))
This code is really not very different from all the MLPs we built in past chapters, but
there are a few things to note:
• We organized the autoencoder into two subcomponents: the encoder and the
Sequential Dense
decoder. Both are regular models with a single layer each, and
Sequential
the autoencoder is a model containing the encoder followed by the
decoder (remember that a model can be used as a layer in another model).
• The autoencoder’s number of outputs is equal to the number of inputs (i.e., 3).
• To perform simple PCA, we do not use any activation function (i.e., all neurons
are linear), and the cost function is the MSE. We will see more complex autoen‐
coders shortly.
Now let’s train the model on a simple generated 3D dataset and use it to encode that
same dataset (i.e., project it to 2D):
history = autoencoder.fit(X_train, X_train, epochs=20)
codings = encoder.predict(X_train)
X_train,
Note that the same dataset, is used as both the inputs and the targets.
Figure 17-2 shows the original 3D dataset (on the left) and the output of the autoen‐
coder’s hidden layer (i.e., the coding layer, on the right). As you can see, the autoen‐
coder found the best 2D plane to project the data onto, preserving as much variance
in the data as it could (just like PCA).
<i>Figure</i> <i>17-2.</i> <i>PCA</i> <i>performed</i> <i>by</i> <i>an</i> <i>undercomplete</i> <i>linear</i> <i>autoencoder</i>