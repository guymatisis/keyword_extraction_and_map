<header><largefont><b>Denoising</b></largefont> <largefont><b>Autoencoders</b></largefont></header>
Another way to force the autoencoder to learn useful features is to add noise to its
inputs, training it to recover the original, noise-free inputs. This idea has been around
since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008
paper,5 Pascal Vincent et al. showed that autoencoders could also be used for feature
6
extraction. In a 2010 paper, Vincent et al. introduced <i>stacked</i> <i>denoising</i> <i>autoencoders.</i>
The noise can be pure Gaussian noise added to the inputs, or it can be randomly
switched-off inputs, just like in dropout (introduced in Chapter 11). Figure 17-8
shows both options.
<i>Figure</i> <i>17-8.</i> <i>Denoising</i> <i>autoencoders,</i> <i>with</i> <i>Gaussian</i> <i>noise</i> <i>(left)</i> <i>or</i> <i>dropout</i> <i>(right)</i>
The implementation is straightforward: it is a regular stacked autoencoder with an
Dropout Gaus
additional layer applied to the encoder’s inputs (or you could use a
sianNoise layer instead). Recall that the Dropout layer is only active during training
GaussianNoise
(and so is the layer):
5 PascalVincentetal.,“ExtractingandComposingRobustFeatureswithDenoisingAutoencoders,”Proceedings
<i>ofthe25thInternationalConferenceonMachineLearning(2008):1096–1103.</i>
6 PascalVincentetal.,“StackedDenoisingAutoencoders:LearningUsefulRepresentationsinaDeepNetwork
withaLocalDenoisingCriterion,”JournalofMachineLearningResearch11(2010):3371–3408.