The dataset methods do <i>not</i> modify datasets, they create new ones,
so make sure to keep a reference to these new datasets (e.g., with
dataset = ... ), or else nothing will happen.
map()
You can also transform the items by calling the method. For example, this cre‐
ates a new dataset with all items doubled:
<b>>>></b> dataset = dataset.map(lambda x: x * 2) <i>#</i> <i>Items:</i> <i>[0,2,4,6,8,10,12]</i>
This function is the one you will call to apply any preprocessing you want to your
data. Sometimes this will include computations that can be quite intensive, such as
reshaping or rotating an image, so you will usually want to spawn multiple threads to
speed things up: it’s as simple as setting the num_parallel_calls argument. Note that
map()
the function you pass to the method must be convertible to a TF Function (see
Chapter 12).
While the map() method applies a transformation to each item, the apply() method
applies a transformation to the dataset as a whole. For example, the following code
unbatch()
applies the function to the dataset (this function is currently experimental,
but it will most likely move to the core API in a future release). Each item in the new
dataset will be a single-integer tensor instead of a batch of seven integers:
<b>>>></b> dataset = dataset.apply(tf.data.experimental.unbatch()) <i>#</i> <i>Items:</i> <i>0,2,4,...</i>
It is also possible to simply filter the dataset using the filter() method:
<b>>>></b> dataset = dataset.filter(lambda x: x < 10) <i>#</i> <i>Items:</i> <i>0</i> <i>2</i> <i>4</i> <i>6</i> <i>8</i> <i>0</i> <i>2</i> <i>4</i> <i>6...</i>
take()
You will often want to look at just a few items from a dataset. You can use the
method for that:
<b>>>></b> <b>for</b> item <b>in</b> dataset.take(3):
<b>...</b> <b>print(item)</b>
<b>...</b>
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
<header><largefont><b>Shuffling</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Data</b></largefont></header>
As you know, Gradient Descent works best when the instances in the training set are
independent and identically distributed (see Chapter 4). A simple way to ensure this
is to shuffle the instances, using the shuffle() method. It will create a new dataset
that will start by filling up a buffer with the first items of the source dataset. Then,
whenever it is asked for an item, it will pull one out randomly from the buffer and
replace it with a fresh one from the source dataset, until it has iterated entirely
through the source dataset. At this point it continues to pull out items randomly from