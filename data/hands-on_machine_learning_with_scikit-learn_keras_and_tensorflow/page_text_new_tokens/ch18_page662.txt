The function first asks the collect policy for its initial state (given the environment
batch size, which is 1 in this case). Since the policy is stateless, this returns an empty
tuple (so we could have written policy_state = () ). Next, we create an iterator over
run()
the dataset, and we run the training loop. At each iteration, we call the driver’s
method, passing it the current time step (initially None ) and the current policy state. It
will run the collect policy and collect experience for four steps (as we configured ear‐
lier), broadcasting the collected trajectories to the replay buffer and the metrics. Next,
we sample one batch of trajectories from the dataset, and we pass it to the agent’s
train() method. It returns a train_loss object which may vary depending on the
type of agent. Next, we display the iteration number and the training loss, and every
train_agent()
1,000 iterations we log all the metrics. Now you can just call for some
number of iterations, and see the agent gradually learn to play <i>Breakout!</i>
train_agent(10000000)
This will take a lot of computing power and a lot of patience (it may take hours, or
even days, depending on your hardware), plus you may need to run the algorithm
several times with different random seeds to get good results, but once it’s done, the
agent will be superhuman (at least at <i>Breakout).</i> You can also try training this DQN
agent on other Atari games: it can achieve superhuman skill at most action games,
but it is not so good at games with long-running storylines. 22
<header><largefont><b>Overview</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Some</b></largefont> <largefont><b>Popular</b></largefont> <largefont><b>RL</b></largefont> <largefont><b>Algorithms</b></largefont></header>
Before we finish this chapter, let’s take a quick look at a few popular RL algorithms:
<i>Actor-Critic</i> <i>algorithms</i>
A family of RL algorithms that combine Policy Gradients with Deep Q-
Networks. An Actor-Critic agent contains two neural networks: a policy net and
a DQN. The DQN is trained normally, by learning from the agent’s experiences.
The policy net learns differently (and much faster) than in regular PG: instead of
estimating the value of each action by going through multiple episodes, then
summing the future discounted rewards for each action, and finally normalizing
them, the agent (actor) relies on the action values estimated by the DQN (critic).
It’s a bit like an athlete (the agent) learning with the help of a coach (the DQN).
<i>Asynchronous</i> <i>Advantage</i> <i>Actor-Critic</i> <i>23</i> <i>(A3C)</i>
An important Actor-Critic variant introduced by DeepMind researchers in 2016,
where multiple agents learn in parallel, exploring different copies of the environ‐
22 Foracomparisonofthisalgorithm’sperformanceonvariousAtarigames,seefigure3inDeepMind’s2015
paper.
23 VolodymyrMnihetal.,“AsynchonousMethodsforDeepReinforcementLearning,”Proceedingsofthe33rd
<i>InternationalConferenceonMachineLearning(2016):1928–1937.</i>