Putting everything together, we can now create a Keras model that can process cate‐
gorical features (along with regular numerical features) and learn an embedding for
each category (as well as for each oov bucket):
regular_inputs = keras.layers.Input(shape=[8])
categories = keras.layers.Input(shape=[], dtype=tf.string)
cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)
cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)
encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])
outputs = keras.layers.Dense(1)(encoded_inputs)
model = keras.models.Model(inputs=[regular_inputs, categories],
outputs=[outputs])
This model takes two inputs: a regular input containing eight numerical features per
instance, plus a categorical input (containing one categorical feature per instance). It
uses a Lambda layer to look up each category’s index, then it looks up the embeddings
for these indices. Next, it concatenates the embeddings and the regular inputs in
order to give the encoded inputs, which are ready to be fed to a neural network. We
could add any kind of neural network at this point, but we just add a dense output
layer, and we create the Keras model.
When the keras.layers.TextVectorization layer is available, you can call its
adapt()
method to make it extract the vocabulary from a data sample (it will take
care of creating the lookup table for you). Then you can add it to your model, and it
will perform the index lookup (replacing the Lambda layer in the previous code
example).
Dense
One-hot encoding followed by a layer (with no activation
Embedding
function and no biases) is equivalent to an layer. How‐
Embedding
ever, the layer uses way fewer computations (the perfor‐
mance difference becomes clear when the size of the embedding
Dense
matrix grows). The layer’s weight matrix plays the role of the
embedding matrix. For example, using one-hot vectors of size 20
Dense Embedding
and a layer with 10 units is equivalent to using an
input_dim=20 output_dim=10
layer with and . As a result, it would
be wasteful to use more embedding dimensions than the number
of units in the layer that follows the Embedding layer.
Now let’s look a bit more closely at the Keras preprocessing layers.
<header><largefont><b>Keras</b></largefont> <largefont><b>Preprocessing</b></largefont> <largefont><b>Layers</b></largefont></header>
The TensorFlow team is working on providing a set of standard Keras preprocessing
layers. They will probably be available by the time you read this; however, the API
may change slightly by then, so please refer to the notebook for this chapter if any‐
thing behaves unexpectedly. This new API will likely supersede the existing Feature