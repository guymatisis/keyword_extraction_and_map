method estimates the log of the <i>probability</i> <i>density</i> <i>function</i> (PDF) at that location.
The greater the score, the higher the density:
<b>>>></b> gm.score_samples(X)
array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,
-4.39802535, -3.80743859])
If you compute the exponential of these scores, you get the value of the PDF at the
location of the given instances. These are not probabilities, but probability <i>densities:</i>
they can take on any positive value, not just a value between 0 and 1. To estimate the
probability that an instance will fall within a particular region, you would have to
integrate the PDF over that region (if you do so over the entire space of possible
instance locations, the result will be 1).
Figure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the
density contours of this model.
<i>Figure</i> <i>9-17.</i> <i>Cluster</i> <i>means,</i> <i>decision</i> <i>boundaries,</i> <i>and</i> <i>density</i> <i>contours</i> <i>of</i> <i>a</i> <i>trained</i>
<i>Gaussian</i> <i>mixture</i> <i>model</i>
Nice! The algorithm clearly found an excellent solution. Of course, we made its task
easy by generating the data using a set of 2D Gaussian distributions (unfortunately,
real-life data is not always so Gaussian and low-dimensional). We also gave the algo‚Äê
rithm the correct number of clusters. When there are many dimensions, or many
clusters, or few instances, EM can struggle to converge to the optimal solution. You
might need to reduce the difficulty of the task by limiting the number of parameters
that the algorithm has to learn. One way to do this is to limit the range of shapes and
orientations that the clusters can have. This can be achieved by imposing constraints
covariance_type
on the covariance matrices. To do this, set the hyperparameter to
one of the following values: