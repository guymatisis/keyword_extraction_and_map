parent7
offspring is a copy of its plus some random variation. The surviving policies
plus their offspring together constitute the second generation. You can continue to
iterate through generations this way until you find a good policy. 8
<i>Figure</i> <i>18-3.</i> <i>Four</i> <i>points</i> <i>in</i> <i>policy</i> <i>space</i> <i>(left)</i> <i>and</i> <i>the</i> <i>agent’s</i> <i>corresponding</i> <i>behavior</i>
<i>(right)</i>
Yet another approach is to use optimization techniques, by evaluating the gradients of
the rewards with regard to the policy parameters, then tweaking these parameters by
following the gradients toward higher rewards.9 We will discuss this approach, is
called <i>policy</i> <i>gradients</i> (PG), in more detail later in this chapter. Going back to the
vacuum cleaner robot, you could slightly increase <i>p</i> and evaluate whether doing so
increases the amount of dust picked up by the robot in 30 minutes; if it does, then
increase <i>p</i> some more, or else reduce <i>p.</i> We will implement a popular PG algorithm
using TensorFlow, but before we do, we need to create an environment for the agent
to live in—so it’s time to introduce OpenAI Gym.
<header><largefont><b>Introduction</b></largefont> <largefont><b>to</b></largefont> <largefont><b>OpenAI</b></largefont> <largefont><b>Gym</b></largefont></header>
One of the challenges of Reinforcement Learning is that in order to train an agent,
you first need to have a working environment. If you want to program an agent that
7 Ifthereisasingleparent,thisiscalledasexualreproduction.Withtwo(ormore)parents,itiscalledsexual
<i>reproduction.Anoffspring’sgenome(inthiscaseasetofpolicyparameters)israndomlycomposedofpartsof</i>
itsparents’genomes.
8 OneinterestingexampleofageneticalgorithmusedforReinforcementLearningistheNeuroEvolutionof
<i>AugmentingTopologies(NEAT)algorithm.</i>
9 ThisiscalledGradientAscent.It’sjustlikeGradientDescentbutintheoppositedirection:maximizinginstead
ofminimizing.