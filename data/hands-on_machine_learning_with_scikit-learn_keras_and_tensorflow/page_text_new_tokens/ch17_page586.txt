<header><largefont><b>Variational</b></largefont> <largefont><b>Autoencoders</b></largefont></header>
Another important category of autoencoders was introduced in 2013 by Diederik
Kingma and Max Welling and quickly became one of the most popular types of
autoencoders: <i>variational</i> <i>autoencoders.7</i>
They are quite different from all the autoencoders we have discussed so far, in these
particular ways:
• They are <i>probabilistic</i> <i>autoencoders,</i> meaning that their outputs are partly deter‐
mined by chance, even after training (as opposed to denoising autoencoders,
which use randomness only during training).
• Most importantly, they are <i>generative</i> <i>autoencoders,</i> meaning that they can gener‐
ate new instances that look like they were sampled from the training set.
Both these properties make them rather similar to RBMs, but they are easier to train,
and the sampling process is much faster (with RBMs you need to wait for the network
to stabilize into a “thermal equilibrium” before you can sample a new instance).
Indeed, as their name suggests, variational autoencoders perform variational Baye‐
sian inference (introduced in Chapter 9), which is an efficient way to perform
approximate Bayesian inference.
Let’s take a look at how they work. Figure 17-12 (left) shows a variational autoen‐
coder. You can recognize the basic structure of all autoencoders, with an encoder fol‐
lowed by a decoder (in this example, they both have two hidden layers), but there is a
twist: instead of directly producing a coding for a given input, the encoder produces a
<i>mean</i> <i>coding</i> <b>μ</b> and a standard deviation <b>σ.</b> The actual coding is then sampled ran‐
domly from a Gaussian distribution with mean <b>μ</b> and standard deviation <b>σ.</b> After that
the decoder decodes the sampled coding normally. The right part of the diagram
shows a training instance going through this autoencoder. First, the encoder pro‐
duces <b>μ</b> and <b>σ,</b> then a coding is sampled randomly (notice that it is not exactly located
at <b>μ),</b> and finally this coding is decoded; the final output resembles the training
instance.
7 DiederikKingmaandMaxWelling,“Auto-EncodingVariationalBayes,”arXivpreprintarXiv:1312.6114
(2013).