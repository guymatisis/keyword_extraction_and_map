<header><largefont><b>Irrelevant</b></largefont> <largefont><b>Features</b></largefont></header>
As the saying goes: garbage in, garbage out. Your system will only be capable of learn‐
ing if the training data contains enough relevant features and not too many irrelevant
ones. A critical part of the success of a Machine Learning project is coming up with a
good set of features to train on. This process, called <i>feature</i> <i>engineering,</i> involves the
following steps:
• <i>Feature</i> <i>selection</i> (selecting the most useful features to train on among existing
features)
• <i>Feature</i> <i>extraction</i> (combining existing features to produce a more useful one—as
we saw earlier, dimensionality reduction algorithms can help)
• Creating new features by gathering new data
Now that we have looked at many examples of bad data, let’s look at a couple of exam‐
ples of bad algorithms.
<header><largefont><b>Overfitting</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Data</b></largefont></header>
Say you are visiting a foreign country and the taxi driver rips you off. You might be
tempted to say that <i>all</i> taxi drivers in that country are thieves. Overgeneralizing is
something that we humans do all too often, and unfortunately machines can fall into
the same trap if we are not careful. In Machine Learning this is called <i>overfitting:</i> it
means that the model performs well on the training data, but it does not generalize
well.
Figure 1-22 shows an example of a high-degree polynomial life satisfaction model
that strongly overfits the training data. Even though it performs much better on the
training data than the simple linear model, would you really trust its predictions?
<i>Figure</i> <i>1-22.</i> <i>Overfitting</i> <i>the</i> <i>training</i> <i>data</i>