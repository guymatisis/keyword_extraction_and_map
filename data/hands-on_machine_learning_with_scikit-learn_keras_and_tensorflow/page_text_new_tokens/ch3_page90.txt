<b>from</b> <b>sklearn.base</b> <b>import</b> BaseEstimator
<b>class</b> <b>Never5Classifier(BaseEstimator):</b>
<b>def</b> fit(self, X, y=None):
<b>return</b> self
<b>def</b> predict(self, X):
<b>return</b> np.zeros((len(X), 1), dtype=bool)
Can you guess this model’s accuracy? Let’s find out:
<b>>>></b> never_5_clf = Never5Classifier()
<b>>>></b> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring="accuracy")
array([0.91125, 0.90855, 0.90915])
That’s right, it has over 90% accuracy! This is simply because only about 10% of the
images are 5s, so if you always guess that an image is <i>not</i> a 5, you will be right about
90% of the time. Beats Nostradamus.
This demonstrates why accuracy is generally not the preferred performance measure
for classifiers, especially when you are dealing with <i>skewed</i> <i>datasets</i> (i.e., when some
classes are much more frequent than others).
<header><largefont><b>Confusion</b></largefont> <largefont><b>Matrix</b></largefont></header>
A much better way to evaluate the performance of a classifier is to look at the <i>confu‐</i>
<i>sion</i> <i>matrix.</i> The general idea is to count the number of times instances of class A are
classified as class B. For example, to know the number of times the classifier confused
images of 5s with 3s, you would look in the fifth row and third column of the confu‐
sion matrix.
To compute the confusion matrix, you first need to have a set of predictions so that
they can be compared to the actual targets. You could make predictions on the test
set, but let’s keep it untouched for now (remember that you want to use the test set
only at the very end of your project, once you have a classifier that you are ready to
cross_val_predict()
launch). Instead, you can use the function:
<b>from</b> <b>sklearn.model_selection</b> <b>import</b> cross_val_predict
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
cross_val_score() cross_val_predict()
Just like the function, performs K-fold
cross-validation, but instead of returning the evaluation scores, it returns the predic‐
tions made on each test fold. This means that you get a clean prediction for each
instance in the training set (“clean” meaning that the prediction is made by a model
that never saw the data during training).
confusion_matrix()
Now you are ready to get the confusion matrix using the func‐
tion. Just pass it the target classes ( y_train_5 ) and the predicted classes
(y_train_pred):