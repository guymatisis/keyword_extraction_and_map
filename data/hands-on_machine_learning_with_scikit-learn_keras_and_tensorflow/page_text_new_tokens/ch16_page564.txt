Transformer-like architecture. The authors pretrained a large but fairly simple
architecture composed of a stack of 12 Transformer modules (using only Masked
Multi-Head Attention layers) on a large dataset, once again trained using self-
supervised learning. Then they fine-tuned it on various language tasks, using
only minor adaptations for each task. The tasks were quite diverse: they included
B),27
text classification, <i>entailment</i> (whether sentence A entails sentence similarity
(e.g., “Nice weather today” is very similar to “It is sunny”), and question answer‐
ing (given a few paragraphs of text giving some context, the model must answer
some multiple-choice questions). Just a few months later, in February 2019, Alec
Radford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper, 28
which proposed a very similar architecture, but larger still (with over 1.5 billion
parameters!) and they showed that it could achieve good performance on many
tasks without any fine-tuning. This is called <i>zero-shot</i> <i>learning</i> (ZSL). A smaller
version of the GPT-2 model (with “just” 117 million parameters) is available at
<i>https://github.com/openai/gpt-2,</i> along with its pretrained weights.
• The BERT paper29 by Jacob Devlin and other Google researchers also demon‐
strates the effectiveness of self-supervised pretraining on a large corpus, using a
similar architecture to GPT but non-masked Multi-Head Attention layers (like in
the Transformer’s encoder). This means that the model is naturally bidirectional;
hence the B in BERT (Bidirectional <i>Encoder</i> <i>Representations</i> <i>from</i> <i>Transformers).</i>
Most importantly, the authors proposed two pretraining tasks that explain most
of the model’s strength:
<i>Masked</i> <i>language</i> <i>model</i> <i>(MLM)</i>
Each word in a sentence has a 15% probability of being masked, and the
model is trained to predict the masked words. For example, if the original
sentence is “She had fun at the birthday party,” then the model may be given
the sentence “She <mask> fun at the <mask> party” and it must predict the
words “had” and “birthday” (the other outputs will be ignored). To be more
precise, each selected word has an 80% chance of being masked, a 10%
chance of being replaced by a random word (to reduce the discrepancy
between pretraining and fine-tuning, since the model will not see <mask>
tokens during fine-tuning), and a 10% chance of being left alone (to bias the
model toward the correct answer).
27 Forexample,thesentence“Janehadalotoffunatherfriend’sbirthdayparty”entails“Janeenjoyedtheparty,”
butitiscontradictedby“Everyonehatedtheparty”anditisunrelatedto“TheEarthisflat.”
28 AlecRadfordetal.,“LanguageModelsAreUnsupervisedMultitaskLearners”(2019).
29 JacobDevlinetal.,“BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding,”
<i>Proceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLin‐</i>
<i>guistics:HumanLanguageTechnologies1(2019).</i>