<b>>>></b> np.linalg.pinv(X_b).dot(y)
array([[4.21509616],
[2.77011339]])
The pseudoinverse itself is computed using a standard matrix factorization technique
called <i>Singular</i> <i>Value</i> <i>Decomposition</i> (SVD) that can decompose the training set
matrix <b>X</b> into the matrix multiplication of three matrices <b>U</b> <b>Σ</b> <b>V</b> ⊺ (see
+ + ⊺
numpy.linalg.svd() ). The pseudoinverse is computed as <b>X</b> = <b>VΣ</b> <b>U</b> . To compute
+
the matrix <b>Σ</b> , the algorithm takes <b>Σ</b> and sets to zero all values smaller than a tiny
threshold value, then it replaces all the nonzero values with their inverse, and finally
it transposes the resulting matrix. This approach is more efficient than computing the
Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may
<b>X⊺X</b>
not work if the matrix is not invertible (i.e., singular), such as if <i>m</i> < <i>n</i> or if some
features are redundant, but the pseudoinverse is always defined.
<header><largefont><b>Computational</b></largefont> <largefont><b>Complexity</b></largefont></header>
<b>X⊺</b>
The Normal Equation computes the inverse of <b>X,</b> which is an (n + 1) × (n + 1)
matrix (where <i>n</i> is the number of features). The <i>computational</i> <i>complexity</i> of inverting
such a matrix is typically about <i>O(n2.4)</i> to <i>O(n3),</i> depending on the implementation. In
other words, if you double the number of features, you multiply the computation
2.4 3
time by roughly 2 = 5.3 to 2 = 8.
The SVD approach used by Scikit-Learn’s LinearRegression class is about <i>O(n2).</i> If
you double the number of features, you multiply the computation time by roughly 4.
Both the Normal Equation and the SVD approach get very slow
when the number of features grows large (e.g., 100,000). On the
positive side, both are linear with regard to the number of instances
in the training set (they are <i>O(m)),</i> so they handle large training
sets efficiently, provided they can fit in memory.
Also, once you have trained your Linear Regression model (using the Normal Equa‐
tion or any other algorithm), predictions are very fast: the computational complexity
is linear with regard to both the number of instances you want to make predictions
on and the number of features. In other words, making predictions on twice as many
instances (or twice as many features) will take roughly twice as much time.
Now we will look at a very different way to train a Linear Regression model, which is
better suited for cases where there are a large number of features or too many training
instances to fit in memory.