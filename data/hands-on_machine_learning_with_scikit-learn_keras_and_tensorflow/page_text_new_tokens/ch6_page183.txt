<header><largefont><b>Regression</b></largefont></header>
Decision Trees are also capable of performing regression tasks. Let’s build a regres‐
DecisionTreeRegressor
sion tree using Scikit-Learn’s class, training it on a noisy
max_depth=2
quadratic dataset with :
<b>from</b> <b>sklearn.tree</b> <b>import</b> DecisionTreeRegressor
tree_reg = DecisionTreeRegressor(max_depth=2)
tree_reg.fit(X, y)
The resulting tree is represented in Figure 6-4.
<i>Figure</i> <i>6-4.</i> <i>A</i> <i>Decision</i> <i>Tree</i> <i>for</i> <i>regression</i>
This tree looks very similar to the classification tree you built earlier. The main differ‐
ence is that instead of predicting a class in each node, it predicts a value. For example,
suppose you want to make a prediction for a new instance with <i>x</i> = 0.6. You traverse
1
the tree starting at the root, and you eventually reach the leaf node that predicts
value=0.111 . This prediction is the average target value of the 110 training instances
associated with this leaf node, and it results in a mean squared error equal to 0.015
over these 110 instances.
This model’s predictions are represented on the left in Figure 6-5. If you set
max_depth=3 , you get the predictions represented on the right. Notice how the pre‐
dicted value for each region is always the average target value of the instances in that
region. The algorithm splits each region in a way that makes most training instances
as close as possible to that predicted value.