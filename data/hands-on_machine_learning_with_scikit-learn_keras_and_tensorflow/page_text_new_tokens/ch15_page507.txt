<i>Figure</i> <i>15-7.</i> <i>Deep</i> <i>RNN</i> <i>(left)</i> <i>unrolled</i> <i>through</i> <i>time</i> <i>(right)</i>
Implementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In
SimpleRNN
this example, we use three layers (but we could add any other type of
LSTM GRU
recurrent layer, such as an layer or a layer, which we will discuss shortly):
model = keras.models.Sequential([
keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
keras.layers.SimpleRNN(20, return_sequences=True),
keras.layers.SimpleRNN(1)
])
Make sure to set return_sequences=True for all recurrent layers
(except the last one, if you only care about the last output). If you
don’t, they will output a 2D array (containing only the output of
the last time step) instead of a 3D array (containing outputs for all
time steps), and the next recurrent layer will complain that you are
not feeding it sequences in the expected 3D format.
If you compile, fit, and evaluate this model, you will find that it reaches an MSE of
0.003. We finally managed to beat the linear model!
Note that the last layer is not ideal: it must have a single unit because we want to fore‐
cast a univariate time series, and this means we must have a single output value per
time step. However, having a single unit means that the hidden state is just a single
number. That’s really not much, and it’s probably not that useful; presumably, the
RNN will mostly use the hidden states of the other recurrent layers to carry over all
the information it needs from time step to time step, and it will not use the final lay‐
er’s hidden state very much. Moreover, since a SimpleRNN layer uses the tanh activa‐
tion function by default, the predicted values must lie within the range –1 to 1. But
what if you want to use another activation function? For both these reasons, it might
Dense
be preferable to replace the output layer with a layer: it would run slightly