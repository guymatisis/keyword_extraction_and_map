5. A custom loss function can be defined by writing a function or by subclassing the
keras.losses.Loss class. When would you use each option?
6. Similarly, a custom metric can be defined in a function or a subclass of
keras.metrics.Metric.
When would you use each option?
7. When should you create a custom layer versus a custom model?
8. What are some use cases that require writing your own custom training loop?
9. Can custom Keras components contain arbitrary Python code, or must they be
convertible to TF Functions?
10. What are the main rules to respect if you want a function to be convertible to a
TF Function?
11. When would you need to create a dynamic Keras model? How do you do that?
Why not make all your models dynamic?
12. Implement a custom layer that performs <i>Layer</i> <i>Normalization</i> (we will use this
type of layer in Chapter 15):
build()
a. The method should define two trainable weights <b>α</b> and <b>β,</b> both of
shape input_shape[-1:] and data type tf.float32 . <b>α</b> should be initialized
with 1s, and <b>β</b> with 0s.
b. The call() method should compute the mean <i>μ</i> and standard deviation <i>σ</i> of
each instance’s features. For this, you can use tf.nn.moments(inputs,
axes=-1, keepdims=True)
, which returns the mean <i>μ</i> and the variance <i>σ</i> 2 of
all instances (compute the square root of the variance to get the standard
deviation). Then the function should compute and return <b>α⊗(X</b> - <i>μ)/(σ</i> + <i>ε)</i> +
⊗ *
<b>β,</b> where represents itemwise multiplication ( ) and <i>ε</i> is a smoothing term
(small constant to avoid division by zero, e.g., 0.001).
c. Ensure that your custom layer produces the same (or very nearly the same)
keras.layers.LayerNormalization
output as the layer.
13. Train a model using a custom training loop to tackle the Fashion MNIST dataset
(see Chapter 10).
a. Display the epoch, iteration, mean training loss, and mean accuracy over each
epoch (updated at each iteration), as well as the validation loss and accuracy at
the end of each epoch.
b. Try using a different optimizer with a different learning rate for the upper lay‐
ers and the lower layers.
Solutions to these exercises are available in Appendix A.