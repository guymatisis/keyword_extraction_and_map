Oops! The algorithm desperately searched for ellipsoids, so it found eight different
clusters instead of two. The density estimation is not too bad, so this model could
perhaps be used for anomaly detection, but it failed to identify the two moons. Let’s
now look at a few clustering algorithms capable of dealing with arbitrarily shaped
clusters.
<header><largefont><b>Other</b></largefont> <largefont><b>Algorithms</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Anomaly</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Novelty</b></largefont> <largefont><b>Detection</b></largefont></header>
Scikit-Learn implements other algorithms dedicated to anomaly detection or novelty
detection:
<i>inverse_transform()</i>
<i>PCA</i> <i>(and</i> <i>other</i> <i>dimensionality</i> <i>reduction</i> <i>techniques</i> <i>with</i> <i>an</i>
<i>method)</i>
If you compare the reconstruction error of a normal instance with the recon‐
struction error of an anomaly, the latter will usually be much larger. This is a sim‐
ple and often quite efficient anomaly detection approach (see this chapter’s
exercises for an application of this approach).
<i>Fast-MCD</i> <i>(minimum</i> <i>covariance</i> <i>determinant)</i>
Implemented by the EllipticEnvelope class, this algorithm is useful for outlier
detection, in particular to clean up a dataset. It assumes that the normal instances
(inliers) are generated from a single Gaussian distribution (not a mixture). It also
assumes that the dataset is contaminated with outliers that were not generated
from this Gaussian distribution. When the algorithm estimates the parameters of
the Gaussian distribution (i.e., the shape of the elliptic envelope around the inli‐
ers), it is careful to ignore the instances that are most likely outliers. This techni‐
que gives a better estimation of the elliptic envelope and thus makes the
algorithm better at identifying the outliers.
<i>Isolation</i> <i>Forest</i>
This is an efficient algorithm for outlier detection, especially in high-dimensional
datasets. The algorithm builds a Random Forest in which each Decision Tree is
grown randomly: at each node, it picks a feature randomly, then it picks a ran‐
dom threshold value (between the min and max values) to split the dataset in
two. The dataset gradually gets chopped into pieces this way, until all instances
end up isolated from the other instances. Anomalies are usually far from other
instances, so on average (across all the Decision Trees) they tend to get isolated in
fewer steps than normal instances.
<i>Local</i> <i>Outlier</i> <i>Factor</i> <i>(LOF)</i>
This algorithm is also good for outlier detection. It compares the density of
instances around a given instance to the density around its neighbors. An anom‐
aly is often more isolated than its <i>k</i> nearest neighbors.