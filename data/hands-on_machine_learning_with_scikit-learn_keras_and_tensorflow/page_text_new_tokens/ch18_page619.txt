Here is the code to build this neural network policy using tf.keras:
<b>import</b> <b>tensorflow</b> <b>as</b> <b>tf</b>
<b>from</b> <b>tensorflow</b> <b>import</b> keras
n_inputs = 4 <i>#</i> <i>==</i> <i>env.observation_space.shape[0]</i>
model = keras.models.Sequential([
keras.layers.Dense(5, activation="elu", input_shape=[n_inputs]),
keras.layers.Dense(1, activation="sigmoid"),
])
Sequential
After the imports, we use a simple model to define the policy network.
The number of inputs is the size of the observation space (which in the case of Cart‐
Pole is 4), and we have just five hidden units because it’s a simple problem. Finally, we
want to output a single probability (the probability of going left), so we have a single
output neuron using the sigmoid activation function. If there were more than two
possible actions, there would be one output neuron per action, and we would use the
softmax activation function instead.
OK, we now have a neural network policy that will take observations and output
action probabilities. But how do we train it?
<header><largefont><b>Evaluating</b></largefont> <largefont><b>Actions:</b></largefont> <largefont><b>The</b></largefont> <largefont><b>Credit</b></largefont> <largefont><b>Assignment</b></largefont> <largefont><b>Problem</b></largefont></header>
If we knew what the best action was at each step, we could train the neural network as
usual, by minimizing the cross entropy between the estimated probability distribu‐
tion and the target probability distribution. It would just be regular supervised learn‐
ing. However, in Reinforcement Learning the only guidance the agent gets is through
rewards, and rewards are typically sparse and delayed. For example, if the agent man‐
ages to balance the pole for 100 steps, how can it know which of the 100 actions it
took were good, and which of them were bad? All it knows is that the pole fell after
the last action, but surely this last action is not entirely responsible. This is called the
<i>credit</i> <i>assignment</i> <i>problem:</i> when the agent gets a reward, it is hard for it to know
which actions should get credited (or blamed) for it. Think of a dog that gets rewar‐
ded hours after it behaved well; will it understand what it is being rewarded for?
To tackle this problem, a common strategy is to evaluate an action based on the sum
of all the rewards that come after it, usually applying a <i>discount</i> <i>factor</i> <i>γ</i> (gamma) at
each step. This sum of discounted rewards is called the action’s <i>return.</i> Consider the
example in Figure 18-6). If an agent decides to go right three times in a row and gets
+10 reward after the first step, 0 after the second step, and finally –50 after the third
step, then assuming we use a discount factor <i>γ</i> = 0.8, the first action will have a return
of 10 + <i>γ</i> × 0 + <i>γ2</i> × (–50) = –22. If the discount factor is close to 0, then future
rewards won’t count for much compared to immediate rewards. Conversely, if the
discount factor is close to 1, then rewards far into the future will count almost as