text|keyphrases
"                                                                      
                                                                      
                                                                      
                                                                      
          the next step, it started detecting part of the topmost rose, and then it detected it
          again once it was shifted one more step to the right. You would then continue to slide
          the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since
          objects can have varying sizes, you would also slide the CNN across regions of differ‐
          ent sizes. For example, once you are done with the 3 × 3 regions, you might want to
          slide the CNN across all 4 × 4 regions as well.             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-24. Detecting multiple objects by sliding a CNN across the image
                                                                      
          This technique is fairly straightforward, but as you can see it will detect the same
          object multiple times, at slightly different positions. Some post-processing will then
          be needed to get rid of all the unnecessary bounding boxes. A common approach for
          this is called non-max suppression. Here’s how you do it:   
           1. First, you need to add an extra objectness output to your CNN, to estimate the
            probability that a flower is indeed present in the image (alternatively, you could
            add a “no-flower” class, but this usually does not work as well). It must use the
            sigmoid activation function, and you can train it using binary cross-entropy loss.
            Then get rid of all the bounding boxes for which the objectness score is below
            some threshold: this will drop all the bounding boxes that don’t actually contain a
            flower.                                                   
           2. Find the bounding box with the highest objectness score, and get rid of all the
            other bounding boxes that overlap a lot with it (e.g., with an IoU greater than
            60%). For example, in Figure 14-24, the bounding box with the max objectness
            score is the thick bounding box over the topmost rose (the objectness score is
            represented by the thickness of the bounding boxes). The other bounding box
                                                                      
                                                                      "|objectness output; non-max suppression
"                                                                      
                                                                      
                                                                      
                                                                      
          the buffer until it is empty. You must specify the buffer size, and it is important to
          make it large enough, or else shuffling will not be very effective.1 Just don’t exceed the
          amount of RAM you have, and even if you have plenty of it, there’s no need to go
          beyond the dataset’s size. You can provide a random seed if you want the same ran‐
          dom order every time you run your program. For example, the following code creates
          and displays a dataset containing the integers 0 to 9, repeated 3 times, shuffled using a
          buffer of size 5 and a random seed of 42, and batched with a batch size of 7:
            >>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times
            >>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)
            >>> for item in dataset:                                  
            ...  print(item)                                          
            ...                                                       
            tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)       
            tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)       
            tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)       
            tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)       
            tf.Tensor([3 6], shape=(2,), dtype=int64)                 
                   If you call repeat() on a shuffled dataset, by default it will generate
                   a new order at every iteration. This is generally a good idea, but if
                   you prefer to reuse the same order at each iteration (e.g., for tests
                   or debugging), you can set reshuffle_each_iteration=False.
                                                                      
          For a large dataset that does not fit in memory, this simple shuffling-buffer approach
          may not be sufficient, since the buffer will be small compared to the dataset. One sol‐
          ution is to shuffle the source data itself (for example, on Linux you can shuffle text
          files using the shuf command). This will definitely improve shuffling a lot! Even if
          the source data is shuffled, you will usually want to shuffle it some more, or else the
          same order will be repeated at each epoch, and the model may end up being biased
          (e.g., due to some spurious patterns present by chance in the source data’s order). To
          shuffle the instances some more, a common approach is to split the source data into
          multiple files, then read them in a random order during training. However, instances
          located in the same file will still end up close to each other. To avoid this you can pick
          multiple files randomly and read them simultaneously, interleaving their records.
          Then on top of that you can add a shuffling buffer using the shuffle() method. If all
                                                                      
                                                                      
                                                                      
                                                                      
          1 Imagine a sorted deck of cards on your left: suppose you just take the top three cards and shuffle them, then
           pick one randomly and put it to your right, keeping the other two in your hands. Take another card on your
           left, shuffle the three cards in your hands and pick one of them randomly, and put it on your right. When you
           are done going through all the cards like this, you will have a deck of cards on your right: do you think it will
           be perfectly shuffled?                                     "|shuffling-buffer approach
"                                                                      
                                                                      
                                                                      
                                                                      
          max_length                                                  
            The maximum size of the replay buffer. We created a large replay buffer that can
            store one million trajectories (as was done in the 2015 DQN paper). This will
            require a lot of RAM.                                     
                                                                      
                   When we store two consecutive trajectories, they contain two con‐
                   secutive observations with four frames each (since we used the Fra
                   meStack4 wrapper), and unfortunately three of the four frames in
                   the second observation are redundant (they are already present in
                   the first observation). In other words, we are using about four
                   times more RAM than necessary. To avoid this, you can instead use
                   a PyHashedReplayBuffer from the tf_agents.replay_buf
                   fers.py_hashed_replay_buffer package: it deduplicates data in
                   the stored trajectories along the last axis of the observations.
                                                                      
          Now we can create the observer that will write the trajectories to the replay buffer. An
          observer is just a function (or a callable object) that takes a trajectory argument, so we
          can directly use the add_method() method (bound to the replay_buffer object) as
          our observer:                                               
            replay_buffer_observer = replay_buffer.add_batch          
          If you wanted to create your own observer, you could write any function with a
          trajectory argument. If it must have a state, you can write a class with a
          __call__(self, trajectory) method. For example, here is a simple observer that
          will increment a counter every time it is called (except when the trajectory represents
          a boundary between two episodes, which does not count as a step), and every 100
          increments it displays the progress up to a given total (the carriage return \r along
          with end="""" ensures that the displayed counter remains on the same line):
                                                                      
            class ShowProgress:                                       
               def __init__(self, total):                             
                 self.counter = 0                                     
                 self.total = total                                   
               def __call__(self, trajectory):                        
                 if not trajectory.is_boundary():                     
                   self.counter += 1                                  
                 if self.counter % 100 == 0:                          
                   print(""\r{}/{}"".format(self.counter, self.total), end="""")
          Now let’s create a few training metrics.                    
          Creating Training Metrics                                   
          TF-Agents implements several RL metrics in the tf_agents.metrics package, some
          purely in Python and some based on TensorFlow. Let’s create a few of them in order
                                                                      "|training metrics
"                                                                      
                                                                      
                                                                      
                                                                      
          class, which can apply different transformers and concatenate their outputs. But you
          cannot specify different columns for each transformer; they all apply to the whole
          data. It is possible to work around this limitation using a custom transformer for col‐
          umn selection (see the Jupyter notebook for an example).    
                                                                      
          Select and Train a Model                                    
                                                                      
          At last! You framed the problem, you got the data and explored it, you sampled a
          training set and a test set, and you wrote transformation pipelines to clean up and
          prepare your data for Machine Learning algorithms automatically. You are now ready
          to select and train a Machine Learning model.               
                                                                      
          Training and Evaluating on the Training Set                 
          The good news is that thanks to all these previous steps, things are now going to be
          much simpler than you might think. Let’s first train a Linear Regression model, like
          we did in the previous chapter:                             
                                                                      
            from sklearn.linear_model import LinearRegression         
            lin_reg = LinearRegression()                              
            lin_reg.fit(housing_prepared, housing_labels)             
          Done! You now have a working Linear Regression model. Let’s try it out on a few
          instances from the training set:                            
                                                                      
            >>> some_data = housing.iloc[:5]                          
            >>> some_labels = housing_labels.iloc[:5]                 
            >>> some_data_prepared = full_pipeline.transform(some_data)
            >>> print(""Predictions:"", lin_reg.predict(some_data_prepared))
            Predictions: [ 210644.6045 317768.8069 210956.4333 59218.9888 189747.5584]
            >>> print(""Labels:"", list(some_labels))                   
            Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] 
          It works, although the predictions are not exactly accurate (e.g., the first prediction is
          off by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐
          ing set using Scikit-Learn’s mean_squared_error() function: 
            >>> from sklearn.metrics import mean_squared_error        
            >>> housing_predictions = lin_reg.predict(housing_prepared)
            >>> lin_mse = mean_squared_error(housing_labels, housing_predictions)
            >>> lin_rmse = np.sqrt(lin_mse)                           
            >>> lin_rmse                                              
            68628.19819848922                                         
          This is better than nothing, but clearly not a great score: most districts’ median_hous
          ing_values range between $120,000 and $265,000, so a typical prediction error of
          $68,628 is not very satisfying. This is an example of a model underfitting the training
          data. When this happens it can mean that the features do not provide enough"|training; model selection and training; mean_squared_error function; example project; data preparation; model selection
"                                                                      
                                                                      
                                                                      
                                                                      
            avoid division by zero). This technique avoids explosions in the activations due
            to excessive competition between the generator and the discriminator.
                                                                      
          The combination of all these techniques allowed the authors to generate extremely
          convincing high-definition images of faces. But what exactly do we call “convincing”?
          Evaluation is one of the big challenges when working with GANs: although it is possi‐
          ble to automatically evaluate the diversity of the generated images, judging their qual‐
          ity is a much trickier and subjective task. One technique is to use human raters, but
          this is costly and time-consuming. So the authors proposed to measure the similarity
          between the local image structure of the generated images and the training images,
          considering every scale. This idea led them to another groundbreaking innovation:
          StyleGANs.                                                  
          StyleGANs                                                   
                                                                      
          The state of the art in high-resolution image generation was advanced once again by
          the same Nvidia team in a 2018 paper18 that introduced the popular StyleGAN archi‐
          tecture. The authors used style transfer techniques in the generator to ensure that the
          generated images have the same local structure as the training images, at every scale,
          greatly improving the quality of the generated images. The discriminator and the loss
          function were not modified, only the generator. Let’s take a look at the StyleGAN. It is
          composed of two networks (see Figure 17-20):                
          Mapping network                                             
            An eight-layer MLP that maps the latent representations z (i.e., the codings) to a
            vector w. This vector is then sent through multiple affine transformations (i.e.,
            Dense layers with no activation functions, represented by the “A” boxes in
            Figure 17-20), which produces multiple vectors. These vectors control the style of
            the generated image at different levels, from fine-grained texture (e.g., hair color)
            to high-level features (e.g., adult or child). In short, the mapping network maps
            the codings to multiple style vectors.                    
                                                                      
          Synthesis network                                           
            Responsible for generating the images. It has a constant learned input (to be
            clear, this input will be constant after training, but during training it keeps getting
            tweaked by backpropagation). It processes this input through multiple convolu‐
            tional and upsampling layers, as earlier, but there are two twists: first, some noise
            is added to the input and to all the outputs of the convolutional layers (before the
            activation function). Second, each noise layer is followed by an Adaptive Instance
            Normalization (AdaIN) layer: it standardizes each feature map independently (by
                                                                      
                                                                      
          18 Tero Karras et al., “A Style-Based Generator Architecture for Generative Adversarial Networks,” arXiv pre‐
           print arXiv:1812.04948 (2018).                             "|adaptive instance normalization (AdaIN); StyleGANs; style transfer; affine transformations
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 17          
                                                                      
             Representation    Learning   and  Generative             
                                                                      
               Learning  Using  Autoencoders     and  GANs            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Autoencoders are artificial neural networks capable of learning dense representations
          of the input data, called latent representations or codings, without any supervision (i.e.,
          the training set is unlabeled). These codings typically have a much lower dimension‐
          ality than the input data, making autoencoders useful for dimensionality reduction
          (see Chapter 8), especially for visualization purposes. Autoencoders also act as feature
          detectors, and they can be used for unsupervised pretraining of deep neural networks
          (as we discussed in Chapter 11). Lastly, some autoencoders are generative models: they
          are capable of randomly generating new data that looks very similar to the training
          data. For example, you could train an autoencoder on pictures of faces, and it would
          then be able to generate new faces. However, the generated images are usually fuzzy
          and not entirely realistic.                                 
          In contrast, faces generated by generative adversarial networks (GANs) are now so
          convincing that it is hard to believe that the people they represent do not exist. You
          can judge so for yourself by visiting https://thispersondoesnotexist.com/, a website that
          shows faces generated by a recent GAN architecture called StyleGAN (you can also
          check out https://thisrentaldoesnotexist.com/ to see some generated Airbnb bed‐
          rooms). GANs are now widely used for super resolution (increasing the resolution of
          an image), colorization, powerful image editing (e.g., replacing photo bombers with
          realistic background), turning a simple sketch into a photorealistic image, predicting
          the next frames in a video, augmenting a dataset (to train other models), generating
          other types of data (such as text, audio, and time series), identifying the weaknesses in
          other models and strengthening them, and more.              
                                                                      
                                                                      
                                                                      "|StyleGANs; latent representations; autoencoders
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-9. MNIST compression that preserves 95% of the variance
          The equation of the inverse transformation is shown in Equation 8-3.
                                                                      
            Equation 8-3. PCA inverse transformation, back to the original number of
            dimensions                                                
                                                                      
                         ⊺                                            
            X     =X   W                                              
             recovered d‐proj d                                       
          Randomized PCA                                              
          If you set the svd_solver hyperparameter to ""randomized"", Scikit-Learn uses a sto‐
          chastic algorithm called Randomized PCA that quickly finds an approximation of the
          first d principal components. Its computational complexity is O(m × d2) + O(d3),
          instead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster
          than full SVD when d is much smaller than n:                
            rnd_pca = PCA(n_components=154, svd_solver=""randomized"")  
            X_reduced = rnd_pca.fit_transform(X_train)                
          By default, svd_solver is actually set to ""auto"": Scikit-Learn automatically uses the
          randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m
          or n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full
          SVD, you can set the svd_solver hyperparameter to ""full"".   
                                                                      
          Incremental PCA                                             
                                                                      
          One problem with the preceding implementations of PCA is that they require the
          whole training set to fit in memory in order for the algorithm to run. Fortunately,
          Incremental PCA (IPCA) algorithms have been developed. They allow you to split the
          training set into mini-batches and feed an IPCA algorithm one mini-batch at a time.
                                                                      "|full SVD approach; randomized; Incremental PCA (IPCA); inverse transformation; incremental; Randomized PCA; Randomized PCA algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
          that was either less than 49% female or more than 54% female. Either way, the survey
          results would be significantly biased.                      
                                                                      
          Suppose you chatted with experts who told you that the median income is a very
          important attribute to predict median housing prices. You may want to ensure that
          the test set is representative of the various categories of incomes in the whole dataset.
          Since the median income is a continuous numerical attribute, you first need to create
          an income category attribute. Let’s look at the median income histogram more closely
          (back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e.,
          $15,000–$60,000), but some median incomes go far beyond 6. It is important to have
          a sufficient number of instances in your dataset for each stratum, or else the estimate
          of a stratum’s importance may be biased. This means that you should not have too
          many strata, and each stratum should be large enough. The following code uses the
          pd.cut() function to create an income category attribute with five categories (labeled
          from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from
          1.5 to 3, and so on:                                        
            housing[""income_cat""] = pd.cut(housing[""median_income""],  
                              bins=[0., 1.5, 3.0, 4.5, 6., np.inf],   
                              labels=[1, 2, 3, 4, 5])                 
          These income categories are represented in Figure 2-9:      
            housing[""income_cat""].hist()                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-9. Histogram of income categories                  
                                                                      
          Now you are ready to do stratified sampling based on the income category. For this
          you can use Scikit-Learn’s StratifiedShuffleSplit class:    
                                                                      
                                                                      
                                                                      "|stratified sampling
"                                                                      
                                                                      
                                                                      
                                                                      
          This is useful for large training sets and for applying PCA online (i.e., on the fly, as
          new instances arrive).                                      
                                                                      
          The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s
          array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5 to
          reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like
          before). Note that you must call the partial_fit() method with each mini-batch,
          rather than the fit() method with the whole training set:   
            from sklearn.decomposition import IncrementalPCA          
                                                                      
            n_batches = 100                                           
            inc_pca = IncrementalPCA(n_components=154)                
            for X_batch in np.array_split(X_train, n_batches):        
               inc_pca.partial_fit(X_batch)                           
            X_reduced = inc_pca.transform(X_train)                    
          Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a
          large array stored in a binary file on disk as if it were entirely in memory; the class
          loads only the data it needs in memory, when it needs it. Since the IncrementalPCA
          class uses only a small part of the array at any given time, the memory usage remains
          under control. This makes it possible to call the usual fit() method, as you can see
          in the following code:                                      
            X_mm = np.memmap(filename, dtype=""float32"", mode=""readonly"", shape=(m, n))
                                                                      
            batch_size = m // n_batches                               
            inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
            inc_pca.fit(X_mm)                                         
          Kernel PCA                                                  
                                                                      
          In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly
          maps instances into a very high-dimensional space (called the feature space), enabling
          nonlinear classification and regression with Support Vector Machines. Recall that a
          linear decision boundary in the high-dimensional feature space corresponds to a
          complex nonlinear decision boundary in the original space.  
          It turns out that the same trick can be applied to PCA, making it possible to perform
          complex nonlinear projections for dimensionality reduction. This is called Kernel
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          5 Scikit-Learn uses the algorithm described in David A. Ross et al., “Incremental Learning for Robust Visual
           Tracking,” International Journal of Computer Vision 77, no. 1–3 (2008): 125–141."|array_split() function; Kernel PCA (kPCA); IncrementalPCA class; kernels; feature space; memmap class; original space
"                                                                      
                                                                      
                                                                      
                                                                      
           • Start the first few epochs using just one replica (this is called the warmup phase).
            Stale gradients tend to be more damaging at the beginning of training, when gra‐
            dients are typically large and the parameters have not settled into a valley of the
            cost function yet, so different replicas may push the parameters in quite different
            directions.                                               
                                                                      
          A paper published by the Google Brain team in 201620 benchmarked various
          approaches and found that using synchronous updates with a few spare replicas was
          more efficient than using asynchronous updates, not only converging faster but also
          producing a better model. However, this is still an active area of research, so you
          should not rule out asynchronous updates just yet.          
                                                                      
          Bandwidth saturation                                        
          Whether you use synchronous or asynchronous updates, data parallelism with cen‐
          tralized parameters still requires communicating the model parameters from the
          parameter servers to every replica at the beginning of each training step, and the gra‐
          dients in the other direction at the end of each training step. Similarly, when using the
          mirrored strategy, the gradients produced by each GPU will need to be shared with
          every other GPU. Unfortunately, there always comes a point where adding an extra
          GPU will not improve performance at all because the time spent moving the data into
          and out of GPU RAM (and across the network in a distributed setup) will outweigh
          the speedup obtained by splitting the computation load. At that point, adding more
          GPUs will just worsen the bandwidth saturation and actually slow down training.
                                                                      
                   For some models, typically relatively small and trained on a very
                   large training set, you are often better off training the model on a
                   single machine with a single powerful GPU with a large memory
                   bandwidth.                                         
                                                                      
          Saturation is more severe for large dense models, since they have a lot of parameters
          and gradients to transfer. It is less severe for small models (but the parallelization gain
          is limited) and for large sparse models, where the gradients are typically mostly zeros
          and so can be communicated efficiently. Jeff Dean, initiator and lead of the Google
          Brain project, reported typical speedups of 25–40× when distributing computations
          across 50 GPUs for dense models, and a 300× speedup for sparser models trained
          across 500 GPUs. As you can see, sparse models really do scale better. Here are a few
          concrete examples:                                          
                                                                      
                                                                      
                                                                      
                                                                      
          20 Jianmin Chen et al., “Revisiting Distributed Synchronous SGD,” arXiv preprint arXiv:1604.00981 (2016)."|bandwidth saturation; warmup phase
"                                                                      
                                                                      
                                                                      
                                                                      
                   The hist() method relies on Matplotlib, which in turn relies on a
                   user-specified graphical backend to draw on your screen. So before
                   you can plot anything, you need to specify which backend Matplot‐
                   lib should use. The simplest option is to use Jupyter’s magic com‐
                   mand %matplotlib inline. This tells Jupyter to set up Matplotlib
                   so it uses Jupyter’s own backend. Plots are then rendered within the
                   notebook itself. Note that calling show() is optional in a Jupyter
                   notebook, as Jupyter will automatically display plots when a cell is
                   executed.                                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-8. A histogram for each numerical attribute        
                                                                      
          There are a few things you might notice in these histograms:
                                                                      
           1. First, the median income attribute does not look like it is expressed in US dollars
            (USD). After checking with the team that collected the data, you are told that the
            data has been scaled and capped at 15 (actually, 15.0001) for higher median
            incomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers
            represent roughly tens of thousands of dollars (e.g., 3 actually means about
            $30,000). Working with preprocessed attributes is common in Machine Learning,
                                                                      
                                                                      "|histograms
"                                                                      
                                                                      
                                                                      
                                                                      
          The converter also optimizes the model, both to shrink it and to reduce its latency. It
          prunes all the operations that are not needed to make predictions (such as training
          operations), and it optimizes computations whenever possible; for example, 3×a +
          4×a + 5×a will be converted to (3 + 4 + 5)×a. It also tries to fuse operations whenever
          possible. For example, Batch Normalization layers end up folded into the previous
          layer’s addition and multiplication operations, whenever possible. To get a good idea
          of how much TFLite can optimize a model, download one of the pretrained TFLite
          models, unzip the archive, then open the excellent Netron graph visualization tool
          and upload the .pb file to view the original model. It’s a big, complex graph, right?
          Next, open the optimized .tflite model and marvel at its beauty!
          Another way you can reduce the model size (other than simply using smaller neural
          network architectures) is by using smaller bit-widths: for example, if you use half-
          floats (16 bits) rather than regular floats (32 bits), the model size will shrink by a fac‐
          tor of 2, at the cost of a (generally small) accuracy drop. Moreover, training will be
          faster, and you will use roughly half the amount of GPU RAM.
                                                                      
          TFLite’s converter can go further than that, by quantizing the model weights down to
          fixed-point, 8-bit integers! This leads to a fourfold size reduction compared to using
          32-bit floats. The simplest approach is called post-training quantization: it just quanti‐
          zes the weights after training, using a fairly basic but efficient symmetrical quantiza‐
          tion technique. It finds the maximum absolute weight value, m, then it maps the
          floating-point range –m to +m to the fixed-point (integer) range –127 to +127. For
          example (see Figure 19-8), if the weights range from –1.5 to +0.8, then the bytes –127,
          0, and +127 will correspond to the floats –1.5, 0.0, and +1.5, respectively. Note that
          0.0 always maps to 0 when using symmetrical quantization (also note that the byte
          values +68 to +127 will not be used, since they map to floats greater than +0.8).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-8. From 32-bit floats to 8-bit integers, using symmetrical quantization
                                                                      
          To perform this post-training quantization, simply add OPTIMIZE_FOR_SIZE to the list
          of converter optimizations before calling the convert() method:
            converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
                                                                      
          This technique dramatically reduces the model’s size, so it’s much faster to download
          and store. However, at runtime the quantized weights get converted back to floats
          before they are used (these recovered floats are not perfectly identical to the original"|post-training quantization
"                                                                      
                                                                      
                                                                      
                                                                      
          but with a regularization constraint. You can see that regularization forced the model
          to have a smaller slope: this model does not fit the training data (circles) as well as the
          first model, but it actually generalizes better to new examples that it did not see dur‐
          ing training (squares).                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-23. Regularization reduces the risk of overfitting 
                                                                      
          The amount of regularization to apply during learning can be controlled by a hyper‐
          parameter. A hyperparameter is a parameter of a learning algorithm (not of the
          model). As such, it is not affected by the learning algorithm itself; it must be set prior
          to training and remains constant during training. If you set the regularization hyper‐
          parameter to a very large value, you will get an almost flat model (a slope close to
          zero); the learning algorithm will almost certainly not overfit the training data, but it
          will be less likely to find a good solution. Tuning hyperparameters is an important
          part of building a Machine Learning system (you will see a detailed example in the
          next chapter).                                              
          Underfitting the Training Data                              
                                                                      
          As you might guess, underfitting is the opposite of overfitting: it occurs when your
          model is too simple to learn the underlying structure of the data. For example, a lin‐
          ear model of life satisfaction is prone to underfit; reality is just more complex than
          the model, so its predictions are bound to be inaccurate, even on the training
          examples.                                                   
          Here are the main options for fixing this problem:          
                                                                      
           • Select a more powerful model, with more parameters.      
                                                                      
           • Feed better features to the learning algorithm (feature engineering).
           • Reduce the constraints on the model (e.g., reduce the regularization hyperpara‐
            meter).                                                   
                                                                      
                                                                      "|underfitting; hyperparameters
"                                                                      
                                                                      
                                                                      
                                                                      
          recommendation system promoting news from last week). Perhaps even more impor‐
          tantly, a long training time will prevent you from experimenting with new ideas. In
          Machine Learning (as in many other fields), it is hard to know in advance which ideas
          will work, so you should try out as many as possible, as fast as possible. One way to
          speed up training is to use hardware accelerators such as GPUs or TPUs. To go even
          faster, you can train a model across multiple machines, each equipped with multiple
          hardware accelerators. TensorFlow’s simple yet powerful Distribution Strategies API
          makes this easy, as we will see.                            
                                                                      
          In this chapter we will look at how to deploy models, first to TF Serving, then to Goo‐
          gle Cloud AI Platform. We will also take a quick look at deploying models to mobile
          apps, embedded devices, and web apps. Lastly, we will discuss how to speed up com‐
          putations using GPUs and how to train models across multiple devices and servers
          using the Distribution Strategies API. That’s a lot of topics to discuss, so let’s get
          started!                                                    
          Serving a TensorFlow Model                                  
                                                                      
          Once you have trained a TensorFlow model, you can easily use it in any Python code:
          if it’s a tf.keras model, just call its predict() method! But as your infrastructure
          grows, there comes a point where it is preferable to wrap your model in a small ser‐
          vice whose sole role is to make predictions and have the rest of the infrastructure
          query it (e.g., via a REST or gRPC API).2 This decouples your model from the rest of
          the infrastructure, making it possible to easily switch model versions or scale the ser‐
          vice up as needed (independently from the rest of your infrastructure), perform A/B
          experiments, and ensure that all your software components rely on the same model
          versions. It also simplifies testing and development, and more. You could create your
          own microservice using any technology you want (e.g., using the Flask library), but
          why reinvent the wheel when you can just use TF Serving?    
                                                                      
          Using TensorFlow Serving                                    
          TF Serving is a very efficient, battle-tested model server that’s written in C++. It can
          sustain a high load, serve multiple versions of your models and watch a model reposi‐
          tory to automatically deploy the latest versions, and more (see Figure 19-1).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          2 A REST (or RESTful) API is an API that uses standard HTTP verbs, such as GET, POST, PUT, and DELETE,
           and uses JSON inputs and outputs. The gRPC protocol is more complex but more efficient. Data is exchanged
           using protocol buffers (see Chapter 13).                   "|Distribution Strategies API; serving TensorFlow models
"                                                                      
                                                                      
                                                                      
                                                                      
            tied_decoder = keras.models.Sequential([                  
               DenseTranspose(dense_2, activation=""selu""),            
               DenseTranspose(dense_1, activation=""sigmoid""),         
               keras.layers.Reshape([28, 28])                         
            ])                                                        
            tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])
          This model achieves a very slightly lower reconstruction error than the previous
          model, with almost half the number of parameters.           
                                                                      
          Training One Autoencoder at a Time                          
                                                                      
          Rather than training the whole stacked autoencoder in one go like we just did, it is
          possible to train one shallow autoencoder at a time, then stack all of them into a sin‐
          gle stacked autoencoder (hence the name), as shown in Figure 17-7. This technique is
          not used as much these days, but you may still run into papers that talk about “greedy
          layerwise training,” so it’s good to know what it means.    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-7. Training one autoencoder at a time             
                                                                      
          During the first phase of training, the first autoencoder learns to reconstruct the
          inputs. Then we encode the whole training set using this first autoencoder, and this
          gives us a new (compressed) training set. We then train a second autoencoder on this
          new dataset. This is the second phase of training. Finally, we build a big sandwich
          using all these autoencoders, as shown in Figure 17-7 (i.e., we first stack the hidden
          layers of each autoencoder, then the output layers in reverse order). This gives us the
          final stacked autoencoder (see the “Training One Autoencoder at a Time” section in
          the notebook for an implementation). We could easily train more autoencoders this
          way, building a very deep stacked autoencoder.              
                                                                      "|greedy layer-wise training
"                                                                      
                                                                      
                                                                      
                                                                      
          Wow! We jumped from 83.3% accuracy to 92.2%, although we are still only training
          the model on 50 instances. Since it is often costly and painful to label instances, espe‐
          cially when it has to be done manually by experts, it is a good idea to label representa‐
          tive instances rather than just random instances.           
                                                                      
          But perhaps we can go one step further: what if we propagated the labels to all the
          other instances in the same cluster? This is called label propagation:
            y_train_propagated = np.empty(len(X_train), dtype=np.int32)
            for i in range(k):                                        
               y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]
          Now let’s train the model again and look at its performance:
            >>> log_reg = LogisticRegression()                        
            >>> log_reg.fit(X_train, y_train_propagated)              
            >>> log_reg.score(X_test, y_test)                         
            0.9333333333333333                                        
          We got a reasonable accuracy boost, but nothing absolutely astounding. The problem
          is that we propagated each representative instance’s label to all the instances in the
          same cluster, including the instances located close to the cluster boundaries, which
          are more likely to be mislabeled. Let’s see what happens if we only propagate the
          labels to the 20% of the instances that are closest to the centroids:
            percentile_closest = 20                                   
                                                                      
            X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
            for i in range(k):                                        
               in_cluster = (kmeans.labels_ == i)                     
               cluster_dist = X_cluster_dist[in_cluster]              
               cutoff_distance = np.percentile(cluster_dist, percentile_closest)
               above_cutoff = (X_cluster_dist > cutoff_distance)      
               X_cluster_dist[in_cluster & above_cutoff] = -1         
            partially_propagated = (X_cluster_dist != -1)             
            X_train_partially_propagated = X_train[partially_propagated]
            y_train_partially_propagated = y_train_propagated[partially_propagated]
          Now let’s train the model again on this partially propagated dataset:
            >>> log_reg = LogisticRegression()                        
            >>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
            >>> log_reg.score(X_test, y_test)                         
            0.94                                                      
          Nice! With just 50 labeled instances (only 5 examples per class on average!), we got
          94.0% accuracy, which is pretty close to the performance of Logistic Regression on
          the fully labeled digits dataset (which was 96.9%). This good performance is due to
          the fact that the propagated labels are actually pretty good—their accuracy is very
          close to 99%, as the following code shows:                  "|label propagation
"                                                                      
                                                                      
                                                                      
                                                                      
                   For each principal component, PCA finds a zero-centered unit vec‐
                   tor pointing in the direction of the PC. Since two opposing unit
                   vectors lie on the same axis, the direction of the unit vectors
                   returned by PCA is not stable: if you perturb the training set
                   slightly and run PCA again, the unit vectors may point in the oppo‐
                   site direction as the original vectors. However, they will generally
                   still lie on the same axes. In some cases, a pair of unit vectors may
                   even rotate or swap (if the variances along these two axes are close),
                   but the plane they define will generally remain the same.
          So how can you find the principal components of a training set? Luckily, there is a
          standard matrix factorization technique called Singular Value Decomposition (SVD)
          that can decompose the training set matrix X into the matrix multiplication of three
          matrices U Σ V⊺, where V contains the unit vectors that define all the principal com‐
          ponents that we are looking for, as shown in Equation 8-1.  
                                                                      
            Equation 8-1. Principal components matrix                 
                                                                      
               ∣ ∣   ∣                                                
            V= c c ⋯ c                                                
                1 2  n                                                
               ∣ ∣   ∣                                                
          The following Python code uses NumPy’s svd() function to obtain all the principal
          components of the training set, then extracts the two unit vectors that define the first
          two PCs:                                                    
                                                                      
            X_centered = X - X.mean(axis=0)                           
            U, s, Vt = np.linalg.svd(X_centered)                      
            c1 = Vt.T[:, 0]                                           
            c2 = Vt.T[:, 1]                                           
                   PCA assumes that the dataset is centered around the origin. As we
                   will see, Scikit-Learn’s PCA classes take care of centering the data
                   for you. If you implement PCA yourself (as in the preceding exam‐
                   ple), or if you use other libraries, don’t forget to center the data
                   first.                                             
          Projecting Down to d Dimensions                             
                                                                      
          Once you have identified all the principal components, you can reduce the dimen‐
          sionality of the dataset down to d dimensions by projecting it onto the hyperplane
          defined by the first d principal components. Selecting this hyperplane ensures that the
          projection will preserve as much variance as possible. For example, in Figure 8-2 the
          3D dataset is projected down to the 2D plane defined by the first two principal
                                                                      "|svd() function; Singular Value Decomposition (SVD); projecting down to d dimensions
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                          Examples of Sampling Bias                   
                                                                      
           Perhaps the most famous example of sampling bias happened during the US presi‐
           dential election in 1936, which pitted Landon against Roosevelt: the Literary Digest
           conducted a very large poll, sending mail to about 10 million people. It got 2.4 million
           answers, and predicted with high confidence that Landon would get 57% of the votes.
           Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest’s
           sampling method:                                           
            • First, to obtain the addresses to send the polls to, the Literary Digest used tele‐
              phone directories, lists of magazine subscribers, club membership lists, and the
              like. All of these lists tended to favor wealthier people, who were more likely to
              vote Republican (hence Landon).                         
            • Second, less than 25% of the people who were polled answered. Again this intro‐
              duced a sampling bias, by potentially ruling out people who didn’t care much
              about politics, people who didn’t like the Literary Digest, and other key groups.
              This is a special type of sampling bias called nonresponse bias.
           Here is another example: say you want to build a system to recognize funk music vid‐
           eos. One way to build your training set is to search for “funk music” on YouTube and
           use the resulting videos. But this assumes that YouTube’s search engine returns a set of
           videos that are representative of all the funk music videos on YouTube. In reality, the
           search results are likely to be biased toward popular artists (and if you live in Brazil
           you will get a lot of “funk carioca” videos, which sound nothing like James Brown).
           On the other hand, how else can you get a large training set?
                                                                      
          Poor-Quality Data                                           
                                                                      
          Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-
          quality measurements), it will make it harder for the system to detect the underlying
          patterns, so your system is less likely to perform well. It is often well worth the effort
          to spend time cleaning up your training data. The truth is, most data scientists spend
          a significant part of their time doing just that. The following are a couple of examples
          of when you’d want to clean up training data:               
                                                                      
           • If some instances are clearly outliers, it may help to simply discard them or try to
            fix the errors manually.                                  
           • If some instances are missing a few features (e.g., 5% of your customers did not
            specify their age), you must decide whether you want to ignore this attribute alto‐
            gether, ignore these instances, fill in the missing values (e.g., with the median
            age), or train one model with the feature and one model without it.
                                                                      
                                                                      "|poor quality
"                                                                      
                                                                      
                                                                      
                                                                      
           • The righthand part is the decoder. During training, it takes the target sentence as
            input (also represented as a sequence of word IDs), shifted one time step to the
            right (i.e., a start-of-sequence token is inserted at the beginning). It also receives
            the outputs of the encoder (i.e., the arrows coming from the left side). Note that
            the top part of the decoder is also stacked N times, and the encoder stack’s final
            outputs are fed to the decoder at each of these N levels. Just like earlier, the
            decoder outputs a probability for each possible next word, at each time step (its
            output shape is [batch size, max output sentence length, vocabulary length]).
           • During inference, the decoder cannot be fed targets, so we feed it the previously
            output words (starting with a start-of-sequence token). So the model needs to be
            called repeatedly, predicting one more word at every round (which is fed to the
            decoder at the next round, until the end-of-sequence token is output).
                                                                      
           • Looking more closely, you can see that you are already familiar with most com‐
            ponents: there are two embedding layers, 5 × N skip connections, each of them
            followed by a layer normalization layer, 2 × N “Feed Forward” modules that are
            composed of two dense layers each (the first one using the ReLU activation func‐
            tion, the second with no activation function), and finally the output layer is a
            dense layer using the softmax activation function. All of these layers are time-
            distributed, so each word is treated independently of all the others. But how can
            we translate a sentence by only looking at one word at a time? Well, that’s where
            the new components come in:                               
             —The encoder’s Multi-Head Attention layer encodes each word’s relationship
              with every other word in the same sentence, paying more attention to the
              most relevant ones. For example, the output of this layer for the word “Queen”
              in the sentence “They welcomed the Queen of the United Kingdom” will
              depend on all the words in the sentence, but it will probably pay more atten‐
              tion to the words “United” and “Kingdom” than to the words “They” or “wel‐
              comed.” This attention mechanism is called self-attention (the sentence is
              paying attention to itself). We will discuss exactly how it works shortly. The
              decoder’s Masked Multi-Head Attention layer does the same thing, but each
              word is only allowed to attend to words located before it. Finally, the decoder’s
              upper Multi-Head Attention layer is where the decoder pays attention to the
              words in the input sentence. For example, the decoder will probably pay close
              attention to the word “Queen” in the input sentence when it is about to output
              this word’s translation.                                
             —The positional embeddings are simply dense vectors (much like word embed‐
              dings) that represent the position of a word in the sentence. The nth positional
              embedding is added to the word embedding of the nth word in each sentence.
              This gives the model access to each word’s position, which is needed because
              the Multi-Head Attention layers do not consider the order or the position of
              the words; they only look at their relationships. Since all the other layers are"|Masked Multi-Head Attention layer; Multi-Head Attention layer; end-of-sequence (EoS) token; dense vectors; positional encodings; self-attention mechanism
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-16. Median income versus median house value        
                                                                      
          This plot reveals a few things. First, the correlation is indeed very strong; you can
          clearly see the upward trend, and the points are not too dispersed. Second, the price
          cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this
          plot reveals other less obvious straight lines: a horizontal line around $450,000,
          another around $350,000, perhaps one around $280,000, and a few more below that.
          You may want to try removing the corresponding districts to prevent your algorithms
          from learning to reproduce these data quirks.               
          Experimenting with Attribute Combinations                   
                                                                      
          Hopefully the previous sections gave you an idea of a few ways you can explore the
          data and gain insights. You identified a few data quirks that you may want to clean up
          before feeding the data to a Machine Learning algorithm, and you found interesting
          correlations between attributes, in particular with the target attribute. You also
          noticed that some attributes have a tail-heavy distribution, so you may want to trans‐
          form them (e.g., by computing their logarithm). Of course, your mileage will vary
          considerably with each project, but the general ideas are similar.
          One last thing you may want to do before preparing the data for Machine Learning
          algorithms is to try out various attribute combinations. For example, the total num‐
          ber of rooms in a district is not very useful if you don’t know how many households
          there are. What you really want is the number of rooms per household. Similarly, the
          total number of bedrooms by itself is not very useful: you probably want to compare
          it to the number of rooms. And the population per household also seems like an
          interesting attribute combination to look at. Let’s create these new attributes:
                                                                      "|attribute combinations
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> ovr_clf.predict([some_digit])                         
            array([5], dtype=uint8)                                   
            >>> len(ovr_clf.estimators_)                              
            10                                                        
          Training an SGDClassifier (or a RandomForestClassifier) is just as easy:
            >>> sgd_clf.fit(X_train, y_train)                         
            >>> sgd_clf.predict([some_digit])                         
            array([5], dtype=uint8)                                   
          This time Scikit-Learn did not have to run OvR or OvO because SGD classifiers can
          directly classify instances into multiple classes. The decision_function() method
          now returns one value per class. Let’s look at the score that the SGD classifier assigned
          to each class:                                              
            >>> sgd_clf.decision_function([some_digit])               
            array([[-15955.22628, -38080.96296, -13326.66695, 573.52692, -17680.68466,
                  2412.53175, -25526.86498, -12290.15705, -7946.05205, -10631.35889]])
          You can see that the classifier is fairly confident about its prediction: almost all scores
          are largely negative, while class 5 has a score of 2412.5. The model has a slight doubt
          regarding class 3, which gets a score of 573.5. Now of course you want to evaluate this
          classifier. As usual, you can use cross-validation. Use the cross_val_score() func‐
          tion to evaluate the SGDClassifier’s accuracy:              
                                                                      
            >>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=""accuracy"")
            array([0.8489802 , 0.87129356, 0.86988048])               
          It gets over 84% on all test folds. If you used a random classifier, you would get 10%
          accuracy, so this is not such a bad score, but you can still do much better. Simply scal‐
          ing the inputs (as discussed in Chapter 2) increases accuracy above 89%:
            >>> from sklearn.preprocessing import StandardScaler      
            >>> scaler = StandardScaler()                             
            >>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))
            >>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=""accuracy"")
            array([0.89707059, 0.8960948 , 0.90693604])               
          Error Analysis                                              
                                                                      
          If this were a real project, you would now follow the steps in your Machine Learning
          project checklist (see Appendix B). You’d explore data preparation options, try out
          multiple models (shortlisting the best ones and fine-tuning their hyperparameters
          using GridSearchCV), and automate as much as possible. Here, we will assume that
          you have found a promising model and you want to find ways to improve it. One way
          to do this is to analyze the types of errors it makes.      
                                                                      
                                                                      
                                                                      "|error analysis
"                                                                      
                                                                      
                                                                      
                                                                      
          way. You get the picture. By looking at this MDP, can you guess which strategy will
          gain the most reward over time? In state s it is clear that action a is the best option,
                                   0            0                     
          and in state s the agent has no choice but to take action a , but in state s it is not
                  2                          1        1               
          obvious whether the agent should stay put (a ) or go through the fire (a ).
                                    0              2                  
          Bellman found a way to estimate the optimal state value of any state s, noted V*(s),
          which is the sum of all discounted future rewards the agent can expect on average
          after it reaches a state s, assuming it acts optimally. He showed that if the agent acts
          optimally, then the Bellman Optimality Equation applies (see Equation 18-1). This
          recursive equation says that if the agent acts optimally, then the optimal value of the
          current state is equal to the reward it will get on average after taking one optimal
          action, plus the expected optimal value of all possible next states that this action can
          lead to.                                                    
            Equation 18-1. Bellman Optimality Equation                
            V* s = max ∑ T s,a,s′ R s,a,s′ +γ·V* s′ for all s         
                   a s                                                
          In this equation:                                           
           • T(s, a, s′) is the transition probability from state s to state s′, given that the agent
            chose action a. For example, in Figure 18-8, T(s , a , s ) = 0.8.
                                        2 1 0                         
           • R(s, a, s′) is the reward that the agent gets when it goes from state s to state s′,
            given that the agent chose action a. For example, in Figure 18-8, R(s , a ,
                                                         2 1          
            s ) = +40.                                                
             0                                                        
           • γ is the discount factor.                                
          This equation leads directly to an algorithm that can precisely estimate the optimal
          state value of every possible state: you first initialize all the state value estimates to
          zero, and then you iteratively update them using the Value Iteration algorithm (see
          Equation 18-2). A remarkable result is that, given enough time, these estimates are
          guaranteed to converge to the optimal state values, corresponding to the optimal
          policy.                                                     
            Equation 18-2. Value Iteration algorithm                  
            V   s  max ∑T s,a,s′ R s,a,s′ +γ·V s′ for all s           
             k+1    a s′             k                                
          In this equation, V(s) is the estimated value of state s at the kth iteration of the
                      k                                               
          algorithm.                                                  
                                                                      
                                                                      "|optimal state value; Value Iteration algorithm; Bellman Optimality Equation
"                                                                      
                                                                      
                                                                      
                                                                      
          This can be written much more concisely using a vectorized form, as shown in Equa‐
          tion 4-2.                                                   
                                                                      
            Equation 4-2. Linear Regression model prediction (vectorized form)
                                                                      
            y =h x =θ·x                                               
               θ                                                      
          In this equation:                                           
                                                                      
           • θ is the model’s parameter vector, containing the bias term θ and the feature
                                                 0                    
            weights θ to θ .                                          
                  1  n                                                
           • x is the instance’s feature vector, containing x to x , with x always equal to 1.
                                      0  n    0                       
           • θ · x is the dot product of the vectors θ and x, which is of course equal to θ x +
                                                         0 0          
            θ x + θ x + ... + θ x .                                   
             1 1 2 2   n n                                            
           • h is the hypothesis function, using the model parameters θ.
             θ                                                        
                   In Machine Learning, vectors are often represented as column vec‐
                   tors, which are 2D arrays with a single column. If θ and x are col‐
                                           ⊺      ⊺                   
                   umn vectors, then the prediction is y =θ x, where θ is the
                                                    ⊺                 
                   transpose of θ (a row vector instead of a column vector) and θ x is
                                   ⊺                                  
                   the matrix multiplication of θ and x. It is of course the same pre‐
                   diction, except that it is now represented as a single-cell matrix
                   rather than a scalar value. In this book I will use this notation to
                   avoid switching between dot products and matrix multiplications.
          OK, that’s the Linear Regression model—but how do we train it? Well, recall that
          training a model means setting its parameters so that the model best fits the training
          set. For this purpose, we first need a measure of how well (or poorly) the model fits
          the training data. In Chapter 2 we saw that the most common performance measure
          of a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There‐
          fore, to train a Linear Regression model, we need to find the value of θ that minimi‐
          zes the RMSE. In practice, it is simpler to minimize the mean squared error (MSE)
          than the RMSE, and it leads to the same result (because the value that minimizes a
          function also minimizes its square root).1                  
          1 It is often the case that a learning algorithm will try to optimize a different function than the performance
           measure used to evaluate the final model. This is generally because that function is easier to compute, because
           it has useful differentiation properties that the performance measure lacks, or because we want to constrain
           the model during training, as you will see when we discuss regularization."|feature vectors; parameter vectors; feature vector; approaches to training; column vectors; parameter vector
"                                                                      
                                                                      
                                                                      
                                                                      
          Next we will look at Polynomial Regression, a more complex model that can fit non‐
          linear datasets. Since this model has more parameters than Linear Regression, it is
          more prone to overfitting the training data, so we will look at how to detect whether
          or not this is the case using learning curves, and then we will look at several regulari‐
          zation techniques that can reduce the risk of overfitting the training set.
                                                                      
          Finally, we will look at two more models that are commonly used for classification
          tasks: Logistic Regression and Softmax Regression.          
                                                                      
                   There will be quite a few math equations in this chapter, using basic
                   notions of linear algebra and calculus. To understand these equa‐
                   tions, you will need to know what vectors and matrices are; how to
                   transpose them, multiply them, and inverse them; and what partial
                   derivatives are. If you are unfamiliar with these concepts, please go
                   through the linear algebra and calculus introductory tutorials avail‐
                   able as Jupyter notebooks in the online supplemental material. For
                   those who are truly allergic to mathematics, you should still go
                   through this chapter and simply skip the equations; hopefully, the
                   text will be sufficient to help you understand most of the concepts.
          Linear Regression                                           
                                                                      
          In Chapter 1 we looked at a simple regression model of life satisfaction: life_satisfac‐
          tion = θ + θ × GDP_per_capita.                              
              0  1                                                    
          This model is just a linear function of the input feature GDP_per_capita. θ and θ are
                                                     0   1            
          the model’s parameters.                                     
          More generally, a linear model makes a prediction by simply computing a weighted
          sum of the input features, plus a constant called the bias term (also called the intercept
          term), as shown in Equation 4-1.                            
            Equation 4-1. Linear Regression model prediction          
            y =θ +θ x +θ x +⋯+θ x                                     
               0  1 1 2 2   n n                                       
          In this equation:                                           
                                                                      
           • ŷ is the predicted value.                                
           • n is the number of features.                             
           • x is the ith feature value.                              
             i                                                        
           • θ is the jth model parameter (including the bias term θ and the feature weights
             j                               0                        
            θ , θ , ⋯, θ ).                                           
             1 2   n                                                  "|bias terms; linear algebra; Polynomial Regression; Linear Regression; Linear Regression model; calculus; intercept terms
"                                                                      
                                                                      
                                                                      
                                                                      
          information to make good predictions, or that the model is not powerful enough. As
          we saw in the previous chapter, the main ways to fix underfitting are to select a more
          powerful model, to feed the training algorithm with better features, or to reduce the
          constraints on the model. This model is not regularized, which rules out the last
          option. You could try to add more features (e.g., the log of the population), but first
          let’s try a more complex model to see how it does.          
                                                                      
          Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding
          complex nonlinear relationships in the data (Decision Trees are presented in more
          detail in Chapter 6). The code should look familiar by now: 
            from sklearn.tree import DecisionTreeRegressor            
            tree_reg = DecisionTreeRegressor()                        
            tree_reg.fit(housing_prepared, housing_labels)            
                                                                      
          Now that the model is trained, let’s evaluate it on the training set:
            >>> housing_predictions = tree_reg.predict(housing_prepared)
            >>> tree_mse = mean_squared_error(housing_labels, housing_predictions)
            >>> tree_rmse = np.sqrt(tree_mse)                         
            >>> tree_rmse                                             
            0.0                                                       
          Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,
          it is much more likely that the model has badly overfit the data. How can you be sure?
          As we saw earlier, you don’t want to touch the test set until you are ready to launch a
          model you are confident about, so you need to use part of the training set for training
          and part of it for model validation.                        
          Better Evaluation Using Cross-Validation                    
                                                                      
          One way to evaluate the Decision Tree model would be to use the
          train_test_split() function to split the training set into a smaller training set and a
          validation set, then train your models against the smaller training set and evaluate
          them against the validation set. It’s a bit of work, but nothing too difficult, and it
          would work fairly well.                                     
          A great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The follow‐
          ing code randomly splits the training set into 10 distinct subsets called folds, then it
          trains and evaluates the Decision Tree model 10 times, picking a different fold for
          evaluation every time and training on the other 9 folds. The result is an array con‐
          taining the 10 evaluation scores:                           
            from sklearn.model_selection import cross_val_score       
            scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                           scoring=""neg_mean_squared_error"", cv=10)   
            tree_rmse_scores = np.sqrt(-scores)                       
                                                                      "|folds; evaluating; K-fold cross-validation; K-fold cross-validation feature; cross-validation
"                                                                      
                                                                      
                                                                      
                                                                      
          ResNet                                                      
                                                                      
          Kaiming He et al. won the ILSVRC 2015 challenge using a Residual Network (or
          ResNet),16 that delivered an astounding top-five error rate under 3.6%. The winning
          variant used an extremely deep CNN composed of 152 layers (other variants had 34,
          50, and 101 layers). It confirmed the general trend: models are getting deeper and
          deeper, with fewer and fewer parameters. The key to being able to train such a deep
          network is to use skip connections (also called shortcut connections): the signal feeding
          into a layer is also added to the output of a layer located a bit higher up the stack. Let’s
          see why this is useful.                                     
          When training a neural network, the goal is to make it model a target function h(x).
          If you add the input x to the output of the network (i.e., you add a skip connection),
          then the network will be forced to model f(x) = h(x) – x rather than h(x). This is
          called residual learning (see Figure 14-15).                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-15. Residual learning                             
                                                                      
          When you initialize a regular neural network, its weights are close to zero, so the net‐
          work just outputs values close to zero. If you add a skip connection, the resulting net‐
          work just outputs a copy of its inputs; in other words, it initially models the identity
          function. If the target function is fairly close to the identity function (which is often
          the case), this will speed up training considerably.        
          Moreover, if you add many skip connections, the network can start making progress
          even if several layers have not started learning yet (see Figure 14-16). Thanks to skip
          connections, the signal can easily make its way across the whole network. The deep
          residual network can be seen as a stack of residual units (RUs), where each residual
          unit is a small neural network with a skip connection.      
                                                                      
                                                                      
                                                                      
          16 Kaiming He et al., “Deep Residual Learning for Image Recognition,” arXiv preprint arXiv:1512:03385 (2015)."|skip connections; ResNet (Residual Network); residual learning; residual units; shortcut connections
"                                                                      
                                                                      
                                                                      
                                                                      
          Complex models such as deep neural networks can detect subtle patterns in the data,
          but if the training set is noisy, or if it is too small (which introduces sampling noise),
          then the model is likely to detect patterns in the noise itself. Obviously these patterns
          will not generalize to new instances. For example, say you feed your life satisfaction
          model many more attributes, including uninformative ones such as the country’s
          name. In that case, a complex model may detect patterns like the fact that all coun‐
          tries in the training data with a w in their name have a life satisfaction greater than 7:
          New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident
          are you that the w-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously
          this pattern occurred in the training data by pure chance, but the model has no way
          to tell whether a pattern is real or simply the result of noise in the data.
                                                                      
                   Overfitting happens when the model is too complex relative to the
                   amount and noisiness of the training data. Here are possible solu‐
                   tions:                                             
                    • Simplify the model by selecting one with fewer parameters
                     (e.g., a linear model rather than a high-degree polynomial
                     model), by reducing the number of attributes in the training
                     data, or by constraining the model.              
                    • Gather more training data.                      
                    • Reduce the noise in the training data (e.g., fix data errors and
                     remove outliers).                                
                                                                      
                                                                      
          Constraining a model to make it simpler and reduce the risk of overfitting is called
          regularization. For example, the linear model we defined earlier has two parameters,
          θ and θ . This gives the learning algorithm two degrees of freedom to adapt the model
           0   1                                                      
          to the training data: it can tweak both the height (θ ) and the slope (θ ) of the line. If
                                        0          1                  
          we forced θ = 0, the algorithm would have only one degree of freedom and would
                 1                                                    
          have a much harder time fitting the data properly: all it could do is move the line up
          or down to get as close as possible to the training instances, so it would end up
          around the mean. A very simple model indeed! If we allow the algorithm to modify θ
                                                           1          
          but we force it to keep it small, then the learning algorithm will effectively have some‐
          where in between one and two degrees of freedom. It will produce a model that’s sim‐
          pler than one with two degrees of freedom, but more complex than one with just one.
          You want to find the right balance between fitting the training data perfectly and
          keeping the model simple enough to ensure that it will generalize well.
          Figure 1-23 shows three models. The dotted line represents the original model that
          was trained on the countries represented as circles (without the countries represented
          as squares), the dashed line is our second model trained with all countries (circles and
          squares), and the solid line is a model trained with the same data as the first model"|regularization
"                                                                      
                                                                      
                                                                      
                                                                      
          Now the model fits the training data as closely as possible (for a linear model), as you
          can see in Figure 1-19.                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-19. The linear model that fits the training data best
                                                                      
          You are finally ready to run the model to make predictions. For example, say you
          want to know how happy Cypriots are, and the OECD data does not have the answer.
          Fortunately, you can use your model to make a good prediction: you look up Cyprus’s
          GDP per capita, find $22,587, and then apply your model and find that life satisfac‐
          tion is likely to be somewhere around 4.85 + 22,587 × 4.91 × 10-5 = 5.96.
          To whet your appetite, Example 1-1 shows the Python code that loads the data, pre‐
          pares it,6 creates a scatterplot for visualization, and then trains a linear model and
          makes a prediction.7                                        
                                                                      
          Example 1-1. Training and running a linear model using Scikit-Learn
                                                                      
          import matplotlib.pyplot as plt                             
          import numpy as np                                          
          import pandas as pd                                         
          import sklearn.linear_model                                 
          # Load the data                                             
          oecd_bli = pd.read_csv(""oecd_bli_2015.csv"", thousands=',')  
          gdp_per_capita = pd.read_csv(""gdp_per_capita.csv"",thousands=',',delimiter='\t',
                           encoding='latin1', na_values=""n/a"")        
                                                                      
                                                                      
                                                                      
          6 The prepare_country_stats() function’s definition is not shown here (see this chapter’s Jupyter notebook if
           you want all the gory details). It’s just boring pandas code that joins the life satisfaction data from the OECD
           with the GDP per capita data from the IMF.                 
          7 It’s OK if you don’t understand all the code yet; we will present Scikit-Learn in the following chapters."|linear model
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Nonlinear SVM Classification                                
                                                                      
          Although linear SVM classifiers are efficient and work surprisingly well in many
          cases, many datasets are not even close to being linearly separable. One approach to
          handling nonlinear datasets is to add more features, such as polynomial features (as
          you did in Chapter 4); in some cases this can result in a linearly separable dataset.
          Consider the left plot in Figure 5-5: it represents a simple dataset with just one fea‐
          ture, x . This dataset is not linearly separable, as you can see. But if you add a second
              1                                                       
          feature x = (x )2, the resulting 2D dataset is perfectly linearly separable.
               2  1                                                   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-5. Adding features to make a dataset linearly separable
          To implement this idea using Scikit-Learn, create a Pipeline containing a Polyno
          mialFeatures transformer (discussed in “Polynomial Regression” on page 128), fol‐
          lowed by a StandardScaler and a LinearSVC. Let’s test this on the moons dataset: this
          is a toy dataset for binary classification in which the data points are shaped as two
          interleaving half circles (see Figure 5-6). You can generate this dataset using the
          make_moons() function:                                      
                                                                      
            from sklearn.datasets import make_moons                   
            from sklearn.pipeline import Pipeline                     
            from sklearn.preprocessing import PolynomialFeatures      
            X, y = make_moons(n_samples=100, noise=0.15)              
            polynomial_svm_clf = Pipeline([                           
                 (""poly_features"", PolynomialFeatures(degree=3)),     
                 (""scaler"", StandardScaler()),                        
                 (""svm_clf"", LinearSVC(C=10, loss=""hinge""))           
               ])                                                     
            polynomial_svm_clf.fit(X, y)                              
                                                                      
                                                                      "|nonlinear SVM classification
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-1. Large margin classification                     
                                                                      
          Notice that adding more training instances “off the street” will not affect the decision
          boundary at all: it is fully determined (or “supported”) by the instances located on the
          edge of the street. These instances are called the support vectors (they are circled in
          Figure 5-1).                                                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-2. Sensitivity to feature scales                   
                                                                      
                   SVMs are sensitive to the feature scales, as you can see in
                   Figure 5-2: in the left plot, the vertical scale is much larger than the
                   horizontal scale, so the widest possible street is close to horizontal.
                   After feature scaling (e.g., using Scikit-Learn’s StandardScaler),
                   the decision boundary in the right plot looks much better.
          Soft Margin Classification                                  
                                                                      
          If we strictly impose that all instances must be off the street and on the right side, this
          is called hard margin classification. There are two main issues with hard margin clas‐
          sification. First, it only works if the data is linearly separable. Second, it is sensitive to
          outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on the left,
          it is impossible to find a hard margin; on the right, the decision boundary ends up
          very different from the one we saw in Figure 5-1 without the outlier, and it will prob‐
          ably not generalize as well.                                
                                                                      
                                                                      
                                                                      "|feature scaling; soft margin classification; support vectors; hard margin classification
"                                                                      
                                                                      
                                                                      
                                                                      
          Look at the Big Picture                                     
                                                                      
          Welcome to the Machine Learning Housing Corporation! Your first task is to use Cal‐
          ifornia census data to build a model of housing prices in the state. This data includes
          metrics such as the population, median income, and median housing price for each
          block group in California. Block groups are the smallest geographical unit for which
          the US Census Bureau publishes sample data (a block group typically has a popula‐
          tion of 600 to 3,000 people). We will call them “districts” for short.
          Your model should learn from this data and be able to predict the median housing
          price in any district, given all the other metrics.         
                                                                      
                                                                      
                   Since you are a well-organized data scientist, the first thing you
                   should do is pull out your Machine Learning project checklist. You
                   can start with the one in Appendix B; it should work reasonably
                   well for most Machine Learning projects, but make sure to adapt it
                   to your needs. In this chapter we will go through many checklist
                   items, but we will also skip a few, either because they are self-
                   explanatory or because they will be discussed in later chapters.
          Frame the Problem                                           
                                                                      
          The first question to ask your boss is what exactly the business objective is. Building a
          model is probably not the end goal. How does the company expect to use and benefit
          from this model? Knowing the objective is important because it will determine how
          you frame the problem, which algorithms you will select, which performance meas‐
          ure you will use to evaluate your model, and how much effort you will spend tweak‐
          ing it.                                                     
          Your boss answers that your model’s output (a prediction of a district’s median hous‐
          ing price) will be fed to another Machine Learning system (see Figure 2-2), along
          with many other signals.3 This downstream system will determine whether it is worth
          investing in a given area or not. Getting this right is critical, as it directly affects
          revenue.                                                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          3 A piece of information fed to a Machine Learning system is often called a signal, in reference to Claude Shan‐
           non’s information theory, which he developed at Bell Labs to improve telecommunications. His theory: you
           want a high signal-to-noise ratio.                         "|project goals; Machine Learning project checklist; framing the problem
"                                                                      
                                                                      
                                                                      
                                                                      
          deep net with over a million layers, and we would have a single (very long) instance
          to train it. Instead, we will use the dataset’s window() method to convert this long
          sequence of characters into many smaller windows of text. Every instance in the data‐
          set will be a fairly short substring of the whole text, and the RNN will be unrolled
          only over the length of these substrings. This is called truncated backpropagation
          through time. Let’s call the window() method to create a dataset of short text windows:
                                                                      
            n_steps = 100                                             
            window_length = n_steps + 1 # target = input shifted 1 character ahead
            dataset = dataset.window(window_length, shift=1, drop_remainder=True)
                   You can try tuning n_steps: it is easier to train RNNs on shorter
                   input sequences, but of course the RNN will not be able to learn
                   any pattern longer than n_steps, so don’t make it too small.
                                                                      
                                                                      
          By default, the window() method creates nonoverlapping windows, but to get the
          largest possible training set we use shift=1 so that the first window contains charac‐
          ters 0 to 100, the second contains characters 1 to 101, and so on. To ensure that all
          windows are exactly 101 characters long (which will allow us to create batches
          without having to do any padding), we set drop_remainder=True (otherwise the last
          100 windows will contain 100 characters, 99 characters, and so on down to 1
          character).                                                 
                                                                      
          The window() method creates a dataset that contains windows, each of which is also
          represented as a dataset. It’s a nested dataset, analogous to a list of lists. This is useful
          when you want to transform each window by calling its dataset methods (e.g., to
          shuffle them or batch them). However, we cannot use a nested dataset directly for
          training, as our model will expect tensors as input, not datasets. So, we must call the
          flat_map() method: it converts a nested dataset into a flat dataset (one that does not
          contain datasets). For example, suppose {1, 2, 3} represents a dataset containing the
          sequence of tensors 1, 2, and 3. If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}},
          you get back the flat dataset {1, 2, 3, 4, 5, 6}. Moreover, the flat_map() method takes
          a function as an argument, which allows you to transform each dataset in the nested
          dataset before flattening. For example, if you pass the function lambda ds:
          ds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5,
          6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset of tensors of size 2. With that
          in mind, we are ready to flatten our dataset:               
            dataset = dataset.flat_map(lambda window: window.batch(window_length))
          Notice that we call batch(window_length) on each window: since all windows have
          exactly that length, we will get a single tensor for each of them. Now the dataset con‐
          tains consecutive windows of 101 characters each. Since Gradient Descent works best
                                                                      "|nested datasets; flat datasets; truncated backpropagation through time
"                                                                      
                                                                      
                                                                      
                                                                      
          You can then compute the reconstruction pre-image error:    
                                                                      
            >>> from sklearn.metrics import mean_squared_error        
            >>> mean_squared_error(X, X_preimage)                     
            32.786308795766132                                        
          Now you can use grid search with cross-validation to find the kernel and hyperpara‐
          meters that minimize this error.                            
          LLE                                                         
                                                                      
          Locally Linear Embedding (LLE)8 is another powerful nonlinear dimensionality reduc‐
          tion (NLDR) technique. It is a Manifold Learning technique that does not rely on
          projections, like the previous algorithms do. In a nutshell, LLE works by first measur‐
          ing how each training instance linearly relates to its closest neighbors (c.n.), and then
          looking for a low-dimensional representation of the training set where these local
          relationships are best preserved (more details shortly). This approach makes it partic‐
          ularly good at unrolling twisted manifolds, especially when there is not too much
          noise.                                                      
                                                                      
          The following code uses Scikit-Learn’s LocallyLinearEmbedding class to unroll the
          Swiss roll:                                                 
            from sklearn.manifold import LocallyLinearEmbedding       
                                                                      
            lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
            X_reduced = lle.fit_transform(X)                          
          The resulting 2D dataset is shown in Figure 8-12. As you can see, the Swiss roll is
          completely unrolled, and the distances between instances are locally well preserved.
          However, distances are not preserved on a larger scale: the left part of the unrolled
          Swiss roll is stretched, while the right part is squeezed. Nevertheless, LLE did a pretty
          good job at modeling the manifold.                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          8 Sam T. Roweis and Lawrence K. Saul, “Nonlinear Dimensionality Reduction by Locally Linear Embedding,”
           Science 290, no. 5500 (2000): 2323–2326.                   "|PCA (Principal Component Analysis); Kernel PCA (kPCA); nonlinear dimensionality reduction (NLDR); LLE (Locally Linear Embedding)
"                                                                      
                                                                      
                                                                      
                                                                      
          One-class SVM                                               
            This algorithm is better suited for novelty detection. Recall that a kernelized
            SVM classifier separates two classes by first (implicitly) mapping all the instances
            to a high-dimensional space, then separating the two classes using a linear SVM
            classifier within this high-dimensional space (see Chapter 5). Since we just have
            one class of instances, the one-class SVM algorithm instead tries to separate the
            instances in high-dimensional space from the origin. In the original space, this
            will correspond to finding a small region that encompasses all the instances. If a
            new instance does not fall within this region, it is an anomaly. There are a few
            hyperparameters to tweak: the usual ones for a kernelized SVM, plus a margin
            hyperparameter that corresponds to the probability of a new instance being mis‐
            takenly considered as novel when it is in fact normal. It works great, especially
            with high-dimensional datasets, but like all SVMs it does not scale to large
            datasets.                                                 
                                                                      
          Exercises                                                   
                                                                      
           1. How would you define clustering? Can you name a few clustering algorithms?
           2. What are some of the main applications of clustering algorithms?
                                                                      
           3. Describe two techniques to select the right number of clusters when using
            K-Means.                                                  
           4. What is label propagation? Why would you implement it, and how?
           5. Can you name two clustering algorithms that can scale to large datasets? And
            two that look for regions of high density?                
           6. Can you think of a use case where active learning would be useful? How would
            you implement it?                                         
                                                                      
           7. What is the difference between anomaly detection and novelty detection?
           8. What is a Gaussian mixture? What tasks can you use it for?
           9. Can you name two techniques to find the right number of clusters when using a
            Gaussian mixture model?                                   
          10. The classic Olivetti faces dataset contains 400 grayscale 64 × 64–pixel images of
            faces. Each image is flattened to a 1D vector of size 4,096. 40 different people
            were photographed (10 times each), and the usual task is to train a model that
            can predict which person is represented in each picture. Load the dataset using
            the sklearn.datasets.fetch_olivetti_faces() function, then split it into a
            training set, a validation set, and a test set (note that the dataset is already scaled
            between 0 and 1). Since the dataset is quite small, you probably want to use strati‐
            fied sampling to ensure that there are the same number of images per person in
            each set. Next, cluster the images using K-Means, and ensure that you have a
                                                                      "|Gaussian mixtures model (GMM); one-class SVM algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
          possible by subnetworks called inception modules,14 which allow GoogLeNet to use
          parameters much more efficiently than previous architectures: GoogLeNet actually
          has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).
                                                                      
          Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +
          1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and ""same"" padding. The input
          signal is first copied and fed to four different layers. All convolutional layers use the
          ReLU activation function. Note that the second set of convolutional layers uses differ‐
          ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different
          scales. Also note that every single layer uses a stride of 1 and ""same"" padding (even
          the max pooling layer), so their outputs all have the same height and width as their
          inputs. This makes it possible to concatenate all the outputs along the depth dimen‐
          sion in the final depth concatenation layer (i.e., stack the feature maps from all four
          top convolutional layers). This concatenation layer can be implemented in Tensor‐
          Flow using the tf.concat() operation, with axis=3 (the axis is the depth).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-13. Inception module                              
                                                                      
          You may wonder why inception modules have convolutional layers with 1 × 1 ker‐
          nels. Surely these layers cannot capture any features because they look at only one
          pixel at a time? In fact, the layers serve three purposes:  
           • Although they cannot capture spatial patterns, they can capture patterns along
            the depth dimension.                                      
           • They are configured to output fewer feature maps than their inputs, so they serve
            as bottleneck layers, meaning they reduce dimensionality. This cuts the computa‐
                                                                      
                                                                      
                                                                      
          14 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams;
           hence the name of these modules.                           "|bottleneck layers; depth concat layer
"                                                                      
                                                                      
                                                                      
                                                                      
               keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=""same"",
                                activation=""selu""),                   
               keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=""same"",
                                activation=""sigmoid""),                
               keras.layers.Reshape([28, 28])                         
            ])                                                        
            conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])
          Recurrent Autoencoders                                      
                                                                      
          If you want to build an autoencoder for sequences, such as time series or text (e.g., for
          unsupervised learning or dimensionality reduction), then recurrent neural networks
          (see Chapter 15) may be better suited than dense networks. Building a recurrent
          autoencoder is straightforward: the encoder is typically a sequence-to-vector RNN
          which compresses the input sequence down to a single vector. The decoder is a
          vector-to-sequence RNN that does the reverse:               
            recurrent_encoder = keras.models.Sequential([             
               keras.layers.LSTM(100, return_sequences=True, input_shape=[None, 28]),
               keras.layers.LSTM(30)                                  
            ])                                                        
            recurrent_decoder = keras.models.Sequential([             
               keras.layers.RepeatVector(28, input_shape=[30]),       
               keras.layers.LSTM(100, return_sequences=True),         
               keras.layers.TimeDistributed(keras.layers.Dense(28, activation=""sigmoid""))
            ])                                                        
            recurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])
          This recurrent autoencoder can process sequences of any length, with 28 dimensions
          per time step. Conveniently, this means it can process Fashion MNIST images by
          treating each image as a sequence of rows: at each time step, the RNN will process a
          single row of 28 pixels. Obviously, you could use a recurrent autoencoder for any
          kind of sequence. Note that we use a RepeatVector layer as the first layer of the
          decoder, to ensure that its input vector gets fed to the decoder at each time step.
          OK, let’s step back for a second. So far we have seen various kinds of autoencoders
          (basic, stacked, convolutional, and recurrent), and we have looked at how to train
          them (either in one shot or layer by layer). We also looked at a couple applications:
          data visualization and unsupervised pretraining.            
          Up to now, in order to force the autoencoder to learn interesting features, we have
          limited the size of the coding layer, making it undercomplete. There are actually
          many other kinds of constraints that can be used, including ones that allow the cod‐
          ing layer to be just as large as the inputs, or even larger, resulting in an overcomplete
          autoencoder. Let’s look at some of those approaches now.    
                                                                      
                                                                      "|overcomplete autoencoders; recurrent autoencoders; recurrent
"                                                                      
                                                                      
                                                                      
                                                                      
          The paper also introduced several other techniques aimed at increasing the diversity
          of the outputs (to avoid mode collapse) and making training more stable:
                                                                      
          Minibatch standard deviation layer                          
            Added near the end of the discriminator. For each position in the inputs, it com‐
            putes the standard deviation across all channels and all instances in the batch
            (S = tf.math.reduce_std(inputs, axis=[0, -1])). These standard deviations
            are then averaged across all points to get a single value (v = tf.reduce_
            mean(S)). Finally, an extra feature map is added to each instance in the batch and
            filled with the computed value (tf.concat([inputs, tf.fill([batch_size,
            height, width, 1], v)], axis=-1)). How does this help? Well, if the genera‐
            tor produces images with little variety, then there will be a small standard devia‐
            tion across feature maps in the discriminator. Thanks to this layer, the
            discriminator will have easy access to this statistic, making it less likely to be
            fooled by a generator that produces too little diversity. This will encourage the
            generator to produce more diverse outputs, reducing the risk of mode collapse.
          Equalized learning rate                                     
            Initializes all weights using a simple Gaussian distribution with mean 0 and stan‐
            dard deviation 1 rather than using He initialization. However, the weights are
            scaled down at runtime (i.e., every time the layer is executed) by the same factor
            as in He initialization: they are divided by 2/n , where n is the number
                                         inputs  inputs               
            of inputs to the layer. The paper demonstrated that this technique significantly
            improved the GAN’s performance when using RMSProp, Adam, or other adap‐
            tive gradient optimizers. Indeed, these optimizers normalize the gradient updates
            by their estimated standard deviation (see Chapter 11), so parameters that have a
            larger dynamic range17 will take longer to train, while parameters with a small
            dynamic range may be updated too quickly, leading to instabilities. By rescaling
            the weights as part of the model itself rather than just rescaling them upon initi‐
            alization, this approach ensures that the dynamic range is the same for all param‐
            eters, throughout training, so they all learn at the same speed. This both speeds
            up and stabilizes training.                               
          Pixelwise normalization layer                               
            Added after each convolutional layer in the generator. It normalizes each activa‐
            tion based on all the activations in the same image and at the same location, but
            across all channels (dividing by the square root of the mean squared activation).
            In TensorFlow code, this is inputs / tf.sqrt(tf.reduce_mean(tf.square(X),
            axis=-1, keepdims=True) + 1e-8) (the smoothing term 1e-8 is needed to
                                                                      
                                                                      
                                                                      
          17 The dynamic range of a variable is the ratio between the highest and the lowest value it may take."|learning rate; normalization; pixelwise normalization layers; minibatch standard deviation layer; equalized learning rates
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-6. A regression problem: predict a value, given an input feature (there are usu‐
          ally multiple input features, and sometimes multiple output values)
                                                                      
          Here are some of the most important supervised learning algorithms (covered in this
          book):                                                      
                                                                      
           • k-Nearest Neighbors                                      
           • Linear Regression                                        
           • Logistic Regression                                      
                                                                      
           • Support Vector Machines (SVMs)                           
           • Decision Trees and Random Forests                        
           • Neural networks2                                         
                                                                      
                                                                      
          Unsupervised learning                                       
          In unsupervised learning, as you might guess, the training data is unlabeled
          (Figure 1-7). The system tries to learn without a teacher.  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann
           machines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining."|unsupervised learning; clustering; supervised learning
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-13. California housing prices: red is expensive, blue is cheap, larger circles indi‐
          cate areas with a larger population                         
                                                                      
          This image tells you that the housing prices are very much related to the location
          (e.g., close to the ocean) and to the population density, as you probably knew already.
          A clustering algorithm should be useful for detecting the main cluster and for adding
          new features that measure the proximity to the cluster centers. The ocean proximity
          attribute may be useful as well, although in Northern California the housing prices in
          coastal districts are not too high, so it is not a simple rule.
          Looking for Correlations                                    
                                                                      
          Since the dataset is not too large, you can easily compute the standard correlation
          coefficient (also called Pearson’s r) between every pair of attributes using the corr()
          method:                                                     
                                                                      
            corr_matrix = housing.corr()                              
          Now let’s look at how much each attribute correlates with the median house value:
            >>> corr_matrix[""median_house_value""].sort_values(ascending=False)
            median_house_value 1.000000                               
            median_income 0.687170                                    
            total_rooms  0.135231                                     
            housing_median_age 0.114220                               "|standard correlation coefficient; computing correlations; Pearson's r; correlation coefficient
"                                                                      
                                                                      
                                                                      
                                                                      
           • There is a gap between the curves. This means that the model performs signifi‐
            cantly better on the training data than on the validation data, which is the hall‐
            mark of an overfitting model. If you used a much larger training set, however, the
            two curves would continue to get closer.                  
                                                                      
                                                                      
                   One way to improve an overfitting model is to feed it more training
                   data until the validation error reaches the training error.
                                                                      
                                                                      
                                                                      
                                                                      
                         The Bias/Variance Trade-off                  
           An important theoretical result of statistics and Machine Learning is the fact that a
           model’s generalization error can be expressed as the sum of three very different
           errors:                                                    
                                                                      
           Bias                                                       
              This part of the generalization error is due to wrong assumptions, such as assum‐
              ing that the data is linear when it is actually quadratic. A high-bias model is most
              likely to underfit the training data.8                  
           Variance                                                   
              This part is due to the model’s excessive sensitivity to small variations in the
              training data. A model with many degrees of freedom (such as a high-degree pol‐
              ynomial model) is likely to have high variance and thus overfit the training data.
           Irreducible error                                          
              This part is due to the noisiness of the data itself. The only way to reduce this
              part of the error is to clean up the data (e.g., fix the data sources, such as broken
              sensors, or detect and remove outliers).                
           Increasing a model’s complexity will typically increase its variance and reduce its bias.
           Conversely, reducing a model’s complexity increases its bias and reduces its variance.
           This is why it is called a trade-off.                      
                                                                      
                                                                      
          Regularized Linear Models                                   
                                                                      
          As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the
          model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be
                                                                      
                                                                      
                                                                      
          8 This notion of bias is not to be confused with the bias term of linear models."|learning curves; bias/variance trade-off; regularized linear models
"                                                                      
                                                                      
                                                                      
                                                                      
          Using masking layers and automatic mask propagation works best for simple
          Sequential models. It will not always work for more complex models, such as when
          you need to mix Conv1D layers with recurrent layers. In such cases, you will need to
          explicitly compute the mask and pass it to the appropriate layers, using either the
          Functional API or the Subclassing API. For example, the following model is identical
          to the previous model, except it is built using the Functional API and handles mask‐
          ing manually:                                               
                                                                      
            K = keras.backend                                         
            inputs = keras.layers.Input(shape=[None])                 
            mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)
            z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)
            z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)
            z = keras.layers.GRU(128)(z, mask=mask)                   
            outputs = keras.layers.Dense(1, activation=""sigmoid"")(z)  
            model = keras.Model(inputs=[inputs], outputs=[outputs])   
          After training for a few epochs, this model will become quite good at judging whether
          a review is positive or not. If you use the TensorBoard() callback, you can visualize
          the embeddings in TensorBoard as they are being learned: it is fascinating to see
          words like “awesome” and “amazing” gradually cluster on one side of the embedding
          space, while words like “awful” and “terrible” cluster on the other side. Some words
          are not as positive as you might expect (at least with this model), such as the word
          “good,” presumably because many negative reviews contain the phrase “not good.” It’s
          impressive that the model is able to learn useful word embeddings based on just
          25,000 movie reviews. Imagine how good the embeddings would be if we had billions
          of reviews to train on! Unfortunately we don’t, but perhaps we can reuse word
          embeddings trained on some other large text corpus (e.g., Wikipedia articles), even if
          it is not composed of movie reviews? After all, the word “amazing” generally has the
          same meaning whether you use it to talk about movies or anything else. Moreover,
          perhaps embeddings would be useful for sentiment analysis even if they were trained
          on another task: since words like “awesome” and “amazing” have a similar meaning,
          they will likely cluster in the embedding space even for other tasks (e.g., predicting
          the next word in a sentence). If all positive words and all negative words form clus‐
          ters, then this will be helpful for sentiment analysis. So instead of using so many
          parameters to learn word embeddings, let’s see if we can’t just reuse pretrained
          embeddings.                                                 
          Reusing Pretrained Embeddings                               
          The TensorFlow Hub project makes it easy to reuse pretrained model components in
          your own models. These model components are called modules. Simply browse the
          TF Hub repository, find the one you need, and copy the code example into your
          project, and the module will be automatically downloaded, along with its pretrained
          weights, and included in your model. Easy!                  "|modules; reusing pretrained embeddings; TensorFlow Hub
"                                                                      
                                                                      
                                                                      
                                                                      
          Centroid initialization methods                             
                                                                      
          If you happen to know approximately where the centroids should be (e.g., if you ran
          another clustering algorithm earlier), then you can set the init hyperparameter to a
          NumPy array containing the list of centroids, and set n_init to 1:
            good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])
            kmeans = KMeans(n_clusters=5, init=good_init, n_init=1)   
          Another solution is to run the algorithm multiple times with different random initial‐
          izations and keep the best solution. The number of random initializations is con‐
          trolled by the n_init hyperparameter: by default, it is equal to 10, which means that
          the whole algorithm described earlier runs 10 times when you call fit(), and Scikit-
          Learn keeps the best solution. But how exactly does it know which solution is the
          best? It uses a performance metric! That metric is called the model’s inertia, which is
          the mean squared distance between each instance and its closest centroid. It is
          roughly equal to 223.3 for the model on the left in Figure 9-5, 237.5 for the model on
          the right in Figure 9-5, and 211.6 for the model in Figure 9-3. The KMeans class runs
          the algorithm n_init times and keeps the model with the lowest inertia. In this
          example, the model in Figure 9-3 will be selected (unless we are very unlucky with
          n_init consecutive random initializations). If you are curious, a model’s inertia is
          accessible via the inertia_ instance variable:              
            >>> kmeans.inertia_                                       
            211.59853725816856                                        
          The score() method returns the negative inertia. Why negative? Because a predic‐
                                                                      
          tor’s score() method must always respect Scikit-Learn’s “greater is better” rule: if a
          predictor is better than another, its score() method should return a greater score.
            >>> kmeans.score(X)                                       
            -211.59853725816856                                       
          An important improvement to the K-Means algorithm, K-Means++, was proposed in
          a 2006 paper by David Arthur and Sergei Vassilvitskii.3 They introduced a smarter
          initialization step that tends to select centroids that are distant from one another, and
          this improvement makes the K-Means algorithm much less likely to converge to a
          suboptimal solution. They showed that the additional computation required for the
          smarter initialization step is well worth it because it makes it possible to drastically
          reduce the number of times the algorithm needs to be run to find the optimal solu‐
          tion. Here is the K-Means++ initialization algorithm:       
                                                                      
           1. Take one centroid c(1), chosen uniformly at random from the dataset.
                                                                      
                                                                      
          3 David Arthur and Sergei Vassilvitskii, “k-Means++: The Advantages of Careful Seeding,” Proceedings of the
           18th Annual ACM-SIAM Symposium on Discrete Algorithms (2007): 1027–1035."|inertia; centroid initialization methods; proposed improvement to
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 19          
                                                                      
                   Training  and  Deploying   TensorFlow              
                                                                      
                                          Models   at Scale           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Once you have a beautiful model that makes amazing predictions, what do you do
          with it? Well, you need to put it in production! This could be as simple as running the
          model on a batch of data and perhaps writing a script that runs this model every
          night. However, it is often much more involved. Various parts of your infrastructure
          may need to use this model on live data, in which case you probably want to wrap
          your model in a web service: this way, any part of your infrastructure can query your
          model at any time using a simple REST API (or some other protocol), as we discussed
          in Chapter 2. But as time passes, you need to regularly retrain your model on fresh
          data and push the updated version to production. You must handle model versioning,
          gracefully transition from one model to the next, possibly roll back to the previous
          model in case of problems, and perhaps run multiple different models in parallel to
          perform A/B experiments.1 If your product becomes successful, your service may start
          to get plenty of queries per second (QPS), and it must scale up to support the load. A
          great solution to scale up your service, as we will see in this chapter, is to use TF Serv‐
          ing, either on your own hardware infrastructure or via a cloud service such as Google
          Cloud AI Platform. It will take care of efficiently serving your model, handle graceful
          model transitions, and more. If you use the cloud platform, you will also get many
          extra features, such as powerful monitoring tools.          
          Moreover, if you have a lot of training data, and compute-intensive models, then
          training time may be prohibitively long. If your product needs to adapt to changes
          quickly, then a long training time can be a showstopper (e.g., think of a news
                                                                      
                                                                      
          1 An A/B experiment consists in testing two different versions of your product on different subsets of users in
           order to check which version works best and get other insights."|queries per second (QPS); A/B experiments; TensorFlow, model deployment at scale
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-11. Tuning the number of trees using early stopping
                                                                      
          It is also possible to implement early stopping by actually stopping training early
          (instead of training a large number of trees first and then looking back to find the
          optimal number). You can do so by setting warm_start=True, which makes Scikit-
          Learn keep existing trees when the fit() method is called, allowing incremental
          training. The following code stops training when the validation error does not
          improve for five iterations in a row:                       
            gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)
                                                                      
            min_val_error = float(""inf"")                              
            error_going_up = 0                                        
            for n_estimators in range(1, 120):                        
               gbrt.n_estimators = n_estimators                       
               gbrt.fit(X_train, y_train)                             
               y_pred = gbrt.predict(X_val)                           
               val_error = mean_squared_error(y_val, y_pred)          
               if val_error < min_val_error:                          
                 min_val_error = val_error                            
                 error_going_up = 0                                   
               else:                                                  
                 error_going_up += 1                                  
                 if error_going_up == 5:                              
                   break # early stopping                             
          The GradientBoostingRegressor class also supports a subsample hyperparameter,
          which specifies the fraction of training instances to be used for training each tree. For
          example, if subsample=0.25, then each tree is trained on 25% of the training instan‐
          ces, selected randomly. As you can probably guess by now, this technique trades a
          higher bias for a lower variance. It also speeds up training considerably. This is called
          Stochastic Gradient Boosting.                               "|Stochastic Gradient Boosting; incremental training
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Equation 5-13. Linear SVM classifier cost function        
                                                                      
                          m                                           
            J w,b = 1 w ⊺ w + C ∑ max 0,1−t i w ⊺ x i +b              
                 2                                                    
                          i=1                                         
          The first sum in the cost function will push the model to have a small weight vector
          w, leading to a larger margin. The second sum computes the total of all margin viola‐
          tions. An instance’s margin violation is equal to 0 if it is located off the street and on
          the correct side, or else it is proportional to the distance to the correct side of the
          street. Minimizing this term ensures that the model makes the margin violations as
          small and as few as possible.                               
                               Hinge Loss                             
                                                                      
           The function max(0, 1 – t) is called the hinge loss function (see the following image).
           It is equal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is
           not differentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”
           on page 137), you can still use Gradient Descent using any subderivative at t = 1 (i.e.,
           any value between –1 and 0).                               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          It is also possible to implement online kernelized SVMs, as described in the papers
          “Incremental and Decremental Support Vector Machine Learning”8 and “Fast Kernel
          Classifiers with Online and Active Learning”.9 These kernelized SVMs are imple‐
                                                                      
                                                                      
                                                                      
          8 Gert Cauwenberghs and Tomaso Poggio, “Incremental and Decremental Support Vector Machine Learning,”
           Proceedings of the 13th International Conference on Neural Information Processing Systems (2000): 388–394.
          9 Antoine Bordes et al., “Fast Kernel Classifiers with Online and Active Learning,” Journal of Machine Learning
           Research 6 (2005): 1579–1619.                              "|hinge loss function; hinge loss; subderivatives
"                                                                      
                                                                      
                                                                      
                                                                      
            ter 10), and get it to output the most likely next purchase. This neural net would
            typically be trained on past sequences of purchases across all clients.
                                                                      
          Building an intelligent bot for a game                      
            This is often tackled using Reinforcement Learning (RL; see Chapter 18), which
            is a branch of Machine Learning that trains agents (such as bots) to pick the
            actions that will maximize their rewards over time (e.g., a bot may get a reward
            every time the player loses some life points), within a given environment (such as
            the game). The famous AlphaGo program that beat the world champion at the
            game of Go was built using RL.                            
          This list could go on and on, but hopefully it gives you a sense of the incredible
          breadth and complexity of the tasks that Machine Learning can tackle, and the types
          of techniques that you would use for each task.             
                                                                      
          Types of Machine Learning Systems                           
                                                                      
          There are so many different types of Machine Learning systems that it is useful to
          classify them in broad categories, based on the following criteria:
                                                                      
           • Whether or not they are trained with human supervision (supervised, unsuper‐
            vised, semisupervised, and Reinforcement Learning)        
           • Whether or not they can learn incrementally on the fly (online versus batch
            learning)                                                 
           • Whether they work by simply comparing new data points to known data points,
            or instead by detecting patterns in the training data and building a predictive
            model, much like scientists do (instance-based versus model-based learning)
                                                                      
          These criteria are not exclusive; you can combine them in any way you like. For
          example, a state-of-the-art spam filter may learn on the fly using a deep neural net‐
          work model trained using examples of spam and ham; this makes it an online, model-
          based, supervised learning system.                          
          Let’s look at each of these criteria a bit more closely.    
                                                                      
          Supervised/Unsupervised Learning                            
                                                                      
          Machine Learning systems can be classified according to the amount and type of
          supervision they get during training. There are four major categories: supervised
          learning, unsupervised learning, semisupervised learning, and Reinforcement
          Learning.                                                   
                                                                      
                                                                      
                                                                      "|labels; supervised learning
"                                                                      
                                                                      
                                                                      
                                                                      
          # Prepare the data                                          
          country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)
          X = np.c_[country_stats[""GDP per capita""]]                  
          y = np.c_[country_stats[""Life satisfaction""]]               
          # Visualize the data                                        
          country_stats.plot(kind='scatter', x=""GDP per capita"", y='Life satisfaction')
          plt.show()                                                  
          # Select a linear model                                     
          model = sklearn.linear_model.LinearRegression()             
                                                                      
          # Train the model                                           
          model.fit(X, y)                                             
          # Make a prediction for Cyprus                              
          X_new = [[22587]] # Cyprus's GDP per capita                 
          print(model.predict(X_new)) # outputs [[ 5.96242338]]       
                                                                      
                   If you had used an instance-based learning algorithm instead, you
                   would have found that Slovenia has the closest GDP per capita to
                   that of Cyprus ($20,732), and since the OECD data tells us that
                   Slovenians’ life satisfaction is 5.7, you would have predicted a life
                   satisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the
                   two next-closest countries, you will find Portugal and Spain with
                   life satisfactions of 5.1 and 6.5, respectively. Averaging these three
                   values, you get 5.77, which is pretty close to your model-based pre‐
                   diction. This simple algorithm is called k-Nearest Neighbors regres‐
                   sion (in this example, k = 3).                     
                   Replacing the Linear Regression model with k-Nearest Neighbors
                   regression in the previous code is as simple as replacing these two
                   lines:                                             
                     import sklearn.linear_model                      
                     model = sklearn.linear_model.LinearRegression()  
                   with these two:                                    
                     import sklearn.neighbors                         
                     model = sklearn.neighbors.KNeighborsRegressor(   
                        n_neighbors=3)                                
          If all went well, your model will make good predictions. If not, you may need to use
          more attributes (employment rate, health, air pollution, etc.), get more or better-
          quality training data, or perhaps select a more powerful model (e.g., a Polynomial
          Regression model).                                          
                                                                      
                                                                      
                                                                      "|k-Nearest Neighbors regression; instance-based learning
"                                                                      
                                                                      
                                                                      
                                                                      
          floats, but not too far off, so the accuracy loss is usually acceptable). To avoid recom‐
          puting them all the time, the recovered floats are cached, so there is no reduction of
          RAM usage. And there is no reduction either in compute speed.
                                                                      
          The most effective way to reduce latency and power consumption is to also quantize
          the activations so that the computations can be done entirely with integers, without
          the need for any floating-point operations. Even when using the same bit-width (e.g.,
          32-bit integers instead of 32-bit floats), integer computations use less CPU cycles,
          consume less energy, and produce less heat. And if you also reduce the bit-width (e.g.,
          down to 8-bit integers), you can get huge speedups. Moreover, some neural network
          accelerator devices (such as the Edge TPU) can only process integers, so full quanti‐
          zation of both weights and activations is compulsory. This can be done post-training;
          it requires a calibration step to find the maximum absolute value of the activations, so
          you need to provide a representative sample of training data to TFLite (it does not
          need to be huge), and it will process the data through the model and measure the
          activation statistics required for quantization (this step is typically fast).
          The main problem with quantization is that it loses a bit of accuracy: it is equivalent
          to adding noise to the weights and activations. If the accuracy drop is too severe, then
          you may need to use quantization-aware training. This means adding fake quantiza‐
          tion operations to the model so it can learn to ignore the quantization noise during
          training; the final weights will then be more robust to quantization. Moreover, the
          calibration step can be taken care of automatically during training, which simplifies
          the whole process.                                          
          I have explained the core concepts of TFLite, but going all the way to coding a mobile
          app or an embedded program would require a whole other book. Fortunately, one
          exists: if you want to learn more about building TensorFlow applications for mobile
          and embedded devices, check out the O’Reilly book TinyML: Machine Learning with
          TensorFlow on Arduino and Ultra-Low Power Micro-Controllers, by Pete Warden (who
          leads the TFLite team) and Daniel Situnayake.               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|fake quantization; quantization-aware training
"                                                                      
                                                                      
                                                                      
                                                                      
          preprocessed once (instead of once per epoch), but the data will still be shuffled dif‐
          ferently at each epoch, and the next batch will still be prepared in advance.
                                                                      
          You now know how to build efficient input pipelines to load and preprocess data
          from multiple text files. We have discussed the most common dataset methods, but
          there are a few more you may want to look at: concatenate(), zip(), window(),
          reduce(), shard(), flat_map(), and padded_batch(). There are also a couple more
          class methods: from_generator() and from_tensors(), which create a new dataset
          from a Python generator or a list of tensors, respectively. Please check the API docu‐
          mentation for more details. Also note that there are experimental features available in
          tf.data.experimental, many of which will likely make it to the core API in future
          releases (e.g., check out the CsvDataset class, as well as the make_csv_dataset()
          method, which takes care of inferring the type of each column).
          Using the Dataset with tf.keras                             
                                                                      
          Now we can use the csv_reader_dataset() function to create a dataset for the train‐
          ing set. Note that we do not need to repeat it, as this will be taken care of by tf.keras.
          We also create datasets for the validation set and the test set:
            train_set = csv_reader_dataset(train_filepaths)           
            valid_set = csv_reader_dataset(valid_filepaths)           
            test_set = csv_reader_dataset(test_filepaths)             
          And now we can simply build and train a Keras model using these datasets.4 All we
          need to do is pass the training and validation datasets to the fit() method, instead of
          X_train, y_train, X_valid, and y_valid:5                    
                                                                      
            model = keras.models.Sequential([...])                    
            model.compile([...])                                      
            model.fit(train_set, epochs=10, validation_data=valid_set)
          Similarly, we can pass a dataset to the evaluate() and predict() methods:
            model.evaluate(test_set)                                  
            new_set = test_set.take(3).map(lambda X, y: X) # pretend we have 3 new instances
            model.predict(new_set) # a dataset containing new instances
          Unlike the other sets, the new_set will usually not contain labels (if it does, Keras will
          ignore them). Note that in all these cases, you can still use NumPy arrays instead of
                                                                      
                                                                      
                                                                      
          4 Support for datasets is specific to tf.keras; this will not work in other implementations of the Keras API.
          5 The fit() method will take care of repeating the training dataset. Alternatively, you could call repeat() on
           the training dataset so that it repeats forever and specify the steps_per_epoch argument when calling the
           fit() method. This may be useful in some rare cases, for example if you want to use a shuffle buffer that
           crosses over epochs.                                       "|using datasets with tf.Keras; using datasets with tf.keras; tf.keras
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Asynchronous updates. With asynchronous updates, whenever a replica has finished
          computing the gradients, it immediately uses them to update the model parameters.
          There is no aggregation (it removes the “mean” step in Figure 19-19) and no synchro‐
          nization. Replicas work independently of the other replicas. Since there is no waiting
          for the other replicas, this approach runs more training steps per minute. Moreover,
          although the parameters still need to be copied to every device at every step, this hap‐
          pens at different times for each replica, so the risk of bandwidth saturation is reduced.
          Data parallelism with asynchronous updates is an attractive choice because of its sim‐
          plicity, the absence of synchronization delay, and a better use of the bandwidth. How‐
          ever, although it works reasonably well in practice, it is almost surprising that it
          works at all! Indeed, by the time a replica has finished computing the gradients based
          on some parameter values, these parameters will have been updated several times by
          other replicas (on average N – 1 times, if there are N replicas), and there is no guaran‐
          tee that the computed gradients will still be pointing in the right direction (see
          Figure 19-20). When gradients are severely out-of-date, they are called stale gradients:
          they can slow down convergence, introducing noise and wobble effects (the learning
          curve may contain temporary oscillations), or they can even make the training algo‐
          rithm diverge.                                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-20. Stale gradients when using asynchronous updates
          There are a few ways you can reduce the effect of stale gradients:
                                                                      
           • Reduce the learning rate.                                
           • Drop stale gradients or scale them down.                 
                                                                      
           • Adjust the mini-batch size.                              
                                                                      
                                                                      "|stale gradients; asynchronous updates
"                                                                      
                                                                      
                                                                      
                                                                      
          Training a Binary Classifier                                
                                                                      
          Let’s simplify the problem for now and only try to identify one digit—for example,
          the number 5. This “5-detector” will be an example of a binary classifier, capable of
          distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for
          this classification task:                                   
            y_train_5 = (y_train == 5) # True for all 5s, False for all other digits
            y_test_5 = (y_test == 5)                                  
                                                                      
          Now let’s pick a classifier and train it. A good place to start is with a Stochastic Gradi‐
          ent Descent (SGD) classifier, using Scikit-Learn’s SGDClassifier class. This classifier
          has the advantage of being capable of handling very large datasets efficiently. This is
          in part because SGD deals with training instances independently, one at a time
          (which also makes SGD well suited for online learning), as we will see later. Let’s cre‐
          ate an SGDClassifier and train it on the whole training set:
            from sklearn.linear_model import SGDClassifier            
            sgd_clf = SGDClassifier(random_state=42)                  
            sgd_clf.fit(X_train, y_train_5)                           
                                                                      
                   The SGDClassifier relies on randomness during training (hence
                   the name “stochastic”). If you want reproducible results, you
                   should set the random_state parameter.             
                                                                      
                                                                      
          Now we can use it to detect images of the number 5:         
                                                                      
            >>> sgd_clf.predict([some_digit])                         
            array([ True])                                            
          The classifier guesses that this image represents a 5 (True). Looks like it guessed right
          in this particular case! Now, let’s evaluate this model’s performance.
                                                                      
          Performance Measures                                        
                                                                      
          Evaluating a classifier is often significantly trickier than evaluating a regressor, so we
          will spend a large part of this chapter on this topic. There are many performance
          measures available, so grab another coffee and get ready to learn many new concepts
          and acronyms!                                               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|performance measures; online learning; binary classifiers; Stochastic Gradient Descent (SGD); SGDClassifier class
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 15          
                                                                      
                            Processing   Sequences   Using            
                                                                      
                                          RNNs   and  CNNs            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          The batter hits the ball. The outfielder immediately starts running, anticipating the
          ball’s trajectory. He tracks it, adapts his movements, and finally catches it (under a
          thunder of applause). Predicting the future is something you do all the time, whether
          you are finishing a friend’s sentence or anticipating the smell of coffee at breakfast. In
          this chapter we will discuss recurrent neural networks (RNNs), a class of nets that can
          predict the future (well, up to a point, of course). They can analyze time series data
          such as stock prices, and tell you when to buy or sell. In autonomous driving systems,
          they can anticipate car trajectories and help avoid accidents. More generally, they can
          work on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the
          nets we have considered so far. For example, they can take sentences, documents, or
          audio samples as input, making them extremely useful for natural language process‐
          ing applications such as automatic translation or speech-to-text.
          In this chapter we will first look at the fundamental concepts underlying RNNs and
          how to train them using backpropagation through time, then we will use them to
          forecast a time series. After that we’ll explore the two main difficulties that RNNs
          face:                                                       
           • Unstable gradients (discussed in Chapter 11), which can be alleviated using vari‐
            ous techniques, including recurrent dropout and recurrent layer normalization
                                                                      
           • A (very) limited short-term memory, which can be extended using LSTM and
            GRU cells                                                 
          RNNs are not the only types of neural networks capable of handling sequential data:
          for small sequences, a regular dense network can do the trick; and for very long
          sequences, such as audio samples or text, convolutional neural networks can actually
                                                                      "|autonomous driving systems; sequences; RNNS; recurrent neural networks (RNNs)
"                                                                      
                                                                      
                                                                      
                                                                      
            cost function is now preferred, as it penalizes bad predictions much more, pro‐
            ducing larger gradients and converging faster.            
                                                                      
          Yann LeCun’s website features great demos of LeNet-5 classifying digits.
                                                                      
          AlexNet                                                     
                                                                      
          The AlexNet CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a
          large margin: it achieved a top-five error rate of 17%, while the second best achieved
          only 26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and
          Geoffrey Hinton. It is similar to LeNet-5, only much larger and deeper, and it was the
          first to stack convolutional layers directly on top of one another, instead of stacking a
          pooling layer on top of each convolutional layer. Table 14-2 presents this architecture.
          Table 14-2. AlexNet architecture                            
                                                                      
          Layer Type Maps Size Kernel size Stride Padding Activation  
          Out Fully connected – 1,000 – – – Softmax                   
          F10 Fully connected – 4,096 – – – ReLU                      
          F9  Fully connected – 4,096 – – – ReLU                      
          S8  Max pooling 256 6 × 6 3 × 3 2 valid –                   
          C7  Convolution 256 13 × 13 3 × 3 1 same ReLU               
          C6  Convolution 384 13 × 13 3 × 3 1 same ReLU               
          C5  Convolution 384 13 × 13 3 × 3 1 same ReLU               
          S4  Max pooling 256 13 × 13 3 × 3 2 valid –                 
          C3  Convolution 256 27 × 27 5 × 5 1 same ReLU               
          S2  Max pooling 96 27 × 27 3 × 3 2 valid –                  
                                                                      
          C1  Convolution 96 55 × 55 11 × 11 4 valid ReLU             
          In  Input  3 (RGB) 227 × 227 – – – –                        
                                                                      
          To reduce overfitting, the authors used two regularization techniques. First, they
          applied dropout (introduced in Chapter 11) with a 50% dropout rate during training
          to the outputs of layers F9 and F10. Second, they performed data augmentation by
          randomly shifting the training images by various offsets, flipping them horizontally,
          and changing the lighting conditions.                       
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          11 Alex Krizhevsky et al., “ImageNet Classification with Deep Convolutional Neural Networks,” _Proceedings of
           the 25th International Conference on Neural Information Processing Systems 1 (2012): 1097–1105."|data augmentation; AlexNet
"                                                                      
                                                                      
                                                                      
                                                                      
          Scikit-Learn uses a multiclass version of AdaBoost called SAMME16 (which stands for
          Stagewise Additive Modeling using a Multiclass Exponential loss function). When there
          are just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate
          class probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use
          a variant of SAMME called SAMME.R (the R stands for “Real”), which relies on class
          probabilities rather than predictions and generally performs better.
                                                                      
          The following code trains an AdaBoost classifier based on 200 Decision Stumps using
          Scikit-Learn’s AdaBoostClassifier class (as you might expect, there is also an Ada
          BoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1—in
          other words, a tree composed of a single decision node plus two leaf nodes. This is
          the default base estimator for the AdaBoostClassifier class:
            from sklearn.ensemble import AdaBoostClassifier           
                                                                      
            ada_clf = AdaBoostClassifier(                             
               DecisionTreeClassifier(max_depth=1), n_estimators=200, 
               algorithm=""SAMME.R"", learning_rate=0.5)                
            ada_clf.fit(X_train, y_train)                             
                   If your AdaBoost ensemble is overfitting the training set, you can
                   try reducing the number of estimators or more strongly regulariz‐
                   ing the base estimator.                            
                                                                      
                                                                      
          Gradient Boosting                                           
                                                                      
          Another very popular boosting algorithm is Gradient Boosting.17 Just like AdaBoost,
          Gradient Boosting works by sequentially adding predictors to an ensemble, each one
          correcting its predecessor. However, instead of tweaking the instance weights at every
          iteration like AdaBoost does, this method tries to fit the new predictor to the residual
          errors made by the previous predictor.                      
          Let’s go through a simple regression example, using Decision Trees as the base predic‐
          tors (of course, Gradient Boosting also works great with regression tasks). This is
          called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let’s
          fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train‐
          ing set):                                                   
                                                                      
                                                                      
                                                                      
          16 For more details, see Ji Zhu et al., “Multi-Class AdaBoost,” Statistics and Its Interface 2, no. 3 (2009): 349–360.
          17 Gradient Boosting was first introduced in Leo Breiman’s 1997 paper “Arcing the Edge” and was further devel‐
           oped in the 1999 paper “Greedy Function Approximation: A Gradient Boosting Machine” by Jerome H. Fried‐
           man.                                                       "|Gradient Tree Boosting; Gradient Boosting; SAMME (Stagewise Additive Modeling using a Multiclass Exponential loss function); AdaBoost version; residual errors; Decision Stumps; Gradient Boosted Regression Trees (GBRT)
"                                                                      
                                                                      
                                                                      
                                                                      
            with a branch for every pair of clusters that merged, you would get a binary tree
            of clusters, where the leaves are the individual instances. This approach scales
            very well to large numbers of instances or clusters. It can capture clusters of vari‐
            ous shapes, it produces a flexible and informative cluster tree instead of forcing
            you to choose a particular cluster scale, and it can be used with any pairwise dis‐
            tance. It can scale nicely to large numbers of instances if you provide a connectiv‐
            ity matrix, which is a sparse m × m matrix that indicates which pairs of instances
            are neighbors (e.g., returned by sklearn.neighbors.kneighbors_graph()).
            Without a connectivity matrix, the algorithm does not scale well to large datasets.
                                                                      
          BIRCH                                                       
            The BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)
            algorithm was designed specifically for very large datasets, and it can be faster
            than batch K-Means, with similar results, as long as the number of features is not
            too large (<20). During training, it builds a tree structure containing just enough
            information to quickly assign each new instance to a cluster, without having to
            store all the instances in the tree: this approach allows it to use limited memory,
            while handling huge datasets.                             
          Mean-Shift                                                  
            This algorithm starts by placing a circle centered on each instance; then for each
            circle it computes the mean of all the instances located within it, and it shifts the
            circle so that it is centered on the mean. Next, it iterates this mean-shifting step
            until all the circles stop moving (i.e., until each of them is centered on the mean
            of the instances it contains). Mean-Shift shifts the circles in the direction of
            higher density, until each of them has found a local density maximum. Finally, all
            the instances whose circles have settled in the same place (or close enough) are
            assigned to the same cluster. Mean-Shift has some of the same features as
            DBSCAN, like how it can find any number of clusters of any shape, it has very
            few hyperparameters (just one—the radius of the circles, called the bandwidth),
            and it relies on local density estimation. But unlike DBSCAN, Mean-Shift tends
            to chop clusters into pieces when they have internal density variations. Unfortu‐
            nately, its computational complexity is O(m2), so it is not suited for large datasets.
          Affinity propagation                                        
            This algorithm uses a voting system, where instances vote for similar instances to
            be their representatives, and once the algorithm converges, each representative
            and its voters form a cluster. Affinity propagation can detect any number of clus‐
            ters of different sizes. Unfortunately, this algorithm has a computational com‐
            plexity of O(m2), so it too is not suited for large datasets.
          Spectral clustering                                         
            This algorithm takes a similarity matrix between the instances and creates a low-
            dimensional embedding from it (i.e., it reduces its dimensionality), then it uses
                                                                      "|Mean-Shift algorithm; spectral clustering; affinity propagation; BIRCH algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
            for episode in range(600):                                
               obs = env.reset()                                      
               for step in range(200):                                
                 epsilon = max(1 - episode / 500, 0.01)               
                 obs, reward, done, info = play_one_step(env, obs, epsilon)
                 if done:                                             
                   break                                              
               if episode > 50:                                       
                 training_step(batch_size)                            
          We run 600 episodes, each for a maximum of 200 steps. At each step, we first com‐
          pute the epsilon value for the ε-greedy policy: it will go from 1 down to 0.01, line‐
          arly, in a bit under 500 episodes. Then we call the play_one_step() function, which
          will use the ε-greedy policy to pick an action, then execute it and record the experi‐
          ence in the replay buffer. If the episode is done, we exit the loop. Finally, if we are past
          the 50th episode, we call the training_step() function to train the model on one
          batch sampled from the replay buffer. The reason we play 50 episodes without train‐
          ing is to give the replay buffer some time to fill up (if we don’t wait enough, then
          there will not be enough diversity in the replay buffer). And that’s it, we just imple‐
          mented the Deep Q-Learning algorithm!                       
          Figure 18-10 shows the total rewards the agent got during each episode.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-10. Learning curve of the Deep Q-Learning algorithm
                                                                      
          As you can see, the algorithm made no apparent progress at all for almost 300 epi‐
          sodes (in part because ε was very high at the beginning), then its performance sud‐
          denly skyrocketed up to 200 (which is the maximum possible performance in this
          environment). That’s great news: the algorithm worked fine, and it actually ran much
          faster than the Policy Gradient algorithm! But wait… just a few episodes later, it for‐
          got everything it knew, and its performance dropped below 25! This is called
                                                                      "|catastrophic forgetting
"                                                                      
                                                                      
                                                                      
                                                                      
                   You can think of autoencoders as a form of self-supervised learning
                   (i.e., using a supervised learning technique with automatically gen‐
                   erated labels, in this case simply equal to the inputs).
                                                                      
                                                                      
          Stacked Autoencoders                                        
                                                                      
          Just like other neural networks we have discussed, autoencoders can have multiple
          hidden layers. In this case they are called stacked autoencoders (or deep autoencoders).
          Adding more layers helps the autoencoder learn more complex codings. That said,
          one must be careful not to make the autoencoder too powerful. Imagine an encoder
          so powerful that it just learns to map each input to a single arbitrary number (and the
          decoder learns the reverse mapping). Obviously such an autoencoder will reconstruct
          the training data perfectly, but it will not have learned any useful data representation
          in the process (and it is unlikely to generalize well to new instances).
          The architecture of a stacked autoencoder is typically symmetrical with regard to the
          central hidden layer (the coding layer). To put it simply, it looks like a sandwich. For
          example, an autoencoder for MNIST (introduced in Chapter 3) may have 784 inputs,
          followed by a hidden layer with 100 neurons, then a central hidden layer of 30 neu‐
          rons, then another hidden layer with 100 neurons, and an output layer with 784 neu‐
          rons. This stacked autoencoder is represented in Figure 17-3.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-3. Stacked autoencoder                            
                                                                      
          Implementing a Stacked Autoencoder Using Keras              
          You can implement a stacked autoencoder very much like a regular deep MLP. In par‐
          ticular, the same techniques we used in Chapter 11 for training deep nets can be
          applied. For example, the following code builds a stacked autoencoder for Fashion
                                                                      "|stacked autoencoders; using Keras; stacked; deep autoencoders
"                                                                      
                                                                      
                                                                      
                                                                      
          In this chapter, we will cover the Data API, the TFRecord format, and how to create
          custom preprocessing layers and use the standard Keras ones. We will also take a
          quick look at a few related projects from TensorFlow’s ecosystem:
                                                                      
          TF Transform (tf.Transform)                                 
            Makes it possible to write a single preprocessing function that can be run in
            batch mode on your full training set, before training (to speed it up), and then
            exported to a TF Function and incorporated into your trained model so that once
            it is deployed in production it can take care of preprocessing new instances on
            the fly.                                                  
          TF Datasets (TFDS)                                          
            Provides a convenient function to download many common datasets of all kinds,
            including large ones like ImageNet, as well as convenient dataset objects to
            manipulate them using the Data API.                       
          So let’s get started!                                       
                                                                      
          The Data API                                                
                                                                      
          The whole Data API revolves around the concept of a dataset: as you might suspect,
          this represents a sequence of data items. Usually you will use datasets that gradually
          read data from disk, but for simplicity let’s create a dataset entirely in RAM using
          tf.data.Dataset.from_tensor_slices():                       
                                                                      
            >>> X = tf.range(10) # any data tensor                    
            >>> dataset = tf.data.Dataset.from_tensor_slices(X)       
            >>> dataset                                               
            <TensorSliceDataset shapes: (), types: tf.int32>          
          The from_tensor_slices() function takes a tensor and creates a tf.data.Dataset
          whose elements are all the slices of X (along the first dimension), so this dataset con‐
          tains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same
          dataset if we had used tf.data.Dataset.range(10).           
          You can simply iterate over a dataset’s items like this:    
            >>> for item in dataset:                                  
            ...  print(item)                                          
            ...                                                       
            tf.Tensor(0, shape=(), dtype=int32)                       
            tf.Tensor(1, shape=(), dtype=int32)                       
            tf.Tensor(2, shape=(), dtype=int32)                       
            [...]                                                     
            tf.Tensor(9, shape=(), dtype=int32)                       
                                                                      
                                                                      "|TF Datasets (TFDS); TensorFlow Data API; datasets; Data API; TF Transform (tf.Transform)
"                                                                      
                                                                      
                                                                      
                                                                      
          the word “are,” and so on. Assuming the vocabulary has 10,000 words, each model
          will output 10,000 probabilities.                           
                                                                      
          Next, we compute the probabilities of each of the 30,000 two-word sentences that
          these models considered (3 × 10,000). We do this by multiplying the estimated condi‐
          tional probability of each word by the estimated probability of the sentence it com‐
          pletes. For example, the estimated probability of the sentence “How” was 75%, while
          the estimated conditional probability of the word “will” (given that the first word is
          “How”) was 36%, so the estimated probability of the sentence “How will” is 75% ×
          36% = 27%. After computing the probabilities of all 30,000 two-word sentences, we
          keep only the top 3. Perhaps they all start with the word “How”: “How will” (27%),
          “How are” (24%), and “How do” (12%). Right now, the sentence “How will” is win‐
          ning, but “How are” has not been eliminated.                
          Then we repeat the same process: we use three models to predict the next word in
          each of these three sentences, and we compute the probabilities of all 30,000 three-
          word sentences we considered. Perhaps the top three are now “How are you” (10%),
          “How do you” (8%), and “How will you” (2%). At the next step we may get “How do
          you do” (7%), “How are you <eos>” (6%), and “How are you doing” (3%). Notice that
          “How will” was eliminated, and we now have three perfectly reasonable translations.
          We boosted our Encoder–Decoder model’s performance without any extra training,
          simply by using it more wisely.                             
          You can implement beam search fairly easily using TensorFlow Addons:
            beam_width = 10                                           
            decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(
               cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)
            decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(
               encoder_state, multiplier=beam_width)                  
            outputs, _, _ = decoder(                                  
               embedding_decoder, start_tokens=start_tokens, end_token=end_token,
               initial_state=decoder_initial_state)                   
          We first create a BeamSearchDecoder, which wraps all the decoder clones (in this case
          10 clones). Then we create one copy of the encoder’s final state for each decoder
          clone, and we pass these states to the decoder, along with the start and end tokens.
          With all this, you can get good translations for fairly short sentences (especially if you
          use pretrained word embeddings). Unfortunately, this model will be really bad at
          translating long sentences. Once again, the problem comes from the limited short-
          term memory of RNNs. Attention mechanisms are the game-changing innovation that
          addressed this problem.                                     
                                                                      
                                                                      
                                                                      
                                                                      "|Encoder–Decoder model; Encoder–Decoder network
"                                                                      
                                                                      
                                                                      
                                                                      
          method estimates the log of the probability density function (PDF) at that location.
          The greater the score, the higher the density:              
                                                                      
            >>> gm.score_samples(X)                                   
            array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,
                -4.39802535, -3.80743859])                            
          If you compute the exponential of these scores, you get the value of the PDF at the
          location of the given instances. These are not probabilities, but probability densities:
          they can take on any positive value, not just a value between 0 and 1. To estimate the
          probability that an instance will fall within a particular region, you would have to
          integrate the PDF over that region (if you do so over the entire space of possible
          instance locations, the result will be 1).                  
          Figure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the
          density contours of this model.                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-17. Cluster means, decision boundaries, and density contours of a trained
          Gaussian mixture model                                      
                                                                      
          Nice! The algorithm clearly found an excellent solution. Of course, we made its task
          easy by generating the data using a set of 2D Gaussian distributions (unfortunately,
          real-life data is not always so Gaussian and low-dimensional). We also gave the algo‐
          rithm the correct number of clusters. When there are many dimensions, or many
          clusters, or few instances, EM can struggle to converge to the optimal solution. You
          might need to reduce the difficulty of the task by limiting the number of parameters
          that the algorithm has to learn. One way to do this is to limit the range of shapes and
          orientations that the clusters can have. This can be achieved by imposing constraints
          on the covariance matrices. To do this, set the covariance_type hyperparameter to
          one of the following values:                                
                                                                      "|probability density function (PDF); density estimation
"                                                                      
                                                                      
                                                                      
                                                                      
          “smartest.” Google’s SentencePiece project provides an open source implementation,
          described in a paper5 by Taku Kudo and John Richardson.     
                                                                      
          Another option was proposed in an earlier paper6 by Rico Sennrich et al. that
          explored other ways of creating subword encodings (e.g., using byte pair encoding).
          Last but not least, the TensorFlow team released the TF.Text library in June 2019,
          which implements various tokenization strategies, including WordPiece7 (a variant of
          byte pair encoding).                                        
          If you want to deploy your model to a mobile device or a web browser, and you don’t
          want to have to write a different preprocessing function every time, then you will
          want to handle preprocessing using only TensorFlow operations, so it can be included
          in the model itself. Let’s see how. First, let’s load the original IMDb reviews, as text
          (byte strings), using TensorFlow Datasets (introduced in Chapter 13):
            import tensorflow_datasets as tfds                        
                                                                      
            datasets, info = tfds.load(""imdb_reviews"", as_supervised=True, with_info=True)
            train_size = info.splits[""train""].num_examples            
          Next, let’s write the preprocessing function:               
            def preprocess(X_batch, y_batch):                         
               X_batch = tf.strings.substr(X_batch, 0, 300)           
               X_batch = tf.strings.regex_replace(X_batch, b""<br\\s*/?>"", b"" "")
               X_batch = tf.strings.regex_replace(X_batch, b""[^a-zA-Z']"", b"" "")
               X_batch = tf.strings.split(X_batch)                    
               return X_batch.to_tensor(default_value=b""<pad>""), y_batch
          It starts by truncating the reviews, keeping only the first 300 characters of each: this
          will speed up training, and it won’t impact performance too much because you can
          generally tell whether a review is positive or not in the first sentence or two. Then it
          uses regular expressions to replace <br /> tags with spaces, and to replace any charac‐
          ters other than letters and quotes with spaces. For example, the text ""Well, I
          can't<br />"" will become ""Well I can't"". Finally, the preprocess() function
          splits the reviews by the spaces, which returns a ragged tensor, and it converts this
          ragged tensor to a dense tensor, padding all reviews with the padding token ""<pad>""
          so that they all have the same length.                      
                                                                      
                                                                      
                                                                      
          5 Taku Kudo and John Richardson, “SentencePiece: A Simple and Language Independent Subword Tokenizer
           and Detokenizer for Neural Text Processing,” arXiv preprint arXiv:1808.06226 (2018).
          6 Rico Sennrich et al., “Neural Machine Translation of Rare Words with Subword Units,” Proceedings of the 54th
           Annual Meeting of the Association for Computational Linguistics 1 (2016): 1715–1725.
          7 Yonghui Wu et al., “Google’s Neural Machine Translation System: Bridging the Gap Between Human and
           Machine Translation,” arXiv preprint arXiv:1609.08144 (2016)."|TF.Text library; Byte-Pair Encoding; word tokenization; regular expressions; tokenization
"                                                                      
                                                                      
                                                                      
                                                                      
          chase, maintain, and upgrade all the hardware infrastructure, you will want to use
          virtual machines on a cloud platform such as Amazon AWS, Microsoft Azure, Google
          Cloud Platform, IBM Cloud, Alibaba Cloud, Oracle Cloud, or some other Platform-
          as-a-Service (PaaS). Managing all the virtual machines, handling container orchestra‐
          tion (even with the help of Kubernetes), taking care of TF Serving configuration,
          tuning and monitoring—all of this can be a full-time job. Fortunately, some service
          providers can take care of all this for you. In this chapter we will use Google Cloud AI
          Platform because it’s the only platform with TPUs today, it supports TensorFlow 2, it
          offers a nice suite of AI services (e.g., AutoML, Vision API, Natural Language API),
          and it is the one I have the most experience with. But there are several other provid‐
          ers in this space, such as Amazon AWS SageMaker and Microsoft AI Platform, which
          are also capable of serving TensorFlow models.              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-2. Scaling up TF Serving with load balancing      
                                                                      
          Now let’s see how to serve our wonderful MNIST model on the cloud!
                                                                      
          Creating a Prediction Service on GCP AI Platform            
                                                                      
          Before you can deploy a model, there’s a little bit of setup to take care of:
           1. Log in to your Google account, and then go to the Google Cloud Platform (GCP)
            console (see Figure 19-3). If you don’t have a Google account, you’ll have to cre‐
            ate one.                                                  
                                                                      
           2. If it is your first time using GCP, you will have to read and accept the terms and
            conditions. Click Tour Console if you want. At the time of this writing, new users
            are offered a free trial, including $300 worth of GCP credit that you can use over
            the course of 12 months. You will only need a small portion of that to pay for the
            services you will use in this chapter. Upon signing up for the free trial, you will
            still need to create a payment profile and enter your credit card number: it is used
            for verification purposes (probably to avoid people using the free trial multiple
            times), but you will not be billed. Activate and upgrade your account if requested.
                                                                      
                                                                      
                                                                      "|prediction service creation; creating on GCP AI
"                                                                      
                                                                      
                                                                      
                                                                      
            def training_step(batch_size):                            
               experiences = sample_experiences(batch_size)           
               states, actions, rewards, next_states, dones = experiences
               next_Q_values = model.predict(next_states)             
               max_next_Q_values = np.max(next_Q_values, axis=1)      
               target_Q_values = (rewards +                           
                          (1 - dones) * discount_factor * max_next_Q_values)
               mask = tf.one_hot(actions, n_outputs)                  
               with tf.GradientTape() as tape:                        
                 all_Q_values = model(states)                         
                 Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)
                 loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))
               grads = tape.gradient(loss, model.trainable_variables) 
               optimizer.apply_gradients(zip(grads, model.trainable_variables))
          Let’s go through this code:                                 
           • First we define some hyperparameters, and we create the optimizer and the loss
            function.                                                 
           • Then we create the training_step() function. It starts by sampling a batch of
            experiences, then it uses the DQN to predict the Q-Value for each possible action
            in each experience’s next state. Since we assume that the agent will be playing
            optimally, we only keep the maximum Q-Value for each next state. Next, we use
            Equation 18-7 to compute the target Q-Value for each experience’s state-action
            pair.                                                     
           • Next, we want to use the DQN to compute the Q-Value for each experienced
            state-action pair. However, the DQN will also output the Q-Values for the other
            possible actions, not just for the action that was actually chosen by the agent. So
            we need to mask out all the Q-Values we do not need. The tf.one_hot() func‐
            tion makes it easy to convert an array of action indices into such a mask. For
            example, if the first three experiences contain actions 1, 1, 0, respectively, then
            the mask will start with [[0, 1], [0, 1], [1, 0],...]. We can then multiply
            the DQN’s output with this mask, and this will zero out all the Q-Values we do
            not want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-
            Values of the experienced state-action pairs. This gives us the Q_values tensor,
            containing one predicted Q-Value for each experience in the batch.
           • Then we compute the loss: it is the mean squared error between the target and
            predicted Q-Values for the experienced state-action pairs.
           • Finally, we perform a Gradient Descent step to minimize the loss with regard to
            the model’s trainable variables.                          
          This was the hardest part. Now training the model is straightforward:
                                                                      
                                                                      
                                                                      "|mean squared error
"                                                                      
                                                                      
                                                                      
                                                                      
          object detection (classifying multiple objects in an image and placing bounding boxes
          around them) and semantic segmentation (classifying each pixel according to the
          class of the object it belongs to).                         
                                                                      
          The Architecture of the Visual Cortex                       
                                                                      
          David H. Hubel and Torsten Wiesel performed a series of experiments on cats in
          19581 and 19592 (and a few years later on monkeys3), giving crucial insights into the
          structure of the visual cortex (the authors received the Nobel Prize in Physiology or
          Medicine in 1981 for their work). In particular, they showed that many neurons in
          the visual cortex have a small local receptive field, meaning they react only to visual
          stimuli located in a limited region of the visual field (see Figure 14-1, in which the
          local receptive fields of five neurons are represented by dashed circles). The receptive
          fields of different neurons may overlap, and together they tile the whole visual field.
          Moreover, the authors showed that some neurons react only to images of horizontal
          lines, while others react only to lines with different orientations (two neurons may
          have the same receptive field but react to different line orientations). They also
          noticed that some neurons have larger receptive fields, and they react to more com‐
          plex patterns that are combinations of the lower-level patterns. These observations
          led to the idea that the higher-level neurons are based on the outputs of neighboring
          lower-level neurons (in Figure 14-1, notice that each neuron is connected only to a
          few neurons from the previous layer). This powerful architecture is able to detect all
          sorts of complex patterns in any area of the visual field.  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          1 David H. Hubel, “Single Unit Activity in Striate Cortex of Unrestrained Cats,” The Journal of Physiology 147
           (1959): 226–238.                                           
          2 David H. Hubel and Torsten N. Wiesel, “Receptive Fields of Single Neurons in the Cat’s Striate Cortex,” The
           Journal of Physiology 148 (1959): 574–591.                 
          3 David H. Hubel and Torsten N. Wiesel, “Receptive Fields and Functional Architecture of Monkey Striate Cor‐
           tex,” The Journal of Physiology 195 (1968): 215–243.       "|architecture of visual cortex
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                   CHAPTER 4          
                                                                      
                                         Training  Models             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          So far we have treated Machine Learning models and their training algorithms mostly
          like black boxes. If you went through some of the exercises in the previous chapters,
          you may have been surprised by how much you can get done without knowing any‐
          thing about what’s under the hood: you optimized a regression system, you improved
          a digit image classifier, and you even built a spam classifier from scratch, all this
          without knowing how they actually work. Indeed, in many situations you don’t really
          need to know the implementation details.                    
          However, having a good understanding of how things work can help you quickly
          home in on the appropriate model, the right training algorithm to use, and a good set
          of hyperparameters for your task. Understanding what’s under the hood will also help
          you debug issues and perform error analysis more efficiently. Lastly, most of the top‐
          ics discussed in this chapter will be essential in understanding, building, and training
          neural networks (discussed in Part II of this book).        
          In this chapter we will start by looking at the Linear Regression model, one of the
          simplest models there is. We will discuss two very different ways to train it:
                                                                      
           • Using a direct “closed-form” equation that directly computes the model parame‐
            ters that best fit the model to the training set (i.e., the model parameters that
            minimize the cost function over the training set).        
           • Using an iterative optimization approach called Gradient Descent (GD) that
            gradually tweaks the model parameters to minimize the cost function over the
            training set, eventually converging to the same set of parameters as the first
            method. We will look at a few variants of Gradient Descent that we will use again
            and again when we study neural networks in Part II: Batch GD, Mini-batch GD,
            and Stochastic GD.                                        
                                                                      
                                                                      "|Gradient Descent (GD); approaches to training; training models
"                                                                      
                                                                      
                                                                      
                                                                      
          Computational Complexity                                    
                                                                      
          The LinearSVC class is based on the liblinear library, which implements an opti‐
          mized algorithm for linear SVMs.1 It does not support the kernel trick, but it scales
          almost linearly with the number of training instances and the number of features. Its
          training time complexity is roughly O(m × n).               
          The algorithm takes longer if you require very high precision. This is controlled by
          the tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification
          tasks, the default tolerance is fine.                       
                                                                      
          The SVC class is based on the libsvm library, which implements an algorithm that
          supports the kernel trick.2 The training time complexity is usually between O(m2 × n)
          and O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐
          ber of training instances gets large (e.g., hundreds of thousands of instances). This
          algorithm is perfect for complex small or medium-sized training sets. It scales well
          with the number of features, especially with sparse features (i.e., when each instance
          has few nonzero features). In this case, the algorithm scales roughly with the average
          number of nonzero features per instance. Table 5-1 compares Scikit-Learn’s SVM
          classification classes.                                     
          Table 5-1. Comparison of Scikit-Learn classes for SVM classification
                                                                      
          Class    Time complexity Out-of-core support Scaling required Kernel trick
          LinearSVC O(m × n) No       Yes     No                      
          SGDClassifier O(m × n) Yes  Yes     No                      
          SVC      O(m² × n) to O(m³ × n) No Yes Yes                  
                                                                      
          SVM Regression                                              
                                                                      
          As mentioned earlier, the SVM algorithm is versatile: not only does it support linear
          and nonlinear classification, but it also supports linear and nonlinear regression. To
          use SVMs for regression instead of classification, the trick is to reverse the objective:
          instead of trying to fit the largest possible street between two classes while limiting
          margin violations, SVM Regression tries to fit as many instances as possible on the
          street while limiting margin violations (i.e., instances off the street). The width of the
          street is controlled by a hyperparameter, ϵ. Figure 5-10 shows two linear SVM
                                                                      
                                                                      
          1 Chih-Jen Lin et al., “A Dual Coordinate Descent Method for Large-Scale Linear SVM,” Proceedings of the 25th
           International Conference on Machine Learning (2008): 408–415.
          2 John Platt, “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines”
           (Microsoft Research technical report, April 21, 1998), https://www.microsoft.com/en-us/research/wp-content/
           uploads/2016/02/tr-98-14.pdf.                              "|libsvm library; liblinear library; SVM classification classes; tolerance hyperparameter; SVM regression; nonlinear SVM classification
"                                                                      
                                                                      
                                                                      
                                                                      
          similar pairs of layers using growing dilation rates: 1, 2, 4, 8, and again 1, 2, 4, 8.
          Finally, we add the output layer: a convolutional layer with 10 filters of size 1 and
          without any activation function. Thanks to the padding layers, every convolutional
          layer outputs a sequence of the same length as the input sequences, so the targets we
          use during training can be the full sequences: no need to crop them or downsample
          them.                                                       
                                                                      
          The last two models offer the best performance so far in forecasting our time series!
          In the WaveNet paper, the authors achieved state-of-the-art performance on various
          audio tasks (hence the name of the architecture), including text-to-speech tasks, pro‐
          ducing incredibly realistic voices across several languages. They also used the model
          to generate music, one audio sample at a time. This feat is all the more impressive
          when you realize that a single second of audio can contain tens of thousands of time
          steps—even LSTMs and GRUs cannot handle such long sequences.
          In Chapter 16, we will continue to explore RNNs, and we will see how they can tackle
          various NLP tasks.                                          
                                                                      
          Exercises                                                   
                                                                      
           1. Can you think of a few applications for a sequence-to-sequence RNN? What
            about a sequence-to-vector RNN, and a vector-to-sequence RNN?
           2. How many dimensions must the inputs of an RNN layer have? What does each
            dimension represent? What about its outputs?              
           3. If you want to build a deep sequence-to-sequence RNN, which RNN layers
            should have return_sequences=True? What about a sequence-to-vector RNN?
                                                                      
           4. Suppose you have a daily univariate time series, and you want to forecast the next
            seven days. Which RNN architecture should you use?        
           5. What are the main difficulties when training RNNs? How can you handle them?
           6. Can you sketch the LSTM cell’s architecture?            
                                                                      
           7. Why would you want to use 1D convolutional layers in an RNN?
           8. Which neural network architecture could you use to classify videos?
           9. Train a classification model for the SketchRNN dataset, available in TensorFlow
            Datasets.                                                 
          10. Download the Bach chorales dataset and unzip it. It is composed of 382 chorales
            composed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long,
            and each time step contains 4 integers, where each integer corresponds to a note’s
            index on a piano (except for the value 0, which means that no note is played).
            Train a model—recurrent, convolutional, or both—that can predict the next time
            step (four notes), given a sequence of time steps from a chorale. Then use this
                                                                      "|handling long sequences; long sequences short-term memory problems; handling long; short-term memory problems
"                                                                      
                                                                      
                                                                      
                                                                      
          This model has two model parameters, θ and θ .5 By tweaking these parameters, you
                                  0   1                               
          can make your model represent any linear function, as shown in Figure 1-18.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-18. A few possible linear models                   
                                                                      
          Before you can use your model, you need to define the parameter values θ and θ .
                                                       0   1          
          How can you know which values will make your model perform best? To answer this
          question, you need to specify a performance measure. You can either define a utility
          function (or fitness function) that measures how good your model is, or you can define
          a cost function that measures how bad it is. For Linear Regression problems, people
          typically use a cost function that measures the distance between the linear model’s
          predictions and the training examples; the objective is to minimize this distance.
          This is where the Linear Regression algorithm comes in: you feed it your training
          examples, and it finds the parameters that make the linear model fit best to your data.
          This is called training the model. In our case, the algorithm finds that the optimal
          parameter values are θ = 4.85 and θ = 4.91 × 10–5.          
                       0       1                                      
                   Confusingly, the same word “model” can refer to a type of model
                   (e.g., Linear Regression), to a fully specified model architecture (e.g.,
                   Linear Regression with one input and one output), or to the final
                   trained model ready to be used for predictions (e.g., Linear Regres‐
                   sion with one input and one output, using θ = 4.85 and θ = 4.91 ×
                                          0       1                   
                   10–5). Model selection consists in choosing the type of model and
                   fully specifying its architecture. Training a model means running
                   an algorithm to find the model parameters that will make it best fit
                   the training data (and hopefully make good predictions on new
                   data).                                             
                                                                      
          5 By convention, the Greek letter θ (theta) is frequently used to represent model parameters."|fitness functions; training; final trained models; training data; utility functions; model parameters; fully-specified model architecture; causal models
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> from sklearn.metrics import confusion_matrix          
            >>> confusion_matrix(y_train_5, y_train_pred)             
            array([[53057, 1522],                                     
                [ 1325, 4096]])                                       
          Each row in a confusion matrix represents an actual class, while each column repre‐
          sents a predicted class. The first row of this matrix considers non-5 images (the nega‐
          tive class): 53,057 of them were correctly classified as non-5s (they are called true
          negatives), while the remaining 1,522 were wrongly classified as 5s (false positives).
          The second row considers the images of 5s (the positive class): 1,325 were wrongly
          classified as non-5s (false negatives), while the remaining 4,096 were correctly classi‐
          fied as 5s (true positives). A perfect classifier would have only true positives and true
          negatives, so its confusion matrix would have nonzero values only on its main diago‐
          nal (top left to bottom right):                             
            >>> y_train_perfect_predictions = y_train_5 # pretend we reached perfection
            >>> confusion_matrix(y_train_5, y_train_perfect_predictions)
            array([[54579, 0],                                        
                [  0, 5421]])                                         
          The confusion matrix gives you a lot of information, but sometimes you may prefer a
          more concise metric. An interesting one to look at is the accuracy of the positive pre‐
          dictions; this is called the precision of the classifier (Equation 3-1).
                                                                      
            Equation 3-1. Precision                                   
                    TP                                                
            precision=                                                
                   TP+FP                                              
          TP is the number of true positives, and FP is the number of false positives.
          A trivial way to have perfect precision is to make one single positive prediction and
          ensure it is correct (precision = 1/1 = 100%). But this would not be very useful, since
          the classifier would ignore all but one positive instance. So precision is typically used
          along with another metric named recall, also called sensitivity or the true positive rate
          (TPR): this is the ratio of positive instances that are correctly detected by the classifier
          (Equation 3-2).                                             
                                                                      
            Equation 3-2. Recall                                      
                                                                      
                  TP                                                  
            recall=                                                   
                 TP+FN                                                
          FN is, of course, the number of false negatives.            
          If you are confused about the confusion matrix, Figure 3-2 may help.
                                                                      
                                                                      "|true positive rate (TPR); sensitivity; recall; precision
"                                                                      
                                                                      
                                                                      
                                                                      
            max_pool = keras.layers.MaxPool2D(pool_size=2)            
                                                                      
          To create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you
          might expect, it works exactly like a max pooling layer, except it computes the mean
          rather than the max. Average pooling layers used to be very popular, but people
          mostly use max pooling layers now, as they generally perform better. This may seem
          surprising, since computing the mean generally loses less information than comput‐
          ing the max. But on the other hand, max pooling preserves only the strongest fea‐
          tures, getting rid of all the meaningless ones, so the next layers get a cleaner signal to
          work with. Moreover, max pooling offers stronger translation invariance than average
          pooling, and it requires slightly less compute.             
          Note that max pooling and average pooling can be performed along the depth dimen‐
          sion rather than the spatial dimensions, although this is not as common. This can
          allow the CNN to learn to be invariant to various features. For example, it could learn
          multiple filters, each detecting a different rotation of the same pattern (such as hand-
          written digits; see Figure 14-10), and the depthwise max pooling layer would ensure
          that the output is the same regardless of the rotation. The CNN could similarly learn
          to be invariant to anything else: thickness, brightness, skew, color, and so on.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-10. Depthwise max pooling can help the CNN learn any invariance
                                                                      
                                                                      "|average pooling layer
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-8. Clustering                                      
                                                                      
          Visualization algorithms are also good examples of unsupervised learning algorithms:
          you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep‐
          resentation of your data that can easily be plotted (Figure 1-9). These algorithms try
          to preserve as much structure as they can (e.g., trying to keep separate clusters in the
          input space from overlapping in the visualization) so that you can understand how
          the data is organized and perhaps identify unsuspected patterns.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3
                                                                      
                                                                      
          3 Notice how animals are rather well separated from vehicles and how horses are close to deer but far from
           birds. Figure reproduced with permission from Richard Socher et al., “Zero-Shot Learning Through Cross-
           Modal Transfer,” Proceedings of the 26th International Conference on Neural Information Processing Systems 1
           (2013): 935–943.                                           "|visualization algorithms
"                                                                      
                                                                      
                                                                      
                                                                      
          In summary:                                                 
                                                                      
           • You studied the data.                                    
           • You selected a model.                                    
                                                                      
           • You trained it on the training data (i.e., the learning algorithm searched for the
            model parameter values that minimize a cost function).    
           • Finally, you applied the model to make predictions on new cases (this is called
            inference), hoping that this model will generalize well.  
                                                                      
          This is what a typical Machine Learning project looks like. In Chapter 2 you will
          experience this firsthand by going through a project end to end.
          We have covered a lot of ground so far: you now know what Machine Learning is
          really about, why it is useful, what some of the most common categories of ML sys‐
          tems are, and what a typical project workflow looks like. Now let’s look at what can go
          wrong in learning and prevent you from making accurate predictions.
                                                                      
          Main Challenges of Machine Learning                         
                                                                      
          In short, since your main task is to select a learning algorithm and train it on some
          data, the two things that can go wrong are “bad algorithm” and “bad data.” Let’s start
          with examples of bad data.                                  
          Insufficient Quantity of Training Data                      
                                                                      
          For a toddler to learn what an apple is, all it takes is for you to point to an apple and
          say “apple” (possibly repeating this procedure a few times). Now the child is able to
          recognize apples in all sorts of colors and shapes. Genius. 
                                                                      
          Machine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐
          ing algorithms to work properly. Even for very simple problems you typically need
          thousands of examples, and for complex problems such as image or speech recogni‐
          tion you may need millions of examples (unless you can reuse parts of an existing
          model).                                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|inference; Machine Learning (ML)
"                                                                      
                                                                      
                                                                      
                                                                      
                   It is possible to use Gradient Boosting with other cost functions.
                   This is controlled by the loss hyperparameter (see Scikit-Learn’s
                   documentation for more details).                   
                                                                      
                                                                      
          It is worth noting that an optimized implementation of Gradient Boosting is available
          in the popular Python library XGBoost, which stands for Extreme Gradient Boosting.
          This package was initially developed by Tianqi Chen as part of the Distributed (Deep)
          Machine Learning Community (DMLC), and it aims to be extremely fast, scalable,
          and portable. In fact, XGBoost is often an important component of the winning
          entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:
                                                                      
            import xgboost                                            
            xgb_reg = xgboost.XGBRegressor()                          
            xgb_reg.fit(X_train, y_train)                             
            y_pred = xgb_reg.predict(X_val)                           
          XGBoost also offers several nice features, such as automatically taking care of early
          stopping:                                                   
            xgb_reg.fit(X_train, y_train,                             
                   eval_set=[(X_val, y_val)], early_stopping_rounds=2)
            y_pred = xgb_reg.predict(X_val)                           
          You should definitely check it out!                         
                                                                      
          Stacking                                                    
                                                                      
          The last Ensemble method we will discuss in this chapter is called stacking (short for
          stacked generalization).18 It is based on a simple idea: instead of using trivial functions
          (such as hard voting) to aggregate the predictions of all predictors in an ensemble,
          why don’t we train a model to perform this aggregation? Figure 7-12 shows such an
          ensemble performing a regression task on a new instance. Each of the bottom three
          predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor
          (called a blender, or a meta learner) takes these predictions as inputs and makes the
          final prediction (3.0).                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          18 David H. Wolpert, “Stacked Generalization,” Neural Networks 5, no. 2 (1992): 241–259."|blenders; stacking; stacked generalization; meta learners; boosting; XGBoost
"                                                                      
                                                                      
                                                                      
                                                                      
          This is just a wrapper around an OpenAI Gym environment, which you can access
          through the gym attribute:                                  
                                                                      
            >>> env.gym                                               
            <gym.envs.atari.atari_env.AtariEnv at 0x24dcab940>        
          TF-Agents environments are very similar to OpenAI Gym environments, but there
          are a few differences. First, the reset() method does not return an observation;
          instead it returns a TimeStep object that wraps the observation, as well as some extra
          information:                                                
            >>> env.reset()                                           
            TimeStep(step_type=array(0, dtype=int32),                 
                  reward=array(0., dtype=float32),                    
                  discount=array(1., dtype=float32),                  
                  observation=array([[[0., 0., 0.], [0., 0., 0.],...]]], dtype=float32))
          The step() method returns a TimeStep object as well:        
            >>> env.step(1) # Fire                                    
            TimeStep(step_type=array(1, dtype=int32),                 
                  reward=array(0., dtype=float32),                    
                  discount=array(1., dtype=float32),                  
                  observation=array([[[0., 0., 0.], [0., 0., 0.],...]]], dtype=float32))
          The reward and observation attributes are self-explanatory, and they are the same as
          for OpenAI Gym (except the reward is represented as a NumPy array). The
          step_type attribute is equal to 0 for the first time step in the episode, 1 for intermedi‐
          ate time steps, and 2 for the final time step. You can call the time step’s is_last()
          method to check whether it is the final one or not. Lastly, the discount attribute indi‐
          cates the discount factor to use at this time step. In this example it is equal to 1, so
          there will be no discount at all. You can define the discount factor by setting the dis
          count parameter when loading the environment.               
                                                                      
                   At any time, you can access the environment’s current time step by
                   calling its current_time_step() method.            
                                                                      
                                                                      
                                                                      
          Environment Specifications                                  
          Conveniently, a TF-Agents environment provides the specifications of the observa‐
          tions, actions, and time steps, including their shapes, data types, and names, as well as
          their minimum and maximum values:                           
                                                                      
                                                                      
                                                                      
                                                                      "|environment specifications
"                                                                      
                                                                      
                                                                      
                                                                      
          catastrophic forgetting, and it is one of the big problems facing virtually all RL algo‐
          rithms: as the agent explores the environment, it updates its policy, but what it learns
          in one part of the environment may break what it learned earlier in other parts of the
          environment. The experiences are quite correlated, and the learning environment
          keeps changing—this is not ideal for Gradient Descent! If you increase the size of the
          replay buffer, the algorithm will be less subject to this problem. Reducing the learning
          rate may also help. But the truth is, Reinforcement Learning is hard: training is often
          unstable, and you may need to try many hyperparameter values and random seeds
          before you find a combination that works well. For example, if you try changing the
          number of neurons per layer in the preceding from 32 to 30 or 34, the performance
          will never go above 100 (the DQN may be more stable with one hidden layer instead
          of two).                                                    
                                                                      
                   Reinforcement Learning is notoriously difficult, largely because of
                   the training instabilities and the huge sensitivity to the choice of
                   hyperparameter values and random seeds.13 As the researcher
                   Andrej Karpathy put it: “[Supervised learning] wants to work. […]
                   RL must be forced to work.” You will need time, patience, persever‐
                   ance, and perhaps a bit of luck too. This is a major reason RL is not
                   as widely adopted as regular Deep Learning (e.g., convolutional
                   nets). But there are a few real-world applications, beyond AlphaGo
                   and Atari games: for example, Google uses RL to optimize its data‐
                   center costs, and it is used in some robotics applications, for hyper‐
                   parameter tuning, and in recommender systems.      
          You might wonder why we didn’t plot the loss. It turns out that loss is a poor indicator
          of the model’s performance. The loss might go down, yet the agent might perform
          worse (e.g., this can happen when the agent gets stuck in one small region of the envi‐
          ronment, and the DQN starts overfitting this region). Conversely, the loss could go
          up, yet the agent might perform better (e.g., if the DQN was underestimating the Q-
          Values, and it starts correctly increasing its predictions, the agent will likely perform
          better, getting more rewards, but the loss might increase because the DQN also sets
          the targets, which will be larger too).                     
          The basic Deep Q-Learning algorithm we’ve been using so far would be too unstable
          to learn to play Atari games. So how did DeepMind do it? Well, they tweaked the
          algorithm!                                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          13 A great 2018 post by Alex Irpan nicely lays out RL’s biggest difficulties and limitations."|Deep Q-Learning
"                                                                      
                                                                      
                                                                      
                                                                      
          these words using a simple encode_words() function that uses the table we just built,
          and finally prefetch the next batch:                        
                                                                      
            def encode_words(X_batch, y_batch):                       
               return table.lookup(X_batch), y_batch                  
            train_set = datasets[""train""].batch(32).map(preprocess)   
            train_set = train_set.map(encode_words).prefetch(1)       
          At last we can create the model and train it:               
            embed_size = 128                                          
            model = keras.models.Sequential([                         
               keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,
                            input_shape=[None]),                      
               keras.layers.GRU(128, return_sequences=True),          
               keras.layers.GRU(128),                                 
               keras.layers.Dense(1, activation=""sigmoid"")            
            ])                                                        
            model.compile(loss=""binary_crossentropy"", optimizer=""adam"",
                    metrics=[""accuracy""])                             
            history = model.fit(train_set, epochs=5)                  
          The first layer is an Embedding layer, which will convert word IDs into embeddings
          (introduced in Chapter 13). The embedding matrix needs to have one row per word
          ID (vocab_size + num_oov_buckets) and one column per embedding dimension
          (this example uses 128 dimensions, but this is a hyperparameter you could tune).
          Whereas the inputs of the model will be 2D tensors of shape [batch size, time steps],
          the output of the Embedding layer will be a 3D tensor of shape [batch size, time steps,
          embedding size].                                            
          The rest of the model is fairly straightforward: it is composed of two GRU layers, with
          the second one returning only the output of the last time step. The output layer is just
          a single neuron using the sigmoid activation function to output the estimated proba‐
          bility that the review expresses a positive sentiment regarding the movie. We then
          compile the model quite simply, and we fit it on the dataset we prepared earlier, for a
          few epochs.                                                 
          Masking                                                     
          As it stands, the model will need to learn that the padding tokens should be ignored.
          But we already know that! Why don’t we tell the model to ignore the padding tokens,
          so that it can focus on the data that actually matters? It’s actually trivial: simply add
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|masking
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Creating the Training Dataset                               
                                                                      
          First, let’s download all of Shakespeare’s work, using Keras’s handy get_file() func‐
          tion and downloading the data from Andrej Karpathy’s Char-RNN project:
            shakespeare_url = ""https://homl.info/shakespeare"" # shortcut URL
            filepath = keras.utils.get_file(""shakespeare.txt"", shakespeare_url)
            with open(filepath) as f:                                 
               shakespeare_text = f.read()                            
          Next, we must encode every character as an integer. One option is to create a custom
          preprocessing layer, as we did in Chapter 13. But in this case, it will be simpler to use
          Keras’s Tokenizer class. First we need to fit a tokenizer to the text: it will find all the
          characters used in the text and map each of them to a different character ID, from 1
          to the number of distinct characters (it does not start at 0, so we can use that value for
          masking, as we will see later in this chapter):             
            tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)
            tokenizer.fit_on_texts([shakespeare_text])                
          We set char_level=True to get character-level encoding rather than the default
          word-level encoding. Note that this tokenizer converts the text to lowercase by
          default (but you can set lower=False if you do not want that). Now the tokenizer can
          encode a sentence (or a list of sentences) to a list of character IDs and back, and it
          tells us how many distinct characters there are and the total number of characters in
          the text:                                                   
                                                                      
            >>> tokenizer.texts_to_sequences([""First""])               
            [[20, 6, 9, 8, 3]]                                        
            >>> tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])      
            ['f i r s t']                                             
            >>> max_id = len(tokenizer.word_index) # number of distinct characters
            >>> dataset_size = tokenizer.document_count # total number of characters
          Let’s encode the full text so each character is represented by its ID (we subtract 1 to
          get IDs from 0 to 38, rather than from 1 to 39):            
            [encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1
          Before we continue, we need to split the dataset into a training set, a validation set,
          and a test set. We can’t just shuffle all the characters in the text, so how do you split a
          sequential dataset?                                         
          How to Split a Sequential Dataset                           
                                                                      
          It is very important to avoid any overlap between the training set, the validation set,
          and the test set. For example, we can take the first 90% of the text for the training set,
          then the next 5% for the validation set, and the final 5% for the test set. It would also
                                                                      "|training dataset creation; splitting sequential datasets
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Equation 16-1. Attention mechanisms                       
                                                                      
                   = ∑α                                               
                 t    t,i i                                           
                    i                                                 
                     exp e                                            
                         t,i                                          
             with α =                                                 
                 t,i                                                  
                    ∑ exp e                                           
                     i′   t,i′                                        
                      ⊺          dot                                  
                      t i                                             
             and e =  ⊺          general                              
                 t,i  t   i                                           
                     ⊺ tanh   ;  concat                               
                             t i                                      
          Here is how you can add Luong attention to an Encoder–Decoder model using Ten‐
          sorFlow Addons:                                             
            attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(
               units, encoder_state, memory_sequence_length=encoder_sequence_length)
            attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(
               decoder_cell, attention_mechanism, attention_layer_size=n_units)
          We simply wrap the decoder cell in an AttentionWrapper, and we provide the desired
          attention mechanism (Luong attention in this example).      
          Visual Attention                                            
          Attention mechanisms are now used for a variety of purposes. One of their first appli‐
          cations beyond NMT was in generating image captions using visual attention:17 a
          convolutional neural network first processes the image and outputs some feature
          maps, then a decoder RNN equipped with an attention mechanism generates the cap‐
          tion, one word at a time. At each decoder time step (each word), the decoder uses the
          attention model to focus on just the right part of the image. For example, in
          Figure 16-7, the model generated the caption “A woman is throwing a frisbee in a
          park,” and you can see what part of the input image the decoder focused its attention
          on when it was about to output the word “frisbee”: clearly, most of its attention was
          focused on the frisbee.                                     
                                                                      
                                                                      
                                                                      
          17 Kelvin Xu et al., “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,” Proceedings
           of the 32nd International Conference on Machine Learning (2015): 2048–2057."|visual attention
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-11. Colab Runtimes and notebooks                  
                                                                      
          Colab does have some restrictions: first, there is a limit to the number of Colab note‐
          books you can run simultaneously (currently 5 per Runtime type). Moreover, as the
          FAQ states, “Colaboratory is intended for interactive use. Long-running background
          computations, particularly on GPUs, may be stopped. Please do not use Colaboratory
          for cryptocurrency mining.” Also, the web interface will automatically disconnect
          from the Colab Runtime if you leave it unattended for a while (~30 minutes). When
          you reconnect to the Colab Runtime, it may have been reset, so make sure you always
          export any data you care about (e.g., download it or save it to Google Drive). Even if
          you never disconnect, the Colab Runtime will automatically shut down after 12
          hours, as it is not meant for long-running computations. Despite these limitations, it’s
          a fantastic tool to run tests easily, get quick results, and collaborate with your
          colleagues.                                                 
          Managing the GPU RAM                                        
                                                                      
          By default TensorFlow automatically grabs all the RAM in all available GPUs the first
          time you run a computation. It does this to limit GPU RAM fragmentation. This
          means that if you try to start a second TensorFlow program (or any program that
          requires the GPU), it will quickly run out of RAM. This does not happen as often as
          you might think, as you will most often have a single TensorFlow program running
          on a machine: usually a training script, a TF Serving node, or a Jupyter notebook. If
          you need to run multiple programs for some reason (e.g., to train two different mod‐
          els in parallel on the same machine), then you will need to split the GPU RAM
          between these processes more evenly.                        
          If you have multiple GPU cards on your machine, a simple solution is to assign each
          of them to a single process. To do this, you can set the CUDA_VISIBLE_DEVICES
          environment variable so that each process only sees the appropriate GPU card(s).
          Also set the CUDA_DEVICE_ORDER environment variable to PCI_BUS_ID to ensure that
                                                                      "|managing GPU RAM
"                                                                      
                                                                      
                                                                      
                                                                      
          Python code). Thanks to the TFPyEnvironment class, TF-Agents supports both pure
          Python environments and TensorFlow-based environments. More generally, TF-
          Agents supports and provides both pure Python and TensorFlow-based components
          (agents, replay buffers, metrics, and so on).               
                                                                      
          Now that we have a nice Breakout environment, with all the appropriate preprocess‐
          ing and TensorFlow support, we must create the DQN agent and the other compo‐
          nents we will need to train it. Let’s look at the architecture of the system we will build.
          Training Architecture                                       
                                                                      
          A TF-Agents training program is usually split into two parts that run in parallel, as
          you can see in Figure 18-13: on the left, a driver explores the environment using a
          collect policy to choose actions, and it collects trajectories (i.e., experiences), sending
          them to an observer, which saves them to a replay buffer; on the right, an agent pulls
          batches of trajectories from the replay buffer and trains some networks, which the col‐
          lect policy uses. In short, the left part explores the environment and collects trajecto‐
          ries, while the right part learns and updates the collect policy.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-13. A typical TF-Agents training architecture     
                                                                      
          This figure begs a few questions, which I’ll attempt to answer here:
                                                                      
           • Why are there multiple environments? Instead of exploring a single environ‐
            ment, you generally want the driver to explore multiple copies of the environ‐
            ment in parallel, taking advantage of the power of all your CPU cores, keeping
                                                                      "|trajectories; replay buffers; training architecture; collect policy
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            final_predictions = final_model.predict(X_test_prepared)  
                                                                      
            final_mse = mean_squared_error(y_test, final_predictions) 
            final_rmse = np.sqrt(final_mse) # => evaluates to 47,730.2
          In some cases, such a point estimate of the generalization error will not be quite
          enough to convince you to launch: what if it is just 0.1% better than the model cur‐
          rently in production? You might want to have an idea of how precise this estimate is.
          For this, you can compute a 95% confidence interval for the generalization error using
          scipy.stats.t.interval():                                   
            >>> from scipy import stats                               
            >>> confidence = 0.95                                     
            >>> squared_errors = (final_predictions - y_test) ** 2    
            >>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,
            ...              loc=squared_errors.mean(),               
            ...              scale=stats.sem(squared_errors)))        
            ...                                                       
            array([45685.10470776, 49691.25001878])                   
          If you did a lot of hyperparameter tuning, the performance will usually be slightly
          worse than what you measured using cross-validation (because your system ends up
          fine-tuned to perform well on the validation data and will likely not perform as well
          on unknown datasets). It is not the case in this example, but when this happens you
          must resist the temptation to tweak the hyperparameters to make the numbers look
          good on the test set; the improvements would be unlikely to generalize to new data.
          Now comes the project prelaunch phase: you need to present your solution (high‐
          lighting what you have learned, what worked and what did not, what assumptions
          were made, and what your system’s limitations are), document everything, and create
          nice presentations with clear visualizations and easy-to-remember statements (e.g.,
          “the median income is the number one predictor of housing prices”). In this Califor‐
          nia housing example, the final performance of the system is not better than the
          experts’ price estimates, which were often off by about 20%, but it may still be a good
          idea to launch it, especially if this frees up some time for the experts so they can work
          on more interesting and productive tasks.                   
          Launch, Monitor, and Maintain Your System                   
                                                                      
          Perfect, you got approval to launch! You now need to get your solution ready for pro‐
          duction (e.g., polish the code, write documentation and tests, and so on). Then you
          can deploy your model to your production environment. One way to do this is to save
          the trained Scikit-Learn model (e.g., using joblib), including the full preprocessing
          and prediction pipeline, then load this trained model within your production envi‐
          ronment and use it to make predictions by calling its predict() method. For exam‐
          ple, perhaps the model will be used within a website: the user will type in some data"|fine-tuning; model fine-tuning; launching, monitoring, and maintaining
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-1. Randomly generated linear dataset               
                                                                      
          Now let’s compute θ using the Normal Equation. We will use the inv() function from
          NumPy’s linear algebra module (np.linalg) to compute the inverse of a matrix, and
          the dot() method for matrix multiplication:                 
            X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance
            theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
          The function that we used to generate the data is y = 4 + 3x + Gaussian noise. Let’s
                                             1                        
          see what the equation found:                                
            >>> theta_best                                            
            array([[4.21509616],                                      
                [2.77011339]])                                        
          We would have hoped for θ = 4 and θ = 3 instead of θ = 4.215 and θ = 2.770. Close
                          0     1         0        1                  
          enough, but the noise made it impossible to recover the exact parameters of the origi‐
          nal function.                                               
          Now we can make predictions using θ:                        
            >>> X_new = np.array([[0], [2]])                          
            >>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance
            >>> y_predict = X_new_b.dot(theta_best)                   
            >>> y_predict                                             
            array([[4.21509616],                                      
                [9.75532293]])                                        
                                                                      
                                                                      
                                                                      "|inv() function
"                                                                      
                                                                      
                                                                      
                                                                      
          Yes! You now have a nice prediction service running on the cloud that can automati‐
          cally scale up to any number of QPS, plus you can query it from anywhere securely.
          Moreover, it costs you close to nothing when you don’t use it: you’ll pay just a few
          cents per month per gigabyte used on GCS. And you can also get detailed logs and
          metrics using Google Stackdriver.                           
                                                                      
          But what if you want to deploy your model to a mobile app? Or to an embedded
          device?                                                     
          Deploying a Model to a Mobile or Embedded Device            
                                                                      
          If you need to deploy your model to a mobile or embedded device, a large model may
          simply take too long to download and use too much RAM and CPU, all of which will
          make your app unresponsive, heat the device, and drain its battery. To avoid this, you
          need to make a mobile-friendly, lightweight, and efficient model, without sacrificing
          too much of its accuracy. The TFLite library provides several tools8 to help you
          deploy your models to mobile and embedded devices, with three main objectives:
                                                                      
           • Reduce the model size, to shorten download time and reduce RAM usage.
           • Reduce the amount of computations needed for each prediction, to reduce
            latency, battery usage, and heating.                      
                                                                      
           • Adapt the model to device-specific constraints.          
          To reduce the model size, TFLite’s model converter can take a SavedModel and com‐
          press it to a much lighter format based on FlatBuffers. This is an efficient cross-
          platform serialization library (a bit like protocol buffers) initially created by Google
          for gaming. It is designed so you can load FlatBuffers straight to RAM without any
          preprocessing: this reduces the loading time and memory footprint. Once the model
          is loaded into a mobile or embedded device, the TFLite interpreter will execute it to
          make predictions. Here is how you can convert a SavedModel to a FlatBuffer and save
          it to a .tflite file:                                       
            converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
            tflite_model = converter.convert()                        
            with open(""converted_model.tflite"", ""wb"") as f:           
               f.write(tflite_model)                                  
                                                                      
                   You can also save a tf.keras model directly to a FlatBuffer using
                   from_keras_model().                                
                                                                      
                                                                      
                                                                      
                                                                      
          8 Also check out TensorFlow’s Graph Transform Tools for modifying and optimizing computational graphs."|prediction service use; embedded devices; serving TensorFlow models; mobile devices; deploying to mobile and embedded devices
"                                                                      
                                                                      
                                                                      
                                                                      
          Manifold Learning                                           
                                                                      
          The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D
          shape that can be bent and twisted in a higher-dimensional space. More generally, a
          d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally
          resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it
          locally resembles a 2D plane, but it is rolled in the third dimension.
          Many dimensionality reduction algorithms work by modeling the manifold on which
          the training instances lie; this is called Manifold Learning. It relies on the manifold
          assumption, also called the manifold hypothesis, which holds that most real-world
          high-dimensional datasets lie close to a much lower-dimensional manifold. This
          assumption is very often empirically observed.              
                                                                      
          Once again, think about the MNIST dataset: all handwritten digit images have some
          similarities. They are made of connected lines, the borders are white, and they are
          more or less centered. If you randomly generated images, only a ridiculously tiny
          fraction of them would look like handwritten digits. In other words, the degrees of
          freedom available to you if you try to create a digit image are dramatically lower than
          the degrees of freedom you would have if you were allowed to generate any image
          you wanted. These constraints tend to squeeze the dataset into a lower-dimensional
          manifold.                                                   
          The manifold assumption is often accompanied by another implicit assumption: that
          the task at hand (e.g., classification or regression) will be simpler if expressed in the
          lower-dimensional space of the manifold. For example, in the top row of Figure 8-6
          the Swiss roll is split into two classes: in the 3D space (on the left), the decision
          boundary would be fairly complex, but in the 2D unrolled manifold space (on the
          right), the decision boundary is a straight line.           
          However, this implicit assumption does not always hold. For example, in the bottom
          row of Figure 8-6, the decision boundary is located at x = 5. This decision boundary
                                          1                           
          looks very simple in the original 3D space (a vertical plane), but it looks more com‐
          plex in the unrolled manifold (a collection of four independent line segments).
          In short, reducing the dimensionality of your training set before training a model will
          usually speed up training, but it may not always lead to a better or simpler solution; it
          all depends on the dataset.                                 
          Hopefully you now have a good sense of what the curse of dimensionality is and how
          dimensionality reduction algorithms can fight it, especially when the manifold
          assumption holds. The rest of this chapter will go through some of the most popular
          algorithms.                                                 
                                                                      
                                                                      
                                                                      "|manifold assumption; manifold hypothesis; Manifold Learning; dimensionality reduction
"                                                                      
                                                                      
                                                                      
                                                                      
          accurate Q-Value estimates (or close enough), then the optimal policy is choosing the
          action that has the highest Q-Value (i.e., the greedy policy).
                                                                      
            Equation 18-5. Q-Learning algorithm                       
                                                                      
            Q s,a r+γ·max Q s′,a′                                     
                α     a′                                              
                                                                      
          For each state-action pair (s, a), this algorithm keeps track of a running average of the
          rewards r the agent gets upon leaving the state s with action a, plus the sum of dis‐
          counted future rewards it expects to get. To estimate this sum, we take the maximum
          of the Q-Value estimates for the next state s′, since we assume that the target policy
          would act optimally from then on.                           
          Let’s implement the Q-Learning algorithm. First, we will need to make an agent
          explore the environment. For this, we need a step function so that the agent can exe‐
          cute one action and get the resulting state and reward:     
                                                                      
            def step(state, action):                                  
               probas = transition_probabilities[state][action]       
               next_state = np.random.choice([0, 1, 2], p=probas)     
               reward = rewards[state][action][next_state]            
               return next_state, reward                              
          Now let’s implement the agent’s exploration policy. Since the state space is pretty
          small, a simple random policy will be sufficient. If we run the algorithm for long
          enough, the agent will visit every state many times, and it will also try every possible
          action many times:                                          
            def exploration_policy(state):                            
               return np.random.choice(possible_actions[state])       
          Next, after we initialize the Q-Values just like earlier, we are ready to run the Q-
          Learning algorithm with learning rate decay (using power scheduling, introduced in
          Chapter 11):                                                
            alpha0 = 0.05 # initial learning rate                     
            decay = 0.005 # learning rate decay                       
            gamma = 0.90 # discount factor                            
            state = 0 # initial state                                 
            for iteration in range(10000):                            
               action = exploration_policy(state)                     
               next_state, reward = step(state, action)               
               next_value = np.max(Q_values[next_state])              
               alpha = alpha0 / (1 + iteration * decay)               
               Q_values[state, action] *= 1 - alpha                   
               Q_values[state, action] += alpha * (reward + gamma * next_value)
               state = next_state                                     "|implementing
"                                                                      
                                                                      
                                                                      
                                                                      
          Policy Search                                               
                                                                      
          The algorithm a software agent uses to determine its actions is called its policy. The
          policy could be a neural network taking observations as inputs and outputting the
          action to take (see Figure 18-2).                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-2. Reinforcement Learning using a neural network policy
                                                                      
          The policy can be any algorithm you can think of, and it does not have to be deter‐
          ministic. In fact, in some cases it does not even have to observe the environment! For
          example, consider a robotic vacuum cleaner whose reward is the amount of dust it
          picks up in 30 minutes. Its policy could be to move forward with some probability p
          every second, or randomly rotate left or right with probability 1 – p. The rotation
          angle would be a random angle between –r and +r. Since this policy involves some
          randomness, it is called a stochastic policy. The robot will have an erratic trajectory,
          which guarantees that it will eventually get to any place it can reach and pick up all
          the dust. The question is, how much dust will it pick up in 30 minutes?
          How would you train such a robot? There are just two policy parameters you can
          tweak: the probability p and the angle range r. One possible learning algorithm could
          be to try out many different values for these parameters, and pick the combination
          that performs best (see Figure 18-3). This is an example of policy search, in this case
          using a brute force approach. When the policy space is too large (which is generally
          the case), finding a good set of parameters this way is like searching for a needle in a
          gigantic haystack.                                          
                                                                      
          Another way to explore the policy space is to use genetic algorithms. For example, you
          could randomly create a first generation of 100 policies and try them out, then “kill”
          the 80 worst policies6 and make the 20 survivors produce 4 offspring each. An
                                                                      
                                                                      
                                                                      
                                                                      
          6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the “gene
           pool.”                                                     "|policy space; policy parameters; stochastic policy; genetic algorithms; policies; policy search
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-3. Hard margin sensitivity to outliers             
                                                                      
          To avoid these issues, use a more flexible model. The objective is to find a good bal‐
          ance between keeping the street as large as possible and limiting the margin violations
          (i.e., instances that end up in the middle of the street or even on the wrong side). This
          is called soft margin classification.                       
          When creating an SVM model using Scikit-Learn, we can specify a number of hyper‐
          parameters. C is one of those hyperparameters. If we set it to a low value, then we end
          up with the model on the left of Figure 5-4. With a high value, we get the model on
          the right. Margin violations are bad. It’s usually better to have few of them. However,
          in this case the model on the left has a lot of margin violations but will probably gen‐
          eralize better.                                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-4. Large margin (left) versus fewer margin violations (right)
                                                                      
                   If your SVM model is overfitting, you can try regularizing it by
                   reducing C.                                        
                                                                      
                                                                      
                                                                      
          The following Scikit-Learn code loads the iris dataset, scales the features, and then
          trains a linear SVM model (using the LinearSVC class with C=1 and the hinge loss
          function, described shortly) to detect Iris virginica flowers:
                                                                      
            import numpy as np                                        
            from sklearn import datasets                              
            from sklearn.pipeline import Pipeline                     "|SVM models; hinge loss function; hinge loss; margin violations
"                                                                      
                                                                      
                                                                      
                                                                      
          Oops! The algorithm desperately searched for ellipsoids, so it found eight different
          clusters instead of two. The density estimation is not too bad, so this model could
          perhaps be used for anomaly detection, but it failed to identify the two moons. Let’s
          now look at a few clustering algorithms capable of dealing with arbitrarily shaped
          clusters.                                                   
                                                                      
          Other Algorithms for Anomaly and Novelty Detection          
                                                                      
          Scikit-Learn implements other algorithms dedicated to anomaly detection or novelty
          detection:                                                  
          PCA (and other dimensionality reduction techniques with an inverse_transform()
          method)                                                     
            If you compare the reconstruction error of a normal instance with the recon‐
            struction error of an anomaly, the latter will usually be much larger. This is a sim‐
            ple and often quite efficient anomaly detection approach (see this chapter’s
            exercises for an application of this approach).           
                                                                      
          Fast-MCD (minimum covariance determinant)                   
            Implemented by the EllipticEnvelope class, this algorithm is useful for outlier
            detection, in particular to clean up a dataset. It assumes that the normal instances
            (inliers) are generated from a single Gaussian distribution (not a mixture). It also
            assumes that the dataset is contaminated with outliers that were not generated
            from this Gaussian distribution. When the algorithm estimates the parameters of
            the Gaussian distribution (i.e., the shape of the elliptic envelope around the inli‐
            ers), it is careful to ignore the instances that are most likely outliers. This techni‐
            que gives a better estimation of the elliptic envelope and thus makes the
            algorithm better at identifying the outliers.             
          Isolation Forest                                            
            This is an efficient algorithm for outlier detection, especially in high-dimensional
            datasets. The algorithm builds a Random Forest in which each Decision Tree is
            grown randomly: at each node, it picks a feature randomly, then it picks a ran‐
            dom threshold value (between the min and max values) to split the dataset in
            two. The dataset gradually gets chopped into pieces this way, until all instances
            end up isolated from the other instances. Anomalies are usually far from other
            instances, so on average (across all the Decision Trees) they tend to get isolated in
            fewer steps than normal instances.                        
          Local Outlier Factor (LOF)                                  
            This algorithm is also good for outlier detection. It compares the density of
            instances around a given instance to the density around its neighbors. An anom‐
            aly is often more isolated than its k nearest neighbors.  
                                                                      
                                                                      "|Local Outlier Factor (LOF); anomaly and novelty detection; Isolation Forest algorithm; Fast-MCD (minimum covariance determinant); novelty detection; additional algorithms for anomaly and novelty detection; for anomaly detection
"                                                                      
                                                                      
                                                                      
                                                                      
            from tf_agents.policies.random_tf_policy import RandomTFPolicy
                                                                      
            initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),
                                    tf_env.action_spec())             
            init_driver = DynamicStepDriver(                          
               tf_env,                                                
               initial_collect_policy,                                
               observers=[replay_buffer.add_batch, ShowProgress(20000)],
               num_steps=20000) # <=> 80,000 ALE frames               
            final_time_step, final_policy_state = init_driver.run()   
          We’re almost ready to run the training loop! We just need one last component: the
          dataset.                                                    
          Creating the Dataset                                        
          To sample a batch of trajectories from the replay buffer, call its get_next() method.
          This returns the batch of trajectories plus a BufferInfo object that contains the sam‐
          ple identifiers and their sampling probabilities (this may be useful for some algo‐
          rithms, such as PER). For example, the following code will sample a small batch of
          two trajectories (subepisodes), each containing three consecutive steps. These
          subepisodes are shown in Figure 18-15 (each row contains three consecutive steps
          from an episode):                                           
            >>> trajectories, buffer_info = replay_buffer.get_next(   
            ...  sample_batch_size=2, num_steps=3)                    
            ...                                                       
            >>> trajectories._fields                                  
            ('step_type', 'observation', 'action', 'policy_info',     
             'next_step_type', 'reward', 'discount')                  
            >>> trajectories.observation.shape                        
            TensorShape([2, 3, 84, 84, 4])                            
            >>> trajectories.step_type.numpy()                        
            array([[1, 1, 1],                                         
                [1, 1, 1]], dtype=int32)                              
          The trajectories object is a named tuple, with seven fields. Each field contains a
          tensor whose first two dimensions are 2 and 3 (since there are two trajectories, each
          with three steps). This explains why the shape of the observation field is [2, 3, 84, 84,
          4]: that’s two trajectories, each with three steps, and each step’s observation is 84 × 84
          × 4. Similarly, the step_type tensor has a shape of [2, 3]: in this example, both trajec‐
          tories contain three consecutive steps in the middle on an episode (types 1, 1, 1). In
          the second trajectory, you can barely see the ball at the lower left of the first observa‐
          tion, and it disappears in the next two observations, so the agent is about to lose a life,
          but the episode will not end immediately because it still has several lives left.
                                                                      
                                                                      "|datasets
"                                                                      
                                                                      
                                                                      
                                                                      
          Peephole connections                                        
                                                                      
          In a regular LSTM cell, the gate controllers can look only at the input x and the pre‐
                                                   (t)                
          vious short-term state h . It may be a good idea to give them a bit more context by
                        (t–1)                                         
          letting them peek at the long-term state as well. This idea was proposed by Felix Gers
          and Jürgen Schmidhuber in 2000.10 They proposed an LSTM variant with extra con‐
          nections called peephole connections: the previous long-term state c is added as an
                                                 (t–1)                
          input to the controllers of the forget gate and the input gate, and the current long-
          term state c is added as input to the controller of the output gate. This often
                 (t)                                                  
          improves performance, but not always, and there is no clear pattern for which tasks
          are better off with or without them: you will have to try it on your task and see if it
          helps.                                                      
          In Keras, the LSTM layer is based on the keras.layers.LSTMCell cell, which does not
          support peepholes. The experimental tf.keras.experimental.PeepholeLSTMCell
          does, however, so you can create a keras.layers.RNN layer and pass a PeepholeLSTM
          Cell to its constructor.                                    
          There are many other variants of the LSTM cell. One particularly popular variant is
          the GRU cell, which we will look at now.                    
          GRU cells                                                   
          The Gated Recurrent Unit (GRU) cell (see Figure 15-10) was proposed by Kyunghyun
          Cho et al. in a 2014 paper11 that also introduced the Encoder–Decoder network we
          discussed earlier.                                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          10 F. A. Gers and J. Schmidhuber, “Recurrent Nets That Time and Count,” Proceedings of the IEEE-INNS-ENNS
           International Joint Conference on Neural Networks (2000): 189–194.
          11 Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical
           Machine Translation,” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing
           (2014): 1724–1734.                                         "|Gated Recurrent Unit (GRU) cell; peephole connections
"                                                                      
                                                                      
                                                                      
                                                                      
          It is a bit ugly due to all the different indices, but all it does is calculate the weighted
          sum of all the inputs, plus the bias term.                  
                                                                      
            Equation 14-1. Computing the output of a neuron in a convolutional layer
                                                                      
                   f h −1 f w −1 f n′ −1    i′=i×s +u                 
                                                h                     
            z  =b + ∑  ∑   ∑  x   .w    with                          
             i,j,k k u=0 v=0 k′=0 i′,j′,k′ u,v,k′,k j′= j×s +v        
                                                 w                    
          In this equation:                                           
           • z is the output of the neuron located in row i, column j in feature map k of the
             i, j, k                                                  
            convolutional layer (layer l).                            
           • As explained earlier, s and s are the vertical and horizontal strides, f and f are
                         h   w                       h   w            
            the height and width of the receptive field, and f is the number of feature maps
                                         n′                           
            in the previous layer (layer l – 1).                      
           • x  is the output of the neuron located in layer l – 1, row i′, column j′, feature
             i′, j′, k′                                               
            map k′ (or channel k′ if the previous layer is the input layer).
           • b is the bias term for feature map k (in layer l). You can think of it as a knob that
             k                                                        
            tweaks the overall brightness of the feature map k.       
           • w   is the connection weight between any neuron in feature map k of the layer
             u, v, k′ ,k                                              
            l and its input located at row u, column v (relative to the neuron’s receptive field),
            and feature map k′.                                       
          TensorFlow Implementation                                   
          In TensorFlow, each input image is typically represented as a 3D tensor of shape
          [height, width, channels]. A mini-batch is represented as a 4D tensor of shape [mini-
          batch size, height, width, channels]. The weights of a convolutional layer are repre‐
          sented as a 4D tensor of shape [f , f , f , f ]. The bias terms of a convolutional layer
                             h w n′ n                                 
          are simply represented as a 1D tensor of shape [f ].        
                                      n                               
          Let’s look at a simple example. The following code loads two sample images, using
          Scikit-Learn’s load_sample_image() (which loads two color images, one of a Chinese
          temple, and the other of a flower), then it creates two filters and applies them to both
          images, and finally it displays one of the resulting feature maps. Note that you must
          pip install the Pillow package to use load_sample_image().  
            from sklearn.datasets import load_sample_image            
            # Load sample images                                      
            china = load_sample_image(""china.jpg"") / 255              
            flower = load_sample_image(""flower.jpg"") / 255            "|convolutional layers; TensorFlow implementation
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-26. Semantic segmentation                         
                                                                      
          There are several solutions available for upsampling (increasing the size of an image),
          such as bilinear interpolation, but that only works reasonably well up to ×4 or ×8.
          Instead, they use a transposed convolutional layer:33 it is equivalent to first stretching
          the image by inserting empty rows and columns (full of zeros), then performing a
          regular convolution (see Figure 14-27). Alternatively, some people prefer to think of
          it as a regular convolutional layer that uses fractional strides (e.g., 1/2 in
          Figure 14-27). The transposed convolutional layer can be initialized to perform
          something close to linear interpolation, but since it is a trainable layer, it will learn to
          do better during training. In tf.keras, you can use the Conv2DTranspose layer.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-27. Upsampling using a transposed convolutional layer
                                                                      
                                                                      
                                                                      
                                                                      
          33 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati‐
           cians call a deconvolution, so this name should be avoided."|transposed convolutional layer; upsampling layer
"                                                                      
                                                                      
                                                                      
                                                                      
          Memory Requirements                                         
                                                                      
          Another problem with CNNs is that the convolutional layers require a huge amount
          of RAM. This is especially true during training, because the reverse pass of backpro‐
          pagation requires all the intermediate values computed during the forward pass.
          For example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature
          maps of size 150 × 100, with stride 1 and ""same"" padding. If the input is a 150 × 100
          RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200
          = 15,200 (the + 1 corresponds to the bias terms), which is fairly small compared to a
          fully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐
          rons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =
          75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐
          nected layer, but still quite computationally intensive. Moreover, if the feature maps
          are represented using 32-bit floats, then the convolutional layer’s output will occupy
          200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one
          instance—if a training batch contains 100 instances, then this layer will use up 1.2 GB
          of RAM!                                                     
          During inference (i.e., when making a prediction for a new instance) the RAM occu‐
          pied by one layer can be released as soon as the next layer has been computed, so you
          only need as much RAM as required by two consecutive layers. But during training
          everything computed during the forward pass needs to be preserved for the reverse
          pass, so the amount of RAM needed is (at least) the total amount of RAM required by
          all layers.                                                 
                                                                      
                   If training crashes because of an out-of-memory error, you can try
                   reducing the mini-batch size. Alternatively, you can try reducing
                   dimensionality using a stride, or removing a few layers. Or you can
                   try using 16-bit floats instead of 32-bit floats. Or you could distrib‐
                   ute the CNN across multiple devices.               
                                                                      
          Now let’s look at the second common building block of CNNs: the pooling layer.
                                                                      
          Pooling Layers                                              
                                                                      
          Once you understand how convolutional layers work, the pooling layers are quite
          easy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to
                                                                      
                                                                      
                                                                      
          7 A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502
           × 1002 × 3 = 675 million parameters!                       
          8 In the international system of units (SI), 1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits."|convolutional layer; pooling layer; subsampling; memory requirements
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Using GPUs to Speed Up Computations                         
                                                                      
          In Chapter 11 we discussed several techniques that can considerably speed up train‐
          ing: better weight initialization, Batch Normalization, sophisticated optimizers, and
          so on. But even with all of these techniques, training a large neural network on a sin‐
          gle machine with a single CPU can take days or even weeks.  
                                                                      
          In this section we will look at how to speed up your models by using GPUs. We will
          also see how to split the computations across multiple devices, including the CPU
          and multiple GPU devices (see Figure 19-9). For now we will run everything on a sin‐
          gle machine, but later in this chapter we will discuss how to distribute computations
          across multiple servers.                                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-9. Executing a TensorFlow graph across multiple devices in parallel
                                                                      
          Thanks to GPUs, instead of waiting for days or weeks for a training algorithm to
          complete, you may end up waiting for just a few minutes or hours. Not only does this
          save an enormous amount of time, but it also means that you can experiment with
          various models much more easily and frequently retrain your models on fresh data.
                                                                      
                   You can often get a major performance boost simply by adding
                   GPU cards to a single machine. In fact, in many cases this will suf‐
                   fice; you won’t need to use multiple machines at all. For example,
                   you can typically train a neural network just as fast using four
                   GPUs on a single machine rather than eight GPUs across multiple
                   machines, due to the extra delay imposed by network communica‐
                   tions in a distributed setup. Similarly, using a single powerful GPU
                   is often preferable to using multiple slower GPUs. "|using GPUs to speed computations; speeding computations with; GPUs (graphics processing units) adding to single machines
"                                                                      
                                                                      
                                                                      
                                                                      
            signature_def['serving_default']:                         
             The given SavedModel SignatureDef contains the following input(s):
               inputs['flatten_input'] tensor_info:                   
                 dtype: DT_FLOAT                                      
                 shape: (-1, 28, 28)                                  
                 name: serving_default_flatten_input:0                
             The given SavedModel SignatureDef contains the following output(s):
               outputs['dense_1'] tensor_info:                        
                 dtype: DT_FLOAT                                      
                 shape: (-1, 10)                                      
                 name: StatefulPartitionedCall:0                      
             Method name is: tensorflow/serving/predict               
          A SavedModel contains one or more metagraphs. A metagraph is a computation
          graph plus some function signature definitions (including their input and output
          names, types, and shapes). Each metagraph is identified by a set of tags. For example,
          you may want to have a metagraph containing the full computation graph, including
          the training operations (this one may be tagged ""train"", for example), and another
          metagraph containing a pruned computation graph with only the prediction opera‐
          tions, including some GPU-specific operations (this metagraph may be tagged
          ""serve"", ""gpu""). However, when you pass a tf.keras model to the
          tf.saved_model.save() function, by default the function saves a much simpler
          SavedModel: it saves a single metagraph tagged ""serve"", which contains two signa‐
          ture definitions, an initialization function (called __saved_model_init_op, which
          you do not need to worry about) and a default serving function (called serv
          ing_default). When saving a tf.keras model, the default serving function corre‐
          sponds to the model’s call() function, which of course makes predictions.
          The saved_model_cli tool can also be used to make predictions (for testing, not
          really for production). Suppose you have a NumPy array (X_new) containing three
          images of handwritten digits that you want to make predictions for. You first need to
          export them to NumPy’s npy format:                          
            np.save(""my_mnist_tests.npy"", X_new)                      
          Next, use the saved_model_cli command like this:            
            $ saved_model_cli run --dir my_mnist_model/0001 --tag_set serve \
                         --signature_def serving_default \            
                         --inputs flatten_input=my_mnist_tests.npy    
            [...] Result for output key dense_1:                      
            [[1.1739199e-04 1.1239604e-07 6.0210604e-04 [...] 3.9471846e-04]
             [1.2294615e-03 2.9207937e-05 9.8599273e-01 [...] 1.1113169e-07]
             [6.4066830e-05 9.6359509e-01 9.0598064e-03 [...] 4.2495009e-04]]
          The tool’s output contains the 10 class probabilities of each of the 3 instances. Great!
          Now that you have a working SavedModel, the next step is to install TF Serving.
                                                                      "|metagraphs
"                                                                      
                                                                      
                                                                      
                                                                      
            another clustering algorithm in this low-dimensional space (Scikit-Learn’s imple‐
            mentation uses K-Means.) Spectral clustering can capture complex cluster struc‐
            tures, and it can also be used to cut graphs (e.g., to identify clusters of friends on
            a social network). It does not scale well to large numbers of instances, and it does
            not behave well when the clusters have very different sizes.
                                                                      
          Now let’s dive into Gaussian mixture models, which can be used for density estima‐
          tion, clustering, and anomaly detection.                    
          Gaussian Mixtures                                           
                                                                      
          A Gaussian mixture model (GMM) is a probabilistic model that assumes that the
          instances were generated from a mixture of several Gaussian distributions whose
          parameters are unknown. All the instances generated from a single Gaussian distri‐
          bution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif‐
          ferent ellipsoidal shape, size, density, and orientation, just like in Figure 9-11. When
          you observe an instance, you know it was generated from one of the Gaussian distri‐
          butions, but you are not told which one, and you do not know what the parameters of
          these distributions are.                                    
                                                                      
          There are several GMM variants. In the simplest variant, implemented in the Gaus
          sianMixture class, you must know in advance the number k of Gaussian distribu‐
          tions. The dataset X is assumed to have been generated through the following
          probabilistic process:                                      
           • For each instance, a cluster is picked randomly from among k clusters. The prob‐
            ability of choosing the jth cluster is defined by the cluster’s weight, ϕ(j).7 The index
            of the cluster chosen for the ith instance is noted z(i). 
                                                                      
           • If z(i)=j, meaning the ith instance has been assigned to the jth cluster, the location
            x(i) of this instance is sampled randomly from the Gaussian distribution with
            mean μ(j) and covariance matrix Σ(j). This is noted xi ∼  μ j,Σ j .
                                                                      
          This generative process can be represented as a graphical model. Figure 9-16 repre‐
          sents the structure of the conditional dependencies between random variables.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          7 Phi (ϕ or φ) is the 21st letter of the Greek alphabet.    "|Gaussian mixture model (GMM); clustering; variants; Gaussian mixtures model (GMM)
"                                                                      
                                                                      
                                                                      
                                                                      
          The general problem formulation is given by Equation 5-5.   
                                                                      
            Equation 5-5. Quadratic Programming problem               
                                                                      
                    1 ⊺     ⊺                                         
            Minimize p Hp + f p                                       
                    2                                                 
               p                                                      
            subject to Ap≤b                                           
                    p is an n ‐dimensional vector (n =number of parameters),
                          p             p                             
                    H is an n ×n matrix,                              
                          p  p                                        
               where f is an n ‐dimensional vector,                   
                          p                                           
                    A is an n ×n matrix (n =number of constraints),   
                          c  p      c                                 
                    b is an n ‐dimensional vector.                    
                          c                                           
          Note that the expression A p ≤ b defines n constraints: p⊺ a(i) ≤ b(i) for i = 1, 2, ⋯, n,
                                   c                       c          
          where a(i) is the vector containing the elements of the ith row of A and b(i) is the ith
          element of b.                                               
          You can easily verify that if you set the QP parameters in the following way, you get
          the hard margin linear SVM classifier objective:            
           • n = n + 1, where n is the number of features (the +1 is for the bias term).
             p                                                        
           • n = m, where m is the number of training instances.      
             c                                                        
           • H is the n × n identity matrix, except with a zero in the top-left cell (to ignore
                  p  p                                                
            the bias term).                                           
           • f = 0, an n -dimensional vector full of 0s.              
                  p                                                   
           • b = –1, an n-dimensional vector full of –1s.             
                   c                                                  
           • a(i) = –t(i) x˙(i), where x˙(i) is equal to x(i) with an extra bias feature x˙ = 1.
                                                 0                    
          One way to train a hard margin linear SVM classifier is to use an off-the-shelf QP
          solver and pass it the preceding parameters. The resulting vector p will contain the
          bias term b = p and the feature weights w = p for i = 1, 2, ⋯, n. Similarly, you can
                   0               i  i                               
          use a QP solver to solve the soft margin problem (see the exercises at the end of the
          chapter).                                                   
          To use the kernel trick, we are going to look at a different constrained optimization
          problem.                                                    
          The Dual Problem                                            
          Given a constrained optimization problem, known as the primal problem, it is possi‐
          ble to express a different but closely related problem, called its dual problem. The"|primal problem; dual problem
"                                                                      
                                                                      
                                                                      
                                                                      
          from.16 When an experience is recorded in the replay buffer, its priority is set to a very
          large value, to ensure that it gets sampled at least once. However, once it is sampled
          (and every time it is sampled), the TD error δ is computed, and this experience’s pri‐
          ority is set to p = |δ| (plus a small constant to ensure that every experience has a non-
          zero probability of being sampled). The probability P of sampling an experience with
          priority p is proportional to pζ, where ζ is a hyperparameter that controls how greedy
          we want importance sampling to be: when ζ = 0, we just get uniform sampling, and
          when ζ = 1, we get full-blown importance sampling. In the paper, the authors used ζ =
          0.6, but the optimal value will depend on the task.         
          There’s one catch, though: since the samples will be biased toward important experi‐
          ences, we must compensate for this bias during training by downweighting the expe‐
          riences according to their importance, or else the model will just overfit the
          important experiences. To be clear, we want important experiences to be sampled
          more often, but this also means we must give them a lower weight during training. To
          do this, we define each experience’s training weight as w = (n P)–β, where n is the
          number of experiences in the replay buffer, and β is a hyperparameter that controls
          how much we want to compensate for the importance sampling bias (0 means not at
          all, while 1 means entirely). In the paper, the authors used β = 0.4 at the beginning of
          training and linearly increased it to β = 1 by the end of training. Again, the optimal
          value will depend on the task, but if you increase one, you will usually want to
          increase the other as well.                                 
                                                                      
          Now let’s look at one last important variant of the DQN algorithm.
          Dueling DQN                                                 
                                                                      
          The Dueling DQN algorithm (DDQN, not to be confused with Double DQN,
          although both techniques can easily be combined) was introduced in yet another
          2015 paper17 by DeepMind researchers. To understand how it works, we must first
          note that the Q-Value of a state-action pair (s, a) can be expressed as Q(s, a) = V(s) +
          A(s, a), where V(s) is the value of state s and A(s, a) is the advantage of taking the
          action a in state s, compared to all other possible actions in that state. Moreover, the
          value of a state is equal to the Q-Value of the best action a* for that state (since we
          assume the optimal policy will pick the best action), so V(s) = Q(s, a*), which implies
          that A(s, a*) = 0. In a Dueling DQN, the model estimates both the value of the state
          and the advantage of each possible action. Since the best action should have an
          advantage of 0, the model subtracts the maximum predicted advantage from all pre‐
                                                                      
                                                                      
          16 It could also just be that the rewards are noisy, in which case there are better methods for estimating an expe‐
           rience’s importance (see the paper for some examples).     
          17 Ziyu Wang et al., “Dueling Network Architectures for Deep Reinforcement Learning,” arXiv preprint arXiv:
           1511.06581 (2015).                                         "|Dueling DQN algorithm; Dueling DQN
"                                                                      
                                                                      
                                                                      
                                                                      
          A SequenceExample contains a Features object for the contextual data and a Fea
          tureLists object that contains one or more named FeatureList objects (e.g., a Fea
          tureList named ""content"" and another named ""comments""). Each FeatureList
          contains a list of Feature objects, each of which may be a list of byte strings, a list of
          64-bit integers, or a list of floats (in this example, each Feature would represent a
          sentence or a comment, perhaps in the form of a list of word identifiers). Building a
          SequenceExample, serializing it, and parsing it is similar to building, serializing, and
          parsing an Example, but you must use tf.io.parse_single_sequence_example() to
          parse a single SequenceExample or tf.io.parse_sequence_example() to parse a
          batch. Both functions return a tuple containing the context features (as a dictionary)
          and the feature lists (also as a dictionary). If the feature lists contain sequences of
          varying sizes (as in the preceding example), you may want to convert them to ragged
          tensors, using tf.RaggedTensor.from_sparse() (see the notebook for the full code):
                                                                      
            parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(
               serialized_sequence_example, context_feature_descriptions,
               sequence_feature_descriptions)                         
            parsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[""content""])
          Now that you know how to efficiently store, load, and parse data, the next step is to
          prepare it so that it can be fed to a neural network.       
          Preprocessing the Input Features                            
                                                                      
          Preparing your data for a neural network requires converting all features into numer‐
          ical features, generally normalizing them, and more. In particular, if your data con‐
          tains categorical features or text features, they need to be converted to numbers. This
          can be done ahead of time when preparing your data files, using any tool you like
          (e.g., NumPy, pandas, or Scikit-Learn). Alternatively, you can preprocess your data on
          the fly when loading it with the Data API (e.g., using the dataset’s map() method, as
          we saw earlier), or you can include a preprocessing layer directly in your model. Let’s
          look at this last option now.                               
          For example, here is how you can implement a standardization layer using a Lambda
          layer. For each feature, it subtracts the mean and divides by its standard deviation
          (plus a tiny smoothing term to avoid division by zero):     
                                                                      
            means = np.mean(X_train, axis=0, keepdims=True)           
            stds = np.std(X_train, axis=0, keepdims=True)             
            eps = keras.backend.epsilon()                             
            model = keras.models.Sequential([                         
               keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),
               [...] # other layers                                   
            ])                                                        
                                                                      "|preprocessing; TFRecord format; preprocessing input features
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
           2. Take a new centroid c(i), choosing an instance x(i) with probability D  i 2 /
            ∑m  D  j 2 , where D(x(i)) is the distance between the instance x(i) and the clos‐
             j=1                                                      
            est centroid that was already chosen. This probability distribution ensures that
            instances farther away from already chosen centroids are much more likely be
            selected as centroids.                                    
           3. Repeat the previous step until all k centroids have been chosen.
          The KMeans class uses this initialization method by default. If you want to force it to
          use the original method (i.e., picking k instances randomly to define the initial cent‐
          roids), then you can set the init hyperparameter to ""random"". You will rarely need to
          do this.                                                    
                                                                      
          Accelerated K-Means and mini-batch K-Means                  
          Another important improvement to the K-Means algorithm was proposed in a 2003
          paper by Charles Elkan.4 It considerably accelerates the algorithm by avoiding many
          unnecessary distance calculations. Elkan achieved this by exploiting the triangle
          inequality (i.e., that a straight line is always the shortest distance between two points5)
          and by keeping track of lower and upper bounds for distances between instances and
          centroids. This is the algorithm the KMeans class uses by default (you can force it to
          use the original algorithm by setting the algorithm hyperparameter to ""full"",
          although you probably will never need to).                  
                                                                      
          Yet another important variant of the K-Means algorithm was proposed in a 2010
          paper by David Sculley.6 Instead of using the full dataset at each iteration, the algo‐
          rithm is capable of using mini-batches, moving the centroids just slightly at each iter‐
          ation. This speeds up the algorithm typically by a factor of three or four and makes it
          possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements
          this algorithm in the MiniBatchKMeans class. You can just use this class like the
          KMeans class:                                               
            from sklearn.cluster import MiniBatchKMeans               
            minibatch_kmeans = MiniBatchKMeans(n_clusters=5)          
            minibatch_kmeans.fit(X)                                   
                                                                      
                                                                      
                                                                      
          4 Charles Elkan, “Using the Triangle Inequality to Accelerate k-Means,” Proceedings of the 20th International
           Conference on Machine Learning (2003): 147–153.            
          5 The triangle inequality is AC ≤ AB + BC where A, B and C are three points and AB, AC, and BC are the
           distances between these points.                            
          6 David Sculley, “Web-Scale K-Means Clustering,” Proceedings of the 19th International Conference on World
           Wide Web (2010): 1177–1178.                                "|accelerated and mini-batch; accelerated K-Means; mini-batch K-Means
"                                                                      
                                                                      
                                                                      
                                                                      
            clf = Pipeline([                                          
                 (""kpca"", KernelPCA(n_components=2)),                 
                 (""log_reg"", LogisticRegression())                    
               ])                                                     
            param_grid = [{                                           
                 ""kpca__gamma"": np.linspace(0.03, 0.05, 10),          
                 ""kpca__kernel"": [""rbf"", ""sigmoid""]                   
               }]                                                     
            grid_search = GridSearchCV(clf, param_grid, cv=3)         
            grid_search.fit(X, y)                                     
          The best kernel and hyperparameters are then available through the best_params_
          variable:                                                   
                                                                      
            >>> print(grid_search.best_params_)                       
            {'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}
          Another approach, this time entirely unsupervised, is to select the kernel and hyper‐
          parameters that yield the lowest reconstruction error. Note that reconstruction is not
          as easy as with linear PCA. Here’s why. Figure 8-11 shows the original Swiss roll 3D
          dataset (top left) and the resulting 2D dataset after kPCA is applied using an RBF ker‐
          nel (top right). Thanks to the kernel trick, this transformation is mathematically
          equivalent to using the feature map φ to map the training set to an infinite-
          dimensional feature space (bottom right), then projecting the transformed training
          set down to 2D using linear PCA.                            
          Notice that if we could invert the linear PCA step for a given instance in the reduced
          space, the reconstructed point would lie in feature space, not in the original space
          (e.g., like the one represented by an X in the diagram). Since the feature space is
          infinite-dimensional, we cannot compute the reconstructed point, and therefore we
          cannot compute the true reconstruction error. Fortunately, it is possible to find a
          point in the original space that would map close to the reconstructed point. This
          point is called the reconstruction pre-image. Once you have this pre-image, you can
          measure its squared distance to the original instance. You can then select the kernel
          and hyperparameters that minimize this reconstruction pre-image error.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|reconstruction pre-images; pre-images; feature maps; kernel trick
"                                                                      
                                                                      
                                                                      
                                                                      
          Visualizing the Reconstructions                             
                                                                      
          One way to ensure that an autoencoder is properly trained is to compare the inputs
          and the outputs: the differences should not be too significant. Let’s plot a few images
          from the validation set, as well as their reconstructions:  
            def plot_image(image):                                    
               plt.imshow(image, cmap=""binary"")                       
               plt.axis(""off"")                                        
            def show_reconstructions(model, n_images=5):              
               reconstructions = model.predict(X_valid[:n_images])    
               fig = plt.figure(figsize=(n_images * 1.5, 3))          
               for image_index in range(n_images):                    
                 plt.subplot(2, n_images, 1 + image_index)            
                 plot_image(X_valid[image_index])                     
                 plt.subplot(2, n_images, 1 + n_images + image_index) 
                 plot_image(reconstructions[image_index])             
            show_reconstructions(stacked_ae)                          
          Figure 17-4 shows the resulting images.                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-4. Original images (top) and their reconstructions (bottom)
                                                                      
          The reconstructions are recognizable, but a bit too lossy. We may need to train the
          model for longer, or make the encoder and decoder deeper, or make the codings
          larger. But if we make the network too powerful, it will manage to make perfect
          reconstructions without having learned any useful patterns in the data. For now, let’s
          go with this model.                                         
                                                                      
          Visualizing the Fashion MNIST Dataset                       
          Now that we have trained a stacked autoencoder, we can use it to reduce the dataset’s
          dimensionality. For visualization, this does not give great results compared to other
          dimensionality reduction algorithms (such as those we discussed in Chapter 8), but
          one big advantage of autoencoders is that they can handle large datasets, with many
          instances and many features. So one strategy is to use an autoencoder to reduce the
          dimensionality down to a reasonable level, then use another dimensionality
                                                                      "|visualizing Fashion MNIST Dataset; Fashion MNIST dataset; visualizing reconstructions
"                                                                      
                                                                      
                                                                      
                                                                      
          Check the Assumptions                                       
                                                                      
          Lastly, it is good practice to list and verify the assumptions that have been made so far
          (by you or others); this can help you catch serious issues early on. For example, the
          district prices that your system outputs are going to be fed into a downstream
          Machine Learning system, and you assume that these prices are going to be used as
          such. But what if the downstream system converts the prices into categories (e.g.,
          “cheap,” “medium,” or “expensive”) and then uses those categories instead of the pri‐
          ces themselves? In this case, getting the price perfectly right is not important at all;
          your system just needs to get the category right. If that’s so, then the problem should
          have been framed as a classification task, not a regression task. You don’t want to find
          this out after working on a regression system for months.   
          Fortunately, after talking with the team in charge of the downstream system, you are
          confident that they do indeed need the actual prices, not just categories. Great! You’re
          all set, the lights are green, and you can start coding now!
                                                                      
          Get the Data                                                
                                                                      
          It’s time to get your hands dirty. Don’t hesitate to pick up your laptop and walk
          through the following code examples in a Jupyter notebook. The full Jupyter note‐
          book is available at https://github.com/ageron/handson-ml2. 
          Create the Workspace                                        
                                                                      
          First you will need to have Python installed. It is probably already installed on your
          system. If not, you can get it at https://www.python.org/.5 
                                                                      
          Next you need to create a workspace directory for your Machine Learning code and
          datasets. Open a terminal and type the following commands (after the $ prompts):
            $ export ML_PATH=""$HOME/ml"" # You can change the path if you prefer
            $ mkdir -p $ML_PATH                                       
          You will need a number of Python modules: Jupyter, NumPy, pandas, Matplotlib, and
          Scikit-Learn. If you already have Jupyter running with all these modules installed,
          you can safely skip to “Download the Data” on page 46. If you don’t have them yet,
          there are many ways to install them (and their dependencies). You can use your
          system’s packaging system (e.g., apt-get on Ubuntu, or MacPorts or Homebrew on
          macOS), install a Scientific Python distribution such as Anaconda and use its packag‐
          ing system, or just use Python’s own packaging system, pip, which is included by
                                                                      
                                                                      
                                                                      
          5 The latest version of Python 3 is recommended. Python 2.7+ may work too, but now that it’s deprecated, all
           major scientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible."|data downloading; installing; verifying assumptions; workspace creation
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-9. In this depiction of Gradient Boosting, the first predictor (top left) is trained
          normally, then each consecutive predictor (middle left and lower left) is trained on the
          previous predictor’s residuals; the right column shows the resulting ensemble’s predictions
                                                                      
          The learning_rate hyperparameter scales the contribution of each tree. If you set it
          to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐
          ing set, but the predictions will usually generalize better. This is a regularization tech‐
          nique called shrinkage. Figure 7-10 shows two GBRT ensembles trained with a low
          learning rate: the one on the left does not have enough trees to fit the training set,
          while the one on the right has too many trees and overfits the training set.
                                                                      
                                                                      
                                                                      
                                                                      "|shrinkage technique; shrinkage
"                                                                      
                                                                      
                                                                      
                                                                      
          Unsupervised Pretraining Using Stacked Autoencoders         
                                                                      
          As we discussed in Chapter 11, if you are tackling a complex supervised task but you
          do not have a lot of labeled training data, one solution is to find a neural network that
          performs a similar task and reuse its lower layers. This makes it possible to train a
          high-performance model using little training data because your neural network won’t
          have to learn all the low-level features; it will just reuse the feature detectors learned
          by the existing network.                                    
          Similarly, if you have a large dataset but most of it is unlabeled, you can first train a
          stacked autoencoder using all the data, then reuse the lower layers to create a neural
          network for your actual task and train it using the labeled data. For example,
          Figure 17-6 shows how to use a stacked autoencoder to perform unsupervised pre‐
          training for a classification neural network. When training the classifier, if you really
          don’t have much labeled training data, you may want to freeze the pretrained layers
          (at least the lower ones).                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-6. Unsupervised pretraining using autoencoders    
                                                                      
                   Having plenty of unlabeled data and little labeled data is common.
                   Building a large unlabeled dataset is often cheap (e.g., a simple
                   script can download millions of images off the internet), but label‐
                   ing those images (e.g., classifying them as cute or not) can usually
                   be done reliably only by humans. Labeling instances is time-
                   consuming and costly, so it’s normal to have only a few thousand
                   human-labeled instances.                           "|unsupervised pretraining using stacked; pretraining using stacked autoencoders; using stacked autoencoders; unsupervised pretraining
"                                                                      
                                                                      
                                                                      
                                                                      
          Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the
          left and accelerates right when the pole is leaning toward the right. We will run this
          policy to see the average rewards it gets over 500 episodes:
                                                                      
            def basic_policy(obs):                                    
               angle = obs[2]                                         
               return 0 if angle < 0 else 1                           
            totals = []                                               
            for episode in range(500):                                
               episode_rewards = 0                                    
               obs = env.reset()                                      
               for step in range(200):                                
                 action = basic_policy(obs)                           
                 obs, reward, done, info = env.step(action)           
                 episode_rewards += reward                            
                 if done:                                             
                   break                                              
               totals.append(episode_rewards)                         
          This code is hopefully self-explanatory. Let’s look at the result:
            >>> import numpy as np                                    
            >>> np.mean(totals), np.std(totals), np.min(totals), np.max(totals)
            (41.718, 8.858356280936096, 24.0, 68.0)                   
          Even with 500 tries, this policy never managed to keep the pole upright for more than
          68 consecutive steps. Not great. If you look at the simulation in the Jupyter note‐
          books, you will see that the cart oscillates left and right more and more strongly until
          the pole tilts too much. Let’s see if a neural network can come up with a better policy.
          Neural Network Policies                                     
          Let’s create a neural network policy. Just like with the policy we hardcoded earlier, this
          neural network will take an observation as input, and it will output the action to be
          executed. More precisely, it will estimate a probability for each action, and then we
          will select an action randomly, according to the estimated probabilities (see
          Figure 18-5). In the case of the CartPole environment, there are just two possible
          actions (left or right), so we only need one output neuron. It will output the probabil‐
          ity p of action 0 (left), and of course the probability of action 1 (right) will be 1 – p.
          For example, if it outputs 0.7, then we will pick action 0 with 70% probability, or
          action 1 with 30% probability.                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|OpenAI Gym; neural network policies
"                                                                      
                                                                      
                                                                      
                                                                      
          Reinforcement Learning is and what it’s good at, then present two of the most impor‐
          tant techniques in Deep Reinforcement Learning: policy gradients and deep Q-
          networks (DQNs), including a discussion of Markov decision processes (MDPs). We
          will use these techniques to train models to balance a pole on a moving cart; then I’ll
          introduce the TF-Agents library, which uses state-of-the-art algorithms that greatly
          simplify building powerful RL systems, and we will use the library to train an agent to
          play Breakout, the famous Atari game. I’ll close the chapter by taking a look at some
          of the latest advances in the field.                        
                                                                      
          Learning to Optimize Rewards                                
                                                                      
          In Reinforcement Learning, a software agent makes observations and takes actions
          within an environment, and in return it receives rewards. Its objective is to learn to act
          in a way that will maximize its expected rewards over time. If you don’t mind a bit of
          anthropomorphism, you can think of positive rewards as pleasure, and negative
          rewards as pain (the term “reward” is a bit misleading in this case). In short, the agent
          acts in the environment and learns by trial and error to maximize its pleasure and
          minimize its pain.                                          
          This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few
          examples (see Figure 18-1):                                 
                                                                      
           a. The agent can be the program controlling a robot. In this case, the environment
            is the real world, the agent observes the environment through a set of sensors
            such as cameras and touch sensors, and its actions consist of sending signals to
            activate motors. It may be programmed to get positive rewards whenever it
            approaches the target destination, and negative rewards whenever it wastes time
            or goes in the wrong direction.                           
           b. The agent can be the program controlling Ms. Pac-Man. In this case, the environ‐
            ment is a simulation of the Atari game, the actions are the nine possible joystick
            positions (upper left, down, center, and so on), the observations are screenshots,
            and the rewards are just the game points.                 
           c. Similarly, the agent can be the program playing a board game such as Go.
           d. The agent does not have to control a physically (or virtually) moving thing. For
            example, it can be a smart thermostat, getting positive rewards whenever it is
            close to the target temperature and saves energy, and negative rewards when
            humans need to tweak the temperature, so the agent must learn to anticipate
            human needs.                                              
                                                                      
           e. The agent can observe stock market prices and decide how much to buy or sell
            every second. Rewards are obviously the monetary gains and losses.
                                                                      
                                                                      "|optimizing rewards
"                                                                      
                                                                      
                                                                      
                                                                      
          this way the GPU will be almost 100% utilized (except for the data transfer time from
          the CPU to the GPU3), and training will run much faster.    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 13-3. With prefetching, the CPU and the GPU work in parallel: as the GPU works
          on one batch, the CPU works on the next                     
                                                                      
                   If you plan to purchase a GPU card, its processing power and its
                   memory size are of course very important (in particular, a large
                   amount of RAM is crucial for computer vision). Just as important
                   to get good performance is its memory bandwidth; this is the num‐
                   ber of gigabytes of data it can get into or out of its RAM per
                   second.                                            
                                                                      
          If the dataset is small enough to fit in memory, you can significantly speed up train‐
          ing by using the dataset’s cache() method to cache its content to RAM. You should
          generally do this after loading and preprocessing the data, but before shuffling,
          repeating, batching, and prefetching. This way, each instance will only be read and
                                                                      
                                                                      
          3 But check out the tf.data.experimental.prefetch_to_device() function, which can prefetch data directly
           to the GPU.                                                "|memory bandwidth
"                                                                      
                                                                      
                                                                      
                                                                      
          Grid Search                                                 
                                                                      
          One option would be to fiddle with the hyperparameters manually, until you find a
          great combination of hyperparameter values. This would be very tedious work, and
          you may not have time to explore many combinations.         
          Instead, you should get Scikit-Learn’s GridSearchCV to search for you. All you need
          to do is tell it which hyperparameters you want it to experiment with and what values
          to try out, and it will use cross-validation to evaluate all the possible combinations of
          hyperparameter values. For example, the following code searches for the best combi‐
          nation of hyperparameter values for the RandomForestRegressor:
                                                                      
            from sklearn.model_selection import GridSearchCV          
            param_grid = [                                            
               {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
               {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
             ]                                                        
            forest_reg = RandomForestRegressor()                      
                                                                      
            grid_search = GridSearchCV(forest_reg, param_grid, cv=5,  
                            scoring='neg_mean_squared_error',         
                            return_train_score=True)                  
            grid_search.fit(housing_prepared, housing_labels)         
                                                                      
                   When you have no idea what value a hyperparameter should have,
                   a simple approach is to try out consecutive powers of 10 (or a
                   smaller number if you want a more fine-grained search, as shown
                   in this example with the n_estimators hyperparameter).
                                                                      
          This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of
          n_estimators and max_features hyperparameter values specified in the first dict
          (don’t worry about what these hyperparameters mean for now; they will be explained
          in Chapter 7), then try all 2 × 3 = 6 combinations of hyperparameter values in the
          second dict, but this time with the bootstrap hyperparameter set to False instead of
          True (which is the default value for this hyperparameter).  
          The grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor
          hyperparameter values, and it will train each model 5 times (since we are using five-
          fold cross validation). In other words, all in all, there will be 18 × 5 = 90 rounds of
          training! It may take quite a long time, but when it is done you can get the best com‐
          bination of parameters like this:                           
                                                                      
            >>> grid_search.best_params_                              
            {'max_features': 8, 'n_estimators': 30}                   "|GridSearchCV
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-6. Linear SVM classifier using polynomial features 
                                                                      
          Polynomial Kernel                                           
                                                                      
          Adding polynomial features is simple to implement and can work great with all sorts
          of Machine Learning algorithms (not just SVMs). That said, at a low polynomial
          degree, this method cannot deal with very complex datasets, and with a high polyno‐
          mial degree it creates a huge number of features, making the model too slow.
          Fortunately, when using SVMs you can apply an almost miraculous mathematical
          technique called the kernel trick (explained in a moment). The kernel trick makes it
          possible to get the same result as if you had added many polynomial features, even
          with very high-degree polynomials, without actually having to add them. So there is
          no combinatorial explosion of the number of features because you don’t actually add
          any features. This trick is implemented by the SVC class. Let’s test it on the moons
          dataset:                                                    
            from sklearn.svm import SVC                               
            poly_kernel_svm_clf = Pipeline([                          
                 (""scaler"", StandardScaler()),                        
                 (""svm_clf"", SVC(kernel=""poly"", degree=3, coef0=1, C=5))
               ])                                                     
            poly_kernel_svm_clf.fit(X, y)                             
          This code trains an SVM classifier using a third-degree polynomial kernel. It is repre‐
          sented on the left in Figure 5-7. On the right is another SVM classifier using a 10th-
          degree polynomial kernel. Obviously, if your model is overfitting, you might want to
          reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing
          it. The hyperparameter coef0 controls how much the model is influenced by high-
          degree polynomials versus low-degree polynomials.           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|polynomial features; kernel trick
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 15-4. Seq-to-seq (top left), seq-to-vector (top right), vector-to-seq (bottom left),
          and Encoder–Decoder (bottom right) networks                 
                                                                      
          Sounds promising, but how do you train a recurrent neural network?
                                                                      
          Training RNNs                                               
                                                                      
          To train an RNN, the trick is to unroll it through time (like we just did) and then
          simply use regular backpropagation (see Figure 15-5). This strategy is called backpro‐
          pagation through time (BPTT).                               
          Just like in regular backpropagation, there is a first forward pass through the unrolled
          network (represented by the dashed arrows). Then the output sequence is evaluated
          using a cost function C(Y , Y , …Y ) (where T is the max time step). Note that this
                         (0) (1) (T)                                  
          cost function may ignore some outputs, as shown in Figure 15-5 (for example, in a
          sequence-to-vector RNN, all outputs are ignored except for the very last one). The
          gradients of that cost function are then propagated backward through the unrolled
          network (represented by the solid arrows). Finally the model parameters are updated
          using the gradients computed during BPTT. Note that the gradients flow backward
          through all the outputs used by the cost function, not just through the final output
          (for example, in Figure 15-5 the cost function is computed using the last three out‐
          puts of the network, Y , Y , and Y , so gradients flow through these three outputs,
                       (2) (3) (4)                                    "|recurrent neurons and layers; training; recurrent; recurrent neurons; backpropagation through time (BPTT)
"                                                                      
                                                                      
                                                                      
                                                                      
          when the instances in the training set are independent and identically distributed (see
          Chapter 4), we need to shuffle these windows. Then we can batch the windows and
          separate the inputs (the first 100 characters) from the target (the last character):
                                                                      
            batch_size = 32                                           
            dataset = dataset.shuffle(10000).batch(batch_size)        
            dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))
          Figure 16-1 summarizes the dataset preparation steps discussed so far (showing win‐
          dows of length 11 rather than 101, and a batch size of 3 instead of 32).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-1. Preparing a dataset of shuffled windows        
                                                                      
          As discussed in Chapter 13, categorical input features should generally be encoded,
          usually as one-hot vectors or as embeddings. Here, we will encode each character
          using a one-hot vector because there are fairly few distinct characters (only 39):
            dataset = dataset.map(                                    
               lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))
                                                                      
          Finally, we just need to add prefetching:                   
            dataset = dataset.prefetch(1)                             
          That’s it! Preparing the dataset was the hardest part. Now let’s create the model.
                                                                      
          Building and Training the Char-RNN Model                    
                                                                      
          To predict the next character based on the previous 100 characters, we can use an
          RNN with 2 GRU layers of 128 units each and 20% dropout on both the inputs (drop
          out) and the hidden states (recurrent_dropout). We can tweak these hyperparame‐
          ters later, if needed. The output layer is a time-distributed Dense layer like we saw in
          Chapter 15. This time this layer must have 39 units (max_id) because there are 39 dis‐
          tinct characters in the text, and we want to output a probability for each possible
          character (at each time step). The output probabilities should sum up to 1 at each
          time step, so we apply the softmax activation function to the outputs of the Dense"|testing and validation; building and training
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-7. An unlabeled training set for unsupervised learning
                                                                      
          Here are some of the most important unsupervised learning algorithms (most of
          these are covered in Chapters 8 and 9):                     
                                                                      
           • Clustering                                               
             —K-Means                                                 
                                                                      
             —DBSCAN                                                  
             —Hierarchical Cluster Analysis (HCA)                     
           • Anomaly detection and novelty detection                  
             —One-class SVM                                           
                                                                      
             —Isolation Forest                                        
           • Visualization and dimensionality reduction               
             —Principal Component Analysis (PCA)                      
             —Kernel PCA                                              
                                                                      
             —Locally Linear Embedding (LLE)                          
             —t-Distributed Stochastic Neighbor Embedding (t-SNE)     
           • Association rule learning                                
                                                                      
             —Apriori                                                 
             —Eclat                                                   
                                                                      
          For example, say you have a lot of data about your blog’s visitors. You may want to
          run a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At
          no point do you tell the algorithm which group a visitor belongs to: it finds those
          connections without your help. For example, it might notice that 40% of your visitors
          are males who love comic books and generally read your blog in the evening, while
          20% are young sci-fi lovers who visit during the weekends. If you use a hierarchical
          clustering algorithm, it may also subdivide each group into smaller groups. This may
          help you target your posts for each group.                  "|classification MLPs; common tasks; hierarchical clustering algorithms; algorithms covered; clustering algorithms
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-5. A bidirectional recurrent layer                
          Beam Search                                                 
                                                                      
          Suppose you train an Encoder–Decoder model, and use it to translate the French sen‐
          tence “Comment vas-tu?” to English. You are hoping that it will output the proper
          translation (“How are you?”), but unfortunately it outputs “How will you?” Looking
          at the training set, you notice many sentences such as “Comment vas-tu jouer?”
          which translates to “How will you play?” So it wasn’t absurd for the model to output
          “How will” after seeing “Comment vas.” Unfortunately, in this case it was a mistake,
          and the model could not go back and fix it, so it tried to complete the sentence as best
          it could. By greedily outputting the most likely word at every step, it ended up with a
          suboptimal translation. How can we give the model a chance to go back and fix mis‐
          takes it made earlier? One of the most common solutions is beam search: it keeps
          track of a short list of the k most promising sentences (say, the top three), and at each
          decoder step it tries to extend them by one word, keeping only the k most likely sen‐
          tences. The parameter k is called the beam width.           
          For example, suppose you use the model to translate the sentence “Comment vas-tu?”
          using beam search with a beam width of 3. At the first decoder step, the model will
          output an estimated probability for each possible word. Suppose the top three words
          are “How” (75% estimated probability), “What” (3%), and “You” (1%). That’s our
          short list so far. Next, we create three copies of our model and use them to find the
          next word for each sentence. Each model will output one estimated probability per
          word in the vocabulary. The first model will try to find the next word in the sentence
          “How,” and perhaps it will output a probability of 36% for the word “will,” 32% for the
          word “are,” 16% for the word “do,” and so on. Note that these are actually conditional
          probabilities, given that the sentence starts with “How.” The second model will try to
          complete the sentence “What”; it might output a conditional probability of 50% for
                                                                      "|beam width; beam search; conditional probability
"                                                                      
                                                                      
                                                                      
                                                                      
          of time, but challenging enough to be fun and rewarding. Keras provides a simple
          function to load it:                                        
                                                                      
            >>> (X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()
            >>> X_train[0][:10]                                       
            [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]             
          Where are the movie reviews? Well, as you can see, the dataset is already prepro‐
          cessed for you: X_train consists of a list of reviews, each of which is represented as a
          NumPy array of integers, where each integer represents a word. All punctuation was
          removed, and then words were converted to lowercase, split by spaces, and finally
          indexed by frequency (so low integers correspond to frequent words). The integers 0,
          1, and 2 are special: they represent the padding token, the start-of-sequence (SSS)
          token, and unknown words, respectively. If you want to visualize a review, you can
          decode it like this:                                        
            >>> word_index = keras.datasets.imdb.get_word_index()     
            >>> id_to_word = {id_ + 3: word for word, id_ in word_index.items()}
            >>> for id_, token in enumerate((""<pad>"", ""<sos>"", ""<unk>"")):
            ...  id_to_word[id_] = token                              
            ...                                                       
            >>> "" "".join([id_to_word[id_] for id_ in X_train[0][:10]])
            '<sos> this film was just brilliant casting location scenery story'
          In a real project, you will have to preprocess the text yourself. You can do that using
          the same Tokenizer class we used earlier, but this time setting char_level=False
          (which is the default). When encoding words, it filters out a lot of characters, includ‐
          ing most punctuation, line breaks, and tabs (but you can change this by setting the
          filters argument). Most importantly, it uses spaces to identify word boundaries.
          This is OK for English and many other scripts (written languages) that use spaces
          between words, but not all scripts use spaces this way. Chinese does not use spaces
          between words, Vietnamese uses spaces even within words, and languages such as
          German often attach multiple words together, without spaces. Even in English, spaces
          are not always the best way to tokenize text: think of “San Francisco” or
          “#ILoveDeepLearning.”                                       
          Fortunately, there are better options! The 2018 paper4 by Taku Kudo introduced an
          unsupervised learning technique to tokenize and detokenize text at the subword level
          in a language-independent way, treating spaces like other characters. With this
          approach, even if your model encounters a word it has never seen before, it can still
          reasonably guess what it means. For example, it may never have seen the word
          “smartest” during training, but perhaps it learned the word “smart” and it also
          learned that the suffix “est” means “the most,” so it can infer the meaning of
                                                                      
          4 Taku Kudo, “Subword Regularization: Improving Neural Network Translation Models with Multiple Subword
           Candidates,” arXiv preprint arXiv:1804.10959 (2018).       "|start of sequence (SoS) token
"                                                                      
                                                                      
                                                                      
                                                                      
          This output tells you that 84.2% of the dataset’s variance lies along the first PC, and
          14.6% lies along the second PC. This leaves less than 1.2% for the third PC, so it is
          reasonable to assume that the third PC probably carries little information.
                                                                      
          Choosing the Right Number of Dimensions                     
                                                                      
          Instead of arbitrarily choosing the number of dimensions to reduce down to, it is
          simpler to choose the number of dimensions that add up to a sufficiently large por‐
          tion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for
          data visualization—in that case you will want to reduce the dimensionality down to 2
          or 3.                                                       
          The following code performs PCA without reducing dimensionality, then computes
          the minimum number of dimensions required to preserve 95% of the training set’s
          variance:                                                   
            pca = PCA()                                               
            pca.fit(X_train)                                          
            cumsum = np.cumsum(pca.explained_variance_ratio_)         
            d = np.argmax(cumsum >= 0.95) + 1                         
          You could then set n_components=d and run PCA again. But there is a much better
          option: instead of specifying the number of principal components you want to pre‐
          serve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio
          of variance you wish to preserve:                           
            pca = PCA(n_components=0.95)                              
            X_reduced = pca.fit_transform(X_train)                    
          Yet another option is to plot the explained variance as a function of the number of
          dimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the
          curve, where the explained variance stops growing fast. In this case, you can see that
          reducing the dimensionality down to about 100 dimensions wouldn’t lose too much
          explained variance.                                         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|choosing dimension number
"                                                                      
                                                                      
                                                                      
                                                                      
          Reinforcement Learning                                      
                                                                      
          Reinforcement Learning is a very different beast. The learning system, called an agent
          in this context, can observe the environment, select and perform actions, and get
          rewards in return (or penalties in the form of negative rewards, as shown in
          Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get
          the most reward over time. A policy defines what action the agent should choose
          when it is in a given situation.                            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-12. Reinforcement Learning                         
                                                                      
          For example, many robots implement Reinforcement Learning algorithms to learn
          how to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement
          Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie
          at the game of Go. It learned its winning policy by analyzing millions of games, and
          then playing many games against itself. Note that learning was turned off during the
          games against the champion; AlphaGo was just applying the policy it had learned.
          Batch and Online Learning                                   
                                                                      
          Another criterion used to classify Machine Learning systems is whether or not the
          system can learn incrementally from a stream of incoming data.
                                                                      
                                                                      
                                                                      "|penalties; policies; agents; Reinforcement Learning (RL); rewards
"                                                                      
                                                                      
                                                                      
                                                                      
          LeNet-5                                                     
                                                                      
          The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As
          mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used
          for handwritten digit recognition (MNIST). It is composed of the layers shown in
          Table 14-1.                                                 
                                                                      
          Table 14-1. LeNet-5 architecture                            
          Layer Type Maps Size Kernel size Stride Activation          
          Out Fully connected – 10 – – RBF                            
          F6  Fully connected – 84 – – tanh                           
          C5  Convolution 120 1 × 1 5 × 5 1 tanh                      
          S4  Avg pooling 16 5 × 5 2 × 2 2 tanh                       
          C3  Convolution 16 10 × 10 5 × 5 1 tanh                     
          S2  Avg pooling 6 14 × 14 2 × 2 2 tanh                      
                                                                      
          C1  Convolution 6 28 × 28 5 × 5 1 tanh                      
          In  Input  1  32 × 32 – –  –                                
                                                                      
          There are a few extra details to be noted:                  
           • MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and
            normalized before being fed to the network. The rest of the network does not use
            any padding, which is why the size keeps shrinking as the image progresses
            through the network.                                      
                                                                      
           • The average pooling layers are slightly more complex than usual: each neuron
            computes the mean of its inputs, then multiplies the result by a learnable coeffi‐
            cient (one per map) and adds a learnable bias term (again, one per map), then
            finally applies the activation function.                  
           • Most neurons in C3 maps are connected to neurons in only three or four S2
            maps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for
            details.                                                  
           • The output layer is a bit special: instead of computing the matrix multiplication
            of the inputs and the weight vector, each neuron outputs the square of the Eucli‐
            dian distance between its input vector and its weight vector. Each output meas‐
            ures how much the image belongs to a particular digit class. The cross-entropy
                                                                      
                                                                      
                                                                      
                                                                      
          10 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE 86,
           no. 11 (1998): 2278–2324.                                  "|LeNet-5
"                                                                      
                                                                      
                                                                      
                                                                      
          Softmax Regression                                          
                                                                      
          The Logistic Regression model can be generalized to support multiple classes directly,
          without having to train and combine multiple binary classifiers (as discussed in
          Chapter 3). This is called Softmax Regression, or Multinomial Logistic Regression.
          The idea is simple: when given an instance x, the Softmax Regression model first
          computes a score s(x) for each class k, then estimates the probability of each class by
                     k                                                
          applying the softmax function (also called the normalized exponential) to the scores.
          The equation to compute s(x) should look familiar, as it is just like the equation for
                          k                                           
          Linear Regression prediction (see Equation 4-19).           
            Equation 4-19. Softmax score for class k                  
                 ⊺ k                                                  
            s x =x θ                                                  
            k                                                         
          Note that each class has its own dedicated parameter vector θ(k). All these vectors are
          typically stored as rows in a parameter matrix Θ.           
          Once you have computed the score of every class for the instance x, you can estimate
          the probability p that the instance belongs to class k by running the scores through
                    k                                                 
          the softmax function (Equation 4-20). The function computes the exponential of
          every score, then normalizes them (dividing by the sum of all the exponentials). The
          scores are generally called logits or log-odds (although they are actually unnormal‐
          ized log-odds).                                             
            Equation 4-20. Softmax function                           
                       exp s x                                        
                          k                                           
            p =σ s x =                                                
             k     k  K                                               
                     ∑  exp s x                                       
                      j=1   j                                         
          In this equation:                                           
           • K is the number of classes.                              
           • s(x) is a vector containing the scores of each class for the instance x.
           • σ(s(x)) is the estimated probability that the instance x belongs to class k, given
                k                                                     
            the scores of each class for that instance.               
                                                                      
                                                                      
                                                                      
                                                                      "|parameter matrix; normalized exponential; Softmax Regression; Multinomial Logistic Regression; softmax function
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> np.linalg.pinv(X_b).dot(y)                            
            array([[4.21509616],                                      
                [2.77011339]])                                        
          The pseudoinverse itself is computed using a standard matrix factorization technique
          called Singular Value Decomposition (SVD) that can decompose the training set
          matrix X into the matrix multiplication of three matrices U Σ V⊺ (see
          numpy.linalg.svd()). The pseudoinverse is computed as X+=VΣ+U ⊺ . To compute
          the matrix Σ+, the algorithm takes Σ and sets to zero all values smaller than a tiny
          threshold value, then it replaces all the nonzero values with their inverse, and finally
          it transposes the resulting matrix. This approach is more efficient than computing the
          Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may
          not work if the matrix X⊺X is not invertible (i.e., singular), such as if m < n or if some
          features are redundant, but the pseudoinverse is always defined.
                                                                      
          Computational Complexity                                    
                                                                      
          The Normal Equation computes the inverse of X⊺ X, which is an (n + 1) × (n + 1)
          matrix (where n is the number of features). The computational complexity of inverting
          such a matrix is typically about O(n2.4) to O(n3), depending on the implementation. In
          other words, if you double the number of features, you multiply the computation
          time by roughly 22.4 = 5.3 to 23 = 8.                       
          The SVD approach used by Scikit-Learn’s LinearRegression class is about O(n2). If
          you double the number of features, you multiply the computation time by roughly 4.
                                                                      
                   Both the Normal Equation and the SVD approach get very slow
                   when the number of features grows large (e.g., 100,000). On the
                   positive side, both are linear with regard to the number of instances
                   in the training set (they are O(m)), so they handle large training
                   sets efficiently, provided they can fit in memory. 
                                                                      
          Also, once you have trained your Linear Regression model (using the Normal Equa‐
          tion or any other algorithm), predictions are very fast: the computational complexity
          is linear with regard to both the number of instances you want to make predictions
          on and the number of features. In other words, making predictions on twice as many
          instances (or twice as many features) will take roughly twice as much time.
          Now we will look at a very different way to train a Linear Regression model, which is
          better suited for cases where there are a large number of features or too many training
          instances to fit in memory.                                 
                                                                      
                                                                      
                                                                      
                                                                      "|Singular Value Decomposition (SVD); Linear Regression; computational complexity
"                                                                      
                                                                      
                                                                      
                                                                      
            table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)
            num_oov_buckets = 2                                       
            table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)
          Let’s go through this code:                                 
                                                                      
           • We first define the vocabulary: this is the list of all possible categories.
           • Then we create a tensor with the corresponding indices (0 to 4).
           • Next, we create an initializer for the lookup table, passing it the list of categories
            and their corresponding indices. In this example, we already have this data, so we
            use a KeyValueTensorInitializer; but if the categories were listed in a text file
            (with one category per line), we would use a TextFileInitializer instead.
                                                                      
           • In the last two lines we create the lookup table, giving it the initializer and speci‐
            fying the number of out-of-vocabulary (oov) buckets. If we look up a category
            that does not exist in the vocabulary, the lookup table will compute a hash of this
            category and use it to assign the unknown category to one of the oov buckets.
            Their indices start after the known categories, so in this example the indices of
            the two oov buckets are 5 and 6.                          
          Why use oov buckets? Well, if the number of categories is large (e.g., zip codes, cities,
          words, products, or users) and the dataset is large as well, or it keeps changing, then
          getting the full list of categories may not be convenient. One solution is to define the
          vocabulary based on a data sample (rather than the whole training set) and add some
          oov buckets for the other categories that were not in the data sample. The more
          unknown categories you expect to find during training, the more oov buckets you
          should use. Indeed, if there are not enough oov buckets, there will be collisions: dif‐
          ferent categories will end up in the same bucket, so the neural network will not be
          able to distinguish them (at least not based on this feature).
          Now let’s use the lookup table to encode a small batch of categorical features to one-
          hot vectors:                                                
                                                                      
            >>> categories = tf.constant([""NEAR BAY"", ""DESERT"", ""INLAND"", ""INLAND""])
            >>> cat_indices = table.lookup(categories)                
            >>> cat_indices                                           
            <tf.Tensor: id=514, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>
            >>> cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)
            >>> cat_one_hot                                           
            <tf.Tensor: id=524, shape=(4, 7), dtype=float32, numpy=   
            array([[0., 0., 0., 1., 0., 0., 0.],                      
                [0., 0., 0., 0., 0., 1., 0.],                         
                [0., 1., 0., 0., 0., 0., 0.],                         
                [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>        
          As you can see, ""NEAR BAY"" was mapped to index 3, the unknown category ""DESERT""
          was mapped to one of the two oov buckets (at index 5), and ""INLAND"" was mapped to"|vocabulary; out-of-vocabulary (oov) buckets
"                                                                      
                                                                      
                                                                      
                                                                      
          Denoising Autoencoders                                      
                                                                      
          Another way to force the autoencoder to learn useful features is to add noise to its
          inputs, training it to recover the original, noise-free inputs. This idea has been around
          since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s thesis). In a 2008
          paper,5 Pascal Vincent et al. showed that autoencoders could also be used for feature
          extraction. In a 2010 paper,6 Vincent et al. introduced stacked denoising autoencoders.
                                                                      
          The noise can be pure Gaussian noise added to the inputs, or it can be randomly
          switched-off inputs, just like in dropout (introduced in Chapter 11). Figure 17-8
          shows both options.                                         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-8. Denoising autoencoders, with Gaussian noise (left) or dropout (right)
          The implementation is straightforward: it is a regular stacked autoencoder with an
          additional Dropout layer applied to the encoder’s inputs (or you could use a Gaus
          sianNoise layer instead). Recall that the Dropout layer is only active during training
                                                                      
          (and so is the GaussianNoise layer):                        
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          5 Pascal Vincent et al., “Extracting and Composing Robust Features with Denoising Autoencoders,” Proceedings
           of the 25th International Conference on Machine Learning (2008): 1096–1103.
          6 Pascal Vincent et al., “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network
           with a Local Denoising Criterion,” Journal of Machine Learning Research 11 (2010): 3371–3408."|stacked denoising autoencoders; denoising; denoising autoencoders
"                                                                      
                                                                      
                                                                      
                                                                      
                   If you deploy your application to a virtual machine on Google
                   Cloud Engine (GCE), or within a container using Google Cloud
                   Kubernetes Engine, or as a web application on Google Cloud App
                   Engine, or as a microservice on Google Cloud Functions, and if the
                   GOOGLE_APPLICATION_CREDENTIALS environment variable is not
                   set, then the library will use the default service account for the host
                   service (e.g., the default GCE service account, if your application
                   runs on GCE).                                      
                                                                      
          Next, you must create a resource object that wraps access to the prediction service:7
            import googleapiclient.discovery                          
                                                                      
            project_id = ""onyx-smoke-242003"" # change this to your project ID
            model_id = ""my_mnist_model""                               
            model_path = ""projects/{}/models/{}"".format(project_id, model_id)
            ml_resource = googleapiclient.discovery.build(""ml"", ""v1"").projects()
          Note that you can append /versions/0001 (or any other version number) to the
          model_path to specify the version you want to query: this can be useful for A/B test‐
          ing or for testing a new version on a small group of users before releasing it widely
          (this is called a canary). Next, let’s write a small function that will use the resource
          object to call the prediction service and get the predictions back:
            def predict(X):                                           
               input_data_json = {""signature_name"": ""serving_default"",
                          ""instances"": X.tolist()}                    
               request = ml_resource.predict(name=model_path, body=input_data_json)
               response = request.execute()                           
               if ""error"" in response:                                
                 raise RuntimeError(response[""error""])                
               return np.array([pred[output_name] for pred in response[""predictions""]])
          The function takes a NumPy array containing the input images and prepares a dictio‐
          nary that the client library will convert to the JSON format (as we did earlier). Then it
          prepares a prediction request, and executes it; it raises an exception if the response
          contains an error, or else it extracts the predictions for each instance and bundles
          them in a NumPy array. Let’s see if it works:               
            >>> Y_probas = predict(X_new)                             
            >>> np.round(Y_probas, 2)                                 
            array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ],
                [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], 
                [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]])
                                                                      
          7 If you get an error saying that module google.appengine was not found, set cache_discovery=False in the
           call to the build() method; see https://stackoverflow.com/q/55561354."|canary testing
"                                                                      
                                                                      
                                                                      
                                                                      
            2015 by Sébastien Jean et al..11 In TensorFlow you can use the tf.nn.sam
            pled_softmax_loss() function for this during training and use the normal soft‐
            max function at inference time (sampled softmax cannot be used at inference
            time because it requires knowing the target).             
                                                                      
          The TensorFlow Addons project includes many sequence-to-sequence tools to let you
          easily build production-ready Encoder–Decoders. For example, the following code
          creates a basic Encoder–Decoder model, similar to the one represented in
          Figure 16-3:                                                
            import tensorflow_addons as tfa                           
                                                                      
            encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)
            decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)
            sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)
            embeddings = keras.layers.Embedding(vocab_size, embed_size)
            encoder_embeddings = embeddings(encoder_inputs)           
            decoder_embeddings = embeddings(decoder_inputs)           
            encoder = keras.layers.LSTM(512, return_state=True)       
            encoder_outputs, state_h, state_c = encoder(encoder_embeddings)
            encoder_state = [state_h, state_c]                        
            sampler = tfa.seq2seq.sampler.TrainingSampler()           
                                                                      
            decoder_cell = keras.layers.LSTMCell(512)                 
            output_layer = keras.layers.Dense(vocab_size)             
            decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,
                                         output_layer=output_layer)   
            final_outputs, final_state, final_sequence_lengths = decoder(
               decoder_embeddings, initial_state=encoder_state,       
               sequence_length=sequence_lengths)                      
            Y_proba = tf.nn.softmax(final_outputs.rnn_output)         
            model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],
                        outputs=[Y_proba])                            
          The code is mostly self-explanatory, but there are a few points to note. First, we set
          return_state=True when creating the LSTM layer so that we can get its final hidden
          state and pass it to the decoder. Since we are using an LSTM cell, it actually returns
          two hidden states (short term and long term). The TrainingSampler is one of several
          samplers available in TensorFlow Addons: their role is to tell the decoder at each step
          what it should pretend the previous output was. During inference, this should be the
                                                                      
          11 Sébastien Jean et al., “On Using Very Large Target Vocabulary for Neural Machine Translation,” Proceedings of
           the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Con‐
           ference on Natural Language Processing of the Asian Federation of Natural Language Processing 1 (2015): 1–10."|TensorFlow Addons
"                                                                      
                                                                      
                                                                      
                                                                      
          that correspond to all the all the necessary statistics computed by Apache Beam (the
          mean, standard deviation, and vocabulary).                  
                                                                      
          With the Data API, TFRecords, the Keras preprocessing layers, and TF Transform,
          you can build highly scalable input pipelines for training and benefit from fast and
          portable data preprocessing in production.                  
          But what if you just wanted to use a standard dataset? Well in that case, things are
          much simpler: just use TFDS!                                
                                                                      
          The TensorFlow Datasets (TFDS) Project                      
                                                                      
          The TensorFlow Datasets project makes it very easy to download common datasets,
          from small ones like MNIST or Fashion MNIST to huge datasets like ImageNet (you
          will need quite a bit of disk space!). The list includes image datasets, text datasets
          (including translation datasets), and audio and video datasets. You can visit https://
          homl.info/tfds to view the full list, along with a description of each dataset.
          TFDS is not bundled with TensorFlow, so you need to install the tensorflow-
          datasets library (e.g., using pip). Then call the tfds.load() function, and it will
          download the data you want (unless it was already downloaded earlier) and return
          the data as a dictionary of datasets (typically one for training and one for testing, but
          this depends on the dataset you choose). For example, let’s download MNIST:
                                                                      
            import tensorflow_datasets as tfds                        
            dataset = tfds.load(name=""mnist"")                         
            mnist_train, mnist_test = dataset[""train""], dataset[""test""]
          You can then apply any transformation you want (typically shuffling, batching, and
          prefetching), and you’re ready to train your model. Here is a simple example:
                                                                      
            mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)
            for item in mnist_train:                                  
               images = item[""image""]                                 
               labels = item[""label""]                                 
               [...]                                                  
                   The load() function shuffles each data shard it downloads (only
                   for the training set). This may not be sufficient, so it’s best to shuf‐
                   fle the training data some more.                   
                                                                      
                                                                      
          Note that each item in the dataset is a dictionary containing both the features and the
          labels. But Keras expects each item to be a tuple containing two elements (again, the
          features and the labels). You could transform the dataset using the map() method, like
          this:                                                       "|TF Datasets (TFDS); TensorFlow Datasets (TFDS) Project
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-10. Analyzing the silhouette diagrams for various values of k
                                                                      
          The vertical dashed lines represent the silhouette score for each number of clusters.
          When most of the instances in a cluster have a lower coefficient than this score (i.e., if
          many of the instances stop short of the dashed line, ending to the left of it), then the
          cluster is rather bad since this means its instances are much too close to other clus‐
          ters. We can see that when k = 3 and when k = 6, we get bad clusters. But when k = 4
          or k = 5, the clusters look pretty good: most instances extend beyond the dashed line,
          to the right and closer to 1.0. When k = 4, the cluster at index 1 (the third from the
          top) is rather big. When k = 5, all clusters have similar sizes. So, even though the
          overall silhouette score from k = 4 is slightly greater than for k = 5, it seems like a
          good idea to use k = 5 to get clusters of similar sizes.    
          Limits of K-Means                                           
                                                                      
          Despite its many merits, most notably being fast and scalable, K-Means is not perfect.
          As we saw, it is necessary to run the algorithm several times to avoid suboptimal solu‐
          tions, plus you need to specify the number of clusters, which can be quite a hassle.
          Moreover, K-Means does not behave very well when the clusters have varying sizes,
                                                                      "|K-Means
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 15-9. LSTM cell                                      
                                                                      
          Now let’s open the box! The key idea is that the network can learn what to store in the
          long-term state, what to throw away, and what to read from it. As the long-term state
          c  traverses the network from left to right, you can see that it first goes through a
           (t–1)                                                      
          forget gate, dropping some memories, and then it adds some new memories via the
          addition operation (which adds the memories that were selected by an input gate).
          The result c is sent straight out, without any further transformation. So, at each time
                 (t)                                                  
          step, some memories are dropped and some memories are added. Moreover, after the
          addition operation, the long-term state is copied and passed through the tanh func‐
          tion, and then the result is filtered by the output gate. This produces the short-term
          state h (which is equal to the cell’s output for this time step, y ). Now let’s look at
              (t)                               (t)                   
          where new memories come from and how the gates work.        
          First, the current input vector x and the previous short-term state h are fed to
                             (t)                    (t–1)             
          four different fully connected layers. They all serve a different purpose:
           • The main layer is the one that outputs g . It has the usual role of analyzing the
                                    (t)                               
            current inputs x and the previous (short-term) state h . In a basic cell, there is
                      (t)                   (t–1)                     
            nothing other than this layer, and its output goes straight out to y and h . In
                                                    (t)  (t)          
            contrast, in an LSTM cell this layer’s output does not go straight out, but instead
            its most important parts are stored in the long-term state (and the rest is
            dropped).                                                 
           • The three other layers are gate controllers. Since they use the logistic activation
            function, their outputs range from 0 to 1. As you can see, their outputs are fed to"|gate controllers; output gate; input gate; forget gate
"                                                                      
                                                                      
                                                                      
                                                                      
          When the cost function is very irregular (as in Figure 4-6), this can actually help the
          algorithm jump out of local minima, so Stochastic Gradient Descent has a better
          chance of finding the global minimum than Batch Gradient Descent does.
                                                                      
          Therefore, randomness is good to escape from local optima, but bad because it means
          that the algorithm can never settle at the minimum. One solution to this dilemma is
          to gradually reduce the learning rate. The steps start out large (which helps make
          quick progress and escape local minima), then get smaller and smaller, allowing the
          algorithm to settle at the global minimum. This process is akin to simulated anneal‐
          ing, an algorithm inspired from the process in metallurgy of annealing, where molten
          metal is slowly cooled down. The function that determines the learning rate at each
          iteration is called the learning schedule. If the learning rate is reduced too quickly, you
          may get stuck in a local minimum, or even end up frozen halfway to the minimum. If
          the learning rate is reduced too slowly, you may jump around the minimum for a
          long time and end up with a suboptimal solution if you halt training too early.
          This code implements Stochastic Gradient Descent using a simple learning schedule:
            n_epochs = 50                                             
            t0, t1 = 5, 50 # learning schedule hyperparameters        
            def learning_schedule(t):                                 
               return t0 / (t + t1)                                   
                                                                      
            theta = np.random.randn(2,1) # random initialization      
            for epoch in range(n_epochs):                             
               for i in range(m):                                     
                 random_index = np.random.randint(m)                  
                 xi = X_b[random_index:random_index+1]                
                 yi = y[random_index:random_index+1]                  
                 gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         
                 eta = learning_schedule(epoch * m + i)               
                 theta = theta - eta * gradients                      
          By convention we iterate by rounds of m iterations; each round is called an epoch.
          While the Batch Gradient Descent code iterated 1,000 times through the whole train‐
          ing set, this code goes through the training set only 50 times and reaches a pretty
          good solution:                                              
            >>> theta                                                 
            array([[4.21076011],                                      
                [2.74856079]])                                        
          Figure 4-10 shows the first 20 steps of training (notice how irregular the steps are).
                                                                      
                                                                      
                                                                      
                                                                      "|learning schedules; epochs; simulated annealing
"                                                                      
                                                                      
                                                                      
                                                                      
          Chaining Transformations                                    
                                                                      
          Once you have a dataset, you can apply all sorts of transformations to it by calling its
          transformation methods. Each method returns a new dataset, so you can chain trans‐
          formations like this (this chain is illustrated in Figure 13-1):
            >>> dataset = dataset.repeat(3).batch(7)                  
            >>> for item in dataset:                                  
            ...  print(item)                                          
            ...                                                       
            tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)       
            tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)       
            tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)       
            tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)       
            tf.Tensor([8 9], shape=(2,), dtype=int32)                 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 13-1. Chaining dataset transformations               
                                                                      
          In this example, we first call the repeat() method on the original dataset, and it
          returns a new dataset that will repeat the items of the original dataset three times. Of
          course, this will not copy all the data in memory three times! (If you call this method
          with no arguments, the new dataset will repeat the source dataset forever, so the code
          that iterates over the dataset will have to decide when to stop.) Then we call the
          batch() method on this new dataset, and again this creates a new dataset. This one
          will group the items of the previous dataset in batches of seven items. Finally, we iter‐
          ate over the items of this final dataset. As you can see, the batch() method had to
          output a final batch of size two instead of seven, but you can call it with drop_remain
          der=True if you want it to drop this final batch so that all batches have the exact same
          size.                                                       
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|chaining transformations; chaining
"                                                                      
                                                                      
                                                                      
                                                                      
            the number of labels available for a subsequent supervised learning algorithm,
            and thus improve its performance.                         
                                                                      
          For search engines                                          
            Some search engines let you search for images that are similar to a reference
            image. To build such a system, you would first apply a clustering algorithm to all
            the images in your database; similar images would end up in the same cluster.
            Then when a user provides a reference image, all you need to do is use the
            trained clustering model to find this image’s cluster, and you can then simply
            return all the images from this cluster.                  
          To segment an image                                         
            By clustering pixels according to their color, then replacing each pixel’s color
            with the mean color of its cluster, it is possible to considerably reduce the num‐
            ber of different colors in the image. Image segmentation is used in many object
            detection and tracking systems, as it makes it easier to detect the contour of each
            object.                                                   
          There is no universal definition of what a cluster is: it really depends on the context,
          and different algorithms will capture different kinds of clusters. Some algorithms
          look for instances centered around a particular point, called a centroid. Others look
          for continuous regions of densely packed instances: these clusters can take on any
          shape. Some algorithms are hierarchical, looking for clusters of clusters. And the list
          goes on.                                                    
                                                                      
          In this section, we will look at two popular clustering algorithms, K-Means and
          DBSCAN, and explore some of their applications, such as nonlinear dimensionality
          reduction, semi-supervised learning, and anomaly detection. 
          K-Means                                                     
                                                                      
          Consider the unlabeled dataset represented in Figure 9-2: you can clearly see five
          blobs of instances. The K-Means algorithm is a simple algorithm capable of clustering
          this kind of dataset very quickly and efficiently, often in just a few iterations. It was
          proposed by Stuart Lloyd at Bell Labs in 1957 as a technique for pulse-code modula‐
          tion, but it was only published outside of the company in 1982.1 In 1965, Edward W.
          Forgy had published virtually the same algorithm, so K-Means is sometimes referred
          to as Lloyd–Forgy.                                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          1 Stuart P. Lloyd, “Least Squares Quantization in PCM,” IEEE Transactions on Information Theory 28, no. 2
           (1982): 129–137.                                           "|Lloyd–Forgy algorithm; search engines; K-Means; K-Means algorithm; for image segmentation; centroids; Lloyd-Forgy algorithm; image segmentation
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-7. Selecting the subspace to project on            
                                                                      
          It seems reasonable to select the axis that preserves the maximum amount of var‐
          iance, as it will most likely lose less information than the other projections. Another
          way to justify this choice is that it is the axis that minimizes the mean squared dis‐
          tance between the original dataset and its projection onto that axis. This is the rather
          simple idea behind PCA.4                                    
          Principal Components                                        
                                                                      
          PCA identifies the axis that accounts for the largest amount of variance in the train‐
          ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the
          first one, that accounts for the largest amount of remaining variance. In this 2D
          example there is no choice: it is the dotted line. If it were a higher-dimensional data‐
          set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,
          a fifth, and so on—as many axes as the number of dimensions in the dataset.
                                                                      
          The ith axis is called the ith principal component (PC) of the data. In Figure 8-7, the
          first PC is the axis on which vector c lies, and the second PC is the axis on which
                                1                                     
          vector c lies. In Figure 8-2 the first two PCs are the orthogonal axes on which the
               2                                                      
          two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane.
                                                                      
                                                                      
                                                                      
                                                                      
          4 Karl Pearson, “On Lines and Planes of Closest Fit to Systems of Points in Space,” The London, Edinburgh, and
           Dublin Philosophical Magazine and Journal of Science 2, no. 11 (1901): 559-572, https://homl.info/pca."|principal component axis
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-1. TF Serving can serve multiple models and automatically deploy the latest
          version of each model                                       
                                                                      
          So let’s suppose you have trained an MNIST model using tf.keras, and you want to
          deploy it to TF Serving. The first thing you have to do is export this model to Tensor‐
          Flow’s SavedModel format.                                   
                                                                      
          Exporting SavedModels                                       
          TensorFlow provides a simple tf.saved_model.save() function to export models to
          the SavedModel format. All you need to do is give it the model, specifying its name
          and version number, and the function will save the model’s computation graph and its
          weights:                                                    
            model = keras.models.Sequential([...])                    
            model.compile([...])                                      
            history = model.fit([...])                                
            model_version = ""0001""                                    
            model_name = ""my_mnist_model""                             
            model_path = os.path.join(model_name, model_version)      
            tf.saved_model.save(model, model_path)                    
          Alternatively, you can just use the model’s save() method (model.save(model_
          path)): as long as the file’s extension is not .h5, the model will be saved using the
          SavedModel format instead of the HDF5 format.               
                                                                      
          It’s usually a good idea to include all the preprocessing layers in the final model you
          export so that it can ingest data in its natural form once it is deployed to production.
          This avoids having to take care of preprocessing separately within the application that
          uses the model. Bundling the preprocessing steps within the model also makes it sim‐
          pler to update them later on and limits the risk of mismatch between a model and the
          preprocessing steps it requires.                            
                                                                      
                                                                      "|SavedModel format
"                                                                      
                                                                      
                                                                      
                                                                      
          Let’s compare the algorithms we’ve discussed so far for Linear Regression6 (recall that
          m is the number of training instances and n is the number of features); see Table 4-1.
                                                                      
          Table 4-1. Comparison of algorithms for Linear Regression   
                                                                      
          Algorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn
          Normal Equation Fast No Slow 0  No     N/A                  
          SVD     Fast No      Slow 0     No     LinearRegression     
          Batch GD Slow No     Fast 2     Yes    SGDRegressor         
          Stochastic GD Fast Yes Fast ≥2  Yes    SGDRegressor         
          Mini-batch GD Fast Yes Fast ≥2  Yes    SGDRegressor         
                                                                      
                   There is almost no difference after training: all these algorithms
                   end up with very similar models and make predictions in exactly
                   the same way.                                      
                                                                      
                                                                      
          Polynomial Regression                                       
                                                                      
          What if your data is more complex than a straight line? Surprisingly, you can use a
          linear model to fit nonlinear data. A simple way to do this is to add powers of each
          feature as new features, then train a linear model on this extended set of features. This
          technique is called Polynomial Regression.                  
          Let’s look at an example. First, let’s generate some nonlinear data, based on a simple
          quadratic equation7 (plus some noise; see Figure 4-12):     
                                                                      
            m = 100                                                   
            X = 6 * np.random.rand(m, 1) - 3                          
            y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          6 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be
           used to train many other models, as we will see.           
          7 A quadratic equation is of the form y = ax2 + bx + c.     "|Gradient Descent; Polynomial Regression
"                                                                      
                                                                      
                                                                      
                                                                      
                   If someone says, “Let’s reach 99% precision,” you should ask, “At
                   what recall?”                                      
                                                                      
                                                                      
                                                                      
          The ROC Curve                                               
                                                                      
          The receiver operating characteristic (ROC) curve is another common tool used with
          binary classifiers. It is very similar to the precision/recall curve, but instead of plot‐
          ting precision versus recall, the ROC curve plots the true positive rate (another name
          for recall) against the false positive rate (FPR). The FPR is the ratio of negative instan‐
          ces that are incorrectly classified as positive. It is equal to 1 – the true negative rate
          (TNR), which is the ratio of negative instances that are correctly classified as negative.
          The TNR is also called specificity. Hence, the ROC curve plots sensitivity (recall) ver‐
          sus 1 – specificity.                                        
          To plot the ROC curve, you first use the roc_curve() function to compute the TPR
          and FPR for various threshold values:                       
                                                                      
            from sklearn.metrics import roc_curve                     
            fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)     
          Then you can plot the FPR against the TPR using Matplotlib. This code produces the
          plot in Figure 3-6:                                         
                                                                      
            def plot_roc_curve(fpr, tpr, label=None):                 
               plt.plot(fpr, tpr, linewidth=2, label=label)           
               plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal      
               [...] # Add axis labels and grid                       
            plot_roc_curve(fpr, tpr)                                  
            plt.show()                                                
          Once again there is a trade-off: the higher the recall (TPR), the more false positives
          (FPR) the classifier produces. The dotted line represents the ROC curve of a purely
          random classifier; a good classifier stays as far away from that line as possible (toward
          the top-left corner).                                       
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|receiver operating characteristic (ROC) curve; false positive rate (FPR); ROC curve; true negative rate (TNR); recall; precision
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Equation 5-11. Making predictions with a kernelized SVM   
                                                                      
                               m        ⊺                             
            h  ϕ x n = w ⊺ ϕ x n +b = ∑ α i t i ϕ x i ϕ x n +b        
             w,b               i=1                                    
                      m                                               
                    = ∑ α i t i ϕ x i ⊺ ϕ x n +b                      
                     i=1                                              
                      m                                               
                    = ∑  α i t i K x i ,x n +b                        
                      i=1                                             
                      i                                               
                     α >0                                             
          Note that since α(i) ≠ 0 only for support vectors, making predictions involves comput‐
          ing the dot product of the new input vector x(n) with only the support vectors, not all
          the training instances. Of course, you need to use the same trick to compute the bias
          term b (Equation 5-12).                                     
            Equation 5-12. Using the kernel trick to compute the bias term
                 m               m      m         ⊺                   
            b = 1 ∑ t i −w ⊺ ϕ x i = 1 ∑ t i − ∑ α j t j ϕ x j ϕ x i  
               n               n                                      
               s i=1           s i=1    j=1                           
                 i               i                                    
                α >0            α >0                                  
                 m       m                                            
             = 1 ∑  t i − ∑ α j t j K x i ,x j                        
               n                                                      
               s i=1    j=1                                           
                 i       j                                            
                α >0    α >0                                          
          If you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side
          effect of the kernel trick.                                 
          Online SVMs                                                 
          Before concluding this chapter, let’s take a quick look at online SVM classifiers (recall
          that online learning means learning incrementally, typically as new instances arrive).
          For linear SVM classifiers, one method for implementing an online SVM classifier is
          to use Gradient Descent (e.g., using SGDClassifier) to minimize the cost function in
          Equation 5-13, which is derived from the primal problem. Unfortunately, Gradient
          Descent converges much more slowly than the methods based on QP.
                                                                      
                                                                      "|online SVMs
"                                                                      
                                                                      
                                                                      
                                                                      
          same. Equation 4-9 shows the closed-form solution, where A is the (n + 1) × (n + 1)
          identity matrix,11 except with a 0 in the top-left cell, corresponding to the bias term.
                                                                      
            Equation 4-9. Ridge Regression closed-form solution       
                                                                      
                ⊺    −1 ⊺                                             
            θ = X X+αA X  y                                           
          Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐
          tion (a variant of Equation 4-9 that uses a matrix factorization technique by André-
          Louis Cholesky):                                            
            >>> from sklearn.linear_model import Ridge                
            >>> ridge_reg = Ridge(alpha=1, solver=""cholesky"")         
            >>> ridge_reg.fit(X, y)                                   
            >>> ridge_reg.predict([[1.5]])                            
            array([[1.55071465]])                                     
          And using Stochastic Gradient Descent:12                    
            >>> sgd_reg = SGDRegressor(penalty=""l2"")                  
            >>> sgd_reg.fit(X, y.ravel())                             
            >>> sgd_reg.predict([[1.5]])                              
            array([1.47012588])                                       
          The penalty hyperparameter sets the type of regularization term to use. Specifying
          ""l2"" indicates that you want SGD to add a regularization term to the cost function
          equal to half the square of the ℓ norm of the weight vector: this is simply Ridge
                              2                                       
          Regression.                                                 
          Lasso Regression                                            
          Least Absolute Shrinkage and Selection Operator Regression (usually simply called
          Lasso Regression) is another regularized version of Linear Regression: just like Ridge
          Regression, it adds a regularization term to the cost function, but it uses the ℓ norm
                                                        1             
          of the weight vector instead of half the square of the ℓ norm (see Equation 4-10).
                                         2                            
            Equation 4-10. Lasso Regression cost function             
                       n                                              
            J θ =MSE θ +α∑ θ                                          
                       i=1 i                                          
                                                                      
                                                                      
          11 A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right).
          12 Alternatively you can use the Ridge class with the ""sag"" solver. Stochastic Average GD is a variant of Stochas‐
           tic GD. For more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient
           Algorithm” by Mark Schmidt et al. from the University of British Columbia."|Lasso Regression; identity matrix
"                                                                      
                                                                      
                                                                      
                                                                      
          set on a graph and often gain some important insights by visually detecting patterns,
          such as clusters. Moreover, DataViz is essential to communicate your conclusions to
          people who are not data scientists—in particular, decision makers who will use your
          results.                                                    
                                                                      
          In this chapter we will discuss the curse of dimensionality and get a sense of what
          goes on in high-dimensional space. Then, we will consider the two main approaches
          to dimensionality reduction (projection and Manifold Learning), and we will go
          through three of the most popular dimensionality reduction techniques: PCA, Kernel
          PCA, and LLE.                                               
          The Curse of Dimensionality                                 
                                                                      
          We are so used to living in three dimensions1 that our intuition fails us when we try
          to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to
          picture in our minds (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a
          1,000-dimensional space.                                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2
                                                                      
          It turns out that many things behave very differently in high-dimensional space. For
          example, if you pick a random point in a unit square (a 1 × 1 square), it will have only
          about a 0.4% chance of being located less than 0.001 from a border (in other words, it
          is very unlikely that a random point will be “extreme” along any dimension). But in a
          10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most
          points in a high-dimensional hypercube are very close to the border.3
                                                                      
                                                                      
          1 Well, four dimensions if you count time, and a few more if you are a string theorist.
          2 Watch a rotating tesseract projected into 3D space at https://homl.info/30. Image by Wikipedia user Nerd‐
           Boy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract.
          3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put
           in their coffee), if you consider enough dimensions.       "|curse of dimensionality
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Efficient Data Representations                              
                                                                      
          Which of the following number sequences do you find the easiest to memorize?
                                                                      
           • 40, 27, 25, 36, 81, 57, 10, 73, 19, 68                   
           • 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14
                                                                      
          At first glance, it would seem that the first sequence should be easier, since it is much
          shorter. However, if you look carefully at the second sequence, you will notice that it
          is just the list of even numbers from 50 down to 14. Once you notice this pattern, the
          second sequence becomes much easier to memorize than the first because you only
          need to remember the pattern (i.e., decreasing even numbers) and the starting and
          ending numbers (i.e., 50 and 14). Note that if you could quickly and easily memorize
          very long sequences, you would not care much about the existence of a pattern in the
          second sequence. You would just learn every number by heart, and that would be
          that. The fact that it is hard to memorize long sequences is what makes it useful to
          recognize patterns, and hopefully this clarifies why constraining an autoencoder dur‐
          ing training pushes it to discover and exploit patterns in the data.
          The relationship between memory, perception, and pattern matching was famously
          studied by William Chase and Herbert Simon in the early 1970s.1 They observed that
          expert chess players were able to memorize the positions of all the pieces in a game by
          looking at the board for just five seconds, a task that most people would find impossi‐
          ble. However, this was only the case when the pieces were placed in realistic positions
          (from actual games), not when the pieces were placed randomly. Chess experts don’t
          have a much better memory than you and I; they just see chess patterns more easily,
          thanks to their experience with the game. Noticing patterns helps them store infor‐
          mation efficiently.                                         
          Just like the chess players in this memory experiment, an autoencoder looks at the
          inputs, converts them to an efficient latent representation, and then spits out some‐
          thing that (hopefully) looks very close to the inputs. An autoencoder is always com‐
          posed of two parts: an encoder (or recognition network) that converts the inputs to a
          latent representation, followed by a decoder (or generative network) that converts the
          internal representation to the outputs (see Figure 17-1).   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          1 William G. Chase and Herbert A. Simon, “Perception in Chess,” Cognitive Psychology 4, no. 1 (1973): 55–81."|pattern matching; encoders; autoencoders; generative network; decoders; efficient data representations; recognition network
"                                                                      
                                                                      
                                                                      
                                                                      
          The Lasso cost function is not differentiable at θ = 0 (for i = 1, 2, ⋯, n), but Gradient
                                      i                               
          Descent still works fine if you use a subgradient vector g13 instead when any θ = 0.
                                                        i             
          Equation 4-11 shows a subgradient vector equation you can use for Gradient Descent
          with the Lasso cost function.                               
            Equation 4-11. Lasso Regression subgradient vector        
                          sign θ                                      
                              1            −1 if θ <0                 
                                               i                      
                          sign θ                                      
            g θ,J = ∇ MSE θ +α 2 where sign θ = 0 if θ =0             
                  θ                      i     i                      
                            ⋮                                         
                                           +1 if θ >0                 
                                               i                      
                          sign θ                                      
                              n                                       
          Here is a small Scikit-Learn example using the Lasso class: 
            >>> from sklearn.linear_model import Lasso                
            >>> lasso_reg = Lasso(alpha=0.1)                          
            >>> lasso_reg.fit(X, y)                                   
            >>> lasso_reg.predict([[1.5]])                            
            array([1.53788174])                                       
          Note that you could instead use SGDRegressor(penalty=""l1""). 
          Elastic Net                                                 
          Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The
          regularization term is a simple mix of both Ridge and Lasso’s regularization terms,
          and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge
          Regression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12).
            Equation 4-12. Elastic Net cost function                  
                        n    1−r n  2                                 
            J θ =MSE θ +rα∑ θ + α∑ θ                                  
                        i=1 i 2  i=1 i                                
          So when should you use plain Linear Regression (i.e., without any regularization),
          Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of
          regularization, so generally you should avoid plain Linear Regression. Ridge is a good
          default, but if you suspect that only a few features are useful, you should prefer Lasso
          or Elastic Net because they tend to reduce the useless features’ weights down to zero,
          as we have discussed. In general, Elastic Net is preferred over Lasso because Lasso
          13 You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra‐
           dient vectors around that point.                           "|subgradient vector; subgradient vectors; Elastic Net
"                                                                      
                                                                      
                                                                      
                                                                      
           • Next, the global average pooling layer outputs the mean of each feature map: this
            drops any remaining spatial information, which is fine because there was not
            much spatial information left at that point. Indeed, GoogLeNet input images are
            typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each
            dividing the height and width by 2, the feature maps are down to 7 × 7. More‐
            over, it is a classification task, not localization, so it does not matter where the
            object is. Thanks to the dimensionality reduction brought by this layer, there is
            no need to have several fully connected layers at the top of the CNN (like in
            AlexNet), and this considerably reduces the number of parameters in the net‐
            work and limits the risk of overfitting.                  
           • The last layers are self-explanatory: dropout for regularization, then a fully con‐
            nected layer with 1,000 units (since there are 1,000 classes) and a softmax activa‐
            tion function to output estimated class probabilities.    
                                                                      
          This diagram is slightly simplified: the original GoogLeNet architecture also included
          two auxiliary classifiers plugged on top of the third and sixth inception modules.
          They were both composed of one average pooling layer, one convolutional layer, two
          fully connected layers, and a softmax activation layer. During training, their loss
          (scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐
          ing gradients problem and regularize the network. However, it was later shown that
          their effect was relatively minor.                          
          Several variants of the GoogLeNet architecture were later proposed by Google
          researchers, including Inception-v3 and Inception-v4, using slightly different incep‐
          tion modules and reaching even better performance.          
                                                                      
          VGGNet                                                      
                                                                      
          The runner-up in the ILSVRC 2014 challenge was VGGNet,15 developed by Karen
          Simonyan and Andrew Zisserman from the Visual Geometry Group (VGG) research
          lab at Oxford University. It had a very simple and classical architecture, with 2 or 3
          convolutional layers and a pooling layer, then again 2 or 3 convolutional layers and a
          pooling layer, and so on (reaching a total of just 16 or 19 convolutional layers,
          depending on the VGG variant), plus a final dense network with 2 hidden layers and
          the output layer. It used only 3 × 3 filters, but many filters.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          15 Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for Large-Scale Image Recog‐
           nition,” arXiv preprint arXiv:1409.1556 (2014).            "|softmax; VGGNet; softmax function
"                                                                      
                                                                      
                                                                      
                                                                      
          To compute the F score, simply call the f1_score() function:
                    1                                                 
            >>> from sklearn.metrics import f1_score                  
            >>> f1_score(y_train_5, y_train_pred)                     
            0.7420962043663375                                        
          The F score favors classifiers that have similar precision and recall. This is not always
             1                                                        
          what you want: in some contexts you mostly care about precision, and in other con‐
          texts you really care about recall. For example, if you trained a classifier to detect vid‐
          eos that are safe for kids, you would probably prefer a classifier that rejects many
          good videos (low recall) but keeps only safe ones (high precision), rather than a clas‐
          sifier that has a much higher recall but lets a few really bad videos show up in your
          product (in such cases, you may even want to add a human pipeline to check the clas‐
          sifier’s video selection). On the other hand, suppose you train a classifier to detect
          shoplifters in surveillance images: it is probably fine if your classifier has only 30%
          precision as long as it has 99% recall (sure, the security guards will get a few false
          alerts, but almost all shoplifters will get caught).        
          Unfortunately, you can’t have it both ways: increasing precision reduces recall, and
          vice versa. This is called the precision/recall trade-off.  
          Precision/Recall Trade-off                                  
          To understand this trade-off, let’s look at how the SGDClassifier makes its classifica‐
          tion decisions. For each instance, it computes a score based on a decision function. If
          that score is greater than a threshold, it assigns the instance to the positive class;
          otherwise it assigns it to the negative class. Figure 3-3 shows a few digits positioned
          from the lowest score on the left to the highest score on the right. Suppose the deci‐
          sion threshold is positioned at the central arrow (between the two 5s): you will find 4
          true positives (actual 5s) on the right of that threshold, and 1 false positive (actually a
          6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6
          actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you raise the
          threshold (move it to the arrow on the right), the false positive (the 6) becomes a true
          negative, thereby increasing the precision (up to 100% in this case), but one true posi‐
          tive becomes a false negative, decreasing recall down to 50%. Conversely, lowering
          the threshold increases recall and reduces precision.       
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|decision function
"                                                                      
                                                                      
                                                                      
                                                                      
          Irrelevant Features                                         
                                                                      
          As the saying goes: garbage in, garbage out. Your system will only be capable of learn‐
          ing if the training data contains enough relevant features and not too many irrelevant
          ones. A critical part of the success of a Machine Learning project is coming up with a
          good set of features to train on. This process, called feature engineering, involves the
          following steps:                                            
                                                                      
           • Feature selection (selecting the most useful features to train on among existing
            features)                                                 
           • Feature extraction (combining existing features to produce a more useful one—as
            we saw earlier, dimensionality reduction algorithms can help)
           • Creating new features by gathering new data              
                                                                      
          Now that we have looked at many examples of bad data, let’s look at a couple of exam‐
          ples of bad algorithms.                                     
          Overfitting the Training Data                               
                                                                      
          Say you are visiting a foreign country and the taxi driver rips you off. You might be
          tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is
          something that we humans do all too often, and unfortunately machines can fall into
          the same trap if we are not careful. In Machine Learning this is called overfitting: it
          means that the model performs well on the training data, but it does not generalize
          well.                                                       
                                                                      
          Figure 1-22 shows an example of a high-degree polynomial life satisfaction model
          that strongly overfits the training data. Even though it performs much better on the
          training data than the simple linear model, would you really trust its predictions?
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-22. Overfitting the training data                  
                                                                      
                                                                      
                                                                      "|feature extraction; overfitting; feature selection; irrelevant features; feature engineering
"                                                                      
                                                                      
                                                                      
                                                                      
          default with the Python binary installers (since Python 2.7.9).6 You can check to see if
          pip is installed by typing the following command:           
                                                                      
            $ python3 -m pip --version                                
            pip 19.3.1 from [...]/lib/python3.7/site-packages/pip (python 3.7)
          You should make sure you have a recent version of pip installed. To upgrade the pip
          module, type the following (the exact version may differ):7 
            $ python3 -m pip install --user -U pip                    
            Collecting pip                                            
            [...]                                                     
            Successfully installed pip-19.3.1                         
                                                                      
                       Creating an Isolated Environment               
           If you would like to work in an isolated environment (which is strongly recom‐
           mended so that you can work on different projects without having conflicting library
           versions), install virtualenv8 by running the following pip command (again, if you
           want virtualenv to be installed for all users on your machine, remove --user and run
           this command with administrator rights):                   
             $ python3 -m pip install --user -U virtualenv            
             Collecting virtualenv                                    
             [...]                                                    
             Successfully installed virtualenv-16.7.6                 
           Now you can create an isolated Python environment by typing this:
             $ cd $ML_PATH                                            
             $ python3 -m virtualenv my_env                           
             Using base prefix '[...]'                                
             New python executable in [...]/ml/my_env/bin/python3     
             Also creating executable in [...]/ml/my_env/bin/python   
             Installing setuptools, pip, wheel...done.                
           Now every time you want to activate this environment, just open a terminal and type
           the following:                                             
                                                                      
                                                                      
                                                                      
          6 I’ll show the installation steps using pip in a bash shell on a Linux or macOS system. You may need to adapt
           these commands to your own system. On Windows, I recommend installing Anaconda instead.
          7 If you want to upgrade pip for all users on your machine rather than just your own user, you should remove
           the --user option and make sure you have administrator rights (e.g., by adding sudo before the whole com‐
           mand on Linux or macOS).                                   
          8 Alternative tools include venv (very similar to virtualenv and included in the standard library), virtualenv‐
           wrapper (provides extra functionalities on top of virtualenv), pyenv (allows easy switching between Python
           versions), and pipenv (a great packaging tool by the same author as the popular requests library, built on top
           of pip and virtualenv).                                    "|isolated environments
"                                                                      
                                                                      
                                                                      
                                                                      
          Here is a more troublesome difference: if you pick two points randomly in a unit
          square, the distance between these two points will be, on average, roughly 0.52. If you
          pick two random points in a unit 3D cube, the average distance will be roughly 0.66.
          But what about two points picked randomly in a 1,000,000-dimensional hypercube?
          The average distance, believe it or not, will be about 408.25 (roughly 1,000,000/6)!
          This is counterintuitive: how can two points be so far apart when they both lie within
          the same unit hypercube? Well, there’s just plenty of space in high dimensions. As a
          result, high-dimensional datasets are at risk of being very sparse: most training
          instances are likely to be far away from each other. This also means that a new
          instance will likely be far away from any training instance, making predictions much
          less reliable than in lower dimensions, since they will be based on much larger extrap‐
          olations. In short, the more dimensions the training set has, the greater the risk of
          overfitting it.                                             
          In theory, one solution to the curse of dimensionality could be to increase the size of
          the training set to reach a sufficient density of training instances. Unfortunately, in
          practice, the number of training instances required to reach a given density grows
          exponentially with the number of dimensions. With just 100 features (significantly
          fewer than in the MNIST problem), you would need more training instances than
          atoms in the observable universe in order for training instances to be within 0.1 of
          each other on average, assuming they were spread out uniformly across all dimen‐
          sions.                                                      
                                                                      
          Main Approaches for Dimensionality Reduction                
                                                                      
          Before we dive into specific dimensionality reduction algorithms, let’s take a look at
          the two main approaches to reducing dimensionality: projection and Manifold
          Learning.                                                   
                                                                      
          Projection                                                  
          In most real-world problems, training instances are not spread out uniformly across
          all dimensions. Many features are almost constant, while others are highly correlated
          (as discussed earlier for MNIST). As a result, all training instances lie within (or close
          to) a much lower-dimensional subspace of the high-dimensional space. This sounds
          very abstract, so let’s look at an example. In Figure 8-2 you can see a 3D dataset repre‐
          sented by circles.                                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|subspace; training instances; projection; dimensionality reduction
"                                                                      
                                                                      
                                                                      
                                                                      
          based only on the original vectors a and b, without having to compute (or even to
          know about) the transformation ϕ. Equation 5-10 lists some of the most commonly
          used kernels.                                               
                                                                      
            Equation 5-10. Common kernels                             
                                                                      
                            ⊺                                         
                Linear: K a,b =a b                                    
                              ⊺  d                                    
              Polynomial: K a,b = γa b+r                              
            Gaussian RBF: K a,b = exp                                 
                               −γ∥a−b∥2                               
                                 ⊺                                    
               Sigmoid: K a,b = tanh γa b+r                           
                             Mercer’s Theorem                         
           According to Mercer’s theorem, if a function K(a, b) respects a few mathematical con‐
           ditions called Mercer’s conditions (e.g., K must be continuous and symmetric in its
           arguments so that K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a
           and b into another space (possibly with much higher dimensions) such that K(a, b) =
           ϕ(a)⊺ ϕ(b). You can use K as a kernel because you know ϕ exists, even if you don’t
           know what ϕ is. In the case of the Gaussian RBF kernel, it can be shown that ϕ maps
           each training instance to an infinite-dimensional space, so it’s a good thing you don’t
           need to actually perform the mapping!                      
           Note that some frequently used kernels (such as the sigmoid kernel) don’t respect all
           of Mercer’s conditions, yet they generally work well in practice.
                                                                      
          There is still one loose end we must tie up. Equation 5-7 shows how to go from the
          dual solution to the primal solution in the case of a linear SVM classifier. But if you
          apply the kernel trick, you end up with equations that include ϕ(x(i)). In fact, w must
          have the same number of dimensions as ϕ(x(i)), which may be huge or even infinite,
          so you can’t compute it. But how can you make predictions without knowing w? Well,
          the good news is that you can plug the formula for w from Equation 5-7 into the deci‐
          sion function for a new instance x(n), and you get an equation with only dot products
          between input vectors. This makes it possible to use the kernel trick (Equation 5-11).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|Mercer's conditions; Mercer's theorem; sigmoid kernel
"                                                                      
                                                                      
                                                                      
                                                                      
            Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions
            for state, actions in enumerate(possible_actions):        
               Q_values[state, actions] = 0.0 # for all possible actions
          Now let’s run the Q-Value Iteration algorithm. It applies Equation 18-3 repeatedly, to
          all Q-Values, for every state and every possible action:    
            gamma = 0.90 # the discount factor                        
                                                                      
            for iteration in range(50):                               
               Q_prev = Q_values.copy()                               
               for s in range(3):                                     
                 for a in possible_actions[s]:                        
                   Q_values[s, a] = np.sum([                          
                        transition_probabilities[s][a][sp]            
                        * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp]))
                      for sp in range(3)])                            
          That’s it! The resulting Q-Values look like this:           
            >>> Q_values                                              
            array([[18.91891892, 17.02702702, 13.62162162],           
                [ 0.   ,     -inf, -4.87971488],                      
                [    -inf, 50.13365013, -inf]])                       
          For example, when the agent is in state s and it chooses action a , the expected sum
                                  0             1                     
          of discounted future rewards is approximately 17.0.         
          For each state, let’s look at the action that has the highest Q-Value:
            >>> np.argmax(Q_values, axis=1) # optimal action for each state
            array([0, 0, 1])                                          
          This gives us the optimal policy for this MDP, when using a discount factor of 0.90: in
          state s choose action a ; in state s choose action a (i.e., stay put); and in state s
              0         0      1         0                 2          
          choose action a (the only possible action). Interestingly, if we increase the discount
                   1                                                  
          factor to 0.95, the optimal policy changes: in state s the best action becomes a (go
                                         1               2            
          through the fire!). This makes sense because the more you value future rewards, the
          more you are willing to put up with some pain now for the promise of future bliss.
          Temporal Difference Learning                                
          Reinforcement Learning problems with discrete actions can often be modeled as
          Markov decision processes, but the agent initially has no idea what the transition
          probabilities are (it does not know T(s, a, s′)), and it does not know what the rewards
          are going to be either (it does not know R(s, a, s′)). It must experience each state and
          each transition at least once to know the rewards, and it must experience them multi‐
          ple times if it is to have a reasonable estimate of the transition probabilities.
          The Temporal Difference Learning (TD Learning) algorithm is very similar to the
          Value Iteration algorithm, but tweaked to take into account the fact that the agent has"|Markov Decision Processes (MDP); Temporal Difference Learning; Temporal Difference Learning (TD Learning)
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-16. A graphical representation of a Gaussian mixture model, including its
          parameters (squares), random variables (circles), and their conditional dependencies
          (solid arrows)                                              
          Here is how to interpret the figure:8                       
                                                                      
           • The circles represent random variables.                  
                                                                      
           • The squares represent fixed values (i.e., parameters of the model).
           • The large rectangles are called plates. They indicate that their content is repeated
            several times.                                            
           • The number at the bottom right of each plate indicates how many times its con‐
            tent is repeated. So, there are m random variables z(i) (from z(1) to z(m)) and m
            random variables x(i). There are also k means μ(j) and k covariance matrices Σ(j).
            Lastly, there is just one weight vector ϕ (containing all the weights ϕ(1) to ϕ(k)).
           • Each variable z(i) is drawn from the categorical distribution with weights ϕ. Each
            variable x(i) is drawn from the normal distribution, with the mean and covariance
            matrix defined by its cluster z(i).                       
           • The solid arrows represent conditional dependencies. For example, the probabil‐
            ity distribution for each random variable z(i) depends on the weight vector ϕ.
            Note that when an arrow crosses a plate boundary, it means that it applies to all
            the repetitions of that plate. For example, the weight vector ϕ conditions the
            probability distributions of all the random variables x(1) to x(m).
                                                                      
           • The squiggly arrow from z(i) to x(i) represents a switch: depending on the value of
            z(i), the instance x(i) will be sampled from a different Gaussian distribution. For
            example, if z(i)=j, then xi ∼  μ j,Σ j .                  
                                                                      
                                                                      
          8 Most of these notations are standard, but a few additional notations were taken from the Wikipedia article on
           plate notation.                                            "|categorical distribution
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 3-2. An illustrated confusion matrix shows examples of true negatives (top left),
          false positives (top right), false negatives (lower left), and true positives (lower right)
                                                                      
          Precision and Recall                                        
                                                                      
          Scikit-Learn provides several functions to compute classifier metrics, including preci‐
          sion and recall:                                            
            >>> from sklearn.metrics import precision_score, recall_score
            >>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)
            0.7290850836596654                                        
            >>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)
            0.7555801512636044                                        
          Now your 5-detector does not look as shiny as it did when you looked at its accuracy.
          When it claims an image represents a 5, it is correct only 72.9% of the time. More‐
          over, it only detects 75.6% of the 5s.                      
          It is often convenient to combine precision and recall into a single metric called the F
                                                           1          
          score, in particular if you need a simple way to compare two classifiers. The F score is
                                                       1              
          the harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean
          treats all values equally, the harmonic mean gives much more weight to low values.
          As a result, the classifier will only get a high F score if both recall and precision are
                                     1                                
          high.                                                       
            Equation 3-3. F                                           
                     1                                                
                   2        precision×recall TP                       
            F =         =2×           =                               
             1   1    1     precision+recall FN+FP                    
                    +                  TP+                            
               precision recall             2                         "|computing classifier metrics; harmonic mean; F1 score
"                                                                      
                                                                      
                                                                      
                                                                      
          Using the Prediction Service                                
                                                                      
          Under the hood, AI Platform just runs TF Serving, so in principle you could use the
          same code as earlier, if you knew which URL to query. There’s just one problem: GCP
          also takes care of encryption and authentication. Encryption is based on SSL/TLS,
          and authentication is token-based: a secret authentication token must be sent to the
          server in every request. So before your code can use the prediction service (or any
          other GCP service), it must obtain a token. We will see how to do this shortly, but
          first you need to configure authentication and give your application the appropriate
          access rights on GCP. You have two options for authentication:
           • Your application (i.e., the client code that will query the prediction service) could
            authenticate using user credentials with your own Google login and password.
            Using user credentials would give your application the exact same rights as on
            GCP, which is certainly way more than it needs. Moreover, you would have to
            deploy your credentials in your application, so anyone with access could steal
            your credentials and fully access your GCP account. In short, do not choose this
            option; it is only needed in very rare cases (e.g., when your application needs to
            access its user’s GCP account).                           
                                                                      
           • The client code can authenticate with a service account. This is an account that
            represents an application, not a user. It is generally given very restricted access
            rights: strictly what it needs, and no more. This is the recommended option.
          So, let’s create a service account for your application: in the navigation menu, go to
          IAM & admin → Service accounts, then click Create Service Account, fill in the form
          (service account name, ID, description), and click Create (see Figure 19-7). Next, you
          must give this account some access rights. Select the ML Engine Developer role: this
          will allow the service account to make predictions, and not much more. Optionally,
          you can grant some users access to the service account (this is useful when your GCP
          user account is part of an organization, and you wish to authorize other users in the
          organization to deploy applications that will be based on this service account or to
          manage the service account itself). Next, click Create Key to export the service
          account’s private key, choose JSON, and click Create. This will download the private
          key in the form of a JSON file. Make sure to keep it private!
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|service account; prediction service use
"                                                                      
                                                                      
                                                                      
                                                                      
            mnist_train = mnist_train.shuffle(10000).batch(32)        
            mnist_train = mnist_train.map(lambda items: (items[""image""], items[""label""]))
            mnist_train = mnist_train.prefetch(1)                     
          But it’s simpler to ask the load() function to do this for you by setting as_super
          vised=True (obviously this works only for labeled datasets). You can also specify the
          batch size if you want. Then you can pass the dataset directly to your tf.keras model:
            dataset = tfds.load(name=""mnist"", batch_size=32, as_supervised=True)
            mnist_train = dataset[""train""].prefetch(1)                
            model = keras.models.Sequential([...])                    
            model.compile(loss=""sparse_categorical_crossentropy"", optimizer=""sgd"")
            model.fit(mnist_train, epochs=5)                          
          This was quite a technical chapter, and you may feel that it is a bit far from the
          abstract beauty of neural networks, but the fact is Deep Learning often involves large
          amounts of data, and knowing how to load, parse, and preprocess it efficiently is a
          crucial skill to have. In the next chapter, we will look at convolutional neural net‐
          works, which are among the most successful neural net architectures for image pro‐
          cessing and many other applications.                        
                                                                      
          Exercises                                                   
                                                                      
           1. Why would you want to use the Data API?                 
           2. What are the benefits of splitting a large dataset into multiple files?
                                                                      
           3. During training, how can you tell that your input pipeline is the bottleneck?
            What can you do to fix it?                                
           4. Can you save any binary data to a TFRecord file, or only serialized protocol
            buffers?                                                  
           5. Why would you go through the hassle of converting all your data to the Example
            protobuf format? Why not use your own protobuf definition?
           6. When using TFRecords, when would you want to activate compression? Why
            not do it systematically?                                 
                                                                      
           7. Data can be preprocessed directly when writing the data files, or within the
            tf.data pipeline, or in preprocessing layers within your model, or using TF Trans‐
            form. Can you list a few pros and cons of each option?    
           8. Name a few common techniques you can use to encode categorical features.
            What about text?                                          
           9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a train‐
            ing set, a validation set, and a test set; shuffle the training set; and save each
            dataset to multiple TFRecord files. Each record should be a serialized Example
            protobuf with two features: the serialized image (use tf.io.serialize_tensor()
                                                                      "|loading and preprocessing with TensorFlow
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-4. Reducing dimensionality using a stride of 2    
                                                                      
          Filters                                                     
                                                                      
          A neuron’s weights can be represented as a small image the size of the receptive field.
          For example, Figure 14-5 shows two possible sets of weights, called filters (or convolu‐
          tion kernels). The first one is represented as a black square with a vertical white line in
          the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of
          1s); neurons using these weights will ignore everything in their receptive field except
          for the central vertical line (since all inputs will get multiplied by 0, except for the
          ones located in the central vertical line). The second filter is a black square with a
          horizontal white line in the middle. Once again, neurons using these weights will
          ignore everything in their receptive field except for the central horizontal line.
          Now if all neurons in a layer use the same vertical line filter (and the same bias term),
          and you feed the network the input image shown in Figure 14-5 (the bottom image),
          the layer will output the top-left image. Notice that the vertical white lines get
          enhanced while the rest gets blurred. Similarly, the upper-right image is what you get
          if all neurons use the same horizontal line filter; notice that the horizontal white lines
          get enhanced while the rest is blurred out. Thus, a layer full of neurons using the
          same filter outputs a feature map, which highlights the areas in an image that activate
          the filter the most. Of course, you do not have to define the filters manually: instead,
          during training the convolutional layer will automatically learn the most useful filters
          for its task, and the layers above will learn to combine them into more complex
          patterns.                                                   
                                                                      
                                                                      
                                                                      
                                                                      "|filters; feature maps; convolution kernels
"                                                                      
                                                                      
                                                                      
                                                                      
          Variational Autoencoders                                    
                                                                      
          Another important category of autoencoders was introduced in 2013 by Diederik
          Kingma and Max Welling and quickly became one of the most popular types of
          autoencoders: variational autoencoders.7                    
                                                                      
          They are quite different from all the autoencoders we have discussed so far, in these
          particular ways:                                            
           • They are probabilistic autoencoders, meaning that their outputs are partly deter‐
            mined by chance, even after training (as opposed to denoising autoencoders,
            which use randomness only during training).               
           • Most importantly, they are generative autoencoders, meaning that they can gener‐
            ate new instances that look like they were sampled from the training set.
                                                                      
          Both these properties make them rather similar to RBMs, but they are easier to train,
          and the sampling process is much faster (with RBMs you need to wait for the network
          to stabilize into a “thermal equilibrium” before you can sample a new instance).
          Indeed, as their name suggests, variational autoencoders perform variational Baye‐
          sian inference (introduced in Chapter 9), which is an efficient way to perform
          approximate Bayesian inference.                             
          Let’s take a look at how they work. Figure 17-12 (left) shows a variational autoen‐
          coder. You can recognize the basic structure of all autoencoders, with an encoder fol‐
          lowed by a decoder (in this example, they both have two hidden layers), but there is a
          twist: instead of directly producing a coding for a given input, the encoder produces a
          mean coding μ and a standard deviation σ. The actual coding is then sampled ran‐
          domly from a Gaussian distribution with mean μ and standard deviation σ. After that
          the decoder decodes the sampled coding normally. The right part of the diagram
          shows a training instance going through this autoencoder. First, the encoder pro‐
          duces μ and σ, then a coding is sampled randomly (notice that it is not exactly located
          at μ), and finally this coding is decoded; the final output resembles the training
          instance.                                                   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          7 Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes,” arXiv preprint arXiv:1312.6114
           (2013).                                                    "|generative; generative autoencoders; mean coding; variational; variational autoencoders; probabilistic; probabilistic autoencoders; Bayesian inference
"                                                                      
                                                                      
                                                                      
                                                                      
          able to flow through the network and reach the final layers of the generator: this
          seems like an unnecessary constraint that probably slowed down training. And
          finally, some visual artifacts may appear because the same noise was used at different
          levels. If instead the generator tried to produce its own pseudorandom noise, this
          noise might not look very convincing, leading to more visual artifacts. Plus, part of
          the generator’s weights would be dedicated to generating pseudorandom noise, which
          again seems wasteful. By adding extra noise inputs, all these issues are avoided; the
          GAN is able to use the provided noise to add the right amount of stochasticity to each
          part of the image.                                          
          The added noise is different for each level. Each noise input consists of a single fea‐
          ture map full of Gaussian noise, which is broadcast to all feature maps (of the given
          level) and scaled using learned per-feature scaling factors (this is represented by the
          “B” boxes in Figure 17-20) before it is added.              
                                                                      
          Finally, StyleGAN uses a technique called mixing regularization (or style mixing),
          where a percentage of the generated images are produced using two different codings.
          Specifically, the codings c and c are sent through the mapping network, giving two
                         1   2                                        
          style vectors w and w . Then the synthesis network generates an image based on the
                  1    2                                              
          styles w for the first levels and the styles w for the remaining levels. The cutoff level
              1                    2                                  
          is picked randomly. This prevents the network from assuming that styles at adjacent
          levels are correlated, which in turn encourages locality in the GAN, meaning that
          each style vector only affects a limited number of traits in the generated image.
          There is such a wide variety of GANs out there that it would require a whole book to
          cover them all. Hopefully this introduction has given you the main ideas, and most
          importantly the desire to learn more. If you’re struggling with a mathematical con‐
          cept, there are probably blog posts out there that will help you understand it better.
          Then go ahead and implement your own GAN, and do not get discouraged if it has
          trouble learning at first: unfortunately, this is normal, and it will require quite a bit of
          patience before it works, but the result is worth it. If you’re struggling with an imple‐
          mentation detail, there are plenty of Keras or TensorFlow implementations that you
          can look at. In fact, if all you want is to get some amazing results quickly, then you
          can just use a pretrained model (e.g., there are pretrained StyleGAN models available
          for Keras).                                                 
          In the next chapter we will move to an entirely different branch of Deep Learning:
          Deep Reinforcement Learning.                                
                                                                      
                                                                      
                                                                      
                                                                      "|mixing regularization; style mixing
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                   CHAPTER 7          
                                                                      
              Ensemble    Learning  and  Random    Forests            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Suppose you pose a complex question to thousands of random people, then aggregate
          their answers. In many cases you will find that this aggregated answer is better than
          an expert’s answer. This is called the wisdom of the crowd. Similarly, if you aggregate
          the predictions of a group of predictors (such as classifiers or regressors), you will
          often get better predictions than with the best individual predictor. A group of pre‐
          dictors is called an ensemble; thus, this technique is called Ensemble Learning, and an
          Ensemble Learning algorithm is called an Ensemble method.   
          As an example of an Ensemble method, you can train a group of Decision Tree classi‐
          fiers, each on a different random subset of the training set. To make predictions, you
          obtain the predictions of all the individual trees, then predict the class that gets the
          most votes (see the last exercise in Chapter 6). Such an ensemble of Decision Trees is
          called a Random Forest, and despite its simplicity, this is one of the most powerful
          Machine Learning algorithms available today.                
          As discussed in Chapter 2, you will often use Ensemble methods near the end of a
          project, once you have already built a few good predictors, to combine them into an
          even better predictor. In fact, the winning solutions in Machine Learning competi‐
          tions often involve several Ensemble methods (most famously in the Netflix Prize
          competition).                                               
                                                                      
          In this chapter we will discuss the most popular Ensemble methods, including bag‐
          ging, boosting, and stacking. We will also explore Random Forests.
          Voting Classifiers                                          
                                                                      
          Suppose you have trained a few classifiers, each one achieving about 80% accuracy.
          You may have a Logistic Regression classifier, an SVM classifier, a Random Forest
          classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1).
                                                                      "|voting classifiers; Ensemble Learning; ensembles; Ensemble methods; prediction problems; wisdom of the crowd; Random Forests
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-6. The decision boundary may not always be simpler with lower dimensions
                                                                      
          PCA                                                         
                                                                      
          Principal Component Analysis (PCA) is by far the most popular dimensionality reduc‐
          tion algorithm. First it identifies the hyperplane that lies closest to the data, and then
          it projects the data onto it, just like in Figure 8-2.      
                                                                      
          Preserving the Variance                                     
                                                                      
          Before you can project the training set onto a lower-dimensional hyperplane, you
          first need to choose the right hyperplane. For example, a simple 2D dataset is repre‐
          sented on the left in Figure 8-7, along with three different axes (i.e., 1D hyperplanes).
          On the right is the result of the projection of the dataset onto each of these axes. As
          you can see, the projection onto the solid line preserves the maximum variance, while
          the projection onto the dotted line preserves very little variance and the projection
          onto the dashed line preserves an intermediate amount of variance.
                                                                      
                                                                      
                                                                      
                                                                      "|PCA (Principal Component Analysis); preserving variance; preserving
"                                                                      
                                                                      
                                                                      
                                                                      
          Creating the Replay Buffer and the Corresponding Observer   
                                                                      
          The TF-Agents library provides various replay buffer implementations in the
          tf_agents.replay_buffers package. Some are purely written in Python (their mod‐
          ule names start with py_), and others are written based on TensorFlow (their module
          names start with tf_). We will use the TFUniformReplayBuffer class in the
          tf_agents.replay_buffers.tf_uniform_replay_buffer package. It provides a
          high-performance implementation of a replay buffer with uniform sampling:21
            from tf_agents.replay_buffers import tf_uniform_replay_buffer
                                                                      
            replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(
               data_spec=agent.collect_data_spec,                     
               batch_size=tf_env.batch_size,                          
               max_length=1000000)                                    
          Let’s look at each of these arguments:                      
          data_spec                                                   
            The specification of the data that will be saved in the replay buffer. The DQN
            agent knowns what the collected data will look like, and it makes the data spec
            available via its collect_data_spec attribute, so that’s what we give the replay
            buffer.                                                   
          batch_size                                                  
                                                                      
            The number of trajectories that will be added at each step. In our case, it will be
            one, since the driver will just execute one action per step and collect one trajec‐
            tory. If the environment were a batched environment, meaning an environment
            that takes a batch of actions at each step and returns a batch of observations, then
            the driver would have to save a batch of trajectories at each step. Since we are
            using a TensorFlow replay buffer, it needs to know the size of the batches it will
            handle (to build the computation graph). An example of a batched environment
            is the ParallelPyEnvironment (from the tf_agents.environments.paral
            lel_py_environment package): it runs multiple environments in parallel in sepa‐
            rate processes (they can be different as long as they have the same action and
            observation specs), and at each step it takes a batch of actions and executes them
            in the environments (one action per environment), then it returns all the result‐
            ing observations.                                         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          21 At the time of this writing, there is no prioritized experience replay buffer yet, but one will likely be open
           sourced soon.                                              "|replay buffer and observer; replay buffers; observers
"                                                                      
                                                                      
                                                                      
                                                                      
            Equation 4-6. Gradient vector of the cost function        
                                                                      
                     ∂                                                
                      MSE θ                                           
                    ∂θ                                                
                      0                                               
                     ∂                                                
            ∇ θ MSE θ = ∂θ 1 MSE θ = m 2 X ⊺ Xθ−y                     
                       ⋮                                              
                     ∂                                                
                       MSE θ                                          
                    ∂θ                                                
                      n                                               
                   Notice that this formula involves calculations over the full training
                   set X, at each Gradient Descent step! This is why the algorithm is
                   called Batch Gradient Descent: it uses the whole batch of training
                   data at every step (actually, Full Gradient Descent would probably
                   be a better name). As a result it is terribly slow on very large train‐
                   ing sets (but we will see much faster Gradient Descent algorithms
                   shortly). However, Gradient Descent scales well with the number of
                   features; training a Linear Regression model when there are hun‐
                   dreds of thousands of features is much faster using Gradient
                   Descent than using the Normal Equation or SVD decomposition.
          Once you have the gradient vector, which points uphill, just go in the opposite direc‐
          tion to go downhill. This means subtracting ∇ MSE(θ) from θ. This is where the
                                      θ                               
          learning rate η comes into play:5 multiply the gradient vector by η to determine the
          size of the downhill step (Equation 4-7).                   
            Equation 4-7. Gradient Descent step                       
            θ next step =θ−η∇ MSE θ                                   
                       θ                                              
          Let’s look at a quick implementation of this algorithm:     
            eta = 0.1 # learning rate                                 
            n_iterations = 1000                                       
            m = 100                                                   
            theta = np.random.randn(2,1) # random initialization      
            for iteration in range(n_iterations):                     
               gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)        
               theta = theta - eta * gradients                        
                                                                      
                                                                      
          5 Eta (η) is the seventh letter of the Greek alphabet.      "|Full Gradient Descent
"                                                                      
                                                                      
                                                                      
                                                                      
          Instead of training the DQN based only on the latest experiences, we will store all
          experiences in a replay buffer (or replay memory), and we will sample a random train‐
          ing batch from it at each training iteration. This helps reduce the correlations
          between the experiences in a training batch, which tremendously helps training. For
          this, we will just use a deque list:                        
                                                                      
            from collections import deque                             
            replay_buffer = deque(maxlen=2000)                        
                                                                      
                   A deque is a linked list, where each element points to the next one
                   and to the previous one. It makes inserting and deleting items very
                   fast, but the longer the deque is, the slower random access will be.
                   If you need a very large replay buffer, use a circular buffer; see the
                   “Deque vs Rotating List” section of the notebook for an
                   implementation.                                    
                                                                      
          Each experience will be composed of five elements: a state, the action the agent took,
          the resulting reward, the next state it reached, and finally a Boolean indicating
          whether the episode ended at that point (done). We will need a small function to sam‐
          ple a random batch of experiences from the replay buffer. It will return five NumPy
          arrays corresponding to the five experience elements:       
            def sample_experiences(batch_size):                       
               indices = np.random.randint(len(replay_buffer), size=batch_size)
               batch = [replay_buffer[index] for index in indices]    
               states, actions, rewards, next_states, dones = [       
                 np.array([experience[field_index] for experience in batch])
                 for field_index in range(5)]                         
               return states, actions, rewards, next_states, dones    
          Let’s also create a function that will play a single step using the ε-greedy policy, then
          store the resulting experience in the replay buffer:        
            def play_one_step(env, state, epsilon):                   
               action = epsilon_greedy_policy(state, epsilon)         
               next_state, reward, done, info = env.step(action)      
               replay_buffer.append((state, action, reward, next_state, done))
               return next_state, reward, done, info                  
          Finally, let’s create one last function that will sample a batch of experiences from the
          replay buffer and train the DQN by performing a single Gradient Descent step on this
          batch:                                                      
            batch_size = 32                                           
            discount_factor = 0.95                                    
            optimizer = keras.optimizers.Adam(lr=1e-3)                
            loss_fn = keras.losses.mean_squared_error                 "|replay memory; deques; replay buffers
"                                                                      
                                                                      
                                                                      
                                                                      
          That’s not too hard! However, you may prefer to use a nice self-contained custom
          layer (much like Scikit-Learn’s StandardScaler), rather than having global variables
          like means and stds dangling around:                        
                                                                      
            class Standardization(keras.layers.Layer):                
               def adapt(self, data_sample):                          
                 self.means_ = np.mean(data_sample, axis=0, keepdims=True)
                 self.stds_ = np.std(data_sample, axis=0, keepdims=True)
               def call(self, inputs):                                
                 return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())
          Before you can use this standardization layer, you will need to adapt it to your dataset
          by calling the adapt() method and passing it a data sample. This will allow it to use
          the appropriate mean and standard deviation for each feature:
            std_layer = Standardization()                             
            std_layer.adapt(data_sample)                              
          This sample must be large enough to be representative of your dataset, but it does not
          have to be the full training set: in general, a few hundred randomly selected instances
          will suffice (however, this depends on your task). Next, you can use this preprocess‐
          ing layer like a normal layer:                              
            model = keras.Sequential()                                
            model.add(std_layer)                                      
            [...] # create the rest of the model                      
            model.compile([...])                                      
            model.fit([...])                                          
          If you are thinking that Keras should contain a standardization layer like this one,
          here’s some good news for you: by the time you read this, the keras.layers.Normal
          ization layer will probably be available. It will work very much like our custom
          Standardization layer: first, create the layer, then adapt it to your dataset by passing
          a data sample to the adapt() method, and finally use the layer normally.
          Now let’s look at categorical features. We will start by encoding them as one-hot
          vectors.                                                    
          Encoding Categorical Features Using One-Hot Vectors         
                                                                      
          Consider the ocean_proximity feature in the California housing dataset we explored
          in Chapter 2: it is a categorical feature with five possible values: ""<1H OCEAN"",
          ""INLAND"", ""NEAR OCEAN"", ""NEAR BAY"", and ""ISLAND"". We need to encode this feature
          before we feed it to a neural network. Since there are very few categories, we can use
          one-hot encoding. For this, we first need to map each category to its index (0 to 4),
          which can be done using a lookup table:                     
            vocab = [""<1H OCEAN"", ""INLAND"", ""NEAR OCEAN"", ""NEAR BAY"", ""ISLAND""]
            indices = tf.range(len(vocab), dtype=tf.int64)            
                                                                      "|encoding using one-hot vectors; one-hot vectors
"                                                                      
                                                                      
                                                                      
                                                                      
          (with 100% probability). It may alternate a number of times between these two states,
          but eventually it will fall into state s and remain there forever (this is a terminal
                                3                                     
          state). Markov chains can have very different dynamics, and they are heavily used in
          thermodynamics, chemistry, statistics, and much more.       
          Markov decision processes were first described in the 1950s by Richard Bellman.12
          They resemble Markov chains but with a twist: at each step, an agent can choose one
          of several possible actions, and the transition probabilities depend on the chosen
          action. Moreover, some state transitions return some reward (positive or negative),
          and the agent’s goal is to find a policy that will maximize reward over time.
          For example, the MDP represented in Figure 18-8 has three states (represented by cir‐
          cles) and up to three possible discrete actions at each step (represented by diamonds).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-8. Example of a Markov decision process           
                                                                      
          If it starts in state s , the agent can choose between actions a , a , or a . If it chooses
                     0                        0 1  2                  
          action a , it just remains in state s with certainty, and without any reward. It can thus
               1             0                                        
          decide to stay there forever if it wants to. But if it chooses action a , it has a 70% prob‐
                                                0                     
          ability of gaining a reward of +10 and remaining in state s . It can then try again and
                                            0                         
          again to gain as much reward as possible, but at one point it is going to end up
          instead in state s . In state s it has only two possible actions: a or a . It can choose to
                    1     1                   0  2                    
          stay put by repeatedly choosing action a , or it can choose to move on to state s and
                                 0                       2            
          get a negative reward of –50 (ouch). In state s it has no other choice than to take
                                      2                               
          action a , which will most likely lead it back to state s , gaining a reward of +40 on the
               1                         0                            
          12 Richard Bellman, “A Markovian Decision Process,” Journal of Mathematics and Mechanics 6, no. 5 (1957):
           679–684.                                                   "|terminal state
"                                                                      
                                                                      
                                                                      
                                                                      
          Convolutional Layers                                        
                                                                      
          The most important building block of a CNN is the convolutional layer:6 neurons in
          the first convolutional layer are not connected to every single pixel in the input image
          (like they were in the layers discussed in previous chapters), but only to pixels in their
          receptive fields (see Figure 14-2). In turn, each neuron in the second convolutional
          layer is connected only to neurons located within a small rectangle in the first layer.
          This architecture allows the network to concentrate on small low-level features in the
          first hidden layer, then assemble them into larger higher-level features in the next
          hidden layer, and so on. This hierarchical structure is common in real-world images,
          which is one of the reasons why CNNs work so well for image recognition.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-2. CNN layers with rectangular local receptive fields
                                                                      
                   All the multilayer neural networks we’ve looked at so far had layers
                   composed of a long line of neurons, and we had to flatten input
                   images to 1D before feeding them to the neural network. In a CNN
                   each layer is represented in 2D, which makes it easier to match
                   neurons with their corresponding inputs.           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          6 A convolution is a mathematical operation that slides one function over another and measures the integral of
           their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform
           and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very
           similar to convolutions (see https://homl.info/76 for more details)."|convolutional layer
"                                                                      
                                                                      
                                                                      
                                                                      
          faster, the accuracy would be roughly the same, and it would allow us to choose any
          output activation function we want. If you make this change, also make sure to
          remove return_sequences=True from the second (now last) recurrent layer:
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
               keras.layers.SimpleRNN(20),                            
               keras.layers.Dense(1)                                  
            ])                                                        
          If you train this model, you will see that it converges faster and performs just as well.
          Plus, you could change the output activation function if you wanted.
          Forecasting Several Time Steps Ahead                        
                                                                      
          So far we have only predicted the value at the next time step, but we could just as
          easily have predicted the value several steps ahead by changing the targets appropri‐
          ately (e.g., to predict 10 steps ahead, just change the targets to be the value 10 steps
          ahead instead of 1 step ahead). But what if we want to predict the next 10 values?
          The first option is to use the model we already trained, make it predict the next value,
          then add that value to the inputs (acting as if this predicted value had actually occur‐
          red), and use the model again to predict the following value, and so on, as in the fol‐
          lowing code:                                                
            series = generate_time_series(1, n_steps + 10)            
            X_new, Y_new = series[:, :n_steps], series[:, n_steps:]   
            X = X_new                                                 
            for step_ahead in range(10):                              
               y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]
               X = np.concatenate([X, y_pred_one], axis=1)            
            Y_pred = X[:, n_steps:]                                   
          As you might expect, the prediction for the next step will usually be more accurate
          than the predictions for later time steps, since the errors might accumulate (as you
          can see in Figure 15-8). If you evaluate this approach on the validation set, you will
          find an MSE of about 0.029. This is much higher than the previous models, but it’s
          also a much harder task, so the comparison doesn’t mean much. It’s much more
          meaningful to compare this performance with naive predictions (just forecasting that
          the time series will remain constant for 10 time steps) or with a simple linear model.
          The naive approach is terrible (it gives an MSE of about 0.223), but the linear model
          gives an MSE of about 0.0188: it’s much better than using our RNN to forecast the
          future one step at a time, and also much faster to train and run. Still, if you only want
          to forecast a few time steps ahead, on more complex tasks, this approach may work
          well.                                                       
                                                                      
                                                                      "|forecasting several steps ahead
"                                                                      
                                                                      
                                                                      
                                                                      
          Autoencoders and GANs are both unsupervised, they both learn dense representa‐
          tions, they can both be used as generative models, and they have many similar appli‐
          cations. However, they work very differently:               
                                                                      
           • Autoencoders simply learn to copy their inputs to their outputs. This may sound
            like a trivial task, but we will see that constraining the network in various ways
            can make it rather difficult. For example, you can limit the size of the latent rep‐
            resentations, or you can add noise to the inputs and train the network to recover
            the original inputs. These constraints prevent the autoencoder from trivially
            copying the inputs directly to the outputs, which forces it to learn efficient ways
            of representing the data. In short, the codings are byproducts of the autoencoder
            learning the identity function under some constraints.    
           • GANs are composed of two neural networks: a generator that tries to generate
            data that looks similar to the training data, and a discriminator that tries to tell
            real data from fake data. This architecture is very original in Deep Learning in
            that the generator and the discriminator compete against each other during
            training: the generator is often compared to a criminal trying to make realistic
            counterfeit money, while the discriminator is like the police investigator trying to
            tell real money from fake. Adversarial training (training competing neural net‐
            works) is widely considered as one of the most important ideas in recent years. In
            2016, Yann LeCun even said that it was “the most interesting idea in the last 10
            years in Machine Learning.”                               
                                                                      
          In this chapter we will start by exploring in more depth how autoencoders work and
          how to use them for dimensionality reduction, feature extraction, unsupervised pre‐
          training, or as generative models. This will naturally lead us to GANs. We will start by
          building a simple GAN to generate fake images, but we will see that training is often
          quite difficult. We will discuss the main difficulties you will encounter with adversa‐
          rial training, as well as some of the main techniques to work around these difficulties.
          Let’s start with autoencoders!                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|adversarial learning; versus Generative Adversarial Networks (GANs); Generative Adversarial Networks (GANs) versus autoencoders; discriminators; generators
"                                                                      
                                                                      
                                                                      
                                                                      
           • Shaded nodes indicate that the value is known. So, in this case, only the random
            variables x(i) have known values: they are called observed variables. The unknown
            random variables z(i) are called latent variables.        
                                                                      
          So, what can you do with such a model? Well, given the dataset X, you typically want
          to start by estimating the weights ϕ and all the distribution parameters μ(1) to μ(k) and
          Σ(1) to Σ(k). Scikit-Learn’s GaussianMixture class makes this super easy:
            from sklearn.mixture import GaussianMixture               
                                                                      
            gm = GaussianMixture(n_components=3, n_init=10)           
            gm.fit(X)                                                 
          Let’s look at the parameters that the algorithm estimated:  
            >>> gm.weights_                                           
            array([0.20965228, 0.4000662 , 0.39028152])               
            >>> gm.means_                                             
            array([[ 3.39909717, 1.05933727],                         
                [-1.40763984, 1.42710194],                            
                [ 0.05135313, 0.07524095]])                           
            >>> gm.covariances_                                       
            array([[[ 1.14807234, -0.03270354],                       
                 [-0.03270354, 0.95496237]],                          
                [[ 0.63478101, 0.72969804],                           
                 [ 0.72969804, 1.1609872 ]],                          
                [[ 0.68809572, 0.79608475],                           
                 [ 0.79608475, 1.21234145]]])                         
          Great, it worked fine! Indeed, the weights that were used to generate the data were
          0.2, 0.4, and 0.4; and similarly, the means and covariance matrices were very close to
          those found by the algorithm. But how? This class relies on the Expectation-
          Maximization (EM) algorithm, which has many similarities with the K-Means algo‐
          rithm: it also initializes the cluster parameters randomly, then it repeats two steps
          until convergence, first assigning instances to clusters (this is called the expectation
          step) and then updating the clusters (this is called the maximization step). Sounds
          familiar, right? In the context of clustering, you can think of EM as a generalization of
          K-Means that not only finds the cluster centers (μ(1) to μ(k)), but also their size, shape,
          and orientation (Σ(1) to Σ(k)), as well as their relative weights (ϕ(1) to ϕ(k)). Unlike K-
          Means, though, EM uses soft cluster assignments, not hard assignments. For each
          instance, during the expectation step, the algorithm estimates the probability that it
          belongs to each cluster (based on the current cluster parameters). Then, during the
          maximization step, each cluster is updated using all the instances in the dataset, with
          each instance weighted by the estimated probability that it belongs to that cluster.
          These probabilities are called the responsibilities of the clusters for the instances.
                                                                      "|maximization step; latent variables; observed variables; expectation step; responsibilities (clustering); Expectation-Maximization (EM) algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
          The fixed-length features are parsed as regular tensors, but the variable-length fea‐
          tures are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor
          using tf.sparse.to_dense(), but in this case it is simpler to just access its values:
                                                                      
            >>> tf.sparse.to_dense(parsed_example[""emails""], default_value=b"""")
            <tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>
            >>> parsed_example[""emails""].values                       
            <tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>
          A BytesList can contain any binary data you want, including any serialized object.
          For example, you can use tf.io.encode_jpeg() to encode an image using the JPEG
          format and put this binary data in a BytesList. Later, when your code reads the
          TFRecord, it will start by parsing the Example, then it will need to call
          tf.io.decode_jpeg() to parse the data and get the original image (or you can use
          tf.io.decode_image(), which can decode any BMP, GIF, JPEG, or PNG image). You
          can also store any tensor you want in a BytesList by serializing the tensor using
          tf.io.serialize_tensor() then putting the resulting byte string in a BytesList
          feature. Later, when you parse the TFRecord, you can parse this data using
          tf.io.parse_tensor().                                       
          Instead of parsing examples one by one using tf.io.parse_single_example(), you
          may want to parse them batch by batch using tf.io.parse_example():
            dataset = tf.data.TFRecordDataset([""my_contacts.tfrecord""]).batch(10)
            for serialized_examples in dataset:                       
               parsed_examples = tf.io.parse_example(serialized_examples,
                                     feature_description)             
          As you can see, the Example protobuf will probably be sufficient for most use cases.
          However, it may be a bit cumbersome to use when you are dealing with lists of lists.
          For example, suppose you want to classify text documents. Each document may be
          represented as a list of sentences, where each sentence is represented as a list of
          words. And perhaps each document also has a list of comments, where each com‐
          ment is represented as a list of words. There may be some contextual data too, such as
          the document’s author, title, and publication date. TensorFlow’s SequenceExample
          protobuf is designed for such use cases.                    
                                                                      
          Handling Lists of Lists Using the SequenceExample Protobuf  
          Here is the definition of the SequenceExample protobuf:     
                                                                      
            message FeatureList { repeated Feature feature = 1; };    
            message FeatureLists { map<string, FeatureList> feature_list = 1; };
            message SequenceExample {                                 
               Features context = 1;                                  
               FeatureLists feature_lists = 2;                        
            };                                                        "|lists of lists, using SequenceExample Protobuf; SequenceExample protobuf (TensorFlow); lists of lists using SequenceExample Proto‐buf
"                                                                      
                                                                      
                                                                      
                                                                      
          will get the documentation for this module. By default, TF Hub will cache the down‐
          loaded files into the local system’s temporary directory. You may prefer to download
          them into a more permanent directory to avoid having to download them again after
          every system cleanup. To do that, set the TFHUB_CACHE_DIR environment variable to
          the directory of your choice (e.g., os.environ[""TFHUB_CACHE_DIR""] = ""./
          my_tfhub_cache"").                                           
                                                                      
          So far, we have looked at time series, text generation using Char-RNN, and sentiment
          analysis using word-level RNN models, training our own word embeddings or reus‐
          ing pretrained embeddings. Let’s now look at another important NLP task: neural
          machine translation (NMT), first using a pure Encoder–Decoder model, then improv‐
          ing it with attention mechanisms, and finally looking the extraordinary Transformer
          architecture.                                               
          An Encoder–Decoder Network for Neural Machine               
                                                                      
          Translation                                                 
                                                                      
          Let’s take a look at a simple neural machine translation model10 that will translate
          English sentences to French (see Figure 16-3).              
          In short, the English sentences are fed to the encoder, and the decoder outputs the
          French translations. Note that the French translations are also used as inputs to the
          decoder, but shifted back by one step. In other words, the decoder is given as input
          the word that it should have output at the previous step (regardless of what it actually
          output). For the very first word, it is given the start-of-sequence (SOS) token. The
          decoder is expected to end the sentence with an end-of-sequence (EOS) token.
          Note that the English sentences are reversed before they are fed to the encoder. For
          example, “I drink milk” is reversed to “milk drink I.” This ensures that the beginning
          of the English sentence will be fed last to the encoder, which is useful because that’s
          generally the first thing that the decoder needs to translate.
                                                                      
          Each word is initially represented by its ID (e.g., 288 for the word “milk”). Next, an
          embedding layer returns the word embedding. These word embeddings are what is
          actually fed to the encoder and the decoder.                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          10 Ilya Sutskever et al., “Sequence to Sequence Learning with Neural Networks,” arXiv preprint arXiv:1409.3215
           (2014).                                                    "|Encoder–Decoder model; sentiment analysis; Encoder–Decoder network; end-of-sequence (EoS) token; neural machine translation (NMT)
"                                                                      
                                                                      
                                                                      
                                                                      
                   The bounding boxes should be normalized so that the horizontal
                   and vertical coordinates, as well as the height and width, all range
                   from 0 to 1. Also, it is common to predict the square root of the
                   height and width rather than the height and width directly: this
                   way, a 10-pixel error for a large bounding box will not be penalized
                   as much as a 10-pixel error for a small bounding box.
                                                                      
          The MSE often works fairly well as a cost function to train the model, but it is not a
          great metric to evaluate how well the model can predict bounding boxes. The most
          common metric for this is the Intersection over Union (IoU): the area of overlap
          between the predicted bounding box and the target bounding box, divided by the
          area of their union (see Figure 14-23). In tf.keras, it is implemented by the
          tf.keras.metrics.MeanIoU class.                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-23. Intersection over Union (IoU) metric for bounding boxes
                                                                      
          Classifying and localizing a single object is nice, but what if the images contain multi‐
          ple objects (as is often the case in the flowers dataset)?  
                                                                      
          Object Detection                                            
                                                                      
          The task of classifying and localizing multiple objects in an image is called object
          detection. Until a few years ago, a common approach was to take a CNN that was
          trained to classify and locate a single object, then slide it across the image, as shown
          in Figure 14-24. In this example, the image was chopped into a 6 × 8 grid, and we
          show a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the
          CNN was looking at the top left of the image, it detected part of the leftmost rose, and
          then it detected that same rose again when it was first shifted one step to the right. At"|object detection
"                                                                      
                                                                      
                                                                      
                                                                      
          For example, let’s use the nnlm-en-dim50 sentence embedding module, version 1, in
          our sentiment analysis model:                               
                                                                      
            import tensorflow_hub as hub                              
            model = keras.Sequential([                                
               hub.KerasLayer(""https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1"",
                       dtype=tf.string, input_shape=[], output_shape=[50]),
               keras.layers.Dense(128, activation=""relu""),            
               keras.layers.Dense(1, activation=""sigmoid"")            
            ])                                                        
            model.compile(loss=""binary_crossentropy"", optimizer=""adam"",
                    metrics=[""accuracy""])                             
          The hub.KerasLayer layer downloads the module from the given URL. This particu‐
          lar module is a sentence encoder: it takes strings as input and encodes each one as a
          single vector (in this case, a 50-dimensional vector). Internally, it parses the string
          (splitting words on spaces) and embeds each word using an embedding matrix that
          was pretrained on a huge corpus: the Google News 7B corpus (seven billion words
          long!). Then it computes the mean of all the word embeddings, and the result is the
          sentence embedding.9 We can then add two simple Dense layers to create a good sen‐
          timent analysis model. By default, a hub.KerasLayer is not trainable, but you can set
          trainable=True when creating it to change that so that you can fine-tune it for your
          task.                                                       
                   Not all TF Hub modules support TensorFlow 2, so make sure you
                   choose a module that does.                         
                                                                      
                                                                      
                                                                      
          Next, we can just load the IMDb reviews dataset—no need to preprocess it (except for
          batching and prefetching)—and directly train the model:     
            datasets, info = tfds.load(""imdb_reviews"", as_supervised=True, with_info=True)
            train_size = info.splits[""train""].num_examples            
            batch_size = 32                                           
            train_set = datasets[""train""].batch(batch_size).prefetch(1)
            history = model.fit(train_set, epochs=5)                  
          Note that the last part of the TF Hub module URL specified that we wanted version 1
          of the model. This versioning ensures that if a new module version is released, it will
          not break our model. Conveniently, if you just enter this URL in a web browser, you
                                                                      
                                                                      
          9 To be precise, the sentence embedding is equal to the mean word embedding multiplied by the square root of
           the number of words in the sentence. This compensates for the fact that the mean of n vectors gets shorter as
           n grows.                                                   "|sentence encoders; Google News 7B corpus
"                                                                      
                                                                      
                                                                      
                                                                      
          pling is performed with replacement, this method is called bagging1 (short for boot‐
          strap aggregating2). When sampling is performed without replacement, it is called
          pasting.3                                                   
                                                                      
          In other words, both bagging and pasting allow training instances to be sampled sev‐
          eral times across multiple predictors, but only bagging allows training instances to be
          sampled several times for the same predictor. This sampling and training process is
          represented in Figure 7-4.                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-4. Bagging and pasting involves training several predictors on different random
          samples of the training set                                 
          Once all predictors are trained, the ensemble can make a prediction for a new
          instance by simply aggregating the predictions of all predictors. The aggregation
          function is typically the statistical mode (i.e., the most frequent prediction, just like a
          hard voting classifier) for classification, or the average for regression. Each individual
          predictor has a higher bias than if it were trained on the original training set, but
          aggregation reduces both bias and variance.4 Generally, the net result is that the
          ensemble has a similar bias but a lower variance than a single predictor trained on the
          original training set.                                      
                                                                      
                                                                      
                                                                      
                                                                      
          1 Leo Breiman, “Bagging Predictors,” Machine Learning 24, no. 2 (1996): 123–140.
          2 In statistics, resampling with replacement is called bootstrapping.
          3 Leo Breiman, “Pasting Small Votes for Classification in Large Databases and On-Line,” Machine Learning 36,
           no. 1–2 (1999): 85–103.                                    
          4 Bias and variance were introduced in Chapter 4.           "|statistical mode
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Download the Data                                           
                                                                      
          In typical environments your data would be available in a relational database (or
          some other common data store) and spread across multiple tables/documents/files.
          To access it, you would first need to get your credentials and access authorizations10
          and familiarize yourself with the data schema. In this project, however, things are
          much simpler: you will just download a single compressed file, housing.tgz, which
          contains a comma-separated values (CSV) file called housing.csv with all the data.
          You could use your web browser to download the file and run tar xzf housing.tgz
          to decompress it and extract the CSV file, but it is preferable to create a small func‐
          tion to do that. Having a function that downloads the data is useful in particular if the
          data changes regularly: you can write a small script that uses the function to fetch the
          latest data (or you can set up a scheduled job to do that automatically at regular inter‐
          vals). Automating the process of fetching the data is also useful if you need to install
          the dataset on multiple machines.                           
          Here is the function to fetch the data:11                   
                                                                      
            import os                                                 
            import tarfile                                            
            import urllib                                             
            DOWNLOAD_ROOT = ""https://raw.githubusercontent.com/ageron/handson-ml2/master/""
            HOUSING_PATH = os.path.join(""datasets"", ""housing"")        
            HOUSING_URL = DOWNLOAD_ROOT + ""datasets/housing/housing.tgz""
            def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
               os.makedirs(housing_path, exist_ok=True)               
               tgz_path = os.path.join(housing_path, ""housing.tgz"")   
               urllib.request.urlretrieve(housing_url, tgz_path)      
               housing_tgz = tarfile.open(tgz_path)                   
               housing_tgz.extractall(path=housing_path)              
               housing_tgz.close()                                    
          Now when you call fetch_housing_data(), it creates a datasets/housing directory in
          your workspace, downloads the housing.tgz file, and extracts the housing.csv file from
          it in this directory.                                       
                                                                      
                                                                      
                                                                      
                                                                      
          10 You might also need to check legal constraints, such as private fields that should never be copied to unsafe
           data stores.                                               
          11 In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter
           notebook.                                                  "|downloading
"                                                                      
                                                                      
                                                                      
                                                                      
          Batch learning                                              
                                                                      
          In batch learning, the system is incapable of learning incrementally: it must be trained
          using all the available data. This will generally take a lot of time and computing
          resources, so it is typically done offline. First the system is trained, and then it is
          launched into production and runs without learning anymore; it just applies what it
          has learned. This is called offline learning.               
          If you want a batch learning system to know about new data (such as a new type of
          spam), you need to train a new version of the system from scratch on the full dataset
          (not just the new data, but also the old data), then stop the old system and replace it
          with the new one.                                           
          Fortunately, the whole process of training, evaluating, and launching a Machine
          Learning system can be automated fairly easily (as shown in Figure 1-3), so even a
          batch learning system can adapt to change. Simply update the data and train a new
          version of the system from scratch as often as needed.      
                                                                      
          This solution is simple and often works fine, but training using the full set of data can
          take many hours, so you would typically train a new system only every 24 hours or
          even just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐
          dict stock prices), then you need a more reactive solution. 
          Also, training on the full set of data requires a lot of computing resources (CPU,
          memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and
          you automate your system to train from scratch every day, it will end up costing you a
          lot of money. If the amount of data is huge, it may even be impossible to use a batch
          learning algorithm.                                         
          Finally, if your system needs to be able to learn autonomously and it has limited
          resources (e.g., a smartphone application or a rover on Mars), then carrying around
          large amounts of training data and taking up a lot of resources to train for hours
          every day is a showstopper.                                 
                                                                      
          Fortunately, a better option in all these cases is to use algorithms that are capable of
          learning incrementally.                                     
          Online learning                                             
                                                                      
          In online learning, you train the system incrementally by feeding it data instances
          sequentially, either individually or in small groups called mini-batches. Each learning
          step is fast and cheap, so the system can learn about new data on the fly, as it arrives
          (see Figure 1-13).                                          
                                                                      
                                                                      
                                                                      
                                                                      "|online learning; offline learning; batch learning; mini-batches
"                                                                      
                                                                      
                                                                      
                                                                      
          on random portions of text at each iteration, without any information on the rest of
          the text), then we will build a stateful RNN (which preserves the hidden state between
          training iterations and continues reading where it left off, allowing it to learn longer
          patterns). Next, we will build an RNN to perform sentiment analysis (e.g., reading
          movie reviews and extracting the rater’s feeling about the movie), this time treating
          sentences as sequences of words, rather than characters. Then we will show how
          RNNs can be used to build an Encoder–Decoder architecture capable of performing
          neural machine translation (NMT). For this, we will use the seq2seq API provided by
          the TensorFlow Addons project.                              
          In the second part of this chapter, we will look at attention mechanisms. As their name
          suggests, these are neural network components that learn to select the part of the
          inputs that the rest of the model should focus on at each time step. First we will see
          how to boost the performance of an RNN-based Encoder–Decoder architecture using
          attention, then we will drop RNNs altogether and look at a very successful attention-
          only architecture called the Transformer. Finally, we will take a look at some of the
          most important advances in NLP in 2018 and 2019, including incredibly powerful
          language models such as GPT-2 and BERT, both based on Transformers.
                                                                      
          Let’s start with a simple and fun model that can write like Shakespeare (well, sort of).
          Generating Shakespearean Text Using a Character RNN         
                                                                      
                                                                      
          In a famous 2015 blog post titled “The Unreasonable Effectiveness of Recurrent Neu‐
          ral Networks,” Andrej Karpathy showed how to train an RNN to predict the next
          character in a sentence. This Char-RNN can then be used to generate novel text, one
          character at a time. Here is a small sample of the text generated by a Char-RNN
          model after it was trained on all of Shakespeare’s work:    
            PANDARUS:                                                 
            Alas, I think he shall be come approached and the day     
            When little srain would be attain’d into being never fed, 
            And who is but a chain and subjects of his death,         
                                                                      
            I should not sleep.                                       
          Not exactly a masterpiece, but it is still impressive that the model was able to learn
          words, grammar, proper punctuation, and more, just by learning to predict the next
          character in a sentence. Let’s look at how to build a Char-RNN, step by step, starting
          with the creation of the dataset.                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|text generation; generating text using character RNNS; sentiment analysis; attention mechanism; character RNNs (Char-RNNs); generating text using character RNNs
"                                                                      
                                                                      
                                                                      
                                                                      
          Anomaly Detection Using Gaussian Mixtures                   
                                                                      
          Anomaly detection (also called outlier detection) is the task of detecting instances that
          deviate strongly from the norm. These instances are called anomalies, or outliers,
          while the normal instances are called inliers. Anomaly detection is useful in a wide
          variety of applications, such as fraud detection, detecting defective products in manu‐
          facturing, or removing outliers from a dataset before training another model (which
          can significantly improve the performance of the resulting model).
          Using a Gaussian mixture model for anomaly detection is quite simple: any instance
          located in a low-density region can be considered an anomaly. You must define what
          density threshold you want to use. For example, in a manufacturing company that
          tries to detect defective products, the ratio of defective products is usually well
          known. Say it is equal to 4%. You then set the density threshold to be the value that
          results in having 4% of the instances located in areas below that threshold density. If
          you notice that you get too many false positives (i.e., perfectly good products that are
          flagged as defective), you can lower the threshold. Conversely, if you have too many
          false negatives (i.e., defective products that the system does not flag as defective), you
          can increase the threshold. This is the usual precision/recall trade-off (see Chapter 3).
          Here is how you would identify the outliers using the fourth percentile lowest density
          as the threshold (i.e., approximately 4% of the instances will be flagged as anomalies):
            densities = gm.score_samples(X)                           
            density_threshold = np.percentile(densities, 4)           
            anomalies = X[densities < density_threshold]              
          Figure 9-19 represents these anomalies as stars.            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-19. Anomaly detection using a Gaussian mixture model
                                                                      
                                                                      
                                                                      "|using Gaussian Mixtures; Gaussian mixture model (GMM); outlier detection; inliers
"                                                                      
                                                                      
                                                                      
                                                                      
          reduce the computational load, the memory usage, and the number of parameters
          (thereby limiting the risk of overfitting).                 
                                                                      
          Just like in convolutional layers, each neuron in a pooling layer is connected to the
          outputs of a limited number of neurons in the previous layer, located within a small
          rectangular receptive field. You must define its size, the stride, and the padding type,
          just like before. However, a pooling neuron has no weights; all it does is aggregate the
          inputs using an aggregation function such as the max or mean. Figure 14-8 shows a
          max pooling layer, which is the most common type of pooling layer. In this example,
          we use a 2 × 2 pooling kernel,9 with a stride of 2 and no padding. Only the max input
          value in each receptive field makes it to the next layer, while the other inputs are
          dropped. For example, in the lower-left receptive field in Figure 14-8, the input values
          are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the
          stride of 2, the output image has half the height and half the width of the input image
          (rounded down since we use no padding).                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)
                                                                      
                   A pooling layer typically works on every input channel independ‐
                   ently, so the output depth is the same as the input depth.
                                                                      
                                                                      
                                                                      
          Other than reducing computations, memory usage, and the number of parameters, a
          max pooling layer also introduces some level of invariance to small translations, as
          shown in Figure 14-9. Here we assume that the bright pixels have a lower value than
          dark pixels, and we consider three images (A, B, C) going through a max pooling
          layer with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but
                                                                      
                                                                      
                                                                      
          9 Other kernels we’ve discussed so far had weights, but pooling kernels do not: they are just stateless sliding
           windows.                                                   "|max pooling layer; invariance; pooling kernel
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-7. SVM classifiers with a polynomial kernel        
                                                                      
                   A common approach to finding the right hyperparameter values is
                   to use grid search (see Chapter 2). It is often faster to first do a very
                   coarse grid search, then a finer grid search around the best values
                   found. Having a good sense of what each hyperparameter actually
                   does can also help you search in the right part of the hyperparame‐
                   ter space.                                         
          Similarity Features                                         
                                                                      
          Another technique to tackle nonlinear problems is to add features computed using a
          similarity function, which measures how much each instance resembles a particular
          landmark. For example, let’s take the 1D dataset discussed earlier and add two land‐
          marks to it at x = –2 and x = 1 (see the left plot in Figure 5-8). Next, let’s define the
                   1      1                                           
          similarity function to be the Gaussian Radial Basis Function (RBF) with γ = 0.3 (see
          Equation 5-1).                                              
            Equation 5-1. Gaussian RBF                                
                                                                      
            ϕ x,ℓ = exp                                               
                     −γ∥x−ℓ∥2                                         
             γ                                                        
          This is a bell-shaped function varying from 0 (very far away from the landmark) to 1
          (at the landmark). Now we are ready to compute the new features. For example, let’s
          look at the instance x = –1: it is located at a distance of 1 from the first landmark and
                      1                                               
          2 from the second landmark. Therefore its new features are x = exp(–0.3 × 12) ≈ 0.74
                                             2                        
          and x = exp(–0.3 × 22) ≈ 0.30. The plot on the right in Figure 5-8 shows the trans‐
             3                                                        
          formed dataset (dropping the original features). As you can see, it is now linearly
          separable.                                                  "|Radial Basis Function (RBF); Gaussian Radial Basis Function (RBF); landmarks; similarity functions
"                                                                      
                                                                      
                                                                      
                                                                      
          distances and the indices of the k nearest neighbors in the training set (two matrices,
          each with k columns):                                       
                                                                      
            >>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
            >>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
            >>> y_pred[y_dist > 0.2] = -1                             
            >>> y_pred.ravel()                                        
            array([-1, 0, 1, -1])                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-15. Decision boundary between two clusters         
                                                                      
          In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any
          number of clusters of any shape. It is robust to outliers, and it has just two hyperpara‐
          meters (eps and min_samples). If the density varies significantly across the clusters,
          however, it can be impossible for it to capture all the clusters properly. Its computa‐
          tional complexity is roughly O(m log m), making it pretty close to linear with regard
          to the number of instances, but Scikit-Learn’s implementation can require up to
          O(m2) memory if eps is large.                               
                                                                      
                   You may also want to try Hierarchical DBSCAN (HDBSCAN),
                   which is implemented in the scikit-learn-contrib project.
                                                                      
                                                                      
                                                                      
          Other Clustering Algorithms                                 
                                                                      
          Scikit-Learn implements several more clustering algorithms that you should take a
          look at. We cannot cover them all in detail here, but here is a brief overview:
          Agglomerative clustering                                    
            A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles
            floating on water and gradually attaching to each other until there’s one big group
            of bubbles. Similarly, at each iteration, agglomerative clustering connects the
            nearest pair of clusters (starting with individual instances). If you drew a tree
                                                                      "|agglomerative clustering; clustering algorithms; additional algorithms; Hierarchical DBSCAN (HDBSCAN)
"                                                                      
                                                                      
                                                                      
                                                                      
          Attention Mechanisms                                        
                                                                      
          Consider the path from the word “milk” to its translation “lait” in Figure 16-3: it is
          quite long! This means that a representation of this word (along with all the other
          words) needs to be carried over many steps before it is actually used. Can’t we make
          this path shorter?                                          
                                                                      
          This was the core idea in a groundbreaking 2014 paper13 by Dzmitry Bahdanau et al.
          They introduced a technique that allowed the decoder to focus on the appropriate
          words (as encoded by the encoder) at each time step. For example, at the time step
          where the decoder needs to output the word “lait,” it will focus its attention on the
          word “milk.” This means that the path from an input word to its translation is now
          much shorter, so the short-term memory limitations of RNNs have much less impact.
          Attention mechanisms revolutionized neural machine translation (and NLP in gen‐
          eral), allowing a significant improvement in the state of the art, especially for long
          sentences (over 30 words).14                                
          Figure 16-6 shows this model’s architecture (slightly simplified, as we will see). On the
          left, you have the encoder and the decoder. Instead of just sending the encoder’s final
          hidden state to the decoder (which is still done, although it is not shown in the fig‐
          ure), we now send all of its outputs to the decoder. At each time step, the decoder’s
          memory cell computes a weighted sum of all these encoder outputs: this determines
          which words it will focus on at this step. The weight α is the weight of the ith
                                            (t,i)                     
          encoder output at the tth decoder time step. For example, if the weight α is much
                                                     (3,2)            
          larger than the weights α and α , then the decoder will pay much more attention
                        (3,0) (3,1)                                   
          to word number 2 (“milk”) than to the other two words, at least at this time step. The
          rest of the decoder works just like earlier: at each time step the memory cell receives
          the inputs we just discussed, plus the hidden state from the previous time step, and
          finally (although it is not represented in the diagram) it receives the target word from
          the previous time step (or at inference time, the output from the previous time step).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          13 Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate,” arXiv pre‐
           print arXiv:1409.0473 (2014).                              
          14 The most common metric used in NMT is the BiLingual Evaluation Understudy (BLEU) score, which com‐
           pares each translation produced by the model with several good translations produced by humans: it counts
           the number of n-grams (sequences of n words) that appear in any of the target translations and adjusts the
           score to take into account the frequency of the produced n-grams in the target translations."|attention mechanism; attention mechanisms
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-6. Creating a new model version on Google Cloud AI Platform
                                                                      
          Congratulations, you have deployed your first model on the cloud! Because you
          selected automatic scaling, AI Platform will start more TF Serving containers when
          the number of queries per second increases, and it will load-balance the queries
          between them. If the QPS goes down, it will stop containers automatically. The cost is
          therefore directly linked to the QPS (as well as the type of machine you choose and
          the amount of data you store on GCS). This pricing model is particularly useful for
          occasional users and for services with important usage spikes, as well as for startups:
          the price remains low until the startup actually starts up. 
                                                                      
                   If you do not use the prediction service, AI Platform will stop all
                   containers. This means you will only pay for the amount of storage
                   you use (a few cents per gigabyte per month). Note that when you
                   query the service, AI Platform will need to start up a TF Serving
                   container, which will take a few seconds. If this delay is unaccepta‐
                   ble, you will have to set the minimum number of TF Serving con‐
                   tainers to 1 when creating the model version. Of course, this means
                   at least one machine will run constantly, so the monthly fee will be
                   higher.                                            
          Now let’s query this prediction service!                    
                                                                      
                                                                      
                                                                      
                                                                      "|prediction service creation; creating on GCP AI
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-7. Visual attention: an input image (left) and the model’s focus before produc‐
          ing the word “frisbee” (right)18                            
                                                                      
                                                                      
                              Explainability                          
           One extra benefit of attention mechanisms is that they make it easier to understand
           what led the model to produce its output. This is called explainability. It can be espe‐
           cially useful when the model makes a mistake: for example, if an image of a dog walk‐
           ing in the snow is labeled as “a wolf walking in the snow,” then you can go back and
           check what the model focused on when it output the word “wolf.” You may find that it
           was paying attention not only to the dog, but also to the snow, hinting at a possible
           explanation: perhaps the way the model learned to distinguish dogs from wolves is by
           checking whether or not there’s a lot of snow around. You can then fix this by training
           the model with more images of wolves without snow, and dogs with snow. This exam‐
           ple comes from a great 2016 paper19 by Marco Tulio Ribeiro et al. that uses a different
           approach to explainability: learning an interpretable model locally around a classi‐
           fier’s prediction.                                         
           In some applications, explainability is not just a tool to debug a model; it can be a
           legal requirement (think of a system deciding whether or not it should grant you a
           loan).                                                     
                                                                      
                                                                      
                                                                      
                                                                      
          18 This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors.
          19 Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” Proceed‐
           ings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):
           1135–1144.                                                 "|explainability; explainability and
"                                                                      
                                                                      
                                                                      
                                                                      
          The dashed lines represent the points where the decision function is equal to 1 or –1:
          they are parallel and at equal distance to the decision boundary, and they form a mar‐
          gin around it. Training a linear SVM classifier means finding the values of w and b
          that make this margin as wide as possible while avoiding margin violations (hard
          margin) or limiting them (soft margin).                     
                                                                      
          Training Objective                                          
                                                                      
          Consider the slope of the decision function: it is equal to the norm of the weight vec‐
          tor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal
          to ±1 are going to be twice as far away from the decision boundary. In other words,
          dividing the slope by 2 will multiply the margin by 2. This may be easier to visualize
          in 2D, as shown in Figure 5-13. The smaller the weight vector w, the larger the
          margin.                                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-13. A smaller weight vector results in a larger margin
                                                                      
          So we want to minimize ∥ w ∥ to get a large margin. If we also want to avoid any
          margin violations (hard margin), then we need the decision function to be greater
          than 1 for all positive training instances and lower than –1 for negative training
          instances. If we define t(i) = –1 for negative instances (if y(i) = 0) and t(i) = 1 for positive
          instances (if y(i) = 1), then we can express this constraint as t(i)(w⊺ x(i) + b) ≥ 1 for all
          instances.                                                  
          We can therefore express the hard margin linear SVM classifier objective as the con‐
          strained optimization problem in Equation 5-3.              
                                                                      
            Equation 5-3. Hard margin linear SVM classifier objective 
                    1 ⊺                                               
             minimize w w                                             
              w,b   2                                                 
             subject to t i w ⊺ x i +b ≥1 for i=1,2,⋯,m               
                                                                      
                                                                      "|training objective; constrained optimization
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Curiosity-based exploration27                               
            A recurring problem in RL is the sparsity of the rewards, which makes learning
            very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have
            proposed an exciting way to tackle this issue: why not ignore the rewards, and
            just make the agent extremely curious to explore the environment? The rewards
            thus become intrinsic to the agent, rather than coming from the environment.
            Similarly, stimulating curiosity in a child is more likely to give good results than
            purely rewarding the child for getting good grades. How does this work? The
            agent continuously tries to predict the outcome of its actions, and it seeks situa‐
            tions where the outcome does not match its predictions. In other words, it wants
            to be surprised. If the outcome is predictable (boring), it goes elsewhere. How‐
            ever, if the outcome is unpredictable but the agent notices that it has no control
            over it, it also gets bored after a while. With only curiosity, the authors succeeded
            in training an agent at many video games: even though the agent gets no penalty
            for losing, the game starts over, which is boring so it learns to avoid it.
          We covered many topics in this chapter: Policy Gradients, Markov chains, Markov
          decision processes, Q-Learning, Approximate Q-Learning, and Deep Q-Learning and
          its main variants (fixed Q-Value targets, Double DQN, Dueling DQN, and prioritized
          experience replay). We discussed how to use TF-Agents to train agents at scale, and
          finally we took a quick look at a few other popular algorithms. Reinforcement Learn‐
          ing is a huge and exciting field, with new ideas and algorithms popping out every day,
          so I hope this chapter sparked your curiosity: there is a whole world to explore!
          Exercises                                                   
                                                                      
                                                                      
           1. How would you define Reinforcement Learning? How is it different from regular
            supervised or unsupervised learning?                      
           2. Can you think of three possible applications of RL that were not mentioned in
            this chapter? For each of them, what is the environment? What is the agent?
            What are some possible actions? What are the rewards?     
           3. What is the discount factor? Can the optimal policy change if you modify the dis‐
            count factor?                                             
                                                                      
           4. How do you measure the performance of a Reinforcement Learning agent?
           5. What is the credit assignment problem? When does it occur? How can you allevi‐
            ate it?                                                   
           6. What is the point of using a replay buffer?             
                                                                      
                                                                      
          27 Deepak Pathak et al., “Curiosity-Driven Exploration by Self-Supervised Prediction,” Proceedings of the 34th
           International Conference on Machine Learning (2017): 2778–2787."|curiosity-based exploration
"                                                                      
                                                                      
                                                                      
                                                                      
          Now let’s create the stateful RNN. First, we need to set stateful=True when creating
          every recurrent layer. Second, the stateful RNN needs to know the batch size (since it
          will preserve a state for each input sequence in the batch), so we must set the
          batch_input_shape argument in the first layer. Note that we can leave the second
          dimension unspecified, since the inputs could have any length:
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.GRU(128, return_sequences=True, stateful=True,
                         dropout=0.2, recurrent_dropout=0.2,          
                         batch_input_shape=[batch_size, None, max_id]),
               keras.layers.GRU(128, return_sequences=True, stateful=True,
                         dropout=0.2, recurrent_dropout=0.2),         
               keras.layers.TimeDistributed(keras.layers.Dense(max_id,
                                           activation=""softmax""))     
            ])                                                        
          At the end of each epoch, we need to reset the states before we go back to the begin‐
          ning of the text. For this, we can use a small callback:    
            class ResetStatesCallback(keras.callbacks.Callback):      
               def on_epoch_begin(self, epoch, logs):                 
                 self.model.reset_states()                            
          And now we can compile and fit the model (for more epochs, because each epoch is
          much shorter than earlier, and there is only one instance per batch):
            model.compile(loss=""sparse_categorical_crossentropy"", optimizer=""adam"")
            model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])
                   After this model is trained, it will only be possible to use it to make
                   predictions for batches of the same size as were used during train‐
                   ing. To avoid this restriction, create an identical stateless model,
                   and copy the stateful model’s weights to this model.
                                                                      
          Now that we have built a character-level model, it’s time to look at word-level models
          and tackle a common natural language processing task: sentiment analysis. In the pro‐
          cess we will learn how to handle sequences of variable lengths using masking.
          Sentiment Analysis                                          
                                                                      
          If MNIST is the “hello world” of computer vision, then the IMDb reviews dataset is
          the “hello world” of natural language processing: it consists of 50,000 movie reviews
          in English (25,000 for training, 25,000 for testing) extracted from the famous Internet
          Movie Database, along with a simple binary target for each review indicating whether
          it is negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular
          for good reasons: it is simple enough to be tackled on a laptop in a reasonable amount
                                                                      
                                                                      "|generating text using character RNNS; sentiment analysis; Internet Movie Database; generating text using character RNNs
"                                                                      
                                                                      
                                                                      
                                                                      
          run on this GPU. The list_physical_devices() function returns the list of all avail‐
          able GPU devices (just one in this example).11              
                                                                      
          Now, what if you don’t want to invest time and money in getting your own GPU
          card? Just use a GPU VM on the cloud!                       
          Using a GPU-Equipped Virtual Machine                        
                                                                      
          All major cloud platforms now offer GPU VMs, some preconfigured with all the driv‐
          ers and libraries you need (including TensorFlow). Google Cloud Platform enforces
          various GPU quotas, both worldwide and per region: you cannot just create thou‐
          sands of GPU VMs without prior authorization from Google.12 By default, the world‐
          wide GPU quota is zero, so you cannot use any GPU VMs. Therefore, the very first
          thing you need to do is to request a higher worldwide quota. In the GCP console,
          open the navigation menu and go to IAM & admin → Quotas. Click Metric, click
          None to uncheck all locations, then search for “GPU” and select “GPUs (all regions)”
          to see the corresponding quota. If this quota’s value is zero (or just insufficient for
          your needs), then check the box next to it (it should be the only selected one) and
          click “Edit quotas.” Fill in the requested information, then click “Submit request.” It
          may take a few hours (or up to a few days) for your quota request to be processed and
          (generally) accepted. By default, there is also a quota of one GPU per region and per
          GPU type. You can request to increase these quotas too: click Metric, select None to
          uncheck all metrics, search for “GPU,” and select the type of GPU you want (e.g.,
          NVIDIA P4 GPUs). Then click the Location drop-down menu, click None to
          uncheck all metrics, and click the location you want; check the boxes next to the
          quota(s) you want to change, and click “Edit quotas” to file a request.
          Once your GPU quota requests are approved, you can in no time create a VM equip‐
          ped with one or more GPUs by using Google Cloud AI Platform’s Deep Learning VM
          Images: go to https://homl.info/dlvm, click View Console, then click “Launch on Com‐
          pute Engine” and fill in the VM configuration form. Note that some locations do not
          have all types of GPUs, and some have no GPUs at all (change the location to see the
          types of GPUs available, if any). Make sure to select TensorFlow 2.0 as the framework,
          and check “Install NVIDIA GPU driver automatically on first startup.” It is also a
          good idea to check “Enable access to JupyterLab via URL instead of SSH”: this will
          make it very easy to start a Jupyter notebook running on this GPU VM, powered by
                                                                      
                                                                      
                                                                      
          11 Many code examples in this chapter use experimental APIs. They are very likely to be moved to the core API
           in future versions. So if an experimental function fails, try simply removing the word experimental, and
           hopefully it will work. If not, then perhaps the API has changed a bit; please check the Jupyter notebook, as I
           will ensure it contains the correct code.                  
          12 Presumably, these quotas are meant to stop bad guys who might be tempted to use GCP with stolen credit
           cards to mine cryptocurrencies.                            "|Deep Learning VM Images; GPU-equipped virtual machines; JupyterLab
"                                                                      
                                                                      
                                                                      
                                                                      
          placement request fails, TensorFlow will fall back to its default placement rules (i.e.,
          GPU 0 by default if it exists and there is a GPU kernel, and CPU 0 otherwise).
                                                                      
          Now how exactly will TensorFlow execute all these operations across multiple
          devices?                                                    
          Parallel Execution Across Multiple Devices                  
                                                                      
          As we saw in Chapter 12, one of the benefits of using TF Functions is parallelism.
          Let’s look at this a bit more closely. When TensorFlow runs a TF Function, it starts by
          analyzing its graph to find the list of operations that need to be evaluated, and it
          counts how many dependencies each of them has. TensorFlow then adds each opera‐
          tion with zero dependencies (i.e., each source operation) to the evaluation queue of
          this operation’s device (see Figure 19-14). Once an operation has been evaluated, the
          dependency counter of each operation that depends on it is decremented. Once an
          operation’s dependency counter reaches zero, it is pushed to the evaluation queue of
          its device. And once all the nodes that TensorFlow needs have been evaluated, it
          returns their outputs.                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-14. Parallelized execution of a TensorFlow graph  
                                                                      
          Operations in the CPU’s evaluation queue are dispatched to a thread pool called the
          inter-op thread pool. If the CPU has multiple cores, then these operations will effec‐
          tively be evaluated in parallel. Some operations have multithreaded CPU kernels:
          these kernels split their tasks into multiple suboperations, which are placed in
          another evaluation queue and dispatched to a second thread pool called the intra-op
                                                                      "|inter-op thread pool; parallel execution across multiple devices; intra-op thread pool
"                                                                      
                                                                      
                                                                      
                                                                      
          each ID always refers to the same GPU card. For example, if you have four GPU
          cards, you could start two programs, assigning two GPUs to each of them, by execut‐
          ing commands like the following in two separate terminal windows:
                                                                      
            $ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py
            # and in another terminal:                                
            $ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py
          Program 1 will then only see GPU cards 0 and 1, named /gpu:0 and /gpu:1 respec‐
          tively, and program 2 will only see GPU cards 2 and 3, named /gpu:1 and /gpu:0
          respectively (note the order). Everything will work fine (see Figure 19-12). Of course,
          you can also define these environment variables in Python by setting os.envi
          ron[""CUDA_DEVICE_ORDER""] and os.environ[""CUDA_VISIBLE_DEVICES""], as long as
          you do so before using TensorFlow.                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-12. Each program gets two GPUs                    
                                                                      
          Another option is to tell TensorFlow to grab only a specific amount of GPU RAM.
          This must be done immediately after importing TensorFlow. For example, to make
          TensorFlow grab only 2 GiB of RAM on each GPU, you must create a virtual GPU
          device (also called a logical GPU device) for each physical GPU device and set its
          memory limit to 2 GiB (i.e., 2,048 MiB):                    
            for gpu in tf.config.experimental.list_physical_devices(""GPU""):
               tf.config.experimental.set_virtual_device_configuration(
                 gpu,                                                 
                 [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])
          Now (supposing you have four GPUs, each with at least 4 GiB of RAM) two programs
          like this one can run in parallel, each using all four GPU cards (see Figure 19-13).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|logical GPU devices; virtual GPU devices
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Image #1                                                  
             n04522168 - vase 46.83%                                  
             n07930864 - cup 7.78%                                    
             n11939491 - daisy 4.87%                                  
          The correct classes (monastery and daisy) appear in the top three results for both
          images. That’s pretty good, considering that the model had to choose from among
          1,000 classes.                                              
          As you can see, it is very easy to create a pretty good image classifier using a pre‐
          trained model. Other vision models are available in keras.applications, including
          several ResNet variants, GoogLeNet variants like Inception-v3 and Xception,
          VGGNet variants, and MobileNet and MobileNetV2 (lightweight models for use in
          mobile applications).                                       
          But what if you want to use an image classifier for classes of images that are not part
          of ImageNet? In that case, you may still benefit from the pretrained models to per‐
          form transfer learning.                                     
                                                                      
          Pretrained Models for Transfer Learning                     
                                                                      
          If you want to build an image classifier but you do not have enough training data,
          then it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐
          cussed in Chapter 11. For example, let’s train a model to classify pictures of flowers,
          reusing a pretrained Xception model. First, let’s load the dataset using TensorFlow
          Datasets (see Chapter 13):                                  
            import tensorflow_datasets as tfds                        
                                                                      
            dataset, info = tfds.load(""tf_flowers"", as_supervised=True, with_info=True)
            dataset_size = info.splits[""train""].num_examples # 3670   
            class_names = info.features[""label""].names # [""dandelion"", ""daisy"", ...]
            n_classes = info.features[""label""].num_classes # 5        
          Note that you can get information about the dataset by setting with_info=True. Here,
          we get the dataset size and the names of the classes. Unfortunately, there is only a
          ""train"" dataset, no test set or validation set, so we need to split the training set. The
          TF Datasets project provides an API for this. For example, let’s take the first 10% of
          the dataset for testing, the next 15% for validation, and the remaining 75% for
          training:                                                   
            test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])
                                                                      
            test_set = tfds.load(""tf_flowers"", split=test_split, as_supervised=True)
            valid_set = tfds.load(""tf_flowers"", split=valid_split, as_supervised=True)
            train_set = tfds.load(""tf_flowers"", split=train_split, as_supervised=True)
                                                                      "|pretrained models for transfer learning; pretraining; transfer learning
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-2. An unlabeled dataset composed of five blobs of instances
          Let’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and
          assign each instance to the closest blob:                   
                                                                      
            from sklearn.cluster import KMeans                        
            k = 5                                                     
            kmeans = KMeans(n_clusters=k)                             
            y_pred = kmeans.fit_predict(X)                            
          Note that you have to specify the number of clusters k that the algorithm must find.
          In this example, it is pretty obvious from looking at the data that k should be set to 5,
          but in general it is not that easy. We will discuss this shortly.
          Each instance was assigned to one of the five clusters. In the context of clustering, an
          instance’s label is the index of the cluster that this instance gets assigned to by the
          algorithm: this is not to be confused with the class labels in classification (remember
          that clustering is an unsupervised learning task). The KMeans instance preserves a
          copy of the labels of the instances it was trained on, available via the labels_ instance
          variable:                                                   
            >>> y_pred                                                
            array([4, 0, 1, ..., 2, 1, 0], dtype=int32)               
            >>> y_pred is kmeans.labels_                              
            True                                                      
          We can also take a look at the five centroids that the algorithm found:
            >>> kmeans.cluster_centers_                               
            array([[-2.80389616, 1.80117999],                         
                [ 0.20876306, 2.25551336],                            
                [-2.79290307, 2.79641063],                            
                [-1.46679593, 2.28585348],                            
                [-2.80037642, 1.30082566]])                           
                                                                      
                                                                      
                                                                      "|labels
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                          TensorFlow in the Browser                   
                                                                      
           What if you want to use your model in a website, running directly in the user’s
           browser? This can be useful in many scenarios, such as:    
            • When your web application is often used in situations where the user’s connec‐
              tivity is intermittent or slow (e.g., a website for hikers), so running the model
              directly on the client side is the only way to make your website reliable.
            • When you need the model’s responses to be as fast as possible (e.g., for an online
              game). Removing the need to query the server to make predictions will definitely
              reduce the latency and make the website much more responsive.
            • When your web service makes predictions based on some private user data, and
              you want to protect the user’s privacy by making the predictions on the client
              side so that the private data never has to leave the user’s machine.9
                                                                      
           For all these scenarios, you can export your model to a special format that can be
           loaded by the TensorFlow.js JavaScript library. This library can then use your model
           to make predictions directly in the user’s browser. The TensorFlow.js project includes
           a tensorflowjs_converter tool that can convert a TensorFlow SavedModel or a
           Keras model file to the TensorFlow.js Layers format: this is a directory containing a set
           of sharded weight files in binary format and a model.json file that describes the mod‐
           el’s architecture and links to the weight files. This format is optimized to be downloa‐
           ded efficiently on the web. Users can then download the model and run predictions in
           the browser using the TensorFlow.js library. Here is a code snippet to give you an idea
           of what the JavaScript API looks like:                     
             import * as tf from '@tensorflow/tfjs';                  
             const model = await tf.loadLayersModel('https://example.com/tfjs/model.json');
             const image = tf.fromPixels(webcamElement);              
             const prediction = model.predict(image);                 
           Once again, doing justice to this topic would require a whole book. If you want to
           learn more about TensorFlow.js, check out the O’Reilly book Practical Deep Learning
           for Cloud, Mobile, and Edge, by Anirudh Koul, Siddha Ganju, and Meher Kasam.
                                                                      
          Next, we will see how to use GPUs to speed up computations! 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          9 If you’re interested in this topic, check out federated learning."|deploying to mobile and embedded devices
"                                                                      
                                                                      
                                                                      
                                                                      
          AdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let’s start with Ada‐
          Boost.                                                      
                                                                      
          AdaBoost                                                    
                                                                      
          One way for a new predictor to correct its predecessor is to pay a bit more attention
          to the training instances that the predecessor underfitted. This results in new predic‐
          tors focusing more and more on the hard cases. This is the technique used by
          AdaBoost.                                                   
          For example, when training an AdaBoost classifier, the algorithm first trains a base
          classifier (such as a Decision Tree) and uses it to make predictions on the training set.
          The algorithm then increases the relative weight of misclassified training instances.
          Then it trains a second classifier, using the updated weights, and again makes predic‐
          tions on the training set, updates the instance weights, and so on (see Figure 7-7).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-7. AdaBoost sequential training with instance weight updates
                                                                      
          Figure 7-8 shows the decision boundaries of five consecutive predictors on the
          moons dataset (in this example, each predictor is a highly regularized SVM classifier
          with an RBF kernel14). The first classifier gets many instances wrong, so their weights
                                                                      
                                                                      
          13 Yoav Freund and Robert E. Schapire, “A Decision-Theoretic Generalization of On-Line Learning and an
           Application to Boosting,” Journal of Computer and System Sciences 55, no. 1 (1997): 119–139.
          14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost; they are slow
           and tend to be unstable with it.                           "|AdaBoost classifiers; Adaptive Boosting; AdaBoost
"                                                                      
                                                                      
                                                                      
                                                                      
          PCA (kPCA).6 It is often good at preserving clusters of instances after projection, or
          sometimes even unrolling datasets that lie close to a twisted manifold.
                                                                      
          The following code uses Scikit-Learn’s KernelPCA class to perform kPCA with an RBF
          kernel (see Chapter 5 for more details about the RBF kernel and other kernels):
            from sklearn.decomposition import KernelPCA               
                                                                      
            rbf_pca = KernelPCA(n_components = 2, kernel=""rbf"", gamma=0.04)
            X_reduced = rbf_pca.fit_transform(X)                      
          Figure 8-10 shows the Swiss roll, reduced to two dimensions using a linear kernel
          (equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-10. Swiss roll reduced to 2D using kPCA with various kernels
                                                                      
          Selecting a Kernel and Tuning Hyperparameters               
                                                                      
          As kPCA is an unsupervised learning algorithm, there is no obvious performance
          measure to help you select the best kernel and hyperparameter values. That said,
          dimensionality reduction is often a preparation step for a supervised learning task
          (e.g., classification), so you can use grid search to select the kernel and hyperparame‐
          ters that lead to the best performance on that task. The following code creates a two-
          step pipeline, first reducing dimensionality to two dimensions using kPCA, then
          applying Logistic Regression for classification. Then it uses GridSearchCV to find the
          best kernel and gamma value for kPCA in order to get the best classification accuracy
          at the end of the pipeline:                                 
            from sklearn.model_selection import GridSearchCV          
            from sklearn.linear_model import LogisticRegression       
            from sklearn.pipeline import Pipeline                     
                                                                      
                                                                      
                                                                      
          6 Bernhard Schölkopf et al., “Kernel Principal Component Analysis,” in Lecture Notes in Computer Science 1327
           (Berlin: Springer, 1997): 583–588.                         "|KernelPCA class
"                                                                      
                                                                      
                                                                      
                                                                      
          the next five file paths from the filepath_dataset and interleave them the same way,
          and so on until it runs out of file paths.                  
                                                                      
                   For interleaving to work best, it is preferable to have files of identi‐
                   cal length; otherwise the ends of the longest files will not be inter‐
                   leaved.                                            
                                                                      
                                                                      
                                                                      
          By default, interleave() does not use parallelism; it just reads one line at a time
          from each file, sequentially. If you want it to actually read files in parallel, you can set
          the num_parallel_calls argument to the number of threads you want (note that the
          map() method also has this argument). You can even set it to tf.data.experimen
          tal.AUTOTUNE to make TensorFlow choose the right number of threads dynamically
          based on the available CPU (however, this is an experimental feature for now). Let’s
          look at what the dataset contains now:                      
            >>> for line in dataset.take(5):                          
            ...  print(line.numpy())                                  
            ...                                                       
            b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'
            b'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'
            b'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'
            b'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'
            b'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'
          These are the first rows (ignoring the header row) of five CSV files, chosen randomly.
          Looks good! But as you can see, these are just byte strings; we need to parse them and
          scale the data.                                             
          Preprocessing the Data                                      
          Let’s implement a small function that will perform this preprocessing:
            X_mean, X_std = [...] # mean and scale of each feature in the training set
            n_inputs = 8                                              
                                                                      
            def preprocess(line):                                     
             defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]
             fields = tf.io.decode_csv(line, record_defaults=defs)    
             x = tf.stack(fields[:-1])                                
             y = tf.stack(fields[-1:])                                
             return (x - X_mean) / X_std, y                           
          Let’s walk through this code:                               
                                                                      
                                                                      
                                                                      "|preprocessing; preprocessing data
"                                                                      
                                                                      
                                                                      
                                                                      
                   The dataset methods do not modify datasets, they create new ones,
                   so make sure to keep a reference to these new datasets (e.g., with
                   dataset = ...), or else nothing will happen.       
                                                                      
                                                                      
          You can also transform the items by calling the map() method. For example, this cre‐
          ates a new dataset with all items doubled:                  
                                                                      
            >>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]
          This function is the one you will call to apply any preprocessing you want to your
          data. Sometimes this will include computations that can be quite intensive, such as
          reshaping or rotating an image, so you will usually want to spawn multiple threads to
          speed things up: it’s as simple as setting the num_parallel_calls argument. Note that
          the function you pass to the map() method must be convertible to a TF Function (see
          Chapter 12).                                                
          While the map() method applies a transformation to each item, the apply() method
          applies a transformation to the dataset as a whole. For example, the following code
          applies the unbatch() function to the dataset (this function is currently experimental,
          but it will most likely move to the core API in a future release). Each item in the new
          dataset will be a single-integer tensor instead of a batch of seven integers:
                                                                      
            >>> dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,...
          It is also possible to simply filter the dataset using the filter() method:
            >>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...
                                                                      
          You will often want to look at just a few items from a dataset. You can use the take()
          method for that:                                            
            >>> for item in dataset.take(3):                          
            ...  print(item)                                          
            ...                                                       
            tf.Tensor(0, shape=(), dtype=int64)                       
            tf.Tensor(2, shape=(), dtype=int64)                       
            tf.Tensor(4, shape=(), dtype=int64)                       
          Shuffling the Data                                          
          As you know, Gradient Descent works best when the instances in the training set are
          independent and identically distributed (see Chapter 4). A simple way to ensure this
          is to shuffle the instances, using the shuffle() method. It will create a new dataset
          that will start by filling up a buffer with the first items of the source dataset. Then,
          whenever it is asked for an item, it will pull one out randomly from the buffer and
          replace it with a fresh one from the source dataset, until it has iterated entirely
          through the source dataset. At this point it continues to pull out items randomly from
                                                                      "|shuffling; shuffling data
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-18. Data parallelism using the mirrored strategy  
                                                                      
          The tricky part when using this approach is to efficiently compute the mean of all the
          gradients from all the GPUs and distribute the result across all the GPUs. This can be
          done using an AllReduce algorithm, a class of algorithms where multiple nodes col‐
          laborate to efficiently perform a reduce operation (such as computing the mean, sum,
          and max), while ensuring that all nodes obtain the same final result. Fortunately,
          there are off-the-shelf implementations of such algorithms, as we will see.
          Data parallelism with centralized parameters                
                                                                      
          Another approach is to store the model parameters outside of the GPU devices per‐
          forming the computations (called workers), for example on the CPU (see
          Figure 19-19). In a distributed setup, you may place all the parameters on one or
          more CPU-only servers called parameter servers, whose only role is to host and
          update the parameters.                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|parameter servers; AllReduce algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
          correlations or new trends, and thereby lead to a better understanding of the prob‐
          lem. Applying ML techniques to dig into large amounts of data can help discover pat‐
          terns that were not immediately apparent. This is called data mining.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-4. Machine Learning can help humans learn          
                                                                      
          To summarize, Machine Learning is great for:                
                                                                      
           • Problems for which existing solutions require a lot of fine-tuning or long lists of
            rules: one Machine Learning algorithm can often simplify code and perform bet‐
            ter than the traditional approach.                        
           • Complex problems for which using a traditional approach yields no good solu‐
            tion: the best Machine Learning techniques can perhaps find a solution.
           • Fluctuating environments: a Machine Learning system can adapt to new data.
           • Getting insights about complex problems and large amounts of data.
                                                                      
                                                                      
          Examples of Applications                                    
                                                                      
          Let’s look at some concrete examples of Machine Learning tasks, along with the tech‐
          niques that can tackle them:                                
          Analyzing images of products on a production line to automatically classify them
            This is image classification, typically performed using convolutional neural net‐
            works (CNNs; see Chapter 14).                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|Machine Learning (ML)
"                                                                      
                                                                      
                                                                      
                                                                      
          Keras does not include a depthwise max pooling layer, but TensorFlow’s low-level
          Deep Learning API does: just use the tf.nn.max_pool() function, and specify the
          kernel size and strides as 4-tuples (i.e., tuples of size 4). The first three values of each
          should be 1: this indicates that the kernel size and stride along the batch, height, and
          width dimensions should be 1. The last value should be whatever kernel size and
          stride you want along the depth dimension—for example, 3 (this must be a divisor of
          the input depth; it will not work if the previous layer outputs 20 feature maps, since
          20 is not a multiple of 3):                                 
            output = tf.nn.max_pool(images,                           
                          ksize=(1, 1, 1, 3),                         
                          strides=(1, 1, 1, 3),                       
                          padding=""valid"")                            
          If you want to include this as a layer in your Keras models, wrap it in a Lambda layer
          (or create a custom Keras layer):                           
                                                                      
            depth_pool = keras.layers.Lambda(                         
               lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),
                             padding=""valid""))                        
          One last type of pooling layer that you will often see in modern architectures is the
          global average pooling layer. It works very differently: all it does is compute the mean
          of each entire feature map (it’s like an average pooling layer using a pooling kernel
          with the same spatial dimensions as the inputs). This means that it just outputs a sin‐
          gle number per feature map and per instance. Although this is of course extremely
          destructive (most of the information in the feature map is lost), it can be useful as the
          output layer, as we will see later in this chapter. To create such a layer, simply use the
          keras.layers.GlobalAvgPool2D class:                         
            global_avg_pool = keras.layers.GlobalAvgPool2D()          
          It’s equivalent to this simple Lambda layer, which computes the mean over the spatial
          dimensions (height and width):                              
            global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))
                                                                      
          Now you know all the building blocks to create convolutional neural networks. Let’s
          see how to assemble them.                                   
          CNN Architectures                                           
                                                                      
          Typical CNN architectures stack a few convolutional layers (each one generally fol‐
          lowed by a ReLU layer), then a pooling layer, then another few convolutional layers
          (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller
          as it progresses through the network, but it also typically gets deeper and deeper (i.e.,
          with more feature maps), thanks to the convolutional layers (see Figure 14-11). At the
          top of the stack, a regular feedforward neural network is added, composed of a few
                                                                      "|global average pooling layer; CNN architectures
"                                                                      
                                                                      
                                                                      
                                                                      
          datasets if you want (but of course they need to have been loaded and preprocessed
          first).                                                     
                                                                      
          If you want to build your own custom training loop (as in Chapter 12), you can just
          iterate over the training set, very naturally:              
            for X_batch, y_batch in train_set:                        
               [...] # perform one Gradient Descent step              
          In fact, it is even possible to create a TF Function (see Chapter 12) that performs the
          whole training loop:                                        
            @tf.function                                              
            def train(model, optimizer, loss_fn, n_epochs, [...]):    
               train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...])
               for X_batch, y_batch in train_set:                     
                 with tf.GradientTape() as tape:                      
                   y_pred = model(X_batch)                            
                   main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))
                   loss = tf.add_n([main_loss] + model.losses)        
                 grads = tape.gradient(loss, model.trainable_variables)
                 optimizer.apply_gradients(zip(grads, model.trainable_variables))
          Congratulations, you now know how to build powerful input pipelines using the Data
          API! However, so far we have used CSV files, which are common, simple, and conve‐
          nient but not really efficient, and do not support large or complex data structures
          (such as images or audio) very well. So let’s see how to use TFRecords instead.
                   If you are happy with CSV files (or whatever other format you are
                   using), you do not have to use TFRecords. As the saying goes, if it
                   ain’t broke, don’t fix it! TFRecords are useful when the bottleneck
                   during training is loading and parsing the data.   
                                                                      
          The TFRecord Format                                         
                                                                      
          The TFRecord format is TensorFlow’s preferred format for storing large amounts of
          data and reading it efficiently. It is a very simple binary format that just contains a
          sequence of binary records of varying sizes (each record is comprised of a length, a
          CRC checksum to check that the length was not corrupted, then the actual data, and
          finally a CRC checksum for the data). You can easily create a TFRecord file using the
          tf.io.TFRecordWriter class:                                 
            with tf.io.TFRecordWriter(""my_data.tfrecord"") as f:       
               f.write(b""This is the first record"")                   
               f.write(b""And this is the second record"")              
                                                                      
                                                                      
                                                                      "|Data API; TFRecord format; pipelines
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 5-11. SVM Regression using a second-degree polynomial kernel
                                                                      
          The following code uses Scikit-Learn’s SVR class (which supports the kernel trick) to
          produce the model represented on the left in Figure 5-11:   
            from sklearn.svm import SVR                               
            svm_poly_reg = SVR(kernel=""poly"", degree=2, C=100, epsilon=0.1)
            svm_poly_reg.fit(X, y)                                    
                                                                      
          The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is
          the regression equivalent of the LinearSVC class. The LinearSVR class scales linearly
          with the size of the training set (just like the LinearSVC class), while the SVR class gets
          much too slow when the training set grows large (just like the SVC class).
                                                                      
                   SVMs can also be used for outlier detection; see Scikit-Learn’s doc‐
                   umentation for more details.                       
                                                                      
                                                                      
                                                                      
          Under the Hood                                              
                                                                      
          This section explains how SVMs make predictions and how their training algorithms
          work, starting with linear SVM classifiers. If you are just getting started with Machine
          Learning, you can safely skip it and go straight to the exercises at the end of this chap‐
          ter, and come back later when you want to get a deeper understanding of SVMs.
          First, a word about notations. In Chapter 4 we used the convention of putting all the
          model parameters in one vector θ, including the bias term θ and the input feature
                                              0                       
          weights θ to θ , and adding a bias input x = 1 to all instances. In this chapter we will
               1  n               0                                   
          use a convention that is more convenient (and more common) when dealing with"|Machine Learning (ML)
"                                                                      
                                                                      
                                                                      
                                                                      
                   By default, recurrent layers in Keras only return the final output. To
                   make them return one output per time step, you must set
                   return_sequences=True, as we will see.             
                                                                      
                                                                      
          If you compile, fit, and evaluate this model (just like earlier, we train for 20 epochs
          using Adam), you will find that its MSE reaches only 0.014, so it is better than the
          naive approach but it does not beat a simple linear model. Note that for each neuron,
          a linear model has one parameter per input and per time step, plus a bias term (in the
          simple linear model we used, that’s a total of 51 parameters). In contrast, for each
          recurrent neuron in a simple RNN, there is just one parameter per input and per hid‐
          den state dimension (in a simple RNN, that’s just the number of recurrent neurons in
          the layer), plus a bias term. In this simple RNN, that’s a total of just three parameters.
                                                                      
                           Trend and Seasonality                      
                                                                      
           There are many other models to forecast time series, such as weighted moving average
           models or autoregressive integrated moving average (ARIMA) models. Some of them
           require you to first remove the trend and seasonality. For example, if you are studying
           the number of active users on your website, and it is growing by 10% every month,
           you would have to remove this trend from the time series. Once the model is trained
           and starts making predictions, you would have to add the trend back to get the final
           predictions. Similarly, if you are trying to predict the amount of sunscreen lotion sold
           every month, you will probably observe strong seasonality: since it sells well every
           summer, a similar pattern will be repeated every year. You would have to remove this
           seasonality from the time series, for example by computing the difference between the
           value at each time step and the value one year earlier (this technique is called differ‐
           encing). Again, after the model is trained and makes predictions, you would have to
           add the seasonal pattern back to get the final predictions.
           When using RNNs, it is generally not necessary to do all this, but it may improve per‐
           formance in some cases, since the model will not have to learn the trend or the
           seasonality.                                               
                                                                      
          Apparently our simple RNN was too simple to get good performance. So let’s try to
          add more recurrent layers!                                  
                                                                      
          Deep RNNs                                                   
          It is quite common to stack multiple layers of cells, as shown in Figure 15-7. This
          gives you a deep RNN.                                       
                                                                      
                                                                      
                                                                      "|autoregressive integrated moving average (ARIMA) models; differencing; weighted moving average model; deep RNNS
"                                                                      
                                                                      
                                                                      
                                                                      
          Next we must preprocess the images. The CNN expects 224 × 224 images, so we need
          to resize them. We also need to run the images through Xception’s prepro
          cess_input() function:                                      
                                                                      
            def preprocess(image, label):                             
               resized_image = tf.image.resize(image, [224, 224])     
               final_image = keras.applications.xception.preprocess_input(resized_image)
               return final_image, label                              
          Let’s apply this preprocessing function to all three datasets, shuffle the training set,
          and add batching and prefetching to all the datasets:       
            batch_size = 32                                           
            train_set = train_set.shuffle(1000)                       
            train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)
            valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)
            test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)
          If you want to perform some data augmentation, change the preprocessing function
          for the training set, adding some random transformations to the training images. For
          example, use tf.image.random_crop() to randomly crop the images, use
          tf.image.random_flip_left_right() to randomly flip the images horizontally, and
          so on (see the “Pretrained Models for Transfer Learning” section of the notebook for
          an example).                                                
                   The keras.preprocessing.image.ImageDataGenerator class
                   makes it easy to load images from disk and augment them in vari‐
                   ous ways: you can shift each image, rotate it, rescale it, flip it hori‐
                   zontally or vertically, shear it, or apply any transformation function
                   you want to it. This is very convenient for simple projects. How‐
                   ever, building a tf.data pipeline has many advantages: it can read
                   the images efficiently (e.g., in parallel) from any source, not just the
                   local disk; you can manipulate the Dataset as you wish; and if you
                   write a preprocessing function based on tf.image operations, this
                   function can be used both in the tf.data pipeline and in the model
                   you will deploy to production (see Chapter 19).    
          Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the
          network by setting include_top=False: this excludes the global average pooling layer
          and the dense output layer. We then add our own global average pooling layer, based
          on the output of the base model, followed by a dense output layer with one unit per
          class, using the softmax activation function. Finally, we create the Keras Model:
                                                                      
            base_model = keras.applications.xception.Xception(weights=""imagenet"",
                                          include_top=False)          
            avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
            output = keras.layers.Dense(n_classes, activation=""softmax"")(avg)
            model = keras.Model(inputs=base_model.input, outputs=output)"|softmax; softmax function
"                                                                      
                                                                      
                                                                      
                                                                      
          Stepping Back                                               
                                                                      
          By now you know a lot about Machine Learning. However, we went through so many
          concepts that you may be feeling a little lost, so let’s step back and look at the big
          picture:                                                    
                                                                      
           • Machine Learning is about making machines get better at some task by learning
            from data, instead of having to explicitly code rules.    
           • There are many different types of ML systems: supervised or not, batch or online,
            instance-based or model-based.                            
           • In an ML project you gather data in a training set, and you feed the training set to
            a learning algorithm. If the algorithm is model-based, it tunes some parameters
            to fit the model to the training set (i.e., to make good predictions on the training
            set itself), and then hopefully it will be able to make good predictions on new
            cases as well. If the algorithm is instance-based, it just learns the examples by
            heart and generalizes to new instances by using a similarity measure to compare
            them to the learned instances.                            
           • The system will not perform well if your training set is too small, or if the data is
            not representative, is noisy, or is polluted with irrelevant features (garbage in,
            garbage out). Lastly, your model needs to be neither too simple (in which case it
            will underfit) nor too complex (in which case it will overfit).
                                                                      
          There’s just one last important topic to cover: once you have trained a model, you
          don’t want to just “hope” it generalizes to new cases. You want to evaluate it and fine-
          tune it if necessary. Let’s see how to do that.             
                                                                      
          Testing and Validating                                      
                                                                      
          The only way to know how well a model will generalize to new cases is to actually try
          it out on new cases. One way to do that is to put your model in production and moni‐
          tor how well it performs. This works well, but if your model is horribly bad, your
          users will complain—not the best idea.                      
          A better option is to split your data into two sets: the training set and the test set. As
          these names imply, you train your model using the training set, and you test it using
          the test set. The error rate on new cases is called the generalization error (or out-of-
          sample error), and by evaluating your model on the test set, you get an estimate of this
          error. This value tells you how well your model will perform on instances it has never
          seen before.                                                
          If the training error is low (i.e., your model makes few mistakes on the training set)
          but the generalization error is high, it means that your model is overfitting the train‐
          ing data.                                                   
                                                                      "|test sets; training sets; generalization error; out-of-sample error; Machine Learning (ML); testing and validating
"                                                                      
                                                                      
                                                                      
                                                                      
          solution to the dual problem typically gives a lower bound to the solution of the pri‐
          mal problem, but under some conditions it can have the same solution as the primal
          problem. Luckily, the SVM problem happens to meet these conditions,6 so you can
          choose to solve the primal problem or the dual problem; both will have the same sol‐
          ution. Equation 5-6 shows the dual form of the linear SVM objective (if you are inter‐
          ested in knowing how to derive the dual problem from the primal problem, see
          Appendix C).                                                
                                                                      
            Equation 5-6. Dual form of the linear SVM objective       
                                                                      
                   m  m                m                              
            minimize 1 ∑ ∑ α i α j t i t j x i⊺ x j − ∑ α i           
              α   2 i=1j=1             i=1                            
                     subject to α i ≥0 for i=1,2,⋯,m                  
          Once you find the vector α that minimizes this equation (using a QP solver), use
          Equation 5-7 to compute w and b that minimize the primal problem.
                                                                      
            Equation 5-7. From the dual solution to the primal solution
                                                                      
                m                                                     
             w= ∑ α i t i x i                                         
               i=1                                                    
                  m                                                   
             b = 1 ∑ t i −w ⊺ x i                                     
               n                                                      
                s i=1                                                 
                  i                                                   
                 α >0                                                 
          The dual problem is faster to solve than the primal one when the number of training
          instances is smaller than the number of features. More importantly, the dual problem
          makes the kernel trick possible, while the primal does not. So what is this kernel trick,
          anyway?                                                     
          Kernelized SVMs                                             
          Suppose you want to apply a second-degree polynomial transformation to a two-
          dimensional training set (such as the moons training set), then train a linear SVM
          classifier on the transformed training set. Equation 5-8 shows the second-degree pol‐
          ynomial mapping function ϕ that you want to apply.          
                                                                      
                                                                      
                                                                      
          6 The objective function is convex, and the inequality constraints are continuously differentiable and convex
           functions.                                                 "|kernelized SVM
"                                                                      
                                                                      
                                                                      
                                                                      
          JupyterLab (this is an alternative web interface to run Jupyter notebooks). Once the
          VM is created, scroll down the navigation menu to the Artificial Intelligence section,
          then click AI Platform → Notebooks. Once the Notebook instance appears in the list
          (this may take a few minutes, so click Refresh once in a while until it appears), click
          its Open JupyterLab link. This will run JupyterLab on the VM and connect your
          browser to it. You can create notebooks and run any code you want on this VM, and
          benefit from its GPUs!                                      
                                                                      
          But if you just want to run some quick tests or easily share notebooks with your col‐
          leagues, then you should try Colaboratory.                  
          Colaboratory                                                
                                                                      
          The simplest and cheapest way to access a GPU VM is to use Colaboratory (or Colab,
          for short). It’s free! Just go to https://colab.research.google.com/ and create a new
          Python 3 notebook: this will create a Jupyter notebook, stored on your Google Drive
          (alternatively, you can open any notebook on GitHub, or on Google Drive, or you can
          even upload your own notebooks). Colab’s user interface is similar to Jupyter’s, except
          you can share and use the notebooks like regular Google Docs, and there are a few
          other minor differences (e.g., you can create handy widgets using special comments
          in your code).                                              
          When you open a Colab notebook, it runs on a free Google VM dedicated to that
          notebook, called a Colab Runtime (see Figure 19-11). By default the Runtime is CPU-
          only, but you can change this by going to Runtime → “Change runtime type,” select‐
          ing GPU in the “Hardware accelerator” drop-down menu, then clicking Save. In fact,
          you could even select TPU! (Yes, you can actually use a TPU for free; we will talk
          about TPUs later in this chapter, though, so for now just select GPU.)
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|Colab Runtime; Colaboratory (Colab)
"                                                                      
                                                                      
                                                                      
                                                                      
          A neuron located in row i, column j of a given layer is connected to the outputs of the
          neurons in the previous layer located in rows i to i + f – 1, columns j to j + f – 1,
                                          h             w             
          where f and f are the height and width of the receptive field (see Figure 14-3). In
              h   w                                                   
          order for a layer to have the same height and width as the previous layer, it is com‐
          mon to add zeros around the inputs, as shown in the diagram. This is called zero
          padding.                                                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-3. Connections between layers and zero padding    
                                                                      
          It is also possible to connect a large input layer to a much smaller layer by spacing out
          the receptive fields, as shown in Figure 14-4. This dramatically reduces the model’s
          computational complexity. The shift from one receptive field to the next is called the
          stride. In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 × 4
          layer, using 3 × 3 receptive fields and a stride of 2 (in this example the stride is the
          same in both directions, but it does not have to be so). A neuron located in row i,
          column j in the upper layer is connected to the outputs of the neurons in the previous
          layer located in rows i × s to i × s + f – 1, columns j × s to j × s + f – 1, where s
                         h    h h           w    w w       h          
          and s are the vertical and horizontal strides.              
             w                                                        
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|stride; zero padding
"                                                                      
                                                                      
                                                                      
                                                                      
          to go ahead and read all three papers: they are quite pleasant to read, and they pro‐
          vide excellent examples of how Deep Learning systems can be incrementally
          improved.                                                   
                                                                      
                                                                      
                        Mean Average Precision (mAP)                  
           A very common metric used in object detection tasks is the mean Average Precision
           (mAP). “Mean Average” sounds a bit redundant, doesn’t it? To understand this met‐
           ric, let’s go back to two classification metrics we discussed in Chapter 3: precision and
           recall. Remember the trade-off: the higher the recall, the lower the precision. You can
           visualize this in a precision/recall curve (see Figure 3-5). To summarize this curve
           into a single number, we could compute its area under the curve (AUC). But note that
           the precision/recall curve may contain a few sections where precision actually goes up
           when recall increases, especially at low recall values (you can see this at the top left of
           Figure 3-5). This is one of the motivations for the mAP metric.
           Suppose the classifier has 90% precision at 10% recall, but 96% precision at 20%
           recall. There’s really no trade-off here: it simply makes more sense to use the classifier
           at 20% recall rather than at 10% recall, as you will get both higher recall and higher
           precision. So instead of looking at the precision at 10% recall, we should really be
           looking at the maximum precision that the classifier can offer with at least 10% recall.
           It would be 96%, not 90%. Therefore, one way to get a fair idea of the model’s perfor‐
           mance is to compute the maximum precision you can get with at least 0% recall, then
           10% recall, 20%, and so on up to 100%, and then calculate the mean of these maxi‐
           mum precisions. This is called the Average Precision (AP) metric. Now when there are
           more than two classes, we can compute the AP for each class, and then compute the
           mean AP (mAP). That’s it!                                  
           In an object detection system, there is an additional level of complexity: what if the
           system detected the correct class, but at the wrong location (i.e., the bounding box is
           completely off)? Surely we should not count this as a positive prediction. One
           approach is to define an IOU threshold: for example, we may consider that a predic‐
           tion is correct only if the IOU is greater than, say, 0.5, and the predicted class is cor‐
           rect. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%, or
           sometimes just AP ). In some competitions (such as the PASCAL VOC challenge),
                      50                                              
           this is what is done. In others (such as the COCO competition), the mAP is computed
           for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the
           mean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Yes, that’s a mean
           mean average.                                              
          Several YOLO implementations built using TensorFlow are available on GitHub. In
          particular, check out Zihao Zang’s TensorFlow 2 implementation. Other object detec‐
          tion models are available in the TensorFlow Models project, many with pretrained
                                                                      "|mean average precision; Average Precision (AP); mean Average Precision (mAP)
"                                                                      
                                                                      
                                                                      
                                                                      
          time steps in the targets (since the kernel’s size is 4, the first output of the convolu‐
          tional layer will be based on the input time steps 0 to 3), and downsample the targets
          by a factor of 2:                                           
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=""valid"",
                          input_shape=[None, 1]),                     
               keras.layers.GRU(20, return_sequences=True),           
               keras.layers.GRU(20, return_sequences=True),           
               keras.layers.TimeDistributed(keras.layers.Dense(10))   
            ])                                                        
            model.compile(loss=""mse"", optimizer=""adam"", metrics=[last_time_step_mse])
            history = model.fit(X_train, Y_train[:, 3::2], epochs=20, 
                        validation_data=(X_valid, Y_valid[:, 3::2]))  
          If you train and evaluate this model, you will find that it is the best model so far. The
          convolutional layer really helps. In fact, it is actually possible to use only 1D convolu‐
          tional layers and drop the recurrent layers entirely!       
          WaveNet                                                     
          In a 2016 paper,13 Aaron van den Oord and other DeepMind researchers introduced
          an architecture called WaveNet. They stacked 1D convolutional layers, doubling the
          dilation rate (how spread apart each neuron’s inputs are) at every layer: the first con‐
          volutional layer gets a glimpse of just two time steps at a time, while the next one sees
          four time steps (its receptive field is four time steps long), the next one sees eight time
          steps, and so on (see Figure 15-11). This way, the lower layers learn short-term pat‐
          terns, while the higher layers learn long-term patterns. Thanks to the doubling dila‐
          tion rate, the network can process extremely large sequences very efficiently.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          13 Aaron van den Oord et al., “WaveNet: A Generative Model for Raw Audio,” arXiv preprint arXiv:1609.03499
           (2016).                                                    "|WaveNet
"                                                                      
                                                                      
                                                                      
                                                                      
            dropout_encoder = keras.models.Sequential([               
               keras.layers.Flatten(input_shape=[28, 28]),            
               keras.layers.Dropout(0.5),                             
               keras.layers.Dense(100, activation=""selu""),            
               keras.layers.Dense(30, activation=""selu"")              
            ])                                                        
            dropout_decoder = keras.models.Sequential([               
               keras.layers.Dense(100, activation=""selu"", input_shape=[30]),
               keras.layers.Dense(28 * 28, activation=""sigmoid""),     
               keras.layers.Reshape([28, 28])                         
            ])                                                        
            dropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])
          Figure 17-9 shows a few noisy images (with half the pixels turned off), and the
          images reconstructed by the dropout-based denoising autoencoder. Notice how the
          autoencoder guesses details that are actually not in the input, such as the top of the
          white shirt (bottom row, fourth image). As you can see, not only can denoising
          autoencoders be used for data visualization or unsupervised pretraining, like the
          other autoencoders we’ve discussed so far, but they can also be used quite simply and
          efficiently to remove noise from images.                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-9. Noisy images (top) and their reconstructions (bottom)
                                                                      
          Sparse Autoencoders                                         
                                                                      
          Another kind of constraint that often leads to good feature extraction is sparsity: by
          adding an appropriate term to the cost function, the autoencoder is pushed to reduce
          the number of active neurons in the coding layer. For example, it may be pushed to
          have on average only 5% significantly active neurons in the coding layer. This forces
          the autoencoder to represent each input as a combination of a small number of acti‐
          vations. As a result, each neuron in the coding layer typically ends up representing a
          useful feature (if you could speak only a few words per month, you would probably
          try to make them worth listening to).                       
          A simple approach is to use the sigmoid activation function in the coding layer (to
          constrain the codings to values between 0 and 1), use a large coding layer (e.g., with
                                                                      
                                                                      "|sparsity; sparse; sparse autoencoders
"                                                                      
                                                                      
                                                                      
                                                                      
          This algorithm will converge to the optimal Q-Values, but it will take many iterations,
          and possibly quite a lot of hyperparameter tuning. As you can see in Figure 18-9, the
          Q-Value Iteration algorithm (left) converges very quickly, in fewer than 20 iterations,
          while the Q-Learning algorithm (right) takes about 8,000 iterations to converge.
          Obviously, not knowing the transition probabilities or the rewards makes finding the
          optimal policy significantly harder!                        
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-9. The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm
          (right)                                                     
                                                                      
          The Q-Learning algorithm is called an off-policy algorithm because the policy being
          trained is not necessarily the one being executed: in the previous code example, the
          policy being executed (the exploration policy) is completely random, while the policy
          being trained will always choose the actions with the highest Q-Values. Conversely,
          the Policy Gradients algorithm is an on-policy algorithm: it explores the world using
          the policy being trained. It is somewhat surprising that Q-Learning is capable of
          learning the optimal policy by just watching an agent act randomly (imagine learning
          to play golf when your teacher is a drunk monkey). Can we do better?
          Exploration Policies                                        
                                                                      
          Of course, Q-Learning can work only if the exploration policy explores the MDP
          thoroughly enough. Although a purely random policy is guaranteed to eventually
          visit every state and every transition many times, it may take an extremely long time
          to do so. Therefore, a better option is to use the ε-greedy policy (ε is epsilon): at each
          step it acts randomly with probability ε, or greedily with probability 1–ε (i.e., choos‐
          ing the action with the highest Q-Value). The advantage of the ε-greedy policy (com‐
          pared to a completely random policy) is that it will spend more and more time
          exploring the interesting parts of the environment, as the Q-Value estimates get better
          and better, while still spending some time visiting unknown regions of the MDP. It is
          quite common to start with a high value for ε (e.g., 1.0) and then gradually reduce it
          (e.g., down to 0.05).                                       "|exploration policy; on-policy algorithms; off-policy algorithms
"                                                                      
                                                                      
                                                                      
                                                                      
          As explained in Chapter 11, it’s usually a good idea to freeze the weights of the pre‐
          trained layers, at least at the beginning of training:      
                                                                      
            for layer in base_model.layers:                           
               layer.trainable = False                                
                   Since our model uses the base model’s layers directly, rather than
                   the base_model object itself, setting base_model.trainable=False
                   would have no effect.                              
                                                                      
                                                                      
          Finally, we can compile the model and start training:       
                                                                      
            optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)
            model.compile(loss=""sparse_categorical_crossentropy"", optimizer=optimizer,
                    metrics=[""accuracy""])                             
            history = model.fit(train_set, epochs=5, validation_data=valid_set)
                   This will be very slow, unless you have a GPU. If you do not, then
                   you should run this chapter’s notebook in Colab, using a GPU run‐
                   time (it’s free!). See the instructions at https://github.com/ageron/
                   handson-ml2.                                       
                                                                      
          After training the model for a few epochs, its validation accuracy should reach about
          75–80% and stop making much progress. This means that the top layers are now
          pretty well trained, so we are ready to unfreeze all the layers (or you could try
          unfreezing just the top ones) and continue training (don’t forget to compile the
          model when you freeze or unfreeze layers). This time we use a much lower learning
          rate to avoid damaging the pretrained weights:              
            for layer in base_model.layers:                           
               layer.trainable = True                                 
                                                                      
            optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)
            model.compile(...)                                        
            history = model.fit(...)                                  
          It will take a while, but this model should reach around 95% accuracy on the test set.
          With that, you can start training amazing image classifiers! But there’s more to com‐
          puter vision than just classification. For example, what if you also want to know where
          the flower is in the picture? Let’s look at this now.       
          Classification and Localization                             
                                                                      
          Localizing an object in a picture can be expressed as a regression task, as discussed in
          Chapter 10: to predict a bounding box around the object, a common approach is to
                                                                      "|localization; classification and localization
"                                                                      
                                                                      
                                                                      
                                                                      
            over that same rose overlaps a lot with the max bounding box, so we will get rid
            of it.                                                    
                                                                      
           3. Repeat step two until there are no more bounding boxes to get rid of.
          This simple approach to object detection works pretty well, but it requires running
          the CNN many times, so it is quite slow. Fortunately, there is a much faster way to
          slide a CNN across an image: using a fully convolutional network (FCN).
                                                                      
          Fully Convolutional Networks                                
                                                                      
          The idea of FCNs was first introduced in a 2015 paper25 by Jonathan Long et al., for
          semantic segmentation (the task of classifying every pixel in an image according to
          the class of the object it belongs to). The authors pointed out that you could replace
          the dense layers at the top of a CNN by convolutional layers. To understand this, let’s
          look at an example: suppose a dense layer with 200 neurons sits on top of a convolu‐
          tional layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map
          size, not the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7
          activations from the convolutional layer (plus a bias term). Now let’s see what hap‐
          pens if we replace the dense layer with a convolutional layer using 200 filters, each of
          size 7 × 7, and with ""valid"" padding. This layer will output 200 feature maps, each 1
          × 1 (since the kernel is exactly the size of the input feature maps and we are using
          ""valid"" padding). In other words, it will output 200 numbers, just like the dense
          layer did; and if you look closely at the computations performed by a convolutional
          layer, you will notice that these numbers will be precisely the same as those the dense
          layer produced. The only difference is that the dense layer’s output was a tensor of
          shape [batch size, 200], while the convolutional layer will output a tensor of shape
          [batch size, 1, 1, 200].                                    
                   To convert a dense layer to a convolutional layer, the number of fil‐
                   ters in the convolutional layer must be equal to the number of units
                   in the dense layer, the filter size must be equal to the size of the
                   input feature maps, and you must use ""valid"" padding. The stride
                   may be set to 1 or more, as we will see shortly.   
                                                                      
          Why is this important? Well, while a dense layer expects a specific input size (since it
          has one weight per input feature), a convolutional layer will happily process images of
          any size26 (however, it does expect its inputs to have a specific number of channels,
                                                                      
                                                                      
          25 Jonathan Long et al., “Fully Convolutional Networks for Semantic Segmentation,” Proceedings of the IEEE
           Conference on Computer Vision and Pattern Recognition (2015): 3431–3440.
          26 There is one small exception: a convolutional layer using ""valid"" padding will complain if the input size is
           smaller than the kernel size.                              "|fully convolutional networks (FCNs)
"                                                                      
                                                                      
                                                                      
                                                                      
          How is this possible? The following analogy can help shed some light on this mystery.
          Suppose you have a slightly biased coin that has a 51% chance of coming up heads
          and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get
          more or less 510 heads and 490 tails, and hence a majority of heads. If you do the
          math, you will find that the probability of obtaining a majority of heads after 1,000
          tosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,
          with 10,000 tosses, the probability climbs over 97%). This is due to the law of large
          numbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the
          probability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You can
          see that as the number of tosses increases, the ratio of heads approaches 51%. Eventu‐
          ally all 10 series end up so close to 51% that they are consistently above 50%.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-3. The law of large numbers                        
                                                                      
          Similarly, suppose you build an ensemble containing 1,000 classifiers that are individ‐
          ually correct only 51% of the time (barely better than random guessing). If you pre‐
          dict the majority voted class, you can hope for up to 75% accuracy! However, this is
          only true if all classifiers are perfectly independent, making uncorrelated errors,
          which is clearly not the case because they are trained on the same data. They are likely
          to make the same types of errors, so there will be many majority votes for the wrong
          class, reducing the ensemble’s accuracy.                    
                                                                      
                   Ensemble methods work best when the predictors are as independ‐
                   ent from one another as possible. One way to get diverse classifiers
                   is to train them using very different algorithms. This increases the
                   chance that they will make very different types of errors, improving
                   the ensemble’s accuracy.                           
                                                                      
          The following code creates and trains a voting classifier in Scikit-Learn, composed of
          three diverse classifiers (the training set is the moons dataset, introduced in Chap‐
          ter 5):                                                     "|voting classifiers; law of large numbers
"                                                                      
                                                                      
                                                                      
                                                                      
          instances with that target label). To do this, simply set average=""weighted"" in the
          preceding code.4                                            
                                                                      
          Multioutput Classification                                  
                                                                      
          The last type of classification task we are going to discuss here is called multioutput–
          multiclass classification (or simply multioutput classification). It is simply a generaliza‐
          tion of multilabel classification where each label can be multiclass (i.e., it can have
          more than two possible values).                             
                                                                      
          To illustrate this, let’s build a system that removes noise from images. It will take as
          input a noisy digit image, and it will (hopefully) output a clean digit image, repre‐
          sented as an array of pixel intensities, just like the MNIST images. Notice that the
          classifier’s output is multilabel (one label per pixel) and each label can have multiple
          values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput
          classification system.                                      
                   The line between classification and regression is sometimes blurry,
                   such as in this example. Arguably, predicting pixel intensity is more
                   akin to regression than to classification. Moreover, multioutput
                   systems are not limited to classification tasks; you could even have
                   a system that outputs multiple labels per instance, including both
                   class labels and value labels.                     
                                                                      
          Let’s start by creating the training and test sets by taking the MNIST images and
          adding noise to their pixel intensities with NumPy’s randint() function. The target
          images will be the original images:                         
            noise = np.random.randint(0, 100, (len(X_train), 784))    
            X_train_mod = X_train + noise                             
            noise = np.random.randint(0, 100, (len(X_test), 784))     
            X_test_mod = X_test + noise                               
            y_train_mod = X_train                                     
            y_test_mod = X_test                                       
          Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so
          you should be frowning right now):                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          4 Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for
           more details.                                              "|computing classifier metrics; multioutput classification; randint() function
"                                                                      
                                                                      
                                                                      
                                                                      
          The biggest difficulty is called mode collapse: this is when the generator’s outputs
          gradually become less diverse. How can this happen? Suppose that the generator gets
          better at producing convincing shoes than any other class. It will fool the discrimina‐
          tor a bit more with shoes, and this will encourage it to produce even more images of
          shoes. Gradually, it will forget how to produce anything else. Meanwhile, the only
          fake images that the discriminator will see will be shoes, so it will also forget how to
          discriminate fake images of other classes. Eventually, when the discriminator man‐
          ages to discriminate the fake shoes from the real ones, the generator will be forced to
          move to another class. It may then become good at shirts, forgetting about shoes, and
          the discriminator will follow. The GAN may gradually cycle across a few classes,
          never really becoming very good at any of them.             
          Moreover, because the generator and the discriminator are constantly pushing against
          each other, their parameters may end up oscillating and becoming unstable. Training
          may begin properly, then suddenly diverge for no apparent reason, due to these insta‐
          bilities. And since many factors affect these complex dynamics, GANs are very sensi‐
          tive to the hyperparameters: you may have to spend a lot of effort fine-tuning them.
                                                                      
          These problems have kept researchers very busy since 2014: many papers were pub‐
          lished on this topic, some proposing new cost functions11 (though a 2018 paper12 by
          Google researchers questions their efficiency) or techniques to stabilize training or to
          avoid the mode collapse issue. For example, a popular technique called experience
          replay consists in storing the images produced by the generator at each iteration in a
          replay buffer (gradually dropping older generated images) and training the discrimi‐
          nator using real images plus fake images drawn from this buffer (rather than just fake
          images produced by the current generator). This reduces the chances that the dis‐
          criminator will overfit the latest generator’s outputs. Another common technique is
          called mini-batch discrimination: it measures how similar images are across the batch
          and provides this statistic to the discriminator, so it can easily reject a whole batch of
          fake images that lack diversity. This encourages the generator to produce a greater
          variety of images, reducing the chance of mode collapse. Other papers simply pro‐
          pose specific architectures that happen to perform well.    
          In short, this is still a very active field of research, and the dynamics of GANs are still
          not perfectly understood. But the good news is that great progress has been made,
          and some of the results are truly astounding! So let’s look at some of the most success‐
          ful architectures, starting with deep convolutional GANs, which were the state of the
          art just a few years ago. Then we will look at two more recent (and more complex)
          architectures.                                              
                                                                      
                                                                      
          11 For a nice comparison of the main GAN losses, check out this great GitHub project by Hwalsuk Lee.
          12 Mario Lucic et al., “Are GANs Created Equal? A Large-Scale Study,” Proceedings of the 32nd International Con‐
           ference on Neural Information Processing Systems (2018): 698–707."|mode collapse; experience replay; minibatch discrimination
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                           No Free Lunch Theorem                      
                                                                      
           A model is a simplified version of the observations. The simplifications are meant to
           discard the superfluous details that are unlikely to generalize to new instances. To
           decide what data to discard and what data to keep, you must make assumptions. For
           example, a linear model makes the assumption that the data is fundamentally linear
           and that the distance between the instances and the straight line is just noise, which
           can safely be ignored.                                     
           In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely
           no assumption about the data, then there is no reason to prefer one model over any
           other. This is called the No Free Lunch (NFL) theorem. For some datasets the best
           model is a linear model, while for other datasets it is a neural network. There is no
           model that is a priori guaranteed to work better (hence the name of the theorem). The
           only way to know for sure which model is best is to evaluate them all. Since this is not
           possible, in practice you make some reasonable assumptions about the data and eval‐
           uate only a few reasonable models. For example, for simple tasks you may evaluate
           linear models with various levels of regularization, and for a complex problem you
           may evaluate various neural networks.                      
          Exercises                                                   
                                                                      
                                                                      
          In this chapter we have covered some of the most important concepts in Machine
          Learning. In the next chapters we will dive deeper and write more code, but before we
          do, make sure you know how to answer the following questions:
           1. How would you define Machine Learning?                  
                                                                      
           2. Can you name four types of problems where it shines?    
           3. What is a labeled training set?                         
           4. What are the two most common supervised tasks?          
           5. Can you name four common unsupervised tasks?            
                                                                      
           6. What type of Machine Learning algorithm would you use to allow a robot to
            walk in various unknown terrains?                         
           7. What type of algorithm would you use to segment your customers into multiple
            groups?                                                   
           8. Would you frame the problem of spam detection as a supervised learning prob‐
            lem or an unsupervised learning problem?                  
                                                                      
                                                                      
          11 David Wolpert, “The Lack of A Priori Distinctions Between Learning Algorithms,” Neural Computation 8, no.
           7 (1996): 1341–1390.                                       "|testing and validating; No Free Lunch (NFL) theorem
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-4. The learning rate is too small                  
          On the other hand, if the learning rate is too high, you might jump across the valley
          and end up on the other side, possibly even higher up than you were before. This
          might make the algorithm diverge, with larger and larger values, failing to find a good
          solution (see Figure 4-5).                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-5. The learning rate is too large                  
                                                                      
          Finally, not all cost functions look like nice, regular bowls. There may be holes, ridges,
          plateaus, and all sorts of irregular terrains, making convergence to the minimum dif‐
          ficult. Figure 4-6 shows the two main challenges with Gradient Descent. If the ran‐
          dom initialization starts the algorithm on the left, then it will converge to a local
          minimum, which is not as good as the global minimum. If it starts on the right, then it
          will take a very long time to cross the plateau. And if you stop too early, you will
          never reach the global minimum.                             
                                                                      
                                                                      "|local minimum; global minimum
"                                                                      
                                                                      
                                                                      
                                                                      
          Alternatively, rather than relying only on chance for exploration, another approach is
          to encourage the exploration policy to try actions that it has not tried much before.
          This can be implemented as a bonus added to the Q-Value estimates, as shown in
          Equation 18-6.                                              
                                                                      
            Equation 18-6. Q-Learning using an exploration function   
                                                                      
            Q s,a r+γ·max f Q s′,a′ ,N s′,a′                          
                α     a′                                              
                                                                      
          In this equation:                                           
                                                                      
           • N(s′, a′) counts the number of times the action a′ was chosen in state s′.
           • f(Q, N) is an exploration function, such as f(Q, N) = Q + κ/(1 + N), where κ is a
            curiosity hyperparameter that measures how much the agent is attracted to the
            unknown.                                                  
                                                                      
          Approximate Q-Learning and Deep Q-Learning                  
                                                                      
          The main problem with Q-Learning is that it does not scale well to large (or even
          medium) MDPs with many states and actions. For example, suppose you wanted to
          use Q-Learning to train an agent to play Ms. Pac-Man (see Figure 18-1). There are
          about 150 pellets that Ms. Pac-Man can eat, each of which can be present or absent
          (i.e., already eaten). So, the number of possible states is greater than 2150 ≈ 1045. And if
          you add all the possible combinations of positions for all the ghosts and Ms. Pac-
          Man, the number of possible states becomes larger than the number of atoms in our
          planet, so there’s absolutely no way you can keep track of an estimate for every single
          Q-Value.                                                    
          The solution is to find a function Q (s, a) that approximates the Q-Value of any state-
                               θ                                      
          action pair (s, a) using a manageable number of parameters (given by the parameter
          vector θ). This is called Approximate Q-Learning. For years it was recommended to
          use linear combinations of handcrafted features extracted from the state (e.g., dis‐
          tance of the closest ghosts, their directions, and so on) to estimate Q-Values, but in
          2013, DeepMind showed that using deep neural networks can work much better,
          especially for complex problems, and it does not require any feature engineering. A
          DNN used to estimate Q-Values is called a Deep Q-Network (DQN), and using a
          DQN for Approximate Q-Learning is called Deep Q-Learning.   
          Now, how can we train a DQN? Well, consider the approximate Q-Value computed
          by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want
          this approximate Q-Value to be as close as possible to the reward r that we actually
          observe after playing action a in state s, plus the discounted value of playing optimally"|Approximate Q-Learning; deep Q-networks (DQNs); Approximate Q-Learning and Deep Q- Learning; Deep Q-Learning
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                   CHAPTER 5          
                                                                      
                               Support   Vector  Machines             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          A Support Vector Machine (SVM) is a powerful and versatile Machine Learning
          model, capable of performing linear or nonlinear classification, regression, and even
          outlier detection. It is one of the most popular models in Machine Learning, and any‐
          one interested in Machine Learning should have it in their toolbox. SVMs are partic‐
          ularly well suited for classification of complex small- or medium-sized datasets.
          This chapter will explain the core concepts of SVMs, how to use them, and how they
          work.                                                       
                                                                      
          Linear SVM Classification                                   
                                                                      
          The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1
          shows part of the iris dataset that was introduced at the end of Chapter 4. The two
          classes can clearly be separated easily with a straight line (they are linearly separable).
          The left plot shows the decision boundaries of three possible linear classifiers. The
          model whose decision boundary is represented by the dashed line is so bad that it
          does not even separate the classes properly. The other two models work perfectly on
          this training set, but their decision boundaries come so close to the instances that
          these models will probably not perform as well on new instances. In contrast, the
          solid line in the plot on the right represents the decision boundary of an SVM classi‐
          fier; this line not only separates the two classes but also stays as far away from the
          closest training instances as possible. You can think of an SVM classifier as fitting the
          widest possible street (represented by the parallel dashed lines) between the classes.
          This is called large margin classification.                 
                                                                      
                                                                      
                                                                      
                                                                      "|large margin classification; linear SVM classification; Support Vector Machines (SVMs)
"                                                                      
                                                                      
                                                                      
                                                                      
          Now let’s look deeper into the heart of the Transformer model: the Multi-Head Atten‐
          tion layer.                                                 
                                                                      
          Multi-Head Attention                                        
                                                                      
          To understand how a Multi-Head Attention layer works, we must first understand the
          Scaled Dot-Product Attention layer, which it is based on. Let’s suppose the encoder
          analyzed the input sentence “They played chess,” and it managed to understand that
          the word “They” is the subject and the word “played” is the verb, so it encoded this
          information in the representations of these words. Now suppose the decoder has
          already translated the subject, and it thinks that it should translate the verb next. For
          this, it needs to fetch the verb from the input sentence. This is analog to a dictionary
          lookup: it’s as if the encoder created a dictionary {“subject”: “They”, “verb”: “played”,
          …} and the decoder wanted to look up the value that corresponds to the key “verb.”
          However, the model does not have discrete tokens to represent the keys (like “subject”
          or “verb”); it has vectorized representations of these concepts (which it learned dur‐
          ing training), so the key it will use for the lookup (called the query) will not perfectly
          match any key in the dictionary. The solution is to compute a similarity measure
          between the query and each key in the dictionary, and then use the softmax function
          to convert these similarity scores to weights that add up to 1. If the key that represents
          the verb is by far the most similar to the query, then that key’s weight will be close to
          1. Then the model can compute a weighted sum of the corresponding values, so if the
          weight of the “verb” key is close to 1, then the weighted sum will be very close to the
          representation of the word “played.” In short, you can think of this whole process as a
          differentiable dictionary lookup. The similarity measure used by the Transformer is
          just the dot product, like in Luong attention. In fact, the equation is the same as for
          Luong attention, except for a scaling factor. The equation is shown in Equation 16-3,
          in a vectorized form.                                       
            Equation 16-3. Scaled Dot-Product Attention               
                                                                      
                                 ⊺                                    
            Attention  ,  ,  = softmax                                
                               d                                      
                                keys                                  
          In this equation:                                           
           • Q is a matrix containing one row per query. Its shape is [n , d ], where
                                                 queries keys         
            n   is the number of queries and d is the number of dimensions of each
             queries               keys                               
            query and each key.                                       
           • K is a matrix containing one row per key. Its shape is [n , d ], where n is
                                              keys keys  keys         
            the number of keys and values.                            "|Scaled Dot-Product Attention layer; Multi-Head Attention layer
"                                                                      
                                                                      
                                                                      
                                                                      
           and max_samples=1.0) but sampling features (by setting bootstrap_features to
          True and/or max_features to a value smaller than 1.0) is called the Random Subspa‐
          ces method.8                                                
                                                                      
          Sampling features results in even more predictor diversity, trading a bit more bias for
          a lower variance.                                           
          Random  Forests                                             
                                                                      
                                                                      
          As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally
          trained via the bagging method (or sometimes pasting), typically with max_samples
          set to the size of the training set. Instead of building a BaggingClassifier and pass‐
          ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier
          class, which is more convenient and optimized for Decision Trees10 (similarly, there is
          a RandomForestRegressor class for regression tasks). The following code uses all
          available CPU cores to train a Random Forest classifier with 500 trees (each limited
          to maximum 16 nodes):                                       
            from sklearn.ensemble import RandomForestClassifier       
            rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)
            rnd_clf.fit(X_train, y_train)                             
                                                                      
            y_pred_rf = rnd_clf.predict(X_test)                       
          With a few exceptions, a RandomForestClassifier has all the hyperparameters of a
          DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara‐
          meters of a BaggingClassifier to control the ensemble itself.11
                                                                      
          The Random Forest algorithm introduces extra randomness when growing trees;
          instead of searching for the very best feature when splitting a node (see Chapter 6), it
          searches for the best feature among a random subset of features. The algorithm
          results in greater tree diversity, which (again) trades a higher bias for a lower var‐
          iance, generally yielding an overall better model. The following BaggingClassifier
          is roughly equivalent to the previous RandomForestClassifier:
                                                                      
                                                                      
          8 Tin Kam Ho, “The Random Subspace Method for Constructing Decision Forests,” IEEE Transactions on Pat‐
           tern Analysis and Machine Intelligence 20, no. 8 (1998): 832–844.
          9 Tin Kam Ho, “Random Decision Forests,” Proceedings of the Third International Conference on Document
           Analysis and Recognition 1 (1995): 278.                    
          10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees.
          11 There are a few notable exceptions: splitter is absent (forced to ""random""), presort is absent (forced to
           False), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi
           fier with the provided hyperparameters).                   "|Random Forests
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-6. Neural machine translation using an Encoder–Decoder network with an
          attention model                                             
                                                                      
          But where do these α weights come from? It’s actually pretty simple: they are gener‐
                      (t,i)                                           
          ated by a type of small neural network called an alignment model (or an attention
          layer), which is trained jointly with the rest of the Encoder–Decoder model. This
          alignment model is illustrated on the righthand side of Figure 16-6. It starts with a
          time-distributed Dense layer15 with a single neuron, which receives as input all the
          encoder outputs, concatenated with the decoder’s previous hidden state (e.g., h ).
                                                          (2)         
          This layer outputs a score (or energy) for each encoder output (e.g., e ): this score
                                                   (3, 2)             
          measures how well each output is aligned with the decoder’s previous hidden state.
          Finally, all the scores go through a softmax layer to get a final weight for each encoder
          output (e.g., α ). All the weights for a given decoder time step add up to 1 (since the
                  (3,2)                                               
          softmax layer is not time-distributed). This particular attention mechanism is called
          Bahdanau attention (named after the paper’s first author). Since it concatenates the
          encoder output with the decoder’s previous hidden state, it is sometimes called con‐
          catenative attention (or additive attention).               
                                                                      
          15 Recall that a time-distributed Dense layer is equivalent to a regular Dense layer that you apply independently
           at each time step (only much faster).                      "|concatenative attention; additive attention; Bahdanau attention
"                                                                      
                                                                      
                                                                      
                                                                      
          Discover and Visualize the Data to Gain Insights            
                                                                      
          So far you have only taken a quick glance at the data to get a general understanding of
          the kind of data you are manipulating. Now the goal is to go into a little more depth.
                                                                      
          First, make sure you have put the test set aside and you are only exploring the train‐
          ing set. Also, if the training set is very large, you may want to sample an exploration
          set, to make manipulations easy and fast. In our case, the set is quite small, so you can
          just work directly on the full set. Let’s create a copy so that you can play with it
          without harming the training set:                           
            housing = strat_train_set.copy()                          
          Visualizing Geographical Data                               
                                                                      
          Since there is geographical information (latitude and longitude), it is a good idea to
          create a scatterplot of all districts to visualize the data (Figure 2-11):
                                                                      
            housing.plot(kind=""scatter"", x=""longitude"", y=""latitude"") 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-11. A geographical scatterplot of the data         
                                                                      
          This looks like California all right, but other than that it is hard to see any particular
          pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places
          where there is a high density of data points (Figure 2-12): 
                                                                      
            housing.plot(kind=""scatter"", x=""longitude"", y=""latitude"", alpha=0.1)
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|data visualization; geographical data; exploration sets; test, training, and exploration sets
"                                                                      
                                                                      
                                                                      
                                                                      
            ment. At regular intervals, but asynchronously (hence the name), each agent
            pushes some weight updates to a master network, then it pulls the latest weights
            from that network. Each agent thus contributes to improving the master network
            and benefits from what the other agents have learned. Moreover, instead of esti‐
            mating the Q-Values, the DQN estimates the advantage of each action (hence the
            second A in the name), which stabilizes training.         
                                                                      
          Advantage Actor-Critic (A2C)                                
            A variant of the A3C algorithm that removes the asynchronicity. All model
            updates are synchronous, so gradient updates are performed over larger batches,
            which allows the model to better utilize the power of the GPU.
          Soft Actor-Critic24 (SAC)                                   
            An Actor-Critic variant proposed in 2018 by Tuomas Haarnoja and other UC
            Berkeley researchers. It learns not only rewards, but also to maximize the entropy
            of its actions. In other words, it tries to be as unpredictable as possible while still
            getting as many rewards as possible. This encourages the agent to explore the
            environment, which speeds up training, and makes it less likely to repeatedly exe‐
            cute the same action when the DQN produces imperfect estimates. This algo‐
            rithm has demonstrated an amazing sample efficiency (contrary to all the
            previous algorithms, which learn very slowly). SAC is available in TF-Agents.
          Proximal Policy Optimization (PPO)25                        
            An algorithm based on A2C that clips the loss function to avoid excessively large
            weight updates (which often lead to training instabilities). PPO is a simplification
            of the previous Trust Region Policy Optimization26 (TRPO) algorithm, also by
            John Schulman and other OpenAI researchers. OpenAI made the news in April
            2019 with their AI called OpenAI Five, based on the PPO algorithm, which
            defeated the world champions at the multiplayer game Dota 2. PPO is also avail‐
            able in TF-Agents.                                        
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          24 Tuomas Haarnoja et al., “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with
           a Stochastic Actor,” Proceedings of the 35th International Conference on Machine Learning (2018): 1856–1865.
          25 John Schulman et al., “Proximal Policy Optimization Algorithms,” arXiv preprint arXiv:1707.06347 (2017).
          26 John Schulman et al., “Trust Region Policy Optimization,” Proceedings of the 32nd International Conference on
           Machine Learning (2015): 1889–1897.                        "|Soft Actor-Critic algorithm; Proximal Policy Optimization (PPO); Advantage Actor-Critic (A2C)
"                                                                      
                                                                      
                                                                      
                                                                      
            from sklearn.base import BaseEstimator                    
                                                                      
            class Never5Classifier(BaseEstimator):                    
               def fit(self, X, y=None):                              
                 return self                                          
               def predict(self, X):                                  
                 return np.zeros((len(X), 1), dtype=bool)             
          Can you guess this model’s accuracy? Let’s find out:        
            >>> never_5_clf = Never5Classifier()                      
            >>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=""accuracy"")
            array([0.91125, 0.90855, 0.90915])                        
          That’s right, it has over 90% accuracy! This is simply because only about 10% of the
          images are 5s, so if you always guess that an image is not a 5, you will be right about
          90% of the time. Beats Nostradamus.                         
          This demonstrates why accuracy is generally not the preferred performance measure
          for classifiers, especially when you are dealing with skewed datasets (i.e., when some
          classes are much more frequent than others).                
                                                                      
          Confusion Matrix                                            
          A much better way to evaluate the performance of a classifier is to look at the confu‐
          sion matrix. The general idea is to count the number of times instances of class A are
          classified as class B. For example, to know the number of times the classifier confused
          images of 5s with 3s, you would look in the fifth row and third column of the confu‐
          sion matrix.                                                
                                                                      
          To compute the confusion matrix, you first need to have a set of predictions so that
          they can be compared to the actual targets. You could make predictions on the test
          set, but let’s keep it untouched for now (remember that you want to use the test set
          only at the very end of your project, once you have a classifier that you are ready to
          launch). Instead, you can use the cross_val_predict() function:
            from sklearn.model_selection import cross_val_predict     
            y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
                                                                      
          Just like the cross_val_score() function, cross_val_predict() performs K-fold
          cross-validation, but instead of returning the evaluation scores, it returns the predic‐
          tions made on each test fold. This means that you get a clean prediction for each
          instance in the training set (“clean” meaning that the prediction is made by a model
          that never saw the data during training).                   
          Now you are ready to get the confusion matrix using the confusion_matrix() func‐
          tion. Just pass it the target classes (y_train_5) and the predicted classes
          (y_train_pred):                                             
                                                                      "|accuracy; confusion matrix; skewed datasets
"                                                                      
                                                                      
                                                                      
                                                                      
          As you can see, on the left the Gradient Descent algorithm goes straight toward the
          minimum, thereby reaching it quickly, whereas on the right it first goes in a direction
          almost orthogonal to the direction of the global minimum, and it ends with a long
          march down an almost flat valley. It will eventually reach the minimum, but it will
          take a long time.                                           
                                                                      
                   When using Gradient Descent, you should ensure that all features
                   have a similar scale (e.g., using Scikit-Learn’s StandardScaler
                   class), or else it will take much longer to converge.
                                                                      
                                                                      
          This diagram also illustrates the fact that training a model means searching for a
          combination of model parameters that minimizes a cost function (over the training
          set). It is a search in the model’s parameter space: the more parameters a model has,
          the more dimensions this space has, and the harder the search is: searching for a nee‐
          dle in a 300-dimensional haystack is much trickier than in 3 dimensions. Fortunately,
          since the cost function is convex in the case of Linear Regression, the needle is simply
          at the bottom of the bowl.                                  
                                                                      
          Batch Gradient Descent                                      
                                                                      
          To implement Gradient Descent, you need to compute the gradient of the cost func‐
          tion with regard to each model parameter θ. In other words, you need to calculate
                                    j                                 
          how much the cost function will change if you change θ just a little bit. This is called
                                           j                          
          a partial derivative. It is like asking “What is the slope of the mountain under my feet
          if I face east?” and then asking the same question facing north (and so on for all other
          dimensions, if you can imagine a universe with more than three dimensions). Equa‐
          tion 4-5 computes the partial derivative of the cost function with regard to parameter
          θ, noted ∂ MSE(θ) / ∂θ.                                     
           j           j                                              
            Equation 4-5. Partial derivatives of the cost function    
                      m                                               
             ∂ MSE θ = 2 ∑ θ ⊺ x i −y i x i                           
            ∂θ      m           j                                     
              j      i=1                                              
          Instead of computing these partial derivatives individually, you can use Equation 4-6
          to compute them all in one go. The gradient vector, noted ∇ MSE(θ), contains all the
                                             θ                        
          partial derivatives of the cost function (one for each model parameter).
                                                                      "|parameter space; Batch Gradient Descent; partial derivatives
"                                                                      
                                                                      
                                                                      
                                                                      
          numbers representing the overall level of response for each filter. The next layer is
          where the “squeeze” happens: this layer has significantly fewer than 256 neurons—
          typically 16 times fewer than the number of feature maps (e.g., 16 neurons)—so the
          256 numbers get compressed into a small vector (e.g., 16 dimensions). This is a low-
          dimensional vector representation (i.e., an embedding) of the distribution of feature
          responses. This bottleneck step forces the SE block to learn a general representation
          of the feature combinations (we will see this principle in action again when we dis‐
          cuss autoencoders in Chapter 17). Finally, the output layer takes the embedding and
          outputs a recalibration vector containing one number per feature map (e.g., 256),
          each between 0 and 1. The feature maps are then multiplied by this recalibration vec‐
          tor, so irrelevant features (with a low recalibration score) get scaled down while rele‐
          vant features (with a recalibration score close to 1) are left alone.
                                                                      
          Implementing a ResNet-34 CNN Using Keras                    
                                                                      
          Most CNN architectures described so far are fairly straightforward to implement
          (although generally you would load a pretrained network instead, as we will see). To
          illustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s
          create a ResidualUnit layer:                                
            class ResidualUnit(keras.layers.Layer):                   
               def __init__(self, filters, strides=1, activation=""relu"", **kwargs):
                 super().__init__(**kwargs)                           
                 self.activation = keras.activations.get(activation)  
                 self.main_layers = [                                 
                   keras.layers.Conv2D(filters, 3, strides=strides,   
                               padding=""same"", use_bias=False),       
                   keras.layers.BatchNormalization(),                 
                   self.activation,                                   
                   keras.layers.Conv2D(filters, 3, strides=1,         
                               padding=""same"", use_bias=False),       
                   keras.layers.BatchNormalization()]                 
                 self.skip_layers = []                                
                 if strides > 1:                                      
                   self.skip_layers = [                               
                      keras.layers.Conv2D(filters, 1, strides=strides,
                                 padding=""same"", use_bias=False),     
                      keras.layers.BatchNormalization()]              
               def call(self, inputs):                                
                 Z = inputs                                           
                 for layer in self.main_layers:                       
                   Z = layer(Z)                                       
                 skip_Z = inputs                                      
                 for layer in self.skip_layers:                       
                   skip_Z = layer(skip_Z)                             
                 return self.activation(Z + skip_Z)                   "|implementing ResNet-34 with; CNN architectures; ResNet-34 CNN; ResNet-34 using Keras
"                                                                      
                                                                      
                                                                      
                                                                      
          A closely related task is novelty detection: it differs from anomaly detection in that the
          algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers,
          whereas anomaly detection does not make this assumption. Indeed, outlier detection
          is often used to clean up a dataset.                        
                                                                      
                   Gaussian mixture models try to fit all the data, including the outli‐
                   ers, so if you have too many of them, this will bias the model’s view
                   of “normality,” and some outliers may wrongly be considered as
                   normal. If this happens, you can try to fit the model once, use it to
                   detect and remove the most extreme outliers, then fit the model
                   again on the cleaned-up dataset. Another approach is to use robust
                   covariance estimation methods (see the EllipticEnvelope class).
                                                                      
          Just like K-Means, the GaussianMixture algorithm requires you to specify the num‐
          ber of clusters. So, how can you find it?                   
          Selecting the Number of Clusters                            
                                                                      
          With K-Means, you could use the inertia or the silhouette score to select the appro‐
          priate number of clusters. But with Gaussian mixtures, it is not possible to use these
          metrics because they are not reliable when the clusters are not spherical or have dif‐
          ferent sizes. Instead, you can try to find the model that minimizes a theoretical infor‐
          mation criterion, such as the Bayesian information criterion (BIC) or the Akaike
          information criterion (AIC), defined in Equation 9-1.       
                                                                      
            Equation 9-1. Bayesian information criterion (BIC) and Akaike information
            criterion (AIC)                                           
            BIC= log m p−2log L                                       
                                                                      
            AIC= 2p−2log L                                            
                                                                      
          In these equations:                                         
                                                                      
           • m is the number of instances, as always.                 
           • p is the number of parameters learned by the model.      
                                                                      
           • L is the maximized value of the likelihood function of the model.
          Both the BIC and the AIC penalize models that have more parameters to learn (e.g.,
          more clusters) and reward models that fit the data well. They often end up selecting
          the same model. When they differ, the model selected by the BIC tends to be simpler
                                                                      
                                                                      
                                                                      "|likelihood function; selecting cluster number; Akaike information criterion (AIC); theoretical information criterion; novelty detection; Bayesian information criterion (BIC)
"                                                                      
                                                                      
                                                                      
                                                                      
          Isomap                                                      
            Creates a graph by connecting each instance to its nearest neighbors, then
            reduces dimensionality while trying to preserve the geodesic distances9 between
            the instances.                                            
                                                                      
          t-Distributed Stochastic Neighbor Embedding (t-SNE)         
            Reduces dimensionality while trying to keep similar instances close and dissimi‐
            lar instances apart. It is mostly used for visualization, in particular to visualize
            clusters of instances in high-dimensional space (e.g., to visualize the MNIST
            images in 2D).                                            
          Linear Discriminant Analysis (LDA)                          
            Is a classification algorithm, but during training it learns the most discriminative
            axes between the classes, and these axes can then be used to define a hyperplane
            onto which to project the data. The benefit of this approach is that the projection
            will keep classes as far apart as possible, so LDA is a good technique to reduce
            dimensionality before running another classification algorithm such as an SVM
            classifier.                                               
          Figure 8-13 shows the results of a few of these techniques. 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-13. Using various techniques to reduce the Swill roll to 2D
                                                                      
          Exercises                                                   
                                                                      
           1. What are the main motivations for reducing a dataset’s dimensionality? What are
            the main drawbacks?                                       
           2. What is the curse of dimensionality?                    
                                                                      
                                                                      
                                                                      
                                                                      
          9 The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between
           these nodes.                                               "|isomap algorithm; Linear Discriminant Analysis (LDA); t-Distributed Stochastic Neighbor Embedding (t-SNE)
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-13. In online learning, a model is trained and launched into production, and
          then it keeps learning as new data comes in                 
                                                                      
          Online learning is great for systems that receive data as a continuous flow (e.g., stock
          prices) and need to adapt to change rapidly or autonomously. It is also a good option
          if you have limited computing resources: once an online learning system has learned
          about new data instances, it does not need them anymore, so you can discard them
          (unless you want to be able to roll back to a previous state and “replay” the data). This
          can save a huge amount of space.                            
          Online learning algorithms can also be used to train systems on huge datasets that
          cannot fit in one machine’s main memory (this is called out-of-core learning). The
          algorithm loads part of the data, runs a training step on that data, and repeats the
          process until it has run on all of the data (see Figure 1-14).
                                                                      
                                                                      
                   Out-of-core learning is usually done offline (i.e., not on the live
                   system), so online learning can be a confusing name. Think of it as
                   incremental learning.                              
                                                                      
                                                                      
          One important parameter of online learning systems is how fast they should adapt to
          changing data: this is called the learning rate. If you set a high learning rate, then your
          system will rapidly adapt to new data, but it will also tend to quickly forget the old
          data (you don’t want a spam filter to flag only the latest kinds of spam it was shown).
          Conversely, if you set a low learning rate, the system will have more inertia; that is, it
          will learn more slowly, but it will also be less sensitive to noise in the new data or to
          sequences of nonrepresentative data points (outliers).      
                                                                      
                                                                      
                                                                      "|learning rate; out-of-core learning; incremental learning
"                                                                      
                                                                      
                                                                      
                                                                      
            • h is your system’s prediction function, also called a hypothesis. When your system
              is given an instance’s feature vector x(i), it outputs a predicted value ŷ(i) = h(x(i))
              for that instance (ŷ is pronounced “y-hat”).            
              —For example, if your system predicts that the median housing price in the first
               district is $158,400, then ŷ(1) = h(x(1)) = 158,400. The prediction error for this
               district is ŷ(1) – y(1) = 2,000.                       
            • RMSE(X,h) is the cost function measured on the set of examples using your
              hypothesis h.                                           
                                                                      
           We use lowercase italic font for scalar values (such as m or y(i)) and function names
           (such as h), lowercase bold font for vectors (such as x(i)), and uppercase bold font for
           matrices (such as X).                                      
                                                                      
          Even though the RMSE is generally the preferred performance measure for regression
          tasks, in some contexts you may prefer to use another function. For example, suppose
          that there are many outlier districts. In that case, you may consider using the mean
          absolute error (MAE, also called the average absolute deviation; see Equation 2-2):
                                                                      
            Equation 2-2. Mean absolute error (MAE)                   
                                                                      
                      m                                               
            MAE X,h = 1 ∑ h x i −y i                                  
                    m                                                 
                     i=1                                              
          Both the RMSE and the MAE are ways to measure the distance between two vectors:
          the vector of predictions and the vector of target values. Various distance measures,
          or norms, are possible:                                     
           • Computing the root of a sum of squares (RMSE) corresponds to the Euclidean
            norm: this is the notion of distance you are familiar with. It is also called the ℓ
                                                           2          
            norm, noted ∥ · ∥ (or just ∥ · ∥).                        
                      2                                               
           • Computing the sum of absolutes (MAE) corresponds to the ℓ norm, noted ∥ · ∥ .
                                                1          1          
            This is sometimes called the Manhattan norm because it measures the distance
            between two points in a city if you can only travel along orthogonal city blocks.
           • More generally, the ℓ norm of a vector v containing n elements is defined as ∥v∥
                         k                                 k          
            = (|v |k + |v |k + ... + |v |k)1/k. ℓ gives the number of nonzero elements in the vec‐
               0   1      n   0                                       
            tor, and ℓ gives the maximum absolute value in the vector.
                  ∞                                                   
           • The higher the norm index, the more it focuses on large values and neglects small
            ones. This is why the RMSE is more sensitive to outliers than the MAE. But when
            outliers are exponentially rare (like in a bell-shaped curve), the RMSE performs
            very well and is generally preferred.                     "|Manhattan norm; mean absolute error (MAE); Euclidean norm; average absolute deviation
"                                                                      
                                                                      
                                                                      
                                                                      
          the evaluations of a model, you get a much more accurate measure of its perfor‐
          mance. There is a drawback, however: the training time is multiplied by the number
          of validation sets.                                         
                                                                      
          Data Mismatch                                               
                                                                      
          In some cases, it’s easy to get a large amount of data for training, but this data proba‐
          bly won’t be perfectly representative of the data that will be used in production. For
          example, suppose you want to create a mobile app to take pictures of flowers and
          automatically determine their species. You can easily download millions of pictures of
          flowers on the web, but they won’t be perfectly representative of the pictures that will
          actually be taken using the app on a mobile device. Perhaps you only have 10,000 rep‐
          resentative pictures (i.e., actually taken with the app). In this case, the most important
          rule to remember is that the validation set and the test set must be as representative as
          possible of the data you expect to use in production, so they should be composed
          exclusively of representative pictures: you can shuffle them and put half in the valida‐
          tion set and half in the test set (making sure that no duplicates or near-duplicates end
          up in both sets). But after training your model on the web pictures, if you observe
          that the performance of the model on the validation set is disappointing, you will not
          know whether this is because your model has overfit the training set, or whether this
          is just due to the mismatch between the web pictures and the mobile app pictures.
          One solution is to hold out some of the training pictures (from the web) in yet
          another set that Andrew Ng calls the train-dev set. After the model is trained (on the
          training set, not on the train-dev set), you can evaluate it on the train-dev set. If it
          performs well, then the model is not overfitting the training set. If it performs poorly
          on the validation set, the problem must be coming from the data mismatch. You can
          try to tackle this problem by preprocessing the web images to make them look more
          like the pictures that will be taken by the mobile app, and then retraining the model.
          Conversely, if the model performs poorly on the train-dev set, then it must have over‐
          fit the training set, so you should try to simplify or regularize the model, get more
          training data, and clean up the training data.              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|data mismatch; train-dev sets
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-7. Bad choices for the number of clusters: when k is too small, separate clusters
          get merged (left), and when k is too large, some clusters get chopped into multiple pieces
          (right)                                                     
                                                                      
          You might be thinking that we could just pick the model with the lowest inertia,
          right? Unfortunately, it is not that simple. The inertia for k=3 is 653.2, which is much
          higher than for k=5 (which was 211.6). But with k=8, the inertia is just 119.1. The
          inertia is not a good performance metric when trying to choose k because it keeps
          getting lower as we increase k. Indeed, the more clusters there are, the closer each
          instance will be to its closest centroid, and therefore the lower the inertia will be. Let’s
          plot the inertia as a function of k (see Figure 9-8).       
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-8. When plotting the inertia as a function of the number of clusters k, the curve
          often contains an inflexion point called the “elbow”        
                                                                      
          As you can see, the inertia drops very quickly as we increase k up to 4, but then it
          decreases much more slowly as we keep increasing k. This curve has roughly the
          shape of an arm, and there is an “elbow” at k = 4. So, if we did not know better, 4
          would be a good choice: any lower value would be dramatic, while any higher value
          would not help much, and we might just be splitting perfectly good clusters in half for
          no good reason.                                             
          This technique for choosing the best value for the number of clusters is rather coarse.
          A more precise approach (but also more computationally expensive) is to use the
          silhouette score, which is the mean silhouette coefficient over all the instances. An
                                                                      "|silhouette coefficient; silhouette score
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                   CHAPTER 8          
                                                                      
                              Dimensionality    Reduction             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Many Machine Learning problems involve thousands or even millions of features for
          each training instance. Not only do all these features make training extremely slow,
          but they can also make it much harder to find a good solution, as we will see. This
          problem is often referred to as the curse of dimensionality.
          Fortunately, in real-world problems, it is often possible to reduce the number of fea‐
          tures considerably, turning an intractable problem into a tractable one. For example,
          consider the MNIST images (introduced in Chapter 3): the pixels on the image bor‐
          ders are almost always white, so you could completely drop these pixels from the
          training set without losing much information. Figure 7-6 confirms that these pixels
          are utterly unimportant for the classification task. Additionally, two neighboring pix‐
          els are often highly correlated: if you merge them into a single pixel (e.g., by taking
          the mean of the two pixel intensities), you will not lose much information.
                                                                      
                   Reducing dimensionality does cause some information loss (just
                   like compressing an image to JPEG can degrade its quality), so
                   even though it will speed up training, it may make your system
                   perform slightly worse. It also makes your pipelines a bit more
                   complex and thus harder to maintain. So, if training is too slow,
                   you should first try to train your system with the original data
                   before considering using dimensionality reduction. In some cases,
                   reducing the dimensionality of the training data may filter out
                   some noise and unnecessary details and thus result in higher per‐
                   formance, but in general it won’t; it will just speed up training.
          Apart from speeding up training, dimensionality reduction is also extremely useful
          for data visualization (or DataViz). Reducing the number of dimensions down to two
          (or three) makes it possible to plot a condensed view of a high-dimensional training
                                                                      "|training sets; high-dimensional training sets; dimensionality reduction
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-12. Variational autoencoder (left) and an instance going through it (right)
                                                                      
          As you can see in the diagram, although the inputs may have a very convoluted distri‐
          bution, a variational autoencoder tends to produce codings that look as though they
          were sampled from a simple Gaussian distribution:8 during training, the cost function
          (discussed next) pushes the codings to gradually migrate within the coding space
          (also called the latent space) to end up looking like a cloud of Gaussian points. One
          great consequence is that after training a variational autoencoder, you can very easily
          generate a new instance: just sample a random coding from the Gaussian distribu‐
          tion, decode it, and voilà!                                 
          Now, let’s look at the cost function. It is composed of two parts. The first is the usual
          reconstruction loss that pushes the autoencoder to reproduce its inputs (we can use
          cross entropy for this, as discussed earlier). The second is the latent loss that pushes
          the autoencoder to have codings that look as though they were sampled from a simple
          Gaussian distribution: it is the KL divergence between the target distribution (i.e., the
          Gaussian distribution) and the actual distribution of the codings. The math is a bit
          more complex than with the sparse autoencoder, in particular because of the Gaus‐
          sian noise, which limits the amount of information that can be transmitted to the
          coding layer (thus pushing the autoencoder to learn useful features). Luckily, the
                                                                      
                                                                      
                                                                      
          8 Variational autoencoders are actually more general; the codings are not limited to Gaussian distributions."|latent loss
"                                                                      
                                                                      
                                                                      
                                                                      
          300 units), and add some ℓ regularization to the coding layer’s activations (the
                           1                                          
          decoder is just a regular decoder):                         
            sparse_l1_encoder = keras.models.Sequential([             
               keras.layers.Flatten(input_shape=[28, 28]),            
               keras.layers.Dense(100, activation=""selu""),            
               keras.layers.Dense(300, activation=""sigmoid""),         
               keras.layers.ActivityRegularization(l1=1e-3)           
            ])                                                        
            sparse_l1_decoder = keras.models.Sequential([             
               keras.layers.Dense(100, activation=""selu"", input_shape=[300]),
               keras.layers.Dense(28 * 28, activation=""sigmoid""),     
               keras.layers.Reshape([28, 28])                         
            ])                                                        
            sparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])
          This ActivityRegularization layer just returns its inputs, but as a side effect it adds
          a training loss equal to the sum of absolute values of its inputs (this layer only has an
          effect during training). Equivalently, you could remove the ActivityRegularization
          layer and set activity_regularizer=keras.regularizers.l1(1e-3) in the previous
          layer. This penalty will encourage the neural network to produce codings close to 0,
          but since it will also be penalized if it does not reconstruct the inputs correctly, it will
          have to output at least a few nonzero values. Using the ℓ norm rather than the ℓ
                                            1              2          
          norm will push the neural network to preserve the most important codings while
          eliminating the ones that are not needed for the input image (rather than just reduc‐
          ing all codings).                                           
          Another approach, which often yields better results, is to measure the actual sparsity
          of the coding layer at each training iteration, and penalize the model when the meas‐
          ured sparsity differs from a target sparsity. We do so by computing the average activa‐
          tion of each neuron in the coding layer, over the whole training batch. The batch size
          must not be too small, or else the mean will not be accurate.
          Once we have the mean activation per neuron, we want to penalize the neurons that
          are too active, or not active enough, by adding a sparsity loss to the cost function. For
          example, if we measure that a neuron has an average activation of 0.3, but the target
          sparsity is 0.1, it must be penalized to activate less. One approach could be simply
          adding the squared error (0.3 – 0.1)2 to the cost function, but in practice a better
          approach is to use the Kullback–Leibler (KL) divergence (briefly discussed in Chap‐
          ter 4), which has much stronger gradients than the mean squared error, as you can
          see in Figure 17-10.                                        
                                                                      
                                                                      
                                                                      "|mean squared error; sparsity loss
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 14          
                                                                      
                           Deep   Computer    Vision Using            
                                                                      
                        Convolutional   Neural   Networks             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐
          parov back in 1996, it wasn’t until fairly recently that computers were able to reliably
          perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing
          spoken words. Why are these tasks so effortless to us humans? The answer lies in the
          fact that perception largely takes place outside the realm of our consciousness, within
          specialized visual, auditory, and other sensory modules in our brains. By the time
          sensory information reaches our consciousness, it is already adorned with high-level
          features; for example, when you look at a picture of a cute puppy, you cannot choose
          not to see the puppy, not to notice its cuteness. Nor can you explain how you recog‐
          nize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective expe‐
          rience: perception is not trivial at all, and to understand it we must look at how the
          sensory modules work.                                       
          Convolutional neural networks (CNNs) emerged from the study of the brain’s visual
          cortex, and they have been used in image recognition since the 1980s. In the last few
          years, thanks to the increase in computational power, the amount of available training
          data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man‐
          aged to achieve superhuman performance on some complex visual tasks. They power
          image search services, self-driving cars, automatic video classification systems, and
          more. Moreover, CNNs are not restricted to visual perception: they are also successful
          at many other tasks, such as voice recognition and natural language processing. How‐
          ever, we will focus on visual applications for now.         
          In this chapter we will explore where CNNs came from, what their building blocks
          look like, and how to implement them using TensorFlow and Keras. Then we will dis‐
          cuss some of the best CNN architectures, as well as other visual tasks, including
                                                                      "|voice recognition; CNNs
"                                                                      
                                                                      
                                                                      
                                                                      
          for it to overfit the data. A simple way to regularize a polynomial model is to reduce
          the number of polynomial degrees.                           
                                                                      
          For a linear model, regularization is typically achieved by constraining the weights of
          the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,
          which implement three different ways to constrain the weights.
          Ridge Regression                                            
                                                                      
          Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin‐
          ear Regression: a regularization term equal to α∑n θ2 is added to the cost function.
                                      i=1 i                           
          This forces the learning algorithm to not only fit the data but also keep the model
          weights as small as possible. Note that the regularization term should only be added
          to the cost function during training. Once the model is trained, you want to use the
          unregularized performance measure to evaluate the model’s performance.
                   It is quite common for the cost function used during training to be
                   different from the performance measure used for testing. Apart
                   from regularization, another reason they might be different is that a
                   good training cost function should have optimization-friendly
                   derivatives, while the performance measure used for testing should
                   be as close as possible to the final objective. For example, classifiers
                   are often trained using a cost function such as the log loss (dis‐
                   cussed in a moment) but evaluated using precision/recall.
                                                                      
          The hyperparameter α controls how much you want to regularize the model. If α = 0,
          then Ridge Regression is just Linear Regression. If α is very large, then all weights end
          up very close to zero and the result is a flat line going through the data’s mean. Equa‐
          tion 4-8 presents the Ridge Regression cost function.9      
                                                                      
            Equation 4-8. Ridge Regression cost function              
                      1 n  2                                          
            J θ =MSE θ +α ∑ θ                                         
                      2 i=1 i                                         
          Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). If we
                         0                                            
          define w as the vector of feature weights (θ to θ ), then the regularization term is
                                    1  n                              
                                                                      
                                                                      
          9 It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this
           notation throughout the rest of this book. The context will make it clear which cost function is being dis‐
           cussed.                                                    "|Tikhonov regularization; regularization terms; Ridge Regression
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> housing_cat = housing[[""ocean_proximity""]]            
            >>> housing_cat.head(10)                                  
                ocean_proximity                                       
            17606  <1H OCEAN                                          
            18632  <1H OCEAN                                          
            14650  NEAR OCEAN                                         
            3230     INLAND                                           
            3555   <1H OCEAN                                          
            19480    INLAND                                           
            8879   <1H OCEAN                                          
            13685    INLAND                                           
            4937   <1H OCEAN                                          
            4861   <1H OCEAN                                          
          It’s not arbitrary text: there are a limited number of possible values, each of which
          represents a category. So this attribute is a categorical attribute. Most Machine Learn‐
          ing algorithms prefer to work with numbers, so let’s convert these categories from
          text to numbers. For this, we can use Scikit-Learn’s OrdinalEncoder class:19
            >>> from sklearn.preprocessing import OrdinalEncoder      
            >>> ordinal_encoder = OrdinalEncoder()                    
            >>> housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
            >>> housing_cat_encoded[:10]                              
            array([[0.],                                              
                [0.],                                                 
                [4.],                                                 
                [1.],                                                 
                [0.],                                                 
                [1.],                                                 
                [0.],                                                 
                [1.],                                                 
                [0.],                                                 
                [0.]])                                                
          You can get the list of categories using the categories_ instance variable. It is a list
          containing a 1D array of categories for each categorical attribute (in this case, a list
          containing a single array since there is just one categorical attribute):
            >>> ordinal_encoder.categories_                           
            [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
                dtype=object)]                                        
          One issue with this representation is that ML algorithms will assume that two nearby
          values are more similar than two distant values. This may be fine in some cases (e.g.,
          for ordered categories such as “bad,” “average,” “good,” and “excellent”), but it is obvi‐
          ously not the case for the ocean_proximity column (for example, categories 0 and 4
          are clearly more similar than categories 0 and 1). To fix this issue, a common solution
          19 This class is available in Scikit-Learn 0.20 and later. If you use an earlier version, please consider upgrading, or
           use the pandas Series.factorize() method.                  "|converting text to numbers
"                                                                      
                                                                      
                                                                      
                                                                      
          ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and
          the fully connected layer)17 containing 3 residual units that output 64 feature maps, 4
          RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐
          ment this architecture later in this chapter.               
                                                                      
          ResNets deeper than that, such as ResNet-152, use slightly different residual units.
          Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three
          convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4
          times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer
          with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature
          maps (4 times 64) that restores the original depth. ResNet-152 contains 3 such RUs
          that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024
          maps, and finally 3 RUs with 2,048 maps.                    
                                                                      
                   Google’s Inception-v418 architecture merged the ideas of GoogLe‐
                   Net and ResNet and achieved a top-five error rate of close to 3% on
                   ImageNet classification.                           
                                                                      
          Xception                                                    
                                                                      
          Another variant of the GoogLeNet architecture is worth noting: Xception19 (which
          stands for Extreme Inception) was proposed in 2016 by François Chollet (the author
          of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350
          million images and 17,000 classes). Just like Inception-v4, it merges the ideas of Goo‐
          gLeNet and ResNet, but it replaces the inception modules with a special type of layer
          called a depthwise separable convolution layer (or separable convolution layer for
          short20). These layers had been used before in some CNN architectures, but they were
          not as central as in the Xception architecture. While a regular convolutional layer
          uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-
          channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer
          makes the strong assumption that spatial patterns and cross-channel patterns can be
          modeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part
          applies a single spatial filter for each input feature map, then the second part looks
                                                                      
                                                                      
          17 It is a common practice when describing a neural network to count only layers with parameters.
          18 Christian Szegedy et al., “Inception–v4, Inception-ResNet and the Impact of Residual Connections on Learn‐
           ing,” arXiv preprint arXiv:1602.07261 (2016).              
          19 François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions,” arXiv preprint arXiv:
           1610.02357 (2016).                                         
          20 This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable
           convolutions” as well.                                     "|separable convolution; depthwise separable convolution; Xception (Extreme Inception)
"                                                                      
                                                                      
                                                                      
                                                                      
            Transformer-like architecture. The authors pretrained a large but fairly simple
            architecture composed of a stack of 12 Transformer modules (using only Masked
            Multi-Head Attention layers) on a large dataset, once again trained using self-
            supervised learning. Then they fine-tuned it on various language tasks, using
            only minor adaptations for each task. The tasks were quite diverse: they included
            text classification, entailment (whether sentence A entails sentence B),27 similarity
            (e.g., “Nice weather today” is very similar to “It is sunny”), and question answer‐
            ing (given a few paragraphs of text giving some context, the model must answer
            some multiple-choice questions). Just a few months later, in February 2019, Alec
            Radford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper,28
            which proposed a very similar architecture, but larger still (with over 1.5 billion
            parameters!) and they showed that it could achieve good performance on many
            tasks without any fine-tuning. This is called zero-shot learning (ZSL). A smaller
            version of the GPT-2 model (with “just” 117 million parameters) is available at
            https://github.com/openai/gpt-2, along with its pretrained weights.
           • The BERT paper29 by Jacob Devlin and other Google researchers also demon‐
            strates the effectiveness of self-supervised pretraining on a large corpus, using a
            similar architecture to GPT but non-masked Multi-Head Attention layers (like in
            the Transformer’s encoder). This means that the model is naturally bidirectional;
            hence the B in BERT (Bidirectional Encoder Representations from Transformers).
            Most importantly, the authors proposed two pretraining tasks that explain most
            of the model’s strength:                                  
            Masked language model (MLM)                               
               Each word in a sentence has a 15% probability of being masked, and the
               model is trained to predict the masked words. For example, if the original
               sentence is “She had fun at the birthday party,” then the model may be given
               the sentence “She <mask> fun at the <mask> party” and it must predict the
               words “had” and “birthday” (the other outputs will be ignored). To be more
               precise, each selected word has an 80% chance of being masked, a 10%
               chance of being replaced by a random word (to reduce the discrepancy
               between pretraining and fine-tuning, since the model will not see <mask>
               tokens during fine-tuning), and a 10% chance of being left alone (to bias the
               model toward the correct answer).                      
                                                                      
                                                                      
                                                                      
          27 For example, the sentence “Jane had a lot of fun at her friend’s birthday party” entails “Jane enjoyed the party,”
           but it is contradicted by “Everyone hated the party” and it is unrelated to “The Earth is flat.”
          28 Alec Radford et al., “Language Models Are Unsupervised Multitask Learners” (2019).
          29 Jacob Devlin et al., “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,”
           Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin‐
           guistics: Human Language Technologies 1 (2019).            "|entailment; masked language model (MLM); zero-shot learning (ZSL)
"                                                                      
                                                                      
                                                                      
                                                                      
                     Always set an alarm to remind yourself to turn services off
                     when you know you will only need them for a few hours, or
                     else you might leave them running for days or months, incur‐
                     ring potentially significant costs.              
                                                                      
           5. Now that you have a GCP account with billing activated, you can start using the
            services. The first one you will need is Google Cloud Storage (GCS): this is where
            you will put the SavedModels, the training data, and more. In the navigation
            menu, scroll down to the Storage section, and click Storage → Browser. All your
            files will go in one or more buckets. Click Create Bucket and choose the bucket
            name (you may need to activate the Storage API first). GCS uses a single world‐
            wide namespace for buckets, so simple names like “machine-learning” will most
            likely not be available. Make sure the bucket name conforms to DNS naming
            conventions, as it may be used in DNS records. Moreover, bucket names are pub‐
            lic, so do not put anything private in there. It is common to use your domain
            name or your company name as a prefix to ensure uniqueness, or simply use a
            random number as part of the name. Choose the location where you want the
            bucket to be hosted, and the rest of the options should be fine by default. Then
            click Create.                                             
           6. Upload the my_mnist_model folder you created earlier (including one or more
            versions) to your bucket. To do this, just go to the GCS Browser, click the bucket,
            then drag and drop the my_mnist_model folder from your system to the bucket
            (see Figure 19-4). Alternatively, you can click “Upload folder” and select the
            my_mnist_model folder to upload. By default, the maximum size for a SavedMo‐
            del is 250 MB, but it is possible to request a higher quota.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Figure 19-4. Uploading a SavedModel to Google Cloud Storage
                                                                      
                                                                      
                                                                      
                                                                      "|Google Cloud Storage (GCS)
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> np.mean(y_train_partially_propagated == y_train[partially_propagated])
            0.9896907216494846                                        
                                                                      
                             Active Learning                          
                                                                      
           To continue improving your model and your training set, the next step could be to do
           a few rounds of active learning, which is when a human expert interacts with the
           learning algorithm, providing labels for specific instances when the algorithm
           requests them. There are many different strategies for active learning, but one of the
           most common ones is called uncertainty sampling. Here is how it works:
            1. The model is trained on the labeled instances gathered so far, and this model is
              used to make predictions on all the unlabeled instances.
            2. The instances for which the model is most uncertain (i.e., when its estimated
              probability is lowest) are given to the expert to be labeled.
                                                                      
            3. You iterate this process until the performance improvement stops being worth
              the labeling effort.                                    
           Other strategies include labeling the instances that would result in the largest model
           change, or the largest drop in the model’s validation error, or the instances that differ‐
           ent models disagree on (e.g., an SVM or a Random Forest).  
                                                                      
                                                                      
          Before we move on to Gaussian mixture models, let’s take a look at DBSCAN,
          another popular clustering algorithm that illustrates a very different approach based
          on local density estimation. This approach allows the algorithm to identify clusters of
          arbitrary shapes.                                           
          DBSCAN                                                      
                                                                      
          This algorithm defines clusters as continuous regions of high density. Here is how it
          works:                                                      
                                                                      
           • For each instance, the algorithm counts how many instances are located within a
            small distance ε (epsilon) from it. This region is called the instance’s ε-
            neighborhood.                                             
           • If an instance has at least min_samples instances in its ε-neighborhood (includ‐
            ing itself), then it is considered a core instance. In other words, core instances are
            those that are located in dense regions.                  
           • All instances in the neighborhood of a core instance belong to the same cluster.
            This neighborhood may include other core instances; therefore, a long sequence
            of neighboring core instances forms a single cluster.     
                                                                      
                                                                      "|DBSCAN; uncertainty sampling; active learning; core instances
"                                                                      
                                                                      
                                                                      
                                                                      
            def split_train_test_by_id(data, test_ratio, id_column):  
               ids = data[id_column]                                  
               in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
               return data.loc[~in_test_set], data.loc[in_test_set]   
          Unfortunately, the housing dataset does not have an identifier column. The simplest
          solution is to use the row index as the ID:                 
            housing_with_id = housing.reset_index() # adds an `index` column
            train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, ""index"")
          If you use the row index as a unique identifier, you need to make sure that new data
          gets appended to the end of the dataset and that no row ever gets deleted. If this is not
          possible, then you can try to use the most stable features to build a unique identifier.
          For example, a district’s latitude and longitude are guaranteed to be stable for a few
          million years, so you could combine them into an ID like so:15
                                                                      
            housing_with_id[""id""] = housing[""longitude""] * 1000 + housing[""latitude""]
            train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, ""id"")
          Scikit-Learn provides a few functions to split datasets into multiple subsets in various
          ways. The simplest function is train_test_split(), which does pretty much the
          same thing as the function split_train_test(), with a couple of additional features.
          First, there is a random_state parameter that allows you to set the random generator
          seed. Second, you can pass it multiple datasets with an identical number of rows, and
          it will split them on the same indices (this is very useful, for example, if you have a
          separate DataFrame for labels):                             
            from sklearn.model_selection import train_test_split      
                                                                      
            train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)
          So far we have considered purely random sampling methods. This is generally fine if
          your dataset is large enough (especially relative to the number of attributes), but if it
          is not, you run the risk of introducing a significant sampling bias. When a survey
          company decides to call 1,000 people to ask them a few questions, they don’t just pick
          1,000 people randomly in a phone book. They try to ensure that these 1,000 people
          are representative of the whole population. For example, the US population is 51.3%
          females and 48.7% males, so a well-conducted survey in the US would try to maintain
          this ratio in the sample: 513 female and 487 male. This is called stratified sampling:
          the population is divided into homogeneous subgroups called strata, and the right
          number of instances are sampled from each stratum to guarantee that the test set is
          representative of the overall population. If the people running the survey used purely
          random sampling, there would be about a 12% chance of sampling a skewed test set
                                                                      
                                                                      
          15 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so
           they will end up in the same set (test or train). This introduces some unfortunate sampling bias."|splitting datasets into subsets; stratified sampling
"                                                                      
                                                                      
                                                                      
                                                                      
          number of filters, the kernel size, and the stride. After these convolutional layers, the
          encoding network will optionally apply a sequence of dense layers, if you set the
          fc_layer_params argument: it must be a list containing the number of neurons for
          each dense layer. Optionally, you can also pass a list of dropout rates (one per dense
          layer) via the dropout_layer_params argument if you want to apply dropout after
          each dense layer. The QNetwork takes the output of this encoding network and passes
          it to the dense output layer (with one unit per action).    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-14. Architecture of an encoding network           
                                                                      
                                                                      
                   The QNetwork class is flexible enough to build many different
                   architectures, but you can always build your own network class if
                   you need extra flexibility: extend the tf_agents.networks.Net
                   work class and implement it like a regular custom Keras layer. The
                   tf_agents.networks.Network class is a subclass of the keras.lay
                   ers.Layer class that adds some functionality required by some
                   agents, such as the possibility to easily create shallow copies of the
                   network (i.e., copying the network’s architecture, but not its
                   weights). For example, the DQNAgent uses this to create a copy of
                   the online model.                                  
          Now that we have the DQN, we are ready to build the DQN agent.
          Creating the DQN Agent                                      
                                                                      
          The TF-Agents library implements many types of agents, located in the tf_agents
          .agents package and its subpackages. We will use the tf_agents.agents
          .dqn.dqn_agent.DqnAgent class:                              
                                                                      
                                                                      "|DQN agents
"                                                                      
                                                                      
                                                                      
                                                                      
          String() are not TensorFlow operations (and neither are the other operations in this
          code), so they cannot be included in a TensorFlow Function (except by wrapping
          them in a tf.py_function() operation, which would make the code slower and less
          portable, as we saw in Chapter 12). Fortunately, TensorFlow does include special pro‐
          tobuf definitions for which it provides parsing operations. 
                                                                      
          TensorFlow Protobufs                                        
                                                                      
          The main protobuf typically used in a TFRecord file is the Example protobuf, which
          represents one instance in a dataset. It contains a list of named features, where each
          feature can either be a list of byte strings, a list of floats, or a list of integers. Here is
          the protobuf definition:                                    
            syntax = ""proto3"";                                        
            message BytesList { repeated bytes value = 1; }           
            message FloatList { repeated float value = 1 [packed = true]; }
            message Int64List { repeated int64 value = 1 [packed = true]; }
            message Feature {                                         
               oneof kind {                                           
                 BytesList bytes_list = 1;                            
                 FloatList float_list = 2;                            
                 Int64List int64_list = 3;                            
               }                                                      
            };                                                        
            message Features { map<string, Feature> feature = 1; };   
            message Example { Features features = 1; };               
          The definitions of BytesList, FloatList, and Int64List are straightforward
          enough. Note that [packed = true] is used for repeated numerical fields, for a more
          efficient encoding. A Feature contains either a BytesList, a FloatList, or an
          Int64List. A Features (with an s) contains a dictionary that maps a feature name to
          the corresponding feature value. And finally, an Example contains only a Features
          object.8 Here is how you could create a tf.train.Example representing the same per‐
          son as earlier and write it to a TFRecord file:             
            from tensorflow.train import BytesList, FloatList, Int64List
            from tensorflow.train import Feature, Features, Example   
            person_example = Example(                                 
               features=Features(                                     
                 feature={                                            
                   ""name"": Feature(bytes_list=BytesList(value=[b""Alice""])),
          8 Why was Example even defined, since it contains no more than a Features object? Well, TensorFlow’s devel‐
           opers may one day decide to add more fields to it. As long as the new Example definition still contains the
           features field, with the same ID, it will be backward compatible. This extensibility is one of the great features
           of protobufs.                                              "|TensorFlow protobufs
"                                                                      
                                                                      
                                                                      
                                                                      
          layer. We can then compile this model, using the ""sparse_categorical_crossen
          tropy"" loss and an Adam optimizer. Finally, we are ready to train the model for sev‐
          eral epochs (this may take many hours, depending on your hardware):
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],
                         dropout=0.2, recurrent_dropout=0.2),         
               keras.layers.GRU(128, return_sequences=True,           
                         dropout=0.2, recurrent_dropout=0.2),         
               keras.layers.TimeDistributed(keras.layers.Dense(max_id,
                                           activation=""softmax""))     
            ])                                                        
            model.compile(loss=""sparse_categorical_crossentropy"", optimizer=""adam"")
            history = model.fit(dataset, epochs=20)                   
          Using the Char-RNN Model                                    
          Now we have a model that can predict the next character in text written by Shake‐
          speare. To feed it some text, we first need to preprocess it like we did earlier, so let’s
          create a little function for this:                          
            def preprocess(texts):                                    
               X = np.array(tokenizer.texts_to_sequences(texts)) - 1  
               return tf.one_hot(X, max_id)                           
          Now let’s use the model to predict the next letter in some text:
            >>> X_new = preprocess([""How are yo""])                    
            >>> Y_pred = model.predict_classes(X_new)                 
            >>> tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char
            'u'                                                       
          Success! The model guessed right. Now let’s use this model to generate new text.
          Generating Fake Shakespearean Text                          
                                                                      
          To generate new text using the Char-RNN model, we could feed it some text, make
          the model predict the most likely next letter, add it at the end of the text, then give the
          extended text to the model to guess the next letter, and so on. But in practice this
          often leads to the same words being repeated over and over again. Instead, we can
          pick the next character randomly, with a probability equal to the estimated probabil‐
          ity, using TensorFlow’s tf.random.categorical() function. This will generate more
          diverse and interesting text. The categorical() function samples random class indi‐
          ces, given the class log probabilities (logits). To have more control over the diversity
          of the generated text, we can divide the logits by a number called the temperature,
          which we can tweak as we wish: a temperature close to 0 will favor the high-
          probability characters, while a very high temperature will give all characters an equal
          probability. The following next_char() function uses this approach to pick the next
          character to add to the input text:                         "|temperature; generating Shakespearean text
"                                                                      
                                                                      
                                                                      
                                                                      
          weights; and some have even been ported to TF Hub, such as SSD31 and Faster-
          RCNN,32 which are both quite popular. SSD is also a “single shot” detection model,
          similar to YOLO. Faster R-CNN is more complex: the image first goes through a
          CNN, then the output is passed to a Region Proposal Network (RPN) that proposes
          bounding boxes that are most likely to contain an object, and a classifier is run for
          each bounding box, based on the cropped output of the CNN.  
                                                                      
          The choice of detection system depends on many factors: speed, accuracy, available
          pretrained models, training time, complexity, etc. The papers contain tables of met‐
          rics, but there is quite a lot of variability in the testing environments, and the technol‐
          ogies evolve so fast that it is difficult to make a fair comparison that will be useful for
          most people and remain valid for more than a few months.    
          So, we can locate objects by drawing bounding boxes around them. Great! But per‐
          haps you want to be a bit more precise. Let’s see how to go down to the pixel level.
                                                                      
          Semantic Segmentation                                       
                                                                      
          In semantic segmentation, each pixel is classified according to the class of the object it
          belongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26. Note
          that different objects of the same class are not distinguished. For example, all the bicy‐
          cles on the right side of the segmented image end up as one big lump of pixels. The
          main difficulty in this task is that when images go through a regular CNN, they grad‐
          ually lose their spatial resolution (due to the layers with strides greater than 1); so, a
          regular CNN may end up knowing that there’s a person somewhere in the bottom left
          of the image, but it will not be much more precise than that.
          Just like for object detection, there are many different approaches to tackle this prob‐
          lem, some quite complex. However, a fairly simple solution was proposed in the 2015
          paper by Jonathan Long et al. we discussed earlier. The authors start by taking a pre‐
          trained CNN and turning it into an FCN. The CNN applies an overall stride of 32 to
          the input image (i.e., if you add up all the strides greater than 1), meaning the last
          layer outputs feature maps that are 32 times smaller than the input image. This is
          clearly too coarse, so they add a single upsampling layer that multiplies the resolution
          by 32.                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          31 Wei Liu et al., “SSD: Single Shot Multibox Detector,” Proceedings of the 14th European Conference on Computer
           Vision 1 (2016): 21–37.                                    
          32 Shaoqing Ren et al., “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,”
           Proceedings of the 28th International Conference on Neural Information Processing Systems 1 (2015): 91–99."|semantic segmentation; object detection; Region Proposal Network (RPN)
"                                                                      
                                                                      
                                                                      
                                                                      
          will learn to play an Atari game, you will need an Atari game simulator. If you want to
          program a walking robot, then the environment is the real world, and you can
          directly train your robot in that environment, but this has its limits: if the robot falls
          off a cliff, you can’t just click Undo. You can’t speed up time either; adding more com‐
          puting power won’t make the robot move any faster. And it’s generally too expensive
          to train 1,000 robots in parallel. In short, training is hard and slow in the real world,
          so you generally need a simulated environment at least for bootstrap training. For
          example, you may use a library like PyBullet or MuJoCo for 3D physics simulation.
                                                                      
          OpenAI Gym10 is a toolkit that provides a wide variety of simulated environments
          (Atari games, board games, 2D and 3D physical simulations, and so on), so you can
          train agents, compare them, or develop new RL algorithms.   
          Before installing the toolkit, if you created an isolated environment using virtualenv,
          you first need to activate it:                              
            $ cd $ML_PATH    # Your ML working directory (e.g., $HOME/ml)
            $ source my_env/bin/activate # on Linux or MacOS          
            $ .\my_env\Scripts\activate # on Windows                  
          Next, install OpenAI Gym (if you are not using a virtual environment, you will need
          to add the --user option, or have administrator rights):    
            $ python3 -m pip install -U gym                           
                                                                      
          Depending on your system, you may also need to install the Mesa OpenGL Utility
          (GLU) library (e.g., on Ubuntu 18.04 you need to run apt install libglu1-mesa).
          This library will be needed to render the first environment. Next, open up a Python
          shell or a Jupyter notebook and create an environment with make():
            >>> import gym                                            
            >>> env = gym.make(""CartPole-v1"")                         
            >>> obs = env.reset()                                     
            >>> obs                                                   
            array([-0.01258566, -0.00156614, 0.04207708, -0.00180545])
          Here, we’ve created a CartPole environment. This is a 2D simulation in which a cart
          can be accelerated left or right in order to balance a pole placed on top of it (see
          Figure 18-4). You can get the list of all available environments by running
          gym.envs.registry.all(). After the environment is created, you must initialize it
          using the reset() method. This returns the first observation. Observations depend
          on the type of environment. For the CartPole environment, each observation is a 1D
          NumPy array containing four floats: these floats represent the cart’s horizontal
                                                                      
                                                                      
                                                                      
          10 OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to pro‐
           mote and develop friendly AIs that will benefit humanity (rather than exterminate it)."|simulated environments
"                                                                      
                                                                      
                                                                      
                                                                      
          Supervised learning                                         
                                                                      
          In supervised learning, the training set you feed to the algorithm includes the desired
          solutions, called labels (Figure 1-5).                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-5. A labeled training set for spam classification (an example of supervised
          learning)                                                   
                                                                      
          A typical supervised learning task is classification. The spam filter is a good example
          of this: it is trained with many example emails along with their class (spam or ham),
          and it must learn how to classify new emails.               
          Another typical task is to predict a target numeric value, such as the price of a car,
          given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is
          called regression (Figure 1-6).1 To train the system, you need to give it many examples
          of cars, including both their predictors and their labels (i.e., their prices).
                                                                      
                   In Machine Learning an attribute is a data type (e.g., “mileage”),
                   while a feature has several meanings, depending on the context, but
                   generally means an attribute plus its value (e.g., “mileage =
                   15,000”). Many people use the words attribute and feature inter‐
                   changeably.                                        
          Note that some regression algorithms can be used for classification as well, and vice
          versa. For example, Logistic Regression is commonly used for classification, as it can
          output a value that corresponds to the probability of belonging to a given class (e.g.,
          20% chance of being spam).                                  
                                                                      
                                                                      
                                                                      
                                                                      
          1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the
           fact that the children of tall people tend to be shorter than their parents. Since the children were shorter, he
           called this regression to the mean. This name was then applied to the methods he used to analyze correlations
           between variables.                                         "|attributes; supervised learning; regression problems; classification with; features; prediction problems
"                                                                      
                                                                      
                                                                      
                                                                      
                   In a transposed convolutional layer, the stride defines how much
                   the input will be stretched, not the size of the filter steps, so the
                   larger the stride, the larger the output (unlike for convolutional lay‐
                   ers or pooling layers).                            
                                                                      
                                                                      
                       TensorFlow Convolution Operations              
                                                                      
           TensorFlow also offers a few other kinds of convolutional layers:
           keras.layers.Conv1D                                        
              Creates a convolutional layer for 1D inputs, such as time series or text (sequences
              of letters or words), as we will see in Chapter 15.     
           keras.layers.Conv3D                                        
              Creates a convolutional layer for 3D inputs, such as 3D PET scans.
                                                                      
           dilation_rate                                              
              Setting the dilation_rate hyperparameter of any convolutional layer to a value
              of 2 or more creates an à-trous convolutional layer (“à trous” is French for “with
              holes”). This is equivalent to using a regular convolutional layer with a filter dila‐
              ted by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter
              equal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated
              filter of [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This lets the convolutional layer have
              a larger receptive field at no computational price and using no extra parameters.
           tf.nn.depthwise_conv2d()                                   
              Can be used to create a depthwise convolutional layer (but you need to create the
              variables yourself). It applies every filter to every individual input channel inde‐
              pendently. Thus, if there are fn filters and fn′ input channels, then this will output
              fn × fn′ feature maps.                                  
                                                                      
          This solution is OK, but still too imprecise. To do better, the authors added skip con‐
          nections from lower layers: for example, they upsampled the output image by a factor
          of 2 (instead of 32), and they added the output of a lower layer that had this double
          resolution. Then they upsampled the result by a factor of 16, leading to a total upsam‐
          pling factor of 32 (see Figure 14-28). This recovered some of the spatial resolution
          that was lost in earlier pooling layers. In their best architecture, they used a second
          similar skip connection to recover even finer details from an even lower layer. In
          short, the output of the original CNN goes through the following extra steps: upscale
          ×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the out‐
          put of an even lower layer, and finally upscale ×8. It is even possible to scale up
          beyond the size of the original image: this can be used to increase the resolution of an
          image, which is a technique called super-resolution.        
                                                                      "|convolution operations
"                                                                      
                                                                      
                                                                      
                                                                      
            the training GPUs busy, and providing less-correlated trajectories to the training
            algorithm.                                                
                                                                      
           • What is a trajectory? It is a concise representation of a transition from one time
            step to the next, or a sequence of consecutive transitions from time step n to time
            step n + t. The trajectories collected by the driver are passed to the observer,
            which saves them in the replay buffer, and they are later sampled by the agent
            and used for training.                                    
           • Why do we need an observer? Can’t the driver save the trajectories directly?
            Indeed, it could, but this would make the architecture less flexible. For example,
            what if you don’t want to use a replay buffer? What if you want to use the trajec‐
            tories for something else, like computing metrics? In fact, an observer is just any
            function that takes a trajectory as an argument. You can use an observer to save
            the trajectories to a replay buffer, or to save them to a TFRecord file (see Chap‐
            ter 13), or to compute metrics, or for anything else. Moreover, you can pass mul‐
            tiple observers to the driver, and it will broadcast the trajectories to all of them.
                                                                      
                   Although this architecture is the most common, you can customize
                   it as you please, and even replace some components with your own.
                   In fact, unless you are researching new RL algorithms, you will
                   most likely want to use a custom environment for your task. For
                   this, you just need to create a custom class that inherits from the
                   PyEnvironment class in the tf_agents.environments.py_environ
                   ment package and overrides the appropriate methods, such as
                   action_spec(), observation_spec(), _reset(), and _step() (see
                   the “Creating a Custom TF_Agents Environment” section of the
                   notebook for an example).                          
                                                                      
          Now we will create all these components: first the Deep Q-Network, then the DQN
          agent (which will take care of creating the collect policy), then the replay buffer and
          the observer to write to it, then a few training metrics, then the driver, and finally the
          dataset. Once we have all the components in place, we will populate the replay buffer
          with some initial trajectories, then we will run the main training loop. So, let’s start by
          creating the Deep Q-Network.                                
          Creating the Deep Q-Network                                 
                                                                      
          The TF-Agents library provides many networks in the tf_agents.networks package
          and its subpackages. We will use the tf_agents.networks.q_network.QNetwork
          class:                                                      
                                                                      
                                                                      
                                                                      "|deep Q-networks (DQNs); trajectory
"                                                                      
                                                                      
                                                                      
                                                                      
          Estimating Probabilities                                    
                                                                      
          So how does Logistic Regression work? Just like a Linear Regression model, a Logistic
          Regression model computes a weighted sum of the input features (plus a bias term),
          but instead of outputting the result directly like the Linear Regression model does, it
          outputs the logistic of this result (see Equation 4-13).    
                                                                      
            Equation 4-13. Logistic Regression model estimated probability (vectorized form)
                     ⊺                                                
            p=h x =σ x θ                                              
               θ                                                      
          The logistic—noted σ(·)—is a sigmoid function (i.e., S-shaped) that outputs a number
          between 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21.
                                                                      
            Equation 4-14. Logistic function                          
                   1                                                  
            σ t =                                                     
                1+ exp −t                                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-21. Logistic function                              
                                                                      
          Once the Logistic Regression model has estimated the probability p = h (x) that an
                                                     θ                
          instance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐
          tion 4-15).                                                 
            Equation 4-15. Logistic Regression model prediction       
               0 if p<0.5                                             
            y =                                                       
               1 if p≥0.5                                             
                                                                      
          Notice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression
          model predicts 1 if x⊺ θ is positive and 0 if it is negative.
                                                                      
                                                                      "|Logistic (sigmoid) function; Logistic (sigmoid); sigmoid (Logistic) activation function; estimating probabilities
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-5. Applying two different filters to get two feature maps
                                                                      
          Stacking Multiple Feature Maps                              
                                                                      
          Up to now, for simplicity, I have represented the output of each convolutional layer as
          a 2D layer, but in reality a convolutional layer has multiple filters (you decide how
          many) and outputs one feature map per filter, so it is more accurately represented in
          3D (see Figure 14-6). It has one neuron per pixel in each feature map, and all neurons
          within a given feature map share the same parameters (i.e., the same weights and bias
          term). Neurons in different feature maps use different parameters. A neuron’s recep‐
          tive field is the same as described earlier, but it extends across all the previous layers’
          feature maps. In short, a convolutional layer simultaneously applies multiple trainable
          filters to its inputs, making it capable of detecting multiple features anywhere in its
          inputs.                                                     
                   The fact that all neurons in a feature map share the same parame‐
                   ters dramatically reduces the number of parameters in the model.
                   Once the CNN has learned to recognize a pattern in one location, it
                   can recognize it in any other location. In contrast, once a regular
                   DNN has learned to recognize a pattern in one location, it can rec‐
                   ognize it only in that particular location.        
                                                                      
          Input images are also composed of multiple sublayers: one per color channel. There
          are typically three: red, green, and blue (RGB). Grayscale images have just one
                                                                      "|stacking multiple feature maps; color channels
"                                                                      
                                                                      
                                                                      
                                                                      
          only partial knowledge of the MDP. In general we assume that the agent initially
          knows only the possible states and actions, and nothing more. The agent uses an
          exploration policy—for example, a purely random policy—to explore the MDP, and as
          it progresses, the TD Learning algorithm updates the estimates of the state values
          based on the transitions and rewards that are actually observed (see Equation 18-4).
                                                                      
            Equation 18-4. TD Learning algorithm                      
                                                                      
            V   s  1−α V s +α r+γ·V s′                                
             k+1       k        k                                     
            or, equivalently:                                         
            V   s  V s +α·δ s,r,s′                                    
             k+1    k    k                                            
            with δ s,r,s′ =r+γ·V s′ −V s                              
               k          k    k                                      
          In this equation:                                           
           • α is the learning rate (e.g., 0.01).                     
           • r + γ · V(s′) is called the TD target.                   
                 k                                                    
           • δ (s, r, s′) is called the TD error.                     
             k                                                        
          A more concise way of writing the first form of this equation is to use the notation
          a  b, which means a ← (1 – α) · a + α ·b. So, the first line of Equation 18-4 can
           α           k+1      k   k                                 
          be rewritten like this: V s r+γ·V s′ .                      
                          α                                           
                   TD Learning has many similarities with Stochastic Gradient
                   Descent, in particular the fact that it handles one sample at a time.
                   Moreover, just like Stochastic GD, it can only truly converge if you
                   gradually reduce the learning rate (otherwise it will keep bouncing
                   around the optimum Q-Values).                      
          For each state s, this algorithm simply keeps track of a running average of the imme‐
          diate rewards the agent gets upon leaving that state, plus the rewards it expects to get
          later (assuming it acts optimally).                         
          Q-Learning                                                  
                                                                      
          Similarly, the Q-Learning algorithm is an adaptation of the Q-Value Iteration algo‐
          rithm to the situation where the transition probabilities and the rewards are initially
          unknown (see Equation 18-5). Q-Learning works by watching an agent play (e.g.,
          randomly) and gradually improving its estimates of the Q-Values. Once it has
                                                                      
                                                                      "|exploration policy; TD target; TD error; Q-Learning
"                                                                      
                                                                      
                                                                      
                                                                      
          the categories. This is called representation learning (we will see other types of repre‐
          sentation learning in Chapter 17).                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 13-4. Embeddings will gradually improve during training
                                                                      
                             Word Embeddings                          
                                                                      
           Not only will embeddings generally be useful representations for the task at hand, but
           quite often these same embeddings can be reused successfully for other tasks. The
           most common example of this is word embeddings (i.e., embeddings of individual
           words): when you are working on a natural language processing task, you are often
           better off reusing pretrained word embeddings than training your own.
           The idea of using vectors to represent words dates back to the 1960s, and many
           sophisticated techniques have been used to generate useful vectors, including using
           neural networks. But things really took off in 2013, when Tomáš Mikolov and other
           Google researchers published a paper9 describing an efficient technique to learn word
           embeddings using neural networks, significantly outperforming previous attempts.
           This allowed them to learn embeddings on a very large corpus of text: they trained a
           neural network to predict the words near any given word, and obtained astounding
           word embeddings. For example, synonyms had very close embeddings, and semanti‐
           cally related words such as France, Spain, and Italy ended up clustered together.
           It’s not just about proximity, though: word embeddings were also organized along
           meaningful axes in the embedding space. Here is a famous example: if you compute
           King – Man + Woman (adding and subtracting the embedding vectors of these
           words), then the result will be very close to the embedding of the word Queen (see
           Figure 13-5). In other words, the word embeddings encode the concept of gender!
                                                                      
                                                                      
          9 Tomas Mikolov et al., “Distributed Representations of Words and Phrases and Their Compositionality,” Pro‐
           ceedings of the 26th International Conference on Neural Information Processing Systems 2 (2013): 3111–3119."|word embeddings; representation learning
"                                                                      
                                                                      
                                                                      
                                                                      
          instance’s silhouette coefficient is equal to (b – a) / max(a, b), where a is the mean
          distance to the other instances in the same cluster (i.e., the mean intra-cluster dis‐
          tance) and b is the mean nearest-cluster distance (i.e., the mean distance to the
          instances of the next closest cluster, defined as the one that minimizes b, excluding
          the instance’s own cluster). The silhouette coefficient can vary between –1 and +1. A
          coefficient close to +1 means that the instance is well inside its own cluster and far
          from other clusters, while a coefficient close to 0 means that it is close to a cluster
          boundary, and finally a coefficient close to –1 means that the instance may have been
          assigned to the wrong cluster.                              
                                                                      
          To compute the silhouette score, you can use Scikit-Learn’s silhouette_score()
          function, giving it all the instances in the dataset and the labels they were assigned:
            >>> from sklearn.metrics import silhouette_score          
            >>> silhouette_score(X, kmeans.labels_)                   
            0.655517642572828                                         
          Let’s compare the silhouette scores for different numbers of clusters (see Figure 9-9).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-9. Selecting the number of clusters k using the silhouette score
                                                                      
          As you can see, this visualization is much richer than the previous one: although it
          confirms that k = 4 is a very good choice, it also underlines the fact that k = 5 is quite
          good as well, and much better than k = 6 or 7. This was not visible when comparing
          inertias.                                                   
          An even more informative visualization is obtained when you plot every instance’s
          silhouette coefficient, sorted by the cluster they are assigned to and by the value of the
          coefficient. This is called a silhouette diagram (see Figure 9-10). Each diagram con‐
          tains one knife shape per cluster. The shape’s height indicates the number of instances
          the cluster contains, and its width represents the sorted silhouette coefficients of the
          instances in the cluster (wider is better). The dashed line indicates the mean silhou‐
          ette coefficient.                                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|silhouette diagram
"                                                                      
                                                                      
                                                                      
                                                                      
          may behave erratically when the number of features is greater than the number of
          training instances or when several features are strongly correlated.
                                                                      
          Here is a short example that uses Scikit-Learn’s ElasticNet (l1_ratio corresponds to
          the mix ratio r):                                           
            >>> from sklearn.linear_model import ElasticNet           
            >>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)     
            >>> elastic_net.fit(X, y)                                 
            >>> elastic_net.predict([[1.5]])                          
            array([1.54333232])                                       
          Early Stopping                                              
                                                                      
          A very different way to regularize iterative learning algorithms such as Gradient
          Descent is to stop training as soon as the validation error reaches a minimum. This is
          called early stopping. Figure 4-20 shows a complex model (in this case, a high-degree
          Polynomial Regression model) being trained with Batch Gradient Descent. As the
          epochs go by the algorithm learns, and its prediction error (RMSE) on the training
          set goes down, along with its prediction error on the validation set. After a while
          though, the validation error stops decreasing and starts to go back up. This indicates
          that the model has started to overfit the training data. With early stopping you just
          stop training as soon as the validation error reaches the minimum. It is such a simple
          and efficient regularization technique that Geoffrey Hinton called it a “beautiful free
          lunch.”                                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-20. Early stopping regularization                  
                                                                      
                                                                      
                                                                      "|early stopping
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-3. A simple machine translation model             
                                                                      
          At each step, the decoder outputs a score for each word in the output vocabulary (i.e.,
          French), and then the softmax layer turns these scores into probabilities. For exam‐
          ple, at the first step the word “Je” may have a probability of 20%, “Tu” may have a
          probability of 1%, and so on. The word with the highest probability is output. This is
          very much like a regular classification task, so you can train the model using the
          ""sparse_categorical_crossentropy"" loss, much like we did in the Char-RNN
          model.                                                      
                                                                      
          Note that at inference time (after training), you will not have the target sentence to
          feed to the decoder. Instead, simply feed the decoder the word that it output at the
          previous step, as shown in Figure 16-4 (this will require an embedding lookup that is
          not shown in the diagram).                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|softmax; softmax function
"                                                                      
                                                                      
                                                                      
                                                                      
          Putting everything together, we can now create a Keras model that can process cate‐
          gorical features (along with regular numerical features) and learn an embedding for
          each category (as well as for each oov bucket):             
                                                                      
            regular_inputs = keras.layers.Input(shape=[8])            
            categories = keras.layers.Input(shape=[], dtype=tf.string)
            cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)
            cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)
            encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])
            outputs = keras.layers.Dense(1)(encoded_inputs)           
            model = keras.models.Model(inputs=[regular_inputs, categories],
                            outputs=[outputs])                        
          This model takes two inputs: a regular input containing eight numerical features per
          instance, plus a categorical input (containing one categorical feature per instance). It
          uses a Lambda layer to look up each category’s index, then it looks up the embeddings
          for these indices. Next, it concatenates the embeddings and the regular inputs in
          order to give the encoded inputs, which are ready to be fed to a neural network. We
          could add any kind of neural network at this point, but we just add a dense output
          layer, and we create the Keras model.                       
          When the keras.layers.TextVectorization layer is available, you can call its
          adapt() method to make it extract the vocabulary from a data sample (it will take
          care of creating the lookup table for you). Then you can add it to your model, and it
          will perform the index lookup (replacing the Lambda layer in the previous code
          example).                                                   
                   One-hot encoding followed by a Dense layer (with no activation
                   function and no biases) is equivalent to an Embedding layer. How‐
                   ever, the Embedding layer uses way fewer computations (the perfor‐
                   mance difference becomes clear when the size of the embedding
                   matrix grows). The Dense layer’s weight matrix plays the role of the
                   embedding matrix. For example, using one-hot vectors of size 20
                   and a Dense layer with 10 units is equivalent to using an Embedding
                   layer with input_dim=20 and output_dim=10. As a result, it would
                   be wasteful to use more embedding dimensions than the number
                   of units in the layer that follows the Embedding layer.
          Now let’s look a bit more closely at the Keras preprocessing layers.
                                                                      
          Keras Preprocessing Layers                                  
                                                                      
          The TensorFlow team is working on providing a set of standard Keras preprocessing
          layers. They will probably be available by the time you read this; however, the API
          may change slightly by then, so please refer to the notebook for this chapter if any‐
          thing behaves unexpectedly. This new API will likely supersede the existing Feature"|preprocessing layers
"                                                                      
                                                                      
                                                                      
                                                                      
          about a new district and click the Estimate Price button. This will send a query con‐
          taining the data to the web server, which will forward it to your web application, and
          finally your code will simply call the model’s predict() method (you want to load the
          model upon server startup, rather than every time the model is used). Alternatively,
          you can wrap the model within a dedicated web service that your web application can
          query through a REST API23 (see Figure 2-17). This makes it easier to upgrade your
          model to new versions without interrupting the main application. It also simplifies
          scaling, since you can start as many web services as needed and load-balance the
          requests coming from your web application across these web services. Moreover, it
          allows your web application to use any language, not just Python.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-17. A model deployed as a web service and used by a web application
                                                                      
          Another popular strategy is to deploy your model on the cloud, for example on Goo‐
          gle Cloud AI Platform (formerly known as Google Cloud ML Engine): just save your
          model using joblib and upload it to Google Cloud Storage (GCS), then head over to
          Google Cloud AI Platform and create a new model version, pointing it to the GCS
          file. That’s it! This gives you a simple web service that takes care of load balancing and
          scaling for you. It take JSON requests containing the input data (e.g., of a district) and
          returns JSON responses containing the predictions. You can then use this web service
          in your website (or whatever production environment you are using). As we will see
          in Chapter 19, deploying TensorFlow models on AI Platform is not much different
          from deploying Scikit-Learn models.                         
          But deployment is not the end of the story. You also need to write monitoring code to
          check your system’s live performance at regular intervals and trigger alerts when it
          drops. This could be a steep drop, likely due to a broken component in your infra‐
          structure, but be aware that it could also be a gentle decay that could easily go unno‐
          ticed for a long time. This is quite common because models tend to “rot” over time:
          indeed, the world changes, so if the model was trained with last year’s data, it may not
          be adapted to today’s data.                                 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          23 In a nutshell, a REST (or RESTful) API is an HTTP-based API that follows some conventions, such as using
           standard HTTP verbs to read, update, create, or delete resources (GET, POST, PUT, and DELETE) and using
           JSON for the inputs and outputs.                           "|deploying on AI platforms
"                                                                      
                                                                      
                                                                      
                                                                      
            housing[""rooms_per_household""] = housing[""total_rooms""]/housing[""households""]
            housing[""bedrooms_per_room""] = housing[""total_bedrooms""]/housing[""total_rooms""]
            housing[""population_per_household""]=housing[""population""]/housing[""households""]
          And now let’s look at the correlation matrix again:         
            >>> corr_matrix = housing.corr()                          
            >>> corr_matrix[""median_house_value""].sort_values(ascending=False)
            median_house_value 1.000000                               
            median_income    0.687160                                 
            rooms_per_household 0.146285                              
            total_rooms      0.135097                                 
            housing_median_age 0.114110                               
            households       0.064506                                 
            total_bedrooms   0.047689                                 
            population_per_household -0.021985                        
            population      -0.026920                                 
            longitude       -0.047432                                 
            latitude        -0.142724                                 
            bedrooms_per_room -0.259984                               
            Name: median_house_value, dtype: float64                  
          Hey, not bad! The new bedrooms_per_room attribute is much more correlated with
          the median house value than the total number of rooms or bedrooms. Apparently
          houses with a lower bedroom/room ratio tend to be more expensive. The number of
          rooms per household is also more informative than the total number of rooms in a
          district—obviously the larger the houses, the more expensive they are.
          This round of exploration does not have to be absolutely thorough; the point is to
          start off on the right foot and quickly gain insights that will help you get a first rea‐
          sonably good prototype. But this is an iterative process: once you get a prototype up
          and running, you can analyze its output to gain more insights and come back to this
          exploration step.                                           
          Prepare the Data for Machine Learning Algorithms            
          It’s time to prepare the data for your Machine Learning algorithms. Instead of doing
          this manually, you should write functions for this purpose, for several good reasons:
                                                                      
           • This will allow you to reproduce these transformations easily on any dataset (e.g.,
            the next time you get a fresh dataset).                   
           • You will gradually build a library of transformation functions that you can reuse
            in future projects.                                       
           • You can use these functions in your live system to transform the new data before
            feeding it to your algorithms.                            
                                                                      
                                                                      
                                                                      "|data visualization; data preparation
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-2. A Machine Learning pipeline for real estate investments
                                                                      
                                                                      
                                Pipelines                             
           A sequence of data processing components is called a data pipeline. Pipelines are very
           common in Machine Learning systems, since there is a lot of data to manipulate and
           many data transformations to apply.                        
                                                                      
           Components typically run asynchronously. Each component pulls in a large amount
           of data, processes it, and spits out the result in another data store. Then, some time
           later, the next component in the pipeline pulls this data and spits out its own output.
           Each component is fairly self-contained: the interface between components is simply
           the data store. This makes the system simple to grasp (with the help of a data flow
           graph), and different teams can focus on different components. Moreover, if a com‐
           ponent breaks down, the downstream components can often continue to run nor‐
           mally (at least for a while) by just using the last output from the broken component.
           This makes the architecture quite robust.                  
           On the other hand, a broken component can go unnoticed for some time if proper
           monitoring is not implemented. The data gets stale and the overall system’s perfor‐
           mance drops.                                               
                                                                      
          The next question to ask your boss is what the current solution looks like (if any).
          The current situation will often give you a reference for performance, as well as
          insights on how to solve the problem. Your boss answers that the district housing pri‐
          ces are currently estimated manually by experts: a team gathers up-to-date informa‐
          tion about a district, and when they cannot get the median housing price, they
          estimate it using complex rules.                            
          This is costly and time-consuming, and their estimates are not great; in cases where
          they manage to find out the actual median housing price, they often realize that their
          estimates were off by more than 20%. This is why the company thinks that it would
          be useful to train a model to predict a district’s median housing price, given other
                                                                      "|pipelines; components
"                                                                      
                                                                      
                                                                      
                                                                      
          To be clear, at time step 0 the model will output a vector containing the forecasts for
          time steps 1 to 10, then at time step 1 the model will forecast time steps 2 to 11, and
          so on. So each target must be a sequence of the same length as the input sequence,
          containing a 10-dimensional vector at each step. Let’s prepare these target sequences:
                                                                      
            Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors
            for step_ahead in range(1, 10 + 1):                       
               Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]
            Y_train = Y[:7000]                                        
            Y_valid = Y[7000:9000]                                    
            Y_test = Y[9000:]                                         
                   It may be surprising that the targets will contain values that appear
                   in the inputs (there is a lot of overlap between X_train and
                   Y_train). Isn’t that cheating? Fortunately, not at all: at each time
                   step, the model only knows about past time steps, so it cannot look
                   ahead. It is said to be a causal model.            
          To turn the model into a sequence-to-sequence model, we must set return_sequen
          ces=True in all recurrent layers (even the last one), and we must apply the output
          Dense layer at every time step. Keras offers a TimeDistributed layer for this very pur‐
          pose: it wraps any layer (e.g., a Dense layer) and applies it at every time step of its
          input sequence. It does this efficiently, by reshaping the inputs so that each time step
          is treated as a separate instance (i.e., it reshapes the inputs from [batch size, time steps,
          input dimensions] to [batch size × time steps, input dimensions]; in this example, the
          number of input dimensions is 20 because the previous SimpleRNN layer has 20 units),
          then it runs the Dense layer, and finally it reshapes the outputs back to sequences (i.e.,
          it reshapes the outputs from [batch size × time steps, output dimensions] to [batch size,
          time steps, output dimensions]; in this example the number of output dimensions is
          10, since the Dense layer has 10 units).2 Here is the updated model:
            model = keras.models.Sequential([                         
               keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),
               keras.layers.SimpleRNN(20, return_sequences=True),     
               keras.layers.TimeDistributed(keras.layers.Dense(10))   
            ])                                                        
          The Dense layer actually supports sequences as inputs (and even higher-dimensional
          inputs): it handles them just like TimeDistributed(Dense(…)), meaning it is applied
          to the last input dimension only (independently across all time steps). Thus, we could
          replace the last layer with just Dense(10). For the sake of clarity, however, we will
          keep using TimeDistributed(Dense(10)) because it makes it clear that the Dense
                                                                      
                                                                      
                                                                      
          2 Note that a TimeDistributed(Dense(n)) layer is equivalent to a Conv1D(n, filter_size=1) layer."|causal models; sequence-to-sequence models
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
           • First, the code assumes that we have precomputed the mean and standard devia‐
            tion of each feature in the training set. X_mean and X_std are just 1D tensors (or
            NumPy arrays) containing eight floats, one per input feature.
           • The preprocess() function takes one CSV line and starts by parsing it. For this
            it uses the tf.io.decode_csv() function, which takes two arguments: the first is
            the line to parse, and the second is an array containing the default value for each
            column in the CSV file. This array tells TensorFlow not only the default value for
            each column, but also the number of columns and their types. In this example,
            we tell it that all feature columns are floats and that missing values should default
            to 0, but we provide an empty array of type tf.float32 as the default value for
            the last column (the target): the array tells TensorFlow that this column contains
            floats, but that there is no default value, so it will raise an exception if it encoun‐
            ters a missing value.                                     
           • The decode_csv() function returns a list of scalar tensors (one per column), but
            we need to return 1D tensor arrays. So we call tf.stack() on all tensors except
            for the last one (the target): this will stack these tensors into a 1D array. We then
            do the same for the target value (this makes it a 1D tensor array with a single
            value, rather than a scalar tensor).                      
           • Finally, we scale the input features by subtracting the feature means and then
            dividing by the feature standard deviations, and we return a tuple containing the
            scaled features and the target.                           
                                                                      
          Let’s test this preprocessing function:                     
            >>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')
            (<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=   
             array([ 0.16579159, 1.216324 , -0.05204564, -0.39215982, -0.5277444 ,
                 -0.2633488 , 0.8543046 , -1.3072058 ], dtype=float32)>,
             <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>) 
          Looks good! We can now apply the function to the dataset.   
          Putting Everything Together                                 
                                                                      
          To make the code reusable, let’s put together everything we have discussed so far into
          a small helper function: it will create and return a dataset that will efficiently load Cal‐
          ifornia housing data from multiple CSV files, preprocess it, shuffle it, optionally
          repeat it, and batch it (see Figure 13-2):                  
            def csv_reader_dataset(filepaths, repeat=1, n_readers=5,  
                          n_read_threads=None, shuffle_buffer_size=10000,
                          n_parse_threads=5, batch_size=32):          
               dataset = tf.data.Dataset.list_files(filepaths)        
               dataset = dataset.interleave(                          
                 lambda filepath: tf.data.TextLineDataset(filepath).skip(1),"|helper function creation; helper functions
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> env.observation_spec()                                
            BoundedArraySpec(shape=(210, 160, 3), dtype=dtype('float32'), name=None,
                      minimum=[[[0. 0. 0.], [0. 0. 0.],...]],         
                      maximum=[[[255., 255., 255.], [255., 255., 255.], ...]])
            >>> env.action_spec()                                     
            BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None,
                      minimum=0, maximum=3)                           
            >>> env.time_step_spec()                                  
            TimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),
                  reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),
                  discount=BoundedArraySpec(shape=(), ..., minimum=0.0, maximum=1.0),
                  observation=BoundedArraySpec(shape=(210, 160, 3), ...))
          As you can see, the observations are simply screenshots of the Atari screen, repre‐
          sented as NumPy arrays of shape [210, 160, 3]. To render an environment, you can
          call env.render(mode=""human""), and if you want to get back the image in the form of
          a NumPy array, just call env.render(mode=""rgb_array"") (unlike in OpenAI Gym,
          this is the default mode).                                  
          There are four actions available. Gym’s Atari environments have an extra method that
          you can call to know what each action corresponds to:       
            >>> env.gym.get_action_meanings()                         
            ['NOOP', 'FIRE', 'RIGHT', 'LEFT']                         
                   Specs can be instances of a specification class, nested lists, or dic‐
                   tionaries of specs. If the specification is nested, then the specified
                   object must match the specification’s nested structure. For example,
                   if the observation spec is {""sensors"": ArraySpec(shape=[2]),
                   ""camera"": ArraySpec(shape=[100, 100])}, then a valid observa‐
                   tion would be {""sensors"": np.array([1.5, 3.5]), ""camera"":
                   np.array(...)}. The tf.nest package provides tools to handle
                   such nested structures (a.k.a. nests).             
          The observations are quite large, so we will downsample them and also convert them
          to grayscale. This will speed up training and use less RAM. For this, we can use an
          environment wrapper.                                        
                                                                      
          Environment Wrappers and Atari Preprocessing                
                                                                      
          TF-Agents provides several environment wrappers in the tf_agents.environ
          ments.wrappers package. As their name suggests, they wrap an environment, for‐
          warding every call to it, but also adding some extra functionality. Here are some of
          the available wrappers:                                     
          ActionClipWrapper                                           
            Clips the actions to the action spec.                     
                                                                      "|Atari preprocessing; environment wrappers
"                                                                      
                                                                      
                                                                      
                                                                      
                   It is common to use 80% of the data for training and hold out 20%
                   for testing. However, this depends on the size of the dataset: if it
                   contains 10 million instances, then holding out 1% means your test
                   set will contain 100,000 instances, probably more than enough to
                   get a good estimate of the generalization error.   
                                                                      
          Hyperparameter Tuning and Model Selection                   
                                                                      
          Evaluating a model is simple enough: just use a test set. But suppose you are hesitat‐
          ing between two types of models (say, a linear model and a polynomial model): how
          can you decide between them? One option is to train both and compare how well
          they generalize using the test set.                         
          Now suppose that the linear model generalizes better, but you want to apply some
          regularization to avoid overfitting. The question is, how do you choose the value of
          the regularization hyperparameter? One option is to train 100 different models using
          100 different values for this hyperparameter. Suppose you find the best hyperparame‐
          ter value that produces a model with the lowest generalization error—say, just 5%
          error. You launch this model into production, but unfortunately it does not perform
          as well as expected and produces 15% errors. What just happened?
          The problem is that you measured the generalization error multiple times on the test
          set, and you adapted the model and hyperparameters to produce the best model for
          that particular set. This means that the model is unlikely to perform as well on new
          data.                                                       
          A common solution to this problem is called holdout validation: you simply hold out
          part of the training set to evaluate several candidate models and select the best one.
          The new held-out set is called the validation set (or sometimes the development set, or
          dev set). More specifically, you train multiple models with various hyperparameters
          on the reduced training set (i.e., the full training set minus the validation set), and
          you select the model that performs best on the validation set. After this holdout vali‐
          dation process, you train the best model on the full training set (including the valida‐
          tion set), and this gives you the final model. Lastly, you evaluate this final model on
          the test set to get an estimate of the generalization error.
                                                                      
          This solution usually works quite well. However, if the validation set is too small, then
          model evaluations will be imprecise: you may end up selecting a suboptimal model by
          mistake. Conversely, if the validation set is too large, then the remaining training set
          will be much smaller than the full training set. Why is this bad? Well, since the final
          model will be trained on the full training set, it is not ideal to compare candidate
          models trained on a much smaller training set. It would be like selecting the fastest
          sprinter to participate in a marathon. One way to solve this problem is to perform
          repeated cross-validation, using many small validation sets. Each model is evaluated
          once per validation set after it is trained on the rest of the data. By averaging out all"|hyperparameter tuning; hold outs; cross-validation; validation sets; development sets (dev sets); holdout validation; model selection
"                                                                      
                                                                      
                                                                      
                                                                      
          As we discussed earlier, one of the triggers of the current tsunami of interest in Deep
          Learning was the discovery in 2006 by Geoffrey Hinton et al. that deep neural net‐
          works can be pretrained in an unsupervised fashion, using this greedy layerwise
          approach. They used restricted Boltzmann machines (RBMs; see Appendix E) for this
          purpose, but in 2007 Yoshua Bengio et al. showed3 that autoencoders worked just as
          well. For several years this was the only efficient way to train deep nets, until many of
          the techniques introduced in Chapter 11 made it possible to just train a deep net in
          one shot.                                                   
                                                                      
          Autoencoders are not limited to dense networks: you can also build convolutional
          autoencoders, or even recurrent autoencoders. Let’s look at these now.
          Convolutional Autoencoders                                  
                                                                      
          If you are dealing with images, then the autoencoders we have seen so far will not
          work well (unless the images are very small): as we saw in Chapter 14, convolutional
          neural networks are far better suited than dense networks to work with images. So if
          you want to build an autoencoder for images (e.g., for unsupervised pretraining or
          dimensionality reduction), you will need to build a convolutional autoencoder.4 The
          encoder is a regular CNN composed of convolutional layers and pooling layers. It
          typically reduces the spatial dimensionality of the inputs (i.e., height and width) while
          increasing the depth (i.e., the number of feature maps). The decoder must do the
          reverse (upscale the image and reduce its depth back to the original dimensions), and
          for this you can use transpose convolutional layers (alternatively, you could combine
          upsampling layers with convolutional layers). Here is a simple convolutional autoen‐
          coder for Fashion MNIST:                                    
            conv_encoder = keras.models.Sequential([                  
               keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]),
               keras.layers.Conv2D(16, kernel_size=3, padding=""same"", activation=""selu""),
               keras.layers.MaxPool2D(pool_size=2),                   
               keras.layers.Conv2D(32, kernel_size=3, padding=""same"", activation=""selu""),
               keras.layers.MaxPool2D(pool_size=2),                   
               keras.layers.Conv2D(64, kernel_size=3, padding=""same"", activation=""selu""),
               keras.layers.MaxPool2D(pool_size=2)                    
            ])                                                        
            conv_decoder = keras.models.Sequential([                  
               keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=""valid"",
                                activation=""selu"",                    
                                input_shape=[3, 3, 64]),              
          3 Yoshua Bengio et al., “Greedy Layer-Wise Training of Deep Networks,” Proceedings of the 19th International
           Conference on Neural Information Processing Systems (2006): 153–160.
          4 Jonathan Masci et al., “Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction,” Proceed‐
           ings of the 21st International Conference on Artificial Neural Networks 1 (2011): 52–59."|unsupervised pretraining using stacked; pretraining using stacked autoencoders; convolutional autoencoders; using stacked autoencoders; convolutional; unsupervised pretraining
"                                                                      
                                                                      
                                                                      
                                                                      
          shifted by one and two pixels to the right. As you can see, the outputs of the max
          pooling layer for images A and B are identical. This is what translation invariance
          means. For image C, the output is different: it is shifted one pixel to the right (but
          there is still 75% invariance). By inserting a max pooling layer every few layers in a
          CNN, it is possible to get some level of translation invariance at a larger scale. More‐
          over, max pooling offers a small amount of rotational invariance and a slight scale
          invariance. Such invariance (even if it is limited) can be useful in cases where the pre‐
          diction should not depend on these details, such as in classification tasks.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-9. Invariance to small translations               
          However, max pooling has some downsides too. Firstly, it is obviously very destruc‐
          tive: even with a tiny 2 × 2 kernel and a stride of 2, the output will be two times
          smaller in both directions (so its area will be four times smaller), simply dropping
          75% of the input values. And in some applications, invariance is not desirable. Take
          semantic segmentation (the task of classifying each pixel in an image according to the
          object that pixel belongs to, which we’ll explore later in this chapter): obviously, if the
          input image is translated by one pixel to the right, the output should also be trans‐
          lated by one pixel to the right. The goal in this case is equivariance, not invariance: a
          small change to the inputs should lead to a corresponding small change in the output.
                                                                      
          TensorFlow Implementation                                   
                                                                      
          Implementing a max pooling layer in TensorFlow is quite easy. The following code
          creates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,
          so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses
          ""valid"" padding (i.e., no padding at all):                  "|semantic segmentation; pooling layer; equivariance
"                                                                      
                                                                      
                                                                      
                                                                      
          In this equation:                                           
                                                                      
           • Y is an m × n matrix containing the layer’s outputs at time step t for each
             (t)     neurons                                          
            instance in the mini-batch (m is the number of instances in the mini-batch and
            n   is the number of neurons).                            
             neurons                                                  
           • X is an m × n matrix containing the inputs for all instances (n is the
             (t)      inputs                          inputs          
            number of input features).                                
           • W is an n × n matrix containing the connection weights for the inputs
              x    inputs neurons                                     
            of the current time step.                                 
           • W is an n × n matrix containing the connection weights for the out‐
              y    neurons neurons                                    
            puts of the previous time step.                           
           • b is a vector of size n containing each neuron’s bias term.
                        neurons                                       
           • The weight matrices W and W are often concatenated vertically into a single
                          x    y                                      
            weight matrix W of shape (n + n ) × n (see the second line of Equa‐
                             inputs neurons neurons                   
            tion 15-2).                                               
           • The notation [X Y ] represents the horizontal concatenation of the matrices
                      (t) (t–1)                                       
            X  and Y .                                                
             (t)  (t–1)                                               
          Notice that Y is a function of X and Y , which is a function of X and Y ,
                  (t)         (t)  (t–1)            (t–1) (t–2)       
          which is a function of X and Y , and so on. This makes Y a function of all the
                        (t–2) (t–3)            (t)                    
          inputs since time t = 0 (that is, X , X , …, X ). At the first time step, t = 0, there are
                             (0) (1) (t)                              
          no previous outputs, so they are typically assumed to be all zeros.
          Memory Cells                                                
          Since the output of a recurrent neuron at time step t is a function of all the inputs
          from previous time steps, you could say it has a form of memory. A part of a neural
          network that preserves some state across time steps is called a memory cell (or simply
          a cell). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell,
          capable of learning only short patterns (typically about 10 steps long, but this varies
          depending on the task). Later in this chapter, we will look at some more complex and
          powerful types of cells capable of learning longer patterns (roughly 10 times longer,
          but again, this depends on the task).                       
          In general a cell’s state at time step t, denoted h (the “h” stands for “hidden”), is a
                                      (t)                             
          function of some inputs at that time step and its state at the previous time step: h =
                                                         (t)          
          f(h , x ). Its output at time step t, denoted y , is also a function of the previous
            (t–1) (t)                 (t)                             
          state and the current inputs. In the case of the basic cells we have discussed so far, the
          output is simply equal to the state, but in more complex cells this is not always the
          case, as shown in Figure 15-3.                              "|basic cells; memory cells
"                                                                      
                                                                      
                                                                      
                                                                      
          mask_zero=True when creating the Embedding layer. This means that padding tokens
          (whose ID is 0)8 will be ignored by all downstream layers. That’s all!
                                                                      
          The way this works is that the Embedding layer creates a mask tensor equal to
          K.not_equal(inputs, 0) (where K = keras.backend): it is a Boolean tensor with
          the same shape as the inputs, and it is equal to False anywhere the word IDs are 0, or
          True otherwise. This mask tensor is then automatically propagated by the model to
          all subsequent layers, as long as the time dimension is preserved. So in this example,
          both GRU layers will receive this mask automatically, but since the second GRU layer
          does not return sequences (it only returns the output of the last time step), the mask
          will not be transmitted to the Dense layer. Each layer may handle the mask differently,
          but in general they simply ignore masked time steps (i.e., time steps for which the
          mask is False). For example, when a recurrent layer encounters a masked time step,
          it simply copies the output from the previous time step. If the mask propagates all the
          way to the output (in models that output sequences, which is not the case in this
          example), then it will be applied to the losses as well, so the masked time steps will
          not contribute to the loss (their loss will be 0).          
                                                                      
                   The LSTM and GRU layers have an optimized implementation for
                   GPUs, based on Nvidia’s cuDNN library. However, this implemen‐
                   tation does not support masking. If your model uses a mask, then
                   these layers will fall back to the (much slower) default implementa‐
                   tion. Note that the optimized implementation also requires you to
                   use the default values for several hyperparameters: activation,
                   recurrent_activation, recurrent_dropout, unroll, use_bias,
                   and reset_after.                                   
          All layers that receive the mask must support masking (or else an exception will be
          raised). This includes all recurrent layers, as well as the TimeDistributed layer and a
          few other layers. Any layer that supports masking must have a supports_masking
          attribute equal to True. If you want to implement your own custom layer with mask‐
          ing support, you should add a mask argument to the call() method (and obviously
          make the method use the mask somehow). Additionally, you should set
          self.supports_masking = True in the constructor. If your layer does not start with
          an Embedding layer, you may use the keras.layers.Masking layer instead: it sets the
          mask to K.any(K.not_equal(inputs, 0), axis=-1), meaning that time steps where
          the last dimension is full of zeros will be masked out in subsequent layers (again, as
          long as the time dimension exists).                         
                                                                      
                                                                      
                                                                      
          8 Their ID is 0 only because they are the most frequent “words” in the dataset. It would probably be a good idea
           to ensure that the padding tokens are always encoded as 0, even if they are not the most frequent."|mask tensors
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-1. Classification (left) versus clustering (right) 
                                                                      
          Clustering is used in a wide variety of applications, including these:
          For customer segmentation                                   
            You can cluster your customers based on their purchases and their activity on
            your website. This is useful to understand who your customers are and what they
            need, so you can adapt your products and marketing campaigns to each segment.
            For example, customer segmentation can be useful in recommender systems to
            suggest content that other users in the same cluster enjoyed.
                                                                      
          For data analysis                                           
            When you analyze a new dataset, it can be helpful to run a clustering algorithm,
            and then analyze each cluster separately.                 
          As a dimensionality reduction technique                     
            Once a dataset has been clustered, it is usually possible to measure each instance’s
            affinity with each cluster (affinity is any measure of how well an instance fits into
            a cluster). Each instance’s feature vector x can then be replaced with the vector of
            its cluster affinities. If there are k clusters, then this vector is k-dimensional. This
            vector is typically much lower-dimensional than the original feature vector, but it
            can preserve enough information for further processing.   
                                                                      
          For anomaly detection (also called outlier detection)       
            Any instance that has a low affinity to all the clusters is likely to be an anomaly.
            For example, if you have clustered the users of your website based on their
            behavior, you can detect users with unusual behavior, such as an unusual number
            of requests per second. Anomaly detection is particularly useful in detecting
            defects in manufacturing, or for fraud detection.         
          For semi-supervised learning                                
            If you only have a few labels, you could perform clustering and propagate the
            labels to all the instances in the same cluster. This technique can greatly increase
                                                                      "|classification MLPs; customer segmentation; fraud detection; analyzing through clustering; recommender systems; outlier detection; affinity; clustering algorithms; using clustering
"                                                                      
                                                                      
                                                                      
                                                                      
          If you want to try using data parallelism with centralized parameters, replace the
          MirroredStrategy with the CentralStorageStrategy:           
                                                                      
            distribution = tf.distribute.experimental.CentralStorageStrategy()
          You can optionally set the compute_devices argument to specify the list of devices
          you want to use as workers (by default it will use all available GPUs), and you can
          optionally set the parameter_device argument to specify the device you want to store
          the parameters on (by default it will use the CPU, or the GPU if there is just one).
          Now let’s see how to train a model across a cluster of TensorFlow servers!
                                                                      
          Training a Model on a TensorFlow Cluster                    
                                                                      
          A TensorFlow cluster is a group of TensorFlow processes running in parallel, usually
          on different machines, and talking to each other to complete some work—for exam‐
          ple, training or executing a neural network. Each TF process in the cluster is called a
          task, or a TF server. It has an IP address, a port, and a type (also called its role or its
          job). The type can be either ""worker"", ""chief"", ""ps"" (parameter server), or
          ""evaluator"":                                                
                                                                      
           • Each worker performs computations, usually on a machine with one or more
            GPUs.                                                     
           • The chief performs computations as well (it is a worker), but it also handles extra
            work such as writing TensorBoard logs or saving checkpoints. There is a single
            chief in a cluster. If no chief is specified, then the first worker is the chief.
           • A parameter server only keeps track of variable values, and it is usually on a CPU-
            only machine. This type of task is only used with the ParameterServerStrategy.
           • An evaluator obviously takes care of evaluation.         
                                                                      
          To start a TensorFlow cluster, you must first specify it. This means defining each
          task’s IP address, TCP port, and type. For example, the following cluster specification
          defines a cluster with three tasks (two workers and one parameter server; see
          Figure 19-21). The cluster spec is a dictionary with one key per job, and the values are
          lists of task addresses (IP:port):                          
            cluster_spec = {                                          
               ""worker"": [                                            
                 ""machine-a.example.com:2222"", # /job:worker/task:0   
                 ""machine-b.example.com:2222"" # /job:worker/task:1    
               ],                                                     
               ""ps"": [""machine-a.example.com:2221""] # /job:ps/task:0  
            }                                                         
                                                                      
                                                                      "|TensorFlow cluster; cluster specification
"                                                                      
                                                                      
                                                                      
                                                                      
            from sklearn.tree import DecisionTreeRegressor            
                                                                      
            tree_reg1 = DecisionTreeRegressor(max_depth=2)            
            tree_reg1.fit(X, y)                                       
          Next, we’ll train a second DecisionTreeRegressor on the residual errors made by the
          first predictor:                                            
            y2 = y - tree_reg1.predict(X)                             
            tree_reg2 = DecisionTreeRegressor(max_depth=2)            
            tree_reg2.fit(X, y2)                                      
          Then we train a third regressor on the residual errors made by the second predictor:
                                                                      
            y3 = y2 - tree_reg2.predict(X)                            
            tree_reg3 = DecisionTreeRegressor(max_depth=2)            
            tree_reg3.fit(X, y3)                                      
          Now we have an ensemble containing three trees. It can make predictions on a new
          instance simply by adding up the predictions of all the trees:
            y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
          Figure 7-9 represents the predictions of these three trees in the left column, and the
          ensemble’s predictions in the right column. In the first row, the ensemble has just one
          tree, so its predictions are exactly the same as the first tree’s predictions. In the second
          row, a new tree is trained on the residual errors of the first tree. On the right you can
          see that the ensemble’s predictions are equal to the sum of the predictions of the first
          two trees. Similarly, in the third row another tree is trained on the residual errors of
          the second tree. You can see that the ensemble’s predictions gradually get better as
          trees are added to the ensemble.                            
          A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe
          gressor class. Much like the RandomForestRegressor class, it has hyperparameters to
          control the growth of Decision Trees (e.g., max_depth, min_samples_leaf), as well as
          hyperparameters to control the ensemble training, such as the number of trees
          (n_estimators). The following code creates the same ensemble as the previous one:
                                                                      
            from sklearn.ensemble import GradientBoostingRegressor    
            gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)
            gbrt.fit(X, y)                                            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|GBRT ensemble training
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                   CHAPTER 3          
                                                                      
                                             Classification           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          In Chapter 1 I mentioned that the most common supervised learning tasks are
          regression (predicting values) and classification (predicting classes). In Chapter 2 we
          explored a regression task, predicting housing values, using various algorithms such
          as Linear Regression, Decision Trees, and Random Forests (which will be explained
          in further detail in later chapters). Now we will turn our attention to classification
          systems.                                                    
          MNIST                                                       
                                                                      
                                                                      
          In this chapter we will be using the MNIST dataset, which is a set of 70,000 small
          images of digits handwritten by high school students and employees of the US Cen‐
          sus Bureau. Each image is labeled with the digit it represents. This set has been stud‐
          ied so much that it is often called the “hello world” of Machine Learning: whenever
          people come up with a new classification algorithm they are curious to see how it will
          perform on MNIST, and anyone who learns Machine Learning tackles this dataset
          sooner or later.                                            
          Scikit-Learn provides many helper functions to download popular datasets. MNIST is
          one of them. The following code fetches the MNIST dataset:1 
            >>> from sklearn.datasets import fetch_openml             
            >>> mnist = fetch_openml('mnist_784', version=1)          
            >>> mnist.keys()                                          
            dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',
                   'categories', 'url'])                              
                                                                      
                                                                      
                                                                      
          1 By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data."|MNIST dataset; dataset dictionary structure
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> from sklearn.datasets import load_iris                
            >>> iris = load_iris()                                    
            >>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)
            >>> rnd_clf.fit(iris[""data""], iris[""target""])             
            >>> for name, score in zip(iris[""feature_names""], rnd_clf.feature_importances_):
            ...  print(name, score)                                   
            ...                                                       
            sepal length (cm) 0.112492250999                          
            sepal width (cm) 0.0231192882825                          
            petal length (cm) 0.441030464364                          
            petal width (cm) 0.423357996355                           
          Similarly, if you train a Random Forest classifier on the MNIST dataset (introduced
          in Chapter 3) and plot each pixel’s importance, you get the image represented in
          Figure 7-6.                                                 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 7-6. MNIST pixel importance (according to a Random Forest classifier)
                                                                      
          Random Forests are very handy to get a quick understanding of what features
          actually matter, in particular if you need to perform feature selection.
                                                                      
          Boosting                                                    
                                                                      
          Boosting (originally called hypothesis boosting) refers to any Ensemble method that
          can combine several weak learners into a strong learner. The general idea of most
          boosting methods is to train predictors sequentially, each trying to correct its prede‐
          cessor. There are many boosting methods available, but by far the most popular are
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|hypothesis boosting; boosting
"                                                                      
                                                                      
                                                                      
                                                                      
            from sklearn.ensemble import RandomForestClassifier       
            from sklearn.ensemble import VotingClassifier             
            from sklearn.linear_model import LogisticRegression       
            from sklearn.svm import SVC                               
            log_clf = LogisticRegression()                            
            rnd_clf = RandomForestClassifier()                        
            svm_clf = SVC()                                           
            voting_clf = VotingClassifier(                            
               estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
               voting='hard')                                         
            voting_clf.fit(X_train, y_train)                          
          Let’s look at each classifier’s accuracy on the test set:   
            >>> from sklearn.metrics import accuracy_score            
            >>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):   
            ...  clf.fit(X_train, y_train)                            
            ...  y_pred = clf.predict(X_test)                         
            ...  print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
            ...                                                       
            LogisticRegression 0.864                                  
            RandomForestClassifier 0.896                              
            SVC 0.888                                                 
            VotingClassifier 0.904                                    
          There you have it! The voting classifier slightly outperforms all the individual
          classifiers.                                                
          If all classifiers are able to estimate class probabilities (i.e., they all have a pre
          dict_proba() method), then you can tell Scikit-Learn to predict the class with the
          highest class probability, averaged over all the individual classifiers. This is called soft
          voting. It often achieves higher performance than hard voting because it gives more
          weight to highly confident votes. All you need to do is replace voting=""hard"" with
          voting=""soft"" and ensure that all classifiers can estimate class probabilities. This is
          not the case for the SVC class by default, so you need to set its probability hyper‐
          parameter to True (this will make the SVC class use cross-validation to estimate class
          probabilities, slowing down training, and it will add a predict_proba() method). If
          you modify the preceding code to use soft voting, you will find that the voting classi‐
          fier achieves over 91.2% accuracy!                          
          Bagging and Pasting                                         
                                                                      
          One way to get a diverse set of classifiers is to use very different training algorithms,
          as just discussed. Another approach is to use the same training algorithm for every
          predictor and train them on different random subsets of the training set. When sam‐
                                                                      
                                                                      "|soft voting; bagging and pasting
"                                                                      
                                                                      
                                                                      
                                                                      
          A related task is dimensionality reduction, in which the goal is to simplify the data
          without losing too much information. One way to do this is to merge several correla‐
          ted features into one. For example, a car’s mileage may be strongly correlated with its
          age, so the dimensionality reduction algorithm will merge them into one feature that
          represents the car’s wear and tear. This is called feature extraction.
                                                                      
                   It is often a good idea to try to reduce the dimension of your train‐
                   ing data using a dimensionality reduction algorithm before you
                   feed it to another Machine Learning algorithm (such as a super‐
                   vised learning algorithm). It will run much faster, the data will take
                   up less disk and memory space, and in some cases it may also per‐
                   form better.                                       
                                                                      
          Yet another important unsupervised task is anomaly detection—for example, detect‐
          ing unusual credit card transactions to prevent fraud, catching manufacturing defects,
          or automatically removing outliers from a dataset before feeding it to another learn‐
          ing algorithm. The system is shown mostly normal instances during training, so it
          learns to recognize them; then, when it sees a new instance, it can tell whether it looks
          like a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar
          task is novelty detection: it aims to detect new instances that look different from all
          instances in the training set. This requires having a very “clean” training set, devoid of
          any instance that you would like the algorithm to detect. For example, if you have
          thousands of pictures of dogs, and 1% of these pictures represent Chihuahuas, then a
          novelty detection algorithm should not treat new pictures of Chihuahuas as novelties.
          On the other hand, anomaly detection algorithms may consider these dogs as so rare
          and so different from other dogs that they would likely classify them as anomalies (no
          offense to Chihuahuas).                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-10. Anomaly detection                              
                                                                      
          Finally, another common unsupervised task is association rule learning, in which the
          goal is to dig into large amounts of data and discover interesting relations between
                                                                      "|anomaly detection; association rule learning; feature extraction; novelty detection
"                                                                      
                                                                      
                                                                      
                                                                      
          Measuring Accuracy Using Cross-Validation                   
                                                                      
          A good way to evaluate a model is to use cross-validation, just as you did in Chap‐
          ter 2.                                                      
                                                                      
                        Implementing Cross-Validation                 
                                                                      
           Occasionally you will need more control over the cross-validation process than what
           Scikit-Learn provides off the shelf. In these cases, you can implement cross-validation
           yourself. The following code does roughly the same thing as Scikit-Learn’s
           cross_val_score() function, and it prints the same result: 
             from sklearn.model_selection import StratifiedKFold      
             from sklearn.base import clone                           
             skfolds = StratifiedKFold(n_splits=3, random_state=42)   
                                                                      
             for train_index, test_index in skfolds.split(X_train, y_train_5):
                clone_clf = clone(sgd_clf)                            
                X_train_folds = X_train[train_index]                  
                y_train_folds = y_train_5[train_index]                
                X_test_fold = X_train[test_index]                     
                y_test_fold = y_train_5[test_index]                   
                clone_clf.fit(X_train_folds, y_train_folds)           
                y_pred = clone_clf.predict(X_test_fold)               
                n_correct = sum(y_pred == y_test_fold)                
                print(n_correct / len(y_pred)) # prints 0.9502, 0.96565, and 0.96495
           The StratifiedKFold class performs stratified sampling (as explained in Chapter 2)
           to produce folds that contain a representative ratio of each class. At each iteration the
           code creates a clone of the classifier, trains that clone on the training folds, and makes
           predictions on the test fold. Then it counts the number of correct predictions and
           outputs the ratio of correct predictions.                  
          Let’s use the cross_val_score() function to evaluate our SGDClassifier model,
          using K-fold cross-validation with three folds. Remember that K-fold cross-validation
          means splitting the training set into K folds (in this case, three), then making predic‐
          tions and evaluating them on each fold using a model trained on the remaining folds
          (see Chapter 2):                                            
                                                                      
            >>> from sklearn.model_selection import cross_val_score   
            >>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=""accuracy"")
            array([0.96355, 0.93795, 0.95615])                        
          Wow! Above 93% accuracy (ratio of correct predictions) on all cross-validation folds?
          This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very
          dumb classifier that just classifies every single image in the “not-5” class:"|K-fold cross-validation; folds; cross-validation; cross_val_score() function
"                                                                      
                                                                      
                                                                      
                                                                      
                   If the input sentence is n words long, and assuming the output sen‐
                   tence is about as long, then this model will need to compute about
                   n2 weights. Fortunately, this quadratic computational complexity is
                   still tractable because even long sentences don’t have thousands of
                   words.                                             
                                                                      
          Another common attention mechanism was proposed shortly after, in a 2015 paper16
          by Minh-Thang Luong et al. Because the goal of the attention mechanism is to meas‐
          ure the similarity between one of the encoder’s outputs and the decoder’s previous
          hidden state, the authors proposed to simply compute the dot product (see Chapter 4)
          of these two vectors, as this is often a fairly good similarity measure, and modern
          hardware can compute it much faster. For this to be possible, both vectors must have
          the same dimensionality. This is called Luong attention (again, after the paper’s first
          author), or sometimes multiplicative attention. The dot product gives a score, and all
          the scores (at a given decoder time step) go through a softmax layer to give the final
          weights, just like in Bahdanau attention. Another simplification they proposed was to
          use the decoder’s hidden state at the current time step rather than at the previous time
          step (i.e., h ) rather than h ), then to use the output of the attention mechanism
                 (t)      (t–1)                                       
          (noted  ) directly to compute the decoder’s predictions (rather than using it to
                t                                                     
          compute the decoder’s current hidden state). They also proposed a variant of the dot
          product mechanism where the encoder outputs first go through a linear transforma‐
          tion (i.e., a time-distributed Dense layer without a bias term) before the dot products
          are computed. This is called the “general” dot product approach. They compared both
          dot product approaches to the concatenative attention mechanism (adding a rescaling
          parameter vector v), and they observed that the dot product variants performed bet‐
          ter than concatenative attention. For this reason, concatenative attention is much less
          used now. The equations for these three attention mechanisms are summarized in
          Equation 16-1.                                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          16 Minh-Thang Luong et al., “Effective Approaches to Attention-Based Neural Machine Translation,” Proceed‐
           ings of the 2015 Conference on Empirical Methods in Natural Language Processing (2015): 1412–1421."|loss functions (see cost functions) Luong attention; multiplicative attention; dot product
"                                                                      
                                                                      
                                                                      
                                                                      
          Instead of just flagging emails that are identical to known spam emails, your spam
          filter could be programmed to also flag emails that are very similar to known spam
          emails. This requires a measure of similarity between two emails. A (very basic) simi‐
          larity measure between two emails could be to count the number of words they have
          in common. The system would flag an email as spam if it has many words in com‐
          mon with a known spam email.                                
                                                                      
          This is called instance-based learning: the system learns the examples by heart, then
          generalizes to new cases by using a similarity measure to compare them to the
          learned examples (or a subset of them). For example, in Figure 1-15 the new instance
          would be classified as a triangle because the majority of the most similar instances
          belong to that class.                                       
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-15. Instance-based learning                        
                                                                      
          Model-based learning                                        
                                                                      
          Another way to generalize from a set of examples is to build a model of these exam‐
          ples and then use that model to make predictions. This is called model-based learning
          (Figure 1-16).                                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-16. Model-based learning                           
                                                                      "|model-based learning; measure of similarity
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-11. Kernel PCA and the reconstruction pre-image error
                                                                      
          You may be wondering how to perform this reconstruction. One solution is to train a
          supervised regression model, with the projected instances as the training set and the
          original instances as the targets. Scikit-Learn will do this automatically if you set
          fit_inverse_transform=True, as shown in the following code:7
            rbf_pca = KernelPCA(n_components = 2, kernel=""rbf"", gamma=0.0433,
                        fit_inverse_transform=True)                   
            X_reduced = rbf_pca.fit_transform(X)                      
            X_preimage = rbf_pca.inverse_transform(X_reduced)         
                   By default, fit_inverse_transform=False and KernelPCA has no
                   inverse_transform() method. This method only gets created
                   when you set fit_inverse_transform=True.           
                                                                      
                                                                      
                                                                      
          7 If you set fit_inverse_transform=True, Scikit-Learn will use the algorithm (based on Kernel Ridge Regres‐
           sion) described in Gokhan H. Bakır et al., “Learning to Find Pre-Images”, Proceedings of the 16th International
           Conference on Neural Information Processing Systems (2004): 449–456."|automatic reconstruction with
"                                                                      
                                                                      
                                                                      
                                                                      
            and it is not necessarily a problem, but you should try to understand how the
            data was computed.                                        
                                                                      
           2. The housing median age and the median house value were also capped. The lat‐
            ter may be a serious problem since it is your target attribute (your labels). Your
            Machine Learning algorithms may learn that prices never go beyond that limit.
            You need to check with your client team (the team that will use your system’s out‐
            put) to see if this is a problem or not. If they tell you that they need precise pre‐
            dictions even beyond $500,000, then you have two options: 
             a. Collect proper labels for the districts whose labels were capped.
            b. Remove those districts from the training set (and also from the test set, since
              your system should not be evaluated poorly if it predicts values beyond
              $500,000).                                              
           3. These attributes have very different scales. We will discuss this later in this chap‐
            ter, when we explore feature scaling.                     
           4. Finally, many histograms are tail-heavy: they extend much farther to the right of
            the median than to the left. This may make it a bit harder for some Machine
            Learning algorithms to detect patterns. We will try transforming these attributes
            later on to have more bell-shaped distributions.          
                                                                      
          Hopefully you now have a better understanding of the kind of data you are dealing
          with.                                                       
                                                                      
                   Wait! Before you look at the data any further, you need to create a
                   test set, put it aside, and never look at it.      
                                                                      
                                                                      
                                                                      
          Create a Test Set                                           
                                                                      
          It may sound strange to voluntarily set aside part of the data at this stage. After all,
          you have only taken a quick glance at the data, and surely you should learn a whole
          lot more about it before you decide what algorithms to use, right? This is true, but
          your brain is an amazing pattern detection system, which means that it is highly
          prone to overfitting: if you look at the test set, you may stumble upon some seemingly
          interesting pattern in the test data that leads you to select a particular kind of
          Machine Learning model. When you estimate the generalization error using the test
          set, your estimate will be too optimistic, and you will launch a system that will not
          perform as well as expected. This is called data snooping bias.
          Creating a test set is theoretically simple: pick some instances randomly, typically
          20% of the dataset (or less if your dataset is very large), and set them aside:
                                                                      "|test sets; tail-heavy histograms; data snooping bias
"                                                                      
                                                                      
                                                                      
                                                                      
          The function first asks the collect policy for its initial state (given the environment
          batch size, which is 1 in this case). Since the policy is stateless, this returns an empty
          tuple (so we could have written policy_state = ()). Next, we create an iterator over
          the dataset, and we run the training loop. At each iteration, we call the driver’s run()
          method, passing it the current time step (initially None) and the current policy state. It
          will run the collect policy and collect experience for four steps (as we configured ear‐
          lier), broadcasting the collected trajectories to the replay buffer and the metrics. Next,
          we sample one batch of trajectories from the dataset, and we pass it to the agent’s
          train() method. It returns a train_loss object which may vary depending on the
          type of agent. Next, we display the iteration number and the training loss, and every
          1,000 iterations we log all the metrics. Now you can just call train_agent() for some
          number of iterations, and see the agent gradually learn to play Breakout!
                                                                      
            train_agent(10000000)                                     
          This will take a lot of computing power and a lot of patience (it may take hours, or
          even days, depending on your hardware), plus you may need to run the algorithm
          several times with different random seeds to get good results, but once it’s done, the
          agent will be superhuman (at least at Breakout). You can also try training this DQN
          agent on other Atari games: it can achieve superhuman skill at most action games,
          but it is not so good at games with long-running storylines.22
          Overview of Some Popular RL Algorithms                      
                                                                      
                                                                      
          Before we finish this chapter, let’s take a quick look at a few popular RL algorithms:
          Actor-Critic algorithms                                     
            A family of RL algorithms that combine Policy Gradients with Deep Q-
            Networks. An Actor-Critic agent contains two neural networks: a policy net and
            a DQN. The DQN is trained normally, by learning from the agent’s experiences.
            The policy net learns differently (and much faster) than in regular PG: instead of
            estimating the value of each action by going through multiple episodes, then
            summing the future discounted rewards for each action, and finally normalizing
            them, the agent (actor) relies on the action values estimated by the DQN (critic).
            It’s a bit like an athlete (the agent) learning with the help of a coach (the DQN).
          Asynchronous Advantage Actor-Critic23 (A3C)                 
            An important Actor-Critic variant introduced by DeepMind researchers in 2016,
            where multiple agents learn in parallel, exploring different copies of the environ‐
                                                                      
                                                                      
          22 For a comparison of this algorithm’s performance on various Atari games, see figure 3 in DeepMind’s 2015
           paper.                                                     
          23 Volodymyr Mnih et al., “Asynchonous Methods for Deep Reinforcement Learning,” Proceedings of the 33rd
           International Conference on Machine Learning (2016): 1928–1937."|TF-Agents library; Actor-Critic algorithms; Asynchronous Advantage Actor-Critic (A3C)
"                                                                      
                                                                      
                                                                      
                                                                      
          Deep Convolutional GANs                                     
                                                                      
          The original GAN paper in 2014 experimented with convolutional layers, but only
          tried to generate small images. Soon after, many researchers tried to build GANs
          based on deeper convolutional nets for larger images. This proved to be tricky, as
          training was very unstable, but Alec Radford et al. finally succeeded in late 2015, after
          experimenting with many different architectures and hyperparameters. They called
          their architecture deep convolutional GANs (DCGANs).13 Here are the main guide‐
          lines they proposed for building stable convolutional GANs: 
           • Replace any pooling layers with strided convolutions (in the discriminator) and
            transposed convolutions (in the generator).               
                                                                      
           • Use Batch Normalization in both the generator and the discriminator, except in
            the generator’s output layer and the discriminator’s input layer.
           • Remove fully connected hidden layers for deeper architectures.
           • Use ReLU activation in the generator for all layers except the output layer, which
            should use tanh.                                          
           • Use leaky ReLU activation in the discriminator for all layers.
                                                                      
          These guidelines will work in many cases, but not always, so you may still need to
          experiment with different hyperparameters (in fact, just changing the random seed
          and training the same model again will sometimes work). For example, here is a small
          DCGAN that works reasonably well with Fashion MNIST:        
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          13 Alec Radford et al., “Unsupervised Representation Learning with Deep Convolutional Generative Adversarial
           Networks,” arXiv preprint arXiv:1511.06434 (2015).         "|deep convolutional GANs (DCGANs); deep convolutional GANs
"                                                                      
                                                                      
                                                                      
                                                                      
          Columns API, which is harder to use and less intuitive (if you want to learn more
          about the Feature Columns API anyway, please check out the notebook for this chap‐
          ter).                                                       
                                                                      
          We already discussed two of these layers: the keras.layers.Normalization layer that
          will perform feature standardization (it will be equivalent to the Standardization
          layer we defined earlier), and the TextVectorization layer that will be capable of
          encoding each word in the inputs into its index in the vocabulary. In both cases, you
          create the layer, you call its adapt() method with a data sample, and then you use the
          layer normally in your model. The other preprocessing layers will follow the same
          pattern.                                                    
          The API will also include a keras.layers.Discretization layer that will chop con‐
          tinuous data into different bins and encode each bin as a one-hot vector. For example,
          you could use it to discretize prices into three categories, (low, medium, high), which
          would be encoded as [1, 0, 0], [0, 1, 0], and [0, 0, 1], respectively. Of course this loses a
          lot of information, but in some cases it can help the model detect patterns that would
          otherwise not be obvious when just looking at the continuous values.
                                                                      
                   The Discretization layer will not be differentiable, and it should
                   only be used at the start of your model. Indeed, the model’s prepro‐
                   cessing layers will be frozen during training, so their parameters
                   will not be affected by Gradient Descent, and thus they do not need
                   to be differentiable. This also means that you should not use an
                   Embedding layer directly in a custom preprocessing layer, if you
                   want it to be trainable: instead, it should be added separately to
                   your model, as in the previous code example.       
          It will also be possible to chain multiple preprocessing layers using the Preproces
          singStage class. For example, the following code will create a preprocessing pipeline
          that will first normalize the inputs, then discretize them (this may remind you of
          Scikit-Learn pipelines). After you adapt this pipeline to a data sample, you can use it
          like a regular layer in your models (but again, only at the start of the model, since it
          contains a nondifferentiable preprocessing layer):          
                                                                      
            normalization = keras.layers.Normalization()              
            discretization = keras.layers.Discretization([...])       
            pipeline = keras.layers.PreprocessingStage([normalization, discretization])
            pipeline.adapt(data_sample)                               
          The TextVectorization layer will also have an option to output word-count vectors
          instead of word indices. For example, if the vocabulary contains three words, say
          [""and"", ""basketball"", ""more""], then the text ""more and more"" will be mapped to
          the vector [1, 0, 2]: the word ""and"" appears once, the word ""basketball"" does not
          appear at all, and the word ""more"" appears twice. This text representation is called a"|bag of words
"                                                                      
                                                                      
                                                                      
                                                                      
          be a good idea to leave a gap between these sets to avoid the risk of a paragraph over‐
          lapping over two sets.                                      
                                                                      
          When dealing with time series, you would in general split across time,: for example,
          you might take the years 2000 to 2012 for the training set, the years 2013 to 2015 for
          the validation set, and the years 2016 to 2018 for the test set. However, in some cases
          you may be able to split along other dimensions, which will give you a longer time
          period to train on. For example, if you have data about the financial health of 10,000
          companies from 2000 to 2018, you might be able to split this data across the different
          companies. It’s very likely that many of these companies will be strongly correlated,
          though (e.g., whole economic sectors may go up or down jointly), and if you have
          correlated companies across the training set and the test set your test set will not be as
          useful, as its measure of the generalization error will be optimistically biased.
          So, it is often safer to split across time—but this implicitly assumes that the patterns
          the RNN can learn in the past (in the training set) will still exist in the future. In other
          words, we assume that the time series is stationary (at least in a wide sense).3 For
          many time series this assumption is reasonable (e.g., chemical reactions should be
          fine, since the laws of chemistry don’t change every day), but for many others it is not
          (e.g., financial markets are notoriously not stationary since patterns disappear as soon
          as traders spot them and start exploiting them). To make sure the time series is
          indeed sufficiently stationary, you can plot the model’s errors on the validation set
          across time: if the model performs much better on the first part of the validation set
          than on the last part, then the time series may not be stationary enough, and you
          might be better off training the model on a shorter time span.
          In short, splitting a time series into a training set, a validation set, and a test set is not
          a trivial task, and how it’s done will depend strongly on the task at hand.
          Now back to Shakespeare! Let’s take the first 90% of the text for the training set
          (keeping the rest for the validation set and the test set), and create a tf.data.Dataset
          that will return each character one by one from this set:   
                                                                      
            train_size = dataset_size * 90 // 100                     
            dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
          Chopping the Sequential Dataset into Multiple Windows       
                                                                      
          The training set now consists of a single sequence of over a million characters, so we
          can’t just train the neural network directly on it: the RNN would be equivalent to a
                                                                      
                                                                      
          3 By definition, a stationary time series’s mean, variance, and autocorrelations (i.e., correlations between values
           in the time series separated by a given interval) do not change over time. This is quite restrictive; for example,
           it excludes time series with trends or cyclical patterns. RNNs are more tolerant in that they can learn trends
           and cyclical patterns.                                     "|chopping sequential datasets
"                                                                      
                                                                      
                                                                      
                                                                      
          from then on. To estimate this sum of future discounted rewards, we can simply exe‐
          cute the DQN on the next state s′ and for all possible actions a′. We get an approxi‐
          mate future Q-Value for each possible action. We then pick the highest (since we
          assume we will be playing optimally) and discount it, and this gives us an estimate of
          the sum of future discounted rewards. By summing the reward r and the future dis‐
          counted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a),
          as shown in Equation 18-7.                                  
                                                                      
            Equation 18-7. Target Q-Value                             
                                                                      
            Q   s,a =r+γ·max Q s′,a′                                  
             target         θ                                         
                        a′                                            
          With this target Q-Value, we can run a training step using any Gradient Descent algo‐
          rithm. Specifically, we generally try to minimize the squared error between the esti‐
          mated Q-Value Q(s, a) and the target Q-Value (or the Huber loss to reduce the
          algorithm’s sensitivity to large errors). And that’s all for the basic Deep Q-Learning
          algorithm! Let’s see how to implement it to solve the CartPole environment.
          Implementing Deep Q-Learning                                
                                                                      
                                                                      
          The first thing we need is a Deep Q-Network. In theory, you need a neural net that
          takes a state-action pair and outputs an approximate Q-Value, but in practice it’s
          much more efficient to use a neural net that takes a state and outputs one approxi‐
          mate Q-Value for each possible action. To solve the CartPole environment, we do not
          need a very complicated neural net; a couple of hidden layers will do:
            env = gym.make(""CartPole-v0"")                             
            input_shape = [4] # == env.observation_space.shape        
            n_outputs = 2 # == env.action_space.n                     
            model = keras.models.Sequential([                         
               keras.layers.Dense(32, activation=""elu"", input_shape=input_shape),
               keras.layers.Dense(32, activation=""elu""),              
               keras.layers.Dense(n_outputs)                          
            ])                                                        
          To select an action using this DQN, we pick the action with the largest predicted Q-
          Value. To ensure that the agent explores the environment, we will use an ε-greedy
          policy (i.e., we will choose a random action with probability ε):
            def epsilon_greedy_policy(state, epsilon=0):              
               if np.random.rand() < epsilon:                         
                 return np.random.randint(2)                          
               else:                                                  
                 Q_values = model.predict(state[np.newaxis])          
                 return np.argmax(Q_values[0])                        "|Q-Learning; Deep Q-Learning
"                                                                      
                                                                      
                                                                      
                                                                      
          them to explore a wider range of features, ultimately improving generalization. Equa‐
          tion 14-2 shows how to apply LRN.                           
                                                                      
            Equation 14-2. Local response normalization (LRN)         
                                                                      
                                         r                            
                    j    −β    j  = min i+ , f −1                     
                    high       high     2  n                          
            b =a k+α ∑ a 2 with                                       
             i i   j= j j                r                            
                     low       j = max 0,i−                           
                               low       2                            
          In this equation:                                           
           • b is the normalized output of the neuron located in feature map i, at some row u
             i                                                        
            and column v (note that in this equation we consider only neurons located at this
            row and column, so u and v are not shown).                
           • a is the activation of that neuron after the ReLU step, but before normalization.
             i                                                        
           • k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth
            radius.                                                   
           • f is the number of feature maps.                         
             n                                                        
          For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation
          of the neurons located in the feature maps immediately above and below its own.
          In AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and
          k = 1. This step can be implemented using the tf.nn.local_response_normaliza
          tion() function (which you can wrap in a Lambda layer if you want to use it in a
          Keras model).                                               
          A variant of AlexNet called ZF Net12 was developed by Matthew Zeiler and Rob Fer‐
          gus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked
          hyperparameters (number of feature maps, kernel size, stride, etc.).
          GoogLeNet                                                   
          The GoogLeNet architecture was developed by Christian Szegedy et al. from Google
          Research,13 and it won the ILSVRC 2014 challenge by pushing the top-five error rate
          below 7%. This great performance came in large part from the fact that the network
          was much deeper than previous CNNs (as you’ll see in Figure 14-14). This was made
                                                                      
          12 Matthew D. Zeiler and Rob Fergus, “Visualizing and Understanding Convolutional Networks,” Proceedings of
           the European Conference on Computer Vision (2014): 818-833.
          13 Christian Szegedy et al., “Going Deeper with Convolutions,” Proceedings of the IEEE Conference on Computer
           Vision and Pattern Recognition (2015): 1–9.                "|GoogLeNet; depth radius; ZF Net
"                                                                      
                                                                      
                                                                      
                                                                      
                   ""id"": Feature(int64_list=Int64List(value=[123])),  
                   ""emails"": Feature(bytes_list=BytesList(value=[b""a@b.com"",
                                              b""c@d.com""]))           
                 }))                                                  
          The code is a bit verbose and repetitive, but it’s rather straightforward (and you could
          easily wrap it inside a small helper function). Now that we have an Example protobuf,
          we can serialize it by calling its SerializeToString() method, then write the result‐
          ing data to a TFRecord file:                                
            with tf.io.TFRecordWriter(""my_contacts.tfrecord"") as f:   
               f.write(person_example.SerializeToString())            
          Normally you would write much more than one Example! Typically, you would create
          a conversion script that reads from your current format (say, CSV files), creates an
          Example protobuf for each instance, serializes them, and saves them to several TFRe‐
          cord files, ideally shuffling them in the process. This requires a bit of work, so once
          again make sure it is really necessary (perhaps your pipeline works fine with CSV
          files).                                                     
                                                                      
          Now that we have a nice TFRecord file containing a serialized Example, let’s try to
          load it.                                                    
          Loading and Parsing Examples                                
                                                                      
          To load the serialized Example protobufs, we will use a tf.data.TFRecordDataset
          once again, and we will parse each Example using tf.io.parse_single_example().
          This is a TensorFlow operation, so it can be included in a TF Function. It requires at
          least two arguments: a string scalar tensor containing the serialized data, and a
          description of each feature. The description is a dictionary that maps each feature
          name to either a tf.io.FixedLenFeature descriptor indicating the feature’s shape,
          type, and default value, or a tf.io.VarLenFeature descriptor indicating only the type
          (if the length of the feature’s list may vary, such as for the ""emails"" feature).
                                                                      
          The following code defines a description dictionary, then it iterates over the TFRecord
          Dataset and parses the serialized Example protobuf this dataset contains:
            feature_description = {                                   
               ""name"": tf.io.FixedLenFeature([], tf.string, default_value=""""),
               ""id"": tf.io.FixedLenFeature([], tf.int64, default_value=0),
               ""emails"": tf.io.VarLenFeature(tf.string),              
            }                                                         
            for serialized_example in tf.data.TFRecordDataset([""my_contacts.tfrecord""]):
               parsed_example = tf.io.parse_single_example(serialized_example,
                                        feature_description)          
                                                                      
                                                                      "|loading and parsing examples
"                                                                      
                                                                      
                                                                      
                                                                      
          index 1, twice. Then we used tf.one_hot() to one-hot encode these indices. Notice
          that we have to tell this function the total number of indices, which is equal to the
          vocabulary size plus the number of oov buckets. Now you know how to encode cate‐
          gorical features to one-hot vectors using TensorFlow!       
                                                                      
          Just like earlier, it wouldn’t be too difficult to bundle all of this logic into a nice self-
          contained class. Its adapt() method would take a data sample and extract all the dis‐
          tinct categories it contains. It would create a lookup table to map each category to its
          index (including unknown categories using oov buckets). Then its call() method
          would use the lookup table to map the input categories to their indices. Well, here’s
          more good news: by the time you read this, Keras will probably include a layer called
          keras.layers.TextVectorization, which will be capable of doing exactly that: its
          adapt() method will extract the vocabulary from a data sample, and its call()
          method will convert each category to its index in the vocabulary. You could add this
          layer at the beginning of your model, followed by a Lambda layer that would apply the
          tf.one_hot() function, if you want to convert these indices to one-hot vectors.
          This may not be the best solution, though. The size of each one-hot vector is the
          vocabulary length plus the number of oov buckets. This is fine when there are just a
          few possible categories, but if the vocabulary is large, it is much more efficient to
          encode them using embeddings instead.                       
                                                                      
                   As a rule of thumb, if the number of categories is lower than 10,
                   then one-hot encoding is generally the way to go (but your mileage
                   may vary!). If the number of categories is greater than 50 (which is
                   often the case when you use hash buckets), then embeddings are
                   usually preferable. In between 10 and 50 categories, you may want
                   to experiment with both options and see which one works best for
                   your use case.                                     
          Encoding Categorical Features Using Embeddings              
                                                                      
          An embedding is a trainable dense vector that represents a category. By default,
          embeddings are initialized randomly, so for example the ""NEAR BAY"" category could
          be represented initially by a random vector such as [0.131, 0.890], while the ""NEAR
          OCEAN"" category might be represented by another random vector such as [0.631,
          0.791]. In this example, we use 2D embeddings, but the number of dimensions is a
          hyperparameter you can tweak. Since these embeddings are trainable, they will grad‐
          ually improve during training; and as they represent fairly similar categories, Gradi‐
          ent Descent will certainly end up pushing them closer together, while it will tend to
          move them away from the ""INLAND"" category’s embedding (see Figure 13-4). Indeed,
          the better the representation, the easier it will be for the neural network to make
          accurate predictions, so training tends to make embeddings useful representations of
                                                                      "|encoding using embeddings; embedding
"                                                                      
                                                                      
                                                                      
                                                                      
          embedding of the token that was actually output. During training, it should be the
          embedding of the previous target token: this is why we used the TrainingSampler. In
          practice, it is often a good idea to start training with the embedding of the target of
          the previous time step and gradually transition to using the embedding of the actual
          token that was output at the previous step. This idea was introduced in a 2015 paper12
          by Samy Bengio et al. The ScheduledEmbeddingTrainingSampler will randomly
          choose between the target or the actual output, with a probability that you can gradu‐
          ally change during training.                                
                                                                      
          Bidirectional RNNs                                          
                                                                      
          A each time step, a regular recurrent layer only looks at past and present inputs
          before generating its output. In other words, it is “causal,” meaning it cannot look into
          the future. This type of RNN makes sense when forecasting time series, but for many
          NLP tasks, such as Neural Machine Translation, it is often preferable to look ahead at
          the next words before encoding a given word. For example, consider the phrases “the
          Queen of the United Kingdom,” “the queen of hearts,” and “the queen bee”: to prop‐
          erly encode the word “queen,” you need to look ahead. To implement this, run two
          recurrent layers on the same inputs, one reading the words from left to right and the
          other reading them from right to left. Then simply combine their outputs at each
          time step, typically by concatenating them. This is called a bidirectional recurrent layer
          (see Figure 16-5).                                          
          To implement a bidirectional recurrent layer in Keras, wrap a recurrent layer in a
          keras.layers.Bidirectional layer. For example, the following code creates a bidir‐
          ectional GRU layer:                                         
            keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))
                                                                      
                   The Bidirectional layer will create a clone of the GRU layer (but in
                   the reverse direction), and it will run both and concatenate their
                   outputs. So although the GRU layer has 10 units, the Bidirectional
                   layer will output 20 values per time step.         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          12 Samy Bengio et al., “Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks,” arXiv
           preprint arXiv:1506.03099 (2015).                          "|bidirectional recurrent layers; bidirectional RNNs; bidirectional recurrent layer
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-16. Trajectories, transitions, time steps, and action steps
                                                                      
          So if you have a batch of trajectories where each trajectory has t + 1 steps (from time
          step n to time step n + t), then it contains all the data from time step n to time step n
          + t, except for the reward and discount from time step n (but it contains the reward
          and discount of time step n + t + 1). This represents t transitions (n to n + 1, n + 1 to
          n + 2, …, n + t – 1 to n + t).                              
          The to_transition() function in the tf_agents.trajectories.trajectory mod‐
          ule converts a batched trajectory into a list containing a batched time_step, a batched
          action_step, and a batched next_time_step. Notice that the second dimension is 2
          instead of 3, since there are t transitions between t + 1 time steps (don’t worry if
          you’re a bit confused; you’ll get the hang of it):          
            >>> from tf_agents.trajectories.trajectory import to_transition
            >>> time_steps, action_steps, next_time_steps = to_transition(trajectories)
            >>> time_steps.observation.shape                          
            TensorShape([2, 2, 84, 84, 4]) # 3 time steps = 2 transitions
                                                                      
                   A sampled trajectory may actually overlap two (or more) episodes!
                   In this case, it will contain boundary transitions, meaning transi‐
                   tions with a step_type equal to 2 (end) and a next_step_type
                   equal to 0 (start). Of course, TF-Agents properly handles such tra‐
                   jectories (e.g., by resetting the policy state when encountering a
                   boundary). The trajectory’s is_boundary() method returns a ten‐
                   sor indicating whether each step is a boundary or not.
                                                                      "|boundary transitions
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-25. The same fully convolutional network processing a small image (left) and a
          large one (right)                                           
          You Only Look Once (YOLO)                                   
                                                                      
          YOLO is an extremely fast and accurate object detection architecture proposed by
          Joseph Redmon et al. in a 2015 paper,28 and subsequently improved in 201629
          (YOLOv2) and in 201830 (YOLOv3). It is so fast that it can run in real time on a video,
          as seen in Redmon’s demo.                                   
                                                                      
          YOLOv3’s architecture is quite similar to the one we just discussed, but with a few
          important differences:                                      
                                                                      
                                                                      
          28 Joseph Redmon et al., “You Only Look Once: Unified, Real-Time Object Detection,” Proceedings of the IEEE
           Conference on Computer Vision and Pattern Recognition (2016): 779–788.
          29 Joseph Redmon and Ali Farhadi, “YOLO9000: Better, Faster, Stronger,” Proceedings of the IEEE Conference on
           Computer Vision and Pattern Recognition (2017): 6517–6525. 
          30 Joseph Redmon and Ali Farhadi, “YOLOv3: An Incremental Improvement,” arXiv preprint arXiv:1804.02767
           (2018).                                                    "|You Only Look Once (YOLO)
"                                                                      
                                                                      
                                                                      
                                                                      
          With that, you have all you need to run any operation on any device, and exploit the
          power of your GPUs! Here are some of the things you could do:
                                                                      
           • You could train several models in parallel, each on its own GPU: just write a
            training script for each model and run them in parallel, setting
            CUDA_DEVICE_ORDER and CUDA_VISIBLE_DEVICES so that each script only sees a
            single GPU device. This is great for hyperparameter tuning, as you can train in
            parallel multiple models with different hyperparameters. If you have a single
            machine with two GPUs, and it takes one hour to train one model on one GPU,
            then training two models in parallel, each on its own dedicated GPU, will take
            just one hour. Simple!                                    
           • You could train a model on a single GPU and perform all the preprocessing in
            parallel on the CPU, using the dataset’s prefetch() method17 to prepare the next
            few batches in advance so that they are ready when the GPU needs them (see
            Chapter 13).                                              
           • If your model takes two images as input and processes them using two CNNs
            before joining their outputs, then it will probably run much faster if you place
            each CNN on a different GPU.                              
                                                                      
           • You can create an efficient ensemble: just place a different trained model on each
            GPU so that you can get all the predictions much faster to produce the ensem‐
            ble’s final prediction.                                   
          But what if you want to train a single model across multiple GPUs?
                                                                      
          Training Models Across Multiple Devices                     
                                                                      
          There are two main approaches to training a single model across multiple devices:
          model parallelism, where the model is split across the devices, and data parallelism,
          where the model is replicated across every device, and each replica is trained on a
          subset of the data. Let’s look at these two options closely before we train a model on
          multiple GPUs.                                              
                                                                      
          Model Parallelism                                           
          So far we have trained each neural network on a single device. What if we want to
          train a single neural network across multiple devices? This requires chopping the
          model into separate chunks and running each chunk on a different device.
                                                                      
                                                                      
                                                                      
          17 At the time of this writing it only prefetches the data to the CPU RAM, but you can use tf.data.experimen
           tal.prefetch_to_device() to make it prefetch the data and push it to the device of your choice so that the
           GPU does not waste time waiting for the data to be transferred."|data parallelism; training models across multiple devices; using GPUs to speed computations; model parallelism; training across multiple devices
"                                                                      
                                                                      
                                                                      
                                                                      
          For our main training loop, instead of calling the get_next() method, we will use a
          tf.data.Dataset. This way, we can benefit from the power of the Data API (e.g., par‐
          allelism and prefetching). For this, we call the replay buffer’s as_dataset() method:
                                                                      
            dataset = replay_buffer.as_dataset(                       
               sample_batch_size=64,                                  
               num_steps=2,                                           
               num_parallel_calls=3).prefetch(3)                      
          We will sample batches of 64 trajectories at each training step (as in the 2015 DQN
          paper), each with 2 steps (i.e., 2 steps = 1 full transition, including the next step’s
          observation). This dataset will process three elements in parallel, and prefetch three
          batches.                                                    
                   For on-policy algorithms such as Policy Gradients, each experience
                   should be sampled once, used from training, and then discarded. In
                   this case, you can still use a replay buffer, but instead of using a
                   Dataset, you would call the replay buffer’s gather_all() method
                   at each training iteration to get a tensor containing all the trajecto‐
                   ries recorded so far, then use them to perform a training step, and
                   finally clear the replay buffer by calling its clear() method.
                                                                      
          Now that we have all the components in place, we are ready to train the model!
                                                                      
          Creating the Training Loop                                  
          To speed up training, we will convert the main functions to TensorFlow Functions.
          For this we will use the tf_agents.utils.common.function() function, which wraps
          tf.function(), with some extra experimental options:        
                                                                      
            from tf_agents.utils.common import function               
            collect_driver.run = function(collect_driver.run)         
            agent.train = function(agent.train)                       
          Let’s create a small function that will run the main training loop for n_iterations:
                                                                      
            def train_agent(n_iterations):                            
               time_step = None                                       
               policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)
               iterator = iter(dataset)                               
               for iteration in range(n_iterations):                  
                 time_step, policy_state = collect_driver.run(time_step, policy_state)
                 trajectories, buffer_info = next(iterator)           
                 train_loss = agent.train(trajectories)               
                 print(""\r{} loss:{:.5f}"".format(                     
                   iteration, train_loss.loss.numpy()), end="""")       
                 if iteration % 1000 == 0:                            
                   log_metrics(train_metrics)                         "|training loops
"                                                                      
                                                                      
                                                                      
                                                                      
          Attention mechanisms are so powerful that you can actually build state-of-the-art
          models using only attention mechanisms.                     
                                                                      
          Attention Is All You Need: The Transformer Architecture     
                                                                      
          In a groundbreaking 2017 paper,20 a team of Google researchers suggested that
          “Attention Is All You Need.” They managed to create an architecture called the Trans‐
          former, which significantly improved the state of the art in NMT without using any
          recurrent or convolutional layers,21 just attention mechanisms (plus embedding lay‐
          ers, dense layers, normalization layers, and a few other bits and pieces). As an extra
          bonus, this architecture was also much faster to train and easier to parallelize, so they
          managed to train it at a fraction of the time and cost of the previous state-of-the-art
          models.                                                     
          The Transformer architecture is represented in Figure 16-8. 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          20 Ashish Vaswani et al., “Attention Is All You Need,” Proceedings of the 31st International Conference on Neural
           Information Processing Systems (2017): 6000–6010.          
          21 Since the Transformer uses time-distributed Dense layers, you could argue that it uses 1D convolutional layers
           with a kernel size of 1.                                   "|Transformer architecture
"                                                                      
                                                                      
                                                                      
                                                                      
          Fighting the Unstable Gradients Problem                     
                                                                      
          Many of the tricks we used in deep nets to alleviate the unstable gradients problem
          can also be used for RNNs: good parameter initialization, faster optimizers, dropout,
          and so on. However, nonsaturating activation functions (e.g., ReLU) may not help as
          much here; in fact, they may actually lead the RNN to be even more unstable during
          training. Why? Well, suppose Gradient Descent updates the weights in a way that
          increases the outputs slightly at the first time step. Because the same weights are used
          at every time step, the outputs at the second time step may also be slightly increased,
          and those at the third, and so on until the outputs explode—and a nonsaturating acti‐
          vation function does not prevent that. You can reduce this risk by using a smaller
          learning rate, but you can also simply use a saturating activation function like the
          hyperbolic tangent (this explains why it is the default). In much the same way, the
          gradients themselves can explode. If you notice that training is unstable, you may
          want to monitor the size of the gradients (e.g., using TensorBoard) and perhaps use
          Gradient Clipping.                                          
          Moreover, Batch Normalization cannot be used as efficiently with RNNs as with deep
          feedforward nets. In fact, you cannot use it between time steps, only between recur‐
          rent layers. To be more precise, it is technically possible to add a BN layer to a mem‐
          ory cell (as we will see shortly) so that it will be applied at each time step (both on the
          inputs for that time step and on the hidden state from the previous step). However,
          the same BN layer will be used at each time step, with the same parameters, regardless
          of the actual scale and offset of the inputs and hidden state. In practice, this does not
          yield good results, as was demonstrated by César Laurent et al. in a 2015 paper:3 the
          authors found that BN was slightly beneficial only when it was applied to the inputs,
          not to the hidden states. In other words, it was slightly better than nothing when
          applied between recurrent layers (i.e., vertically in Figure 15-7), but not within recur‐
          rent layers (i.e., horizontally). In Keras this can be done simply by adding a Batch
          Normalization layer before each recurrent layer, but don’t expect too much from it.
          Another form of normalization often works better with RNNs: Layer Normalization.
          This idea was introduced by Jimmy Lei Ba et al. in a 2016 paper:4 it is very similar to
          Batch Normalization, but instead of normalizing across the batch dimension, it nor‐
          malizes across the features dimension. One advantage is that it can compute the
          required statistics on the fly, at each time step, independently for each instance. This
          also means that it behaves the same way during training and testing (as opposed to
          BN), and it does not need to use exponential moving averages to estimate the feature
          statistics across all instances in the training set. Like BN, Layer Normalization learns a
                                                                      
                                                                      
          3 César Laurent et al., “Batch Normalized Recurrent Neural Networks,” Proceedings of the IEEE International
           Conference on Acoustics, Speech, and Signal Processing (2016): 2657–2661.
          4 Jimmy Lei Ba et al., “Layer Normalization,” arXiv preprint arXiv:1607.06450 (2016)."|Layer Normalization; unstable gradients problem
"                                                                      
                                                                      
                                                                      
                                                                      
                 cycle_length=n_readers, num_parallel_calls=n_read_threads)
               dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)
               dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat)
               return dataset.batch(batch_size).prefetch(1)           
          Everything should make sense in this code, except the very last line (prefetch(1)),
          which is important for performance.                         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 13-2. Loading and preprocessing data from multiple CSV files
                                                                      
          Prefetching                                                 
                                                                      
          By calling prefetch(1) at the end, we are creating a dataset that will do its best to
          always be one batch ahead.2 In other words, while our training algorithm is working
          on one batch, the dataset will already be working in parallel on getting the next batch
          ready (e.g., reading the data from disk and preprocessing it). This can improve per‐
          formance dramatically, as is illustrated in Figure 13-3. If we also ensure that loading
          and preprocessing are multithreaded (by setting num_parallel_calls when calling
          interleave() and map()), we can exploit multiple cores on the CPU and hopefully
          make preparing one batch of data shorter than running a training step on the GPU:
                                                                      
                                                                      
                                                                      
          2 In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐
           tively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE (this is an
           experimental feature for now).                             "|prefetching data; prefetching
"                                                                      
                                                                      
                                                                      
                                                                      
          Clustering                                                  
            The goal is to group similar instances together into clusters. Clustering is a great
            tool for data analysis, customer segmentation, recommender systems, search
            engines, image segmentation, semi-supervised learning, dimensionality reduc‐
            tion, and more.                                           
                                                                      
          Anomaly detection                                           
            The objective is to learn what “normal” data looks like, and then use that to
            detect abnormal instances, such as defective items on a production line or a new
            trend in a time series.                                   
          Density estimation                                          
            This is the task of estimating the probability density function (PDF) of the random
            process that generated the dataset. Density estimation is commonly used for
            anomaly detection: instances located in very low-density regions are likely to be
            anomalies. It is also useful for data analysis and visualization.
          Ready for some cake? We will start with clustering, using K-Means and DBSCAN,
          and then we will discuss Gaussian mixture models and see how they can be used for
          density estimation, clustering, and anomaly detection.      
                                                                      
          Clustering                                                  
                                                                      
          As you enjoy a hike in the mountains, you stumble upon a plant you have never seen
          before. You look around and you notice a few more. They are not identical, yet they
          are sufficiently similar for you to know that they most likely belong to the same spe‐
          cies (or at least the same genus). You may need a botanist to tell you what species that
          is, but you certainly don’t need an expert to identify groups of similar-looking objects.
          This is called clustering: it is the task of identifying similar instances and assigning
          them to clusters, or groups of similar instances.           
          Just like in classification, each instance gets assigned to a group. However, unlike clas‐
          sification, clustering is an unsupervised task. Consider Figure 9-1: on the left is the
          iris dataset (introduced in Chapter 4), where each instance’s species (i.e., its class) is
          represented with a different marker. It is a labeled dataset, for which classification
          algorithms such as Logistic Regression, SVMs, or Random Forest classifiers are well
          suited. On the right is the same dataset, but without the labels, so you cannot use a
          classification algorithm anymore. This is where clustering algorithms step in: many of
          them can easily detect the lower-left cluster. It is also quite easy to see with our own
          eyes, but it is not so obvious that the upper-right cluster is composed of two distinct
          sub-clusters. That said, the dataset has two additional features (sepal length and
          width), not represented here, and clustering algorithms can make good use of all fea‐
          tures, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mix‐
          ture model, only 5 instances out of 150 are assigned to the wrong cluster).
                                                                      "|clustering; probability density function (PDF); density estimation; anomaly detection; K-Means
"                                                                      
                                                                      
                                                                      
                                                                      
            from sklearn.model_selection import StratifiedShuffleSplit
                                                                      
            split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
            for train_index, test_index in split.split(housing, housing[""income_cat""]):
               strat_train_set = housing.loc[train_index]             
               strat_test_set = housing.loc[test_index]               
          Let’s see if this worked as expected. You can start by looking at the income category
          proportions in the test set:                                
            >>> strat_test_set[""income_cat""].value_counts() / len(strat_test_set)
            3  0.350533                                               
            2  0.318798                                               
            4  0.176357                                               
            5  0.114583                                               
            1  0.039729                                               
            Name: income_cat, dtype: float64                          
          With similar code you can measure the income category proportions in the full data‐
          set. Figure 2-10 compares the income category proportions in the overall dataset, in
          the test set generated with stratified sampling, and in a test set generated using purely
          random sampling. As you can see, the test set generated using stratified sampling has
          income category proportions almost identical to those in the full dataset, whereas the
          test set generated using purely random sampling is skewed.  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 2-10. Sampling bias comparison of stratified versus purely random sampling
                                                                      
          Now you should remove the income_cat attribute so the data is back to its original
          state:                                                      
            for set_ in (strat_train_set, strat_test_set):            
               set_.drop(""income_cat"", axis=1, inplace=True)          
                                                                      
          We spent quite a bit of time on test set generation for a good reason: this is an often
          neglected but critical part of a Machine Learning project. Moreover, many of these
          ideas will be useful later when we discuss cross-validation. Now it’s time to move on
          to the next stage: exploring the data.                      
                                                                      
                                                                      "|data downloading
"                                                                      
                                                                      
                                                                      
                                                                      
          bag of words, since it completely loses the order of the words. Common words like
          ""and"" will have a large value in most texts, even though they are usually the least
          interesting (e.g., in the text ""more and more basketball"" the word ""basketball"" is
          clearly the most important, precisely because it is not a very frequent word). So, the
          word counts should be normalized in a way that reduces the importance of frequent
          words. A common way to do this is to divide each word count by the log of the total
          number of training instances in which the word appears. This technique is called
          Term-Frequency × Inverse-Document-Frequency (TF-IDF). For example, let’s imagine
          that the words ""and"", ""basketball"", and ""more"" appear respectively in 200, 10, and
          100 text instances in the training set: in this case, the final vector will be [1/
          log(200), 0/log(10), 2/log(100)], which is approximately equal to [0.19, 0.,
          0.43]. The TextVectorization layer will (likely) have an option to perform TF-IDF.
                                                                      
                   If the standard preprocessing layers are insufficient for your task,
                   you will still have the option to create your own custom prepro‐
                   cessing layer, much like we did earlier with the Standardization
                   class. Create a subclass of the keras.layers.PreprocessingLayer
                   class with an adapt() method, which should take a data_sample
                   argument and optionally an extra reset_state argument: if True,
                   then the adapt() method should reset any existing state before
                   computing the new state; if False, it should try to update the exist‐
                   ing state.                                         
                                                                      
          As you can see, these Keras preprocessing layers will make preprocessing much eas‐
          ier! Now, whether you choose to write your own preprocessing layers or use Keras’s
          (or even use the Feature Columns API), all the preprocessing will be done on the fly.
          During training, however, it may be preferable to perform preprocessing ahead of
          time. Let’s see why we’d want to do that and how we’d go about it.
          TF Transform                                                
                                                                      
          If preprocessing is computationally expensive, then handling it before training rather
          than on the fly may give you a significant speedup: the data will be preprocessed just
          once per instance before training, rather than once per instance and per epoch during
          training. As mentioned earlier, if the dataset is small enough to fit in RAM, you can
          use its cache() method. But if it is too large, then tools like Apache Beam or Spark
          will help. They let you run efficient data processing pipelines over large amounts of
          data, even distributed across multiple servers, so you can use them to preprocess all
          the training data before training.                          
                                                                      
          This works great and indeed can speed up training, but there is one problem: once
          your model is trained, suppose you want to deploy it to a mobile app. In that case you
          will need to write some code in your app to take care of preprocessing the data before"|TF Transform; Term-Frequency × Inverse-Document-Frequency (TF-IDF); preprocessing; TF Transform (tf.Transform); preprocessing input features
"                                                                      
                                                                      
                                                                      
                                                                      
          Bayes’ theorem (Equation 9-2) tells us how to update the probability distribution over
          the latent variables after we observe some data X. It computes the posterior distribu‐
          tion p(z|X), which is the conditional probability of z given X.
                                                                      
            Equation 9-2. Bayes’ theorem                              
                                                                      
                         likelihood × prior p X z p z                 
            p z X =posterior=      =                                  
                           evidence    p X                            
          Unfortunately, in a Gaussian mixture model (and many other problems), the denomi‐
          nator p(x) is intractable, as it requires integrating over all the possible values of z
          (Equation 9-3), which would require considering all possible combinations of cluster
          parameters and cluster assignments.                         
            Equation 9-3. The evidence p(X) is often intractable      
                                                                      
                ∫                                                     
            p X = p X z p z dz                                        
                                                                      
          This intractability is one of the central problems in Bayesian statistics, and there are
          several approaches to solving it. One of them is variational inference, which picks a
          family of distributions q(z; λ) with its own variational parameters λ (lambda), then
          optimizes these parameters to make q(z) a good approximation of p(z|X). This is
          achieved by finding the value of λ that minimizes the KL divergence from q(z) to
          p(z|X), noted D (q‖p). The KL divergence equation is shown in Equation 9-4, and it
                   KL                                                 
          can be rewritten as the log of the evidence (log p(X)) minus the evidence lower bound
          (ELBO). Since the log of the evidence does not depend on q, it is a constant term, so
          minimizing the KL divergence just requires maximizing the ELBO.
            Equation 9-4. KL divergence from q(z) to p(z|X)           
                          q z                                         
            D  q∥ p =  log                                            
             KL      q   p z X                                        
                   =   logq z − log p z X                             
                     q                                                
                               p z,X                                  
                   =   logq z − log                                   
                     q          p X                                   
                   =   logq z − log p z,X + log p X                   
                     q                                                
                   =   logq z −  log p z,X +  log p X                 
                     q        q         q                             
                   =   log p X −  log p z,X −  log q z                
                     q         q         q                            
                   = log p X −ELBO                                    
                    where ELBO=  log p z,X −  log q z                 
                              q         q                             "|p (posterior) distribution; variational parameters; evidence lower bound (ELBO); variational inference
"                                                                      
                                                                      
                                                                      
                                                                      
           6. What is the most important layer in the Transformer architecture? What is its
            purpose?                                                  
                                                                      
           7. When would you need to use sampled softmax?             
           8. Embedded Reber grammars were used by Hochreiter and Schmidhuber in their
            paper about LSTMs. They are artificial grammars that produce strings such as
            “BPBTSXXVPSEPE.” Check out Jenny Orr’s nice introduction to this topic.
            Choose a particular embedded Reber grammar (such as the one represented on
            Jenny Orr’s page), then train an RNN to identify whether a string respects that
            grammar or not. You will first need to write a function capable of generating a
            training batch containing about 50% strings that respect the grammar, and 50%
            that don’t.                                               
           9. Train an Encoder–Decoder model that can convert a date string from one format
            to another (e.g., from “April 22, 2019” to “2019-04-22”). 
          10. Go through TensorFlow’s Neural Machine Translation with Attention tutorial.
          11. Use one of the recent language models (e.g., BERT) to generate more convincing
            Shakespearean text.                                       
                                                                      
          Solutions to these exercises are available in Appendix A.   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|Embedded Reber grammars
"                                                                      
                                                                      
                                                                      
                                                                      
          work quite well too. We will discuss both of these possibilities, and we will finish this
          chapter by implementing a WaveNet: this is a CNN architecture capable of handling
          sequences of tens of thousands of time steps. In Chapter 16, we will continue to
          explore RNNs and see how to use them for natural language processing, along with
          more recent architectures based on attention mechanisms. Let’s get started!
                                                                      
          Recurrent Neurons and Layers                                
                                                                      
          Up to now we have focused on feedforward neural networks, where the activations
          flow only in one direction, from the input layer to the output layer (a few exceptions
          are discussed in Appendix E). A recurrent neural network looks very much like a
          feedforward neural network, except it also has connections pointing backward. Let’s
          look at the simplest possible RNN, composed of one neuron receiving inputs, pro‐
          ducing an output, and sending that output back to itself, as shown in Figure 15-1
          (left). At each time step t (also called a frame), this recurrent neuron receives the inputs
          x as well as its own output from the previous time step, y . Since there is no previ‐
           (t)                              (t–1)                     
          ous output at the first time step, it is generally set to 0. We can represent this tiny net‐
          work against the time axis, as shown in Figure 15-1 (right). This is called unrolling the
          network through time (it’s the same recurrent neuron represented once per time step).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 15-1. A recurrent neuron (left) unrolled through time (right)
                                                                      
          You can easily create a layer of recurrent neurons. At each time step t, every neuron
          receives both the input vector x and the output vector from the previous time step
                             (t)                                      
          y , as shown in Figure 15-2. Note that both the inputs and outputs are vectors now
           (t–1)                                                      
          (when there was just a single neuron, the output was a scalar).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|recurrent neurons and layers; time step; unrolling the network through time; WaveNet; recurrent; recurrent neurons
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                              Cross Entropy                           
                                                                      
           Cross entropy originated from information theory. Suppose you want to efficiently
           transmit information about the weather every day. If there are eight options (sunny,
           rainy, etc.), you could encode each option using three bits because 23 = 8. However, if
           you think it will be sunny almost every day, it would be much more efficient to code
           “sunny” on just one bit (0) and the other seven options on four bits (starting with a
           1). Cross entropy measures the average number of bits you actually send per option.
           If your assumption about the weather is perfect, cross entropy will be equal to the
           entropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump‐
           tions are wrong (e.g., if it rains often), cross entropy will be greater by an amount
           called the Kullback–Leibler (KL) divergence.               
           The cross entropy between two probability distributions p and q is defined as H(p,q)
           = —Σx p(x) log q(x) (at least when the distributions are discrete). For more details,
           check out my video on the subject.                         
                                                                      
          The gradient vector of this cost function with regard to θ(k) is given by Equation 4-23.
                                                                      
            Equation 4-23. Cross entropy gradient vector for class k  
                     m                                                
            ∇  J Θ = 1 ∑ p i −y i x i                                 
             θ k   m i=1 k k                                          
          Now you can compute the gradient vector for every class, then use Gradient Descent
          (or any other optimization algorithm) to find the parameter matrix Θ that minimizes
          the cost function.                                          
                                                                      
          Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-
          Learn’s LogisticRegression uses one-versus-the-rest by default when you train it on
          more than two classes, but you can set the multi_class hyperparameter to ""multino
          mial"" to switch it to Softmax Regression. You must also specify a solver that supports
          Softmax Regression, such as the ""lbfgs"" solver (see Scikit-Learn’s documentation for
          more details). It also applies ℓ regularization by default, which you can control using
                           2                                          
          the hyperparameter C:                                       
            X = iris[""data""][:, (2, 3)] # petal length, petal width   
            y = iris[""target""]                                        
            softmax_reg = LogisticRegression(multi_class=""multinomial"",solver=""lbfgs"", C=10)
            softmax_reg.fit(X, y)                                     
          So the next time you find an iris with petals that are 5 cm long and 2 cm wide, you
          can ask your model to tell you what type of iris it is, and it will answer Iris virginica
          (class 2) with 94.2% probability (or Iris versicolor with 5.8% probability):"|Kullback–Leibler divergence
"                                                                      
                                                                      
                                                                      
                                                                      
                   If a categorical attribute has a large number of possible categories
                   (e.g., country code, profession, species), then one-hot encoding will
                   result in a large number of input features. This may slow down
                   training and degrade performance. If this happens, you may want
                   to replace the categorical input with useful numerical features
                   related to the categories: for example, you could replace the
                   ocean_proximity feature with the distance to the ocean (similarly,
                   a country code could be replaced with the country’s population and
                   GDP per capita). Alternatively, you could replace each category
                   with a learnable, low-dimensional vector called an embedding. Each
                   category’s representation would be learned during training. This is
                   an example of representation learning (see Chapters 13 and 17 for
                   more details).                                     
          Custom Transformers                                         
                                                                      
          Although Scikit-Learn provides many useful transformers, you will need to write
          your own for tasks such as custom cleanup operations or combining specific
          attributes. You will want your transformer to work seamlessly with Scikit-Learn func‐
          tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐
          itance), all you need to do is create a class and implement three methods: fit()
          (returning self), transform(), and fit_transform().         
          You can get the last one for free by simply adding TransformerMixin as a base class.
          If you add BaseEstimator as a base class (and avoid *args and **kargs in your con‐
          structor), you will also get two extra methods (get_params() and set_params()) that
          will be useful for automatic hyperparameter tuning.         
          For example, here is a small transformer class that adds the combined attributes we
          discussed earlier:                                          
                                                                      
            from sklearn.base import BaseEstimator, TransformerMixin  
            rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6
                                                                      
            class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
               def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs
                 self.add_bedrooms_per_room = add_bedrooms_per_room   
               def fit(self, X, y=None):                              
                 return self # nothing else to do                     
               def transform(self, X):                                
                 rooms_per_household = X[:, rooms_ix] / X[:, households_ix]
                 population_per_household = X[:, population_ix] / X[:, households_ix]
                 if self.add_bedrooms_per_room:                       
                   bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
                   return np.c_[X, rooms_per_household, population_per_household,
                           bedrooms_per_room]                         "|transformations; custom transformers; transformers and; duck typing; embedding; representation learning
"                                                                      
                                                                      
                                                                      
                                                                      
                   If you already know all the Machine Learning basics, you may want
                   to skip directly to Chapter 2. If you are not sure, try to answer all
                   the questions listed at the end of the chapter before moving on.
                                                                      
                                                                      
          What Is Machine Learning?                                   
                                                                      
          Machine Learning is the science (and art) of programming computers so they can
          learn from data.                                            
                                                                      
          Here is a slightly more general definition:                 
            [Machine Learning is the] field of study that gives computers the ability to learn
            without being explicitly programmed.                      
              —Arthur Samuel, 1959                                    
                                                                      
          And a more engineering-oriented one:                        
            A computer program is said to learn from experience E with respect to some task T
            and some performance measure P, if its performance on T, as measured by P,
            improves with experience E.                               
              —Tom Mitchell, 1997                                     
          Your spam filter is a Machine Learning program that, given examples of spam emails
          (e.g., flagged by users) and examples of regular (nonspam, also called “ham”) emails,
          can learn to flag spam. The examples that the system uses to learn are called the train‐
          ing set. Each training example is called a training instance (or sample). In this case, the
          task T is to flag spam for new emails, the experience E is the training data, and the
          performance measure P needs to be defined; for example, you can use the ratio of
          correctly classified emails. This particular performance measure is called accuracy,
          and it is often used in classification tasks.               
                                                                      
          If you just download a copy of Wikipedia, your computer has a lot more data, but it is
          not suddenly better at any task. Thus, downloading a copy of Wikipedia is not
          Machine Learning.                                           
          Why Use Machine Learning?                                   
                                                                      
          Consider how you would write a spam filter using traditional programming techni‐
          ques (Figure 1-1):                                          
                                                                      
           1. First you would consider what spam typically looks like. You might notice that
            some words or phrases (such as “4U,” “credit card,” “free,” and “amazing”) tend to
            come up a lot in the subject line. Perhaps you would also notice a few other pat‐
            terns in the sender’s name, the email’s body, and other parts of the email.
                                                                      "|training sets; Machine Learning (ML); training samples; spam filters; training instances
"                                                                      
                                                                      
                                                                      
                                                                      
          Now you can fit the imputer instance to the training data using the fit() method:
                                                                      
            imputer.fit(housing_num)                                  
          The imputer has simply computed the median of each attribute and stored the result
          in its statistics_ instance variable. Only the total_bedrooms attribute had missing
          values, but we cannot be sure that there won’t be any missing values in new data after
          the system goes live, so it is safer to apply the imputer to all the numerical attributes:
            >>> imputer.statistics_                                   
            array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])
            >>> housing_num.median().values                           
            array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])
          Now you can use this “trained” imputer to transform the training set by replacing
          missing values with the learned medians:                    
            X = imputer.transform(housing_num)                        
                                                                      
          The result is a plain NumPy array containing the transformed features. If you want to
          put it back into a pandas DataFrame, it’s simple:           
            housing_tr = pd.DataFrame(X, columns=housing_num.columns, 
                            index=housing_num.index)                  
                                                                      
                            Scikit-Learn Design                       
                                                                      
           Scikit-Learn’s API is remarkably well designed. These are the main design principles:17
           Consistency                                                
              All objects share a consistent and simple interface:    
                                                                      
              Estimators                                              
                Any object that can estimate some parameters based on a dataset is called an
                estimator (e.g., an imputer is an estimator). The estimation itself is per‐
                formed by the fit() method, and it takes only a dataset as a parameter (or
                two for supervised learning algorithms; the second dataset contains the
                labels). Any other parameter needed to guide the estimation process is con‐
                sidered a hyperparameter (such as an imputer’s strategy), and it must be
                set as an instance variable (generally via a constructor parameter).
              Transformers                                            
                Some estimators (such as an imputer) can also transform a dataset; these are
                called transformers. Once again, the API is simple: the transformation is
                performed by the transform() method with the dataset to transform as a
                                                                      
                                                                      
          17 For more details on the design principles, see Lars Buitinck et al., “API Design for Machine Learning Software:
           Experiences from the Scikit-Learn Project” ,” arXiv preprint arXiv:1309.0238 (2013)."|transformations; estimators; design principles
"                                                                      
                                                                      
                                                                      
                                                                      
          Let’s plot this model’s predictions (Figure 4-2):           
                                                                      
            plt.plot(X_new, y_predict, ""r-"")                          
            plt.plot(X, y, ""b."")                                      
            plt.axis([0, 2, 0, 15])                                   
            plt.show()                                                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-2. Linear Regression model predictions             
                                                                      
          Performing Linear Regression using Scikit-Learn is simple:2 
            >>> from sklearn.linear_model import LinearRegression     
            >>> lin_reg = LinearRegression()                          
            >>> lin_reg.fit(X, y)                                     
            >>> lin_reg.intercept_, lin_reg.coef_                     
            (array([4.21509616]), array([[2.77011339]]))              
            >>> lin_reg.predict(X_new)                                
            array([[4.21509616],                                      
                [9.75532293]])                                        
          The LinearRegression class is based on the scipy.linalg.lstsq() function (the
          name stands for “least squares”), which you could call directly:
            >>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
            >>> theta_best_svd                                        
            array([[4.21509616],                                      
                [2.77011339]])                                        
          This function computes θ =X+y, where  + is the pseudoinverse of X (specifically,
          the Moore-Penrose inverse). You can use np.linalg.pinv() to compute the
          pseudoinverse directly:                                     
                                                                      
                                                                      
          2 Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_)."|linear regression
"                                                                      
                                                                      
                                                                      
                                                                      
          And you can then use a tf.data.TFRecordDataset to read one or more TFRecord
          files:                                                      
                                                                      
            filepaths = [""my_data.tfrecord""]                          
            dataset = tf.data.TFRecordDataset(filepaths)              
            for item in dataset:                                      
               print(item)                                            
          This will output:                                           
            tf.Tensor(b'This is the first record', shape=(), dtype=string)
            tf.Tensor(b'And this is the second record', shape=(), dtype=string)
                                                                      
                   By default, a TFRecordDataset will read files one by one, but you
                   can make it read multiple files in parallel and interleave their
                   records by setting num_parallel_reads. Alternatively, you could
                   obtain the same result by using list_files() and interleave()
                   as we did earlier to read multiple CSV files.      
          Compressed TFRecord Files                                   
                                                                      
          It can sometimes be useful to compress your TFRecord files, especially if they need to
          be loaded via a network connection. You can create a compressed TFRecord file by
          setting the options argument:                               
            options = tf.io.TFRecordOptions(compression_type=""GZIP"")  
            with tf.io.TFRecordWriter(""my_compressed.tfrecord"", options) as f:
             [...]                                                    
          When reading a compressed TFRecord file, you need to specify the compression type:
            dataset = tf.data.TFRecordDataset([""my_compressed.tfrecord""],
                                compression_type=""GZIP"")              
                                                                      
          A Brief Introduction to Protocol Buffers                    
                                                                      
          Even though each record can use any binary format you want, TFRecord files usually
          contain serialized protocol buffers (also called protobufs). This is a portable, extensi‐
          ble, and efficient binary format developed at Google back in 2001 and made open
          source in 2008; protobufs are now widely used, in particular in gRPC, Google’s
          remote procedure call system. They are defined using a simple language that looks
          like this:                                                  
            syntax = ""proto3"";                                        
            message Person {                                          
             string name = 1;                                         
             int32 id = 2;                                            
             repeated string email = 3;                               
            }                                                         "|protocol buffers (protobufs); compressed TFRecord files
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                               Notations                              
                                                                      
           This equation introduces several very common Machine Learning notations that we
           will use throughout this book:                             
            • m is the number of instances in the dataset you are measuring the RMSE on.
              —For example, if you are evaluating the RMSE on a validation set of 2,000 dis‐
               tricts, then m = 2,000.                                
                                                                      
            • x(i) is a vector of all the feature values (excluding the label) of the ith instance in
              the dataset, and y(i) is its label (the desired output value for that instance).
              —For example, if the first district in the dataset is located at longitude –118.29°,
               latitude 33.91°, and it has 1,416 inhabitants with a median income of $38,372,
               and the median house value is $156,400 (ignoring the other features for now),
               then:                                                  
                      −118.29                                         
                                                                      
                       33.91                                          
                   1                                                  
                  x =                                                 
                       1,416                                          
                      38,372                                          
               and:                                                   
                   1                                                  
                  y =156,400                                          
            • X is a matrix containing all the feature values (excluding labels) of all instances in
              the dataset. There is one row per instance, and the ith row is equal to the trans‐
              pose of x(i), noted (x(i))⊺.4                           
              —For example, if the first district is as just described, then the matrix X looks
               like this:                                             
                                                                      
                       1 ⊺                                            
                      x                                               
                       2 ⊺                                            
                      x                                               
                            −118.29 33.91 1,416 38,372                
                  X=   ⋮   =                                          
                              ⋮    ⋮  ⋮   ⋮                           
                      1999 ⊺                                          
                     x                                                
                      2000 ⊺                                          
                     x                                                
          4 Recall that the transpose operator flips a column vector into a row vector (and vice versa)."|Machine Learning (ML)
"                                                                      
                                                                      
                                                                      
                                                                      
          but not through Y and Y ). Moreover, since the same parameters W and b are used
                     (0) (1)                                          
          at each time step, backpropagation will do the right thing and sum over all time steps.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 15-5. Backpropagation through time                   
                                                                      
          Fortunately, tf.keras takes care of all of this complexity for you—so let’s start coding!
          Forecasting a Time Series                                   
                                                                      
          Suppose you are studying the number of active users per hour on your website, or the
          daily temperature in your city, or your company’s financial health, measured quar‐
          terly using multiple metrics. In all these cases, the data will be a sequence of one or
          more values per time step. This is called a time series. In the first two examples there
          is a single value per time step, so these are univariate time series, while in the financial
          example there are multiple values per time step (e.g., the company’s revenue, debt,
          and so on), so it is a multivariate time series. A typical task is to predict future values,
          which is called forecasting. Another common task is to fill in the blanks: to predict (or
          rather “postdict”) missing values from the past. This is called imputation. For exam‐
          ple, Figure 15-6 shows 3 univariate time series, each of them 50 time steps long, and
          the goal here is to forecast the value at the next time step (represented by the X) for
          each of them.                                               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|multivariate time series; forecasting time series; forecasting; time series data; univariate time series; imputation
"                                                                      
                                                                      
                                                                      
                                                                      
            from sklearn.linear_model import SGDRegressor             
            sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)
            sgd_reg.fit(X, y.ravel())                                 
          Once again, you find a solution quite close to the one returned by the Normal
          Equation:                                                   
            >>> sgd_reg.intercept_, sgd_reg.coef_                     
            (array([4.24365286]), array([2.8250878]))                 
                                                                      
          Mini-batch Gradient Descent                                 
                                                                      
          The last Gradient Descent algorithm we will look at is called Mini-batch Gradient
          Descent. It is simple to understand once you know Batch and Stochastic Gradient
          Descent: at each step, instead of computing the gradients based on the full training set
          (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD
          computes the gradients on small random sets of instances called mini-batches. The
          main advantage of Mini-batch GD over Stochastic GD is that you can get a perfor‐
          mance boost from hardware optimization of matrix operations, especially when using
          GPUs.                                                       
          The algorithm’s progress in parameter space is less erratic than with Stochastic GD,
          especially with fairly large mini-batches. As a result, Mini-batch GD will end up walk‐
          ing around a bit closer to the minimum than Stochastic GD—but it may be harder for
          it to escape from local minima (in the case of problems that suffer from local minima,
          unlike Linear Regression). Figure 4-11 shows the paths taken by the three Gradient
          Descent algorithms in parameter space during training. They all end up near the
          minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic
          GD and Mini-batch GD continue to walk around. However, don’t forget that Batch
          GD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD
          would also reach the minimum if you used a good learning schedule.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-11. Gradient Descent paths in parameter space      
                                                                      "|Mini-batch Gradient Descent; mini-batches
"                                                                      
                                                                      
                                                                      
                                                                      
          offspring is a copy of its parent7 plus some random variation. The surviving policies
          plus their offspring together constitute the second generation. You can continue to
          iterate through generations this way until you find a good policy.8
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-3. Four points in policy space (left) and the agent’s corresponding behavior
          (right)                                                     
                                                                      
          Yet another approach is to use optimization techniques, by evaluating the gradients of
          the rewards with regard to the policy parameters, then tweaking these parameters by
          following the gradients toward higher rewards.9 We will discuss this approach, is
          called policy gradients (PG), in more detail later in this chapter. Going back to the
          vacuum cleaner robot, you could slightly increase p and evaluate whether doing so
          increases the amount of dust picked up by the robot in 30 minutes; if it does, then
          increase p some more, or else reduce p. We will implement a popular PG algorithm
          using TensorFlow, but before we do, we need to create an environment for the agent
          to live in—so it’s time to introduce OpenAI Gym.            
                                                                      
          Introduction to OpenAI Gym                                  
                                                                      
          One of the challenges of Reinforcement Learning is that in order to train an agent,
          you first need to have a working environment. If you want to program an agent that
                                                                      
                                                                      
          7 If there is a single parent, this is called asexual reproduction. With two (or more) parents, it is called sexual
           reproduction. An offspring’s genome (in this case a set of policy parameters) is randomly composed of parts of
           its parents’ genomes.                                      
          8 One interesting example of a genetic algorithm used for Reinforcement Learning is the NeuroEvolution of
           Augmenting Topologies (NEAT) algorithm.                    
          9 This is called Gradient Ascent. It’s just like Gradient Descent but in the opposite direction: maximizing instead
           of minimizing.                                             "|OpenAI Gym; policy gradients (PG)
"                                                                      
                                                                      
                                                                      
                                                                      
          is to create one binary attribute per category: one attribute equal to 1 when the cate‐
          gory is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the cate‐
          gory is “INLAND” (and 0 otherwise), and so on. This is called one-hot encoding,
          because only one attribute will be equal to 1 (hot), while the others will be 0 (cold).
          The new attributes are sometimes called dummy attributes. Scikit-Learn provides a
          OneHotEncoder class to convert categorical values into one-hot vectors:20
                                                                      
            >>> from sklearn.preprocessing import OneHotEncoder       
            >>> cat_encoder = OneHotEncoder()                         
            >>> housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
            >>> housing_cat_1hot                                      
            <16512x5 sparse matrix of type '<class 'numpy.float64'>'  
             with 16512 stored elements in Compressed Sparse Row format>
          Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very
          useful when you have categorical attributes with thousands of categories. After one-
          hot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s
          except for a single 1 per row. Using up tons of memory mostly to store zeros would
          be very wasteful, so instead a sparse matrix only stores the location of the nonzero
          elements. You can use it mostly like a normal 2D array,21 but if you really want to con‐
          vert it to a (dense) NumPy array, just call the toarray() method:
            >>> housing_cat_1hot.toarray()                            
            array([[1., 0., 0., 0., 0.],                              
                [1., 0., 0., 0., 0.],                                 
                [0., 0., 0., 0., 1.],                                 
                ...,                                                  
                [0., 1., 0., 0., 0.],                                 
                [1., 0., 0., 0., 0.],                                 
                [0., 0., 0., 1., 0.]])                                
          Once again, you can get the list of categories using the encoder’s categories_
          instance variable:                                          
            >>> cat_encoder.categories_                               
            [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
                dtype=object)]                                        
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          20 Before Scikit-Learn 0.20, the method could only encode integer categorical values, but since 0.20 it can also
           handle other types of inputs, including text categorical inputs.
          21 See SciPy’s documentation for more details.              "|dummy attributes; dense arrays; sparse matrix; one-hot vectors; one-hot encoding
"                                                                      
                                                                      
                                                                      
                                                                      
          The simple policy gradients algorithm we just trained solved the CartPole task, but it
          would not scale well to larger and more complex tasks. Indeed, it is highly sample
          inefficient, meaning it needs to explore the game for a very long time before it can
          make significant progress. This is due to the fact that it must run multiple episodes to
          estimate the advantage of each action, as we have seen. However, it is the foundation
          of more powerful algorithms, such as Actor-Critic algorithms (which we will discuss
          briefly at the end of this chapter).                        
                                                                      
          We will now look at another popular family of algorithms. Whereas PG algorithms
          directly try to optimize the policy to increase rewards, the algorithms we will look at
          now are less direct: the agent learns to estimate the expected return for each state, or
          for each action in each state, then it uses this knowledge to decide how to act. To
          understand these algorithms, we must first introduce Markov decision processes.
          Markov Decision Processes                                   
                                                                      
          In the early 20th century, the mathematician Andrey Markov studied stochastic pro‐
          cesses with no memory, called Markov chains. Such a process has a fixed number of
          states, and it randomly evolves from one state to another at each step. The probability
          for it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s, s
          ′), not on past states (this is why we say that the system has no memory).
                                                                      
          Figure 18-7 shows an example of a Markov chain with four states.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-7. Example of a Markov chain                      
                                                                      
          Suppose that the process starts in state s , and there is a 70% chance that it will
                                   0                                  
          remain in that state at the next step. Eventually it is bound to leave that state and
          never come back because no other state points back to s . If it goes to state s , it will
                                           0           1              
          then most likely go to state s (90% probability), then immediately back to state s
                           2                               1          "|Markov Decision Processes (MDP); policy gradients; sample inefficiency; Markov chains; policy gradients (PG); Actor-Critic algorithms
"                                                                      
                                                                      
                                                                      
                                                                      
          Baseline Metrics                                            
                                                                      
          Before we start using RNNs, it is often a good idea to have a few baseline metrics, or
          else we may end up thinking our model works great when in fact it is doing worse
          than basic models. For example, the simplest approach is to predict the last value in
          each series. This is called naive forecasting, and it is sometimes surprisingly difficult to
          outperform. In this case, it gives us a mean squared error of about 0.020:
            >>> y_pred = X_valid[:, -1]                               
            >>> np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
            0.020211367                                               
          Another simple approach is to use a fully connected network. Since it expects a flat
          list of features for each input, we need to add a Flatten layer. Let’s just use a simple
          Linear Regression model so that each prediction will be a linear combination of the
          values in the time series:                                  
            model = keras.models.Sequential([                         
               keras.layers.Flatten(input_shape=[50, 1]),             
               keras.layers.Dense(1)                                  
            ])                                                        
          If we compile this model using the MSE loss and the default Adam optimizer, then fit
          it on the training set for 20 epochs and evaluate it on the validation set, we get an
          MSE of about 0.004. That’s much better than the naive approach!
                                                                      
          Implementing a Simple RNN                                   
          Let’s see if we can beat that with a simple RNN:            
                                                                      
            model = keras.models.Sequential([                         
             keras.layers.SimpleRNN(1, input_shape=[None, 1])         
            ])                                                        
          That’s really the simplest RNN you can build. It just contains a single layer, with a sin‐
          gle neuron, as we saw in Figure 15-1. We do not need to specify the length of the
          input sequences (unlike in the previous model), since a recurrent neural network can
          process any number of time steps (this is why we set the first input dimension to
          None). By default, the SimpleRNN layer uses the hyperbolic tangent activation func‐
          tion. It works exactly as we saw earlier: the initial state h is set to 0, and it is passed
                                          (init)                      
          to a single recurrent neuron, along with the value of the first time step, x . The neu‐
                                                    (0)               
          ron computes a weighted sum of these values and applies the hyperbolic tangent acti‐
          vation function to the result, and this gives the first output, y . In a simple RNN, this
                                              0                       
          output is also the new state h . This new state is passed to the same recurrent neuron
                           0                                          
          along with the next input value, x , and the process is repeated until the last time
                              (1)                                     
          step. Then the layer just outputs the last value, y . All of this is performed simultane‐
                                      49                              
          ously for every time series.                                "|mean squared error; simple RNNs; naive forecasting; baseline metrics
"                                                                      
                                                                      
                                                                      
                                                                      
                   Separable convolutional layers use fewer parameters, less memory,
                   and fewer computations than regular convolutional layers, and in
                   general they even perform better, so you should consider using
                   them by default (except after layers with few channels).
                                                                      
          The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐
          versity of Hong Kong. They used an ensemble of many different techniques, includ‐
          ing a sophisticated object-detection system called GBD-Net,21 to achieve a top-five
          error rate below 3%. Although this result is unquestionably impressive, the complex‐
          ity of the solution contrasted with the simplicity of ResNets. Moreover, one year later
          another fairly simple architecture performed even better, as we will see now.
                                                                      
          SENet                                                       
                                                                      
          The winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-
          Excitation Network (SENet).22 This architecture extends existing architectures such as
          inception networks and ResNets, and boosts their performance. This allowed SENet
          to win the competition with an astonishing 2.25% top-five error rate! The extended
          versions of inception networks and ResNets are called SE-Inception and SE-ResNet,
          respectively. The boost comes from the fact that a SENet adds a small neural network,
          called an SE block, to every unit in the original architecture (i.e., every inception
          module or every residual unit), as shown in Figure 14-20.   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-20. SE-Inception module (left) and SE-ResNet unit (right)
                                                                      
                                                                      
                                                                      
          21 Xingyu Zeng et al., “Crafting GBD-Net for Object Detection,” IEEE Transactions on Pattern Analysis and
           Machine Intelligence 40, no. 9 (2018): 2109–2123.          
          22 Jie Hu et al., “Squeeze-and-Excitation Networks,” Proceedings of the IEEE Conference on Computer Vision and
           Pattern Recognition (2018): 7132–7141.                     "|SE-Inception; SE block; SENet (Squeeze-and-Excitation Network); SE-ResNet
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-14. Using online learning to handle huge datasets  
                                                                      
          A big challenge with online learning is that if bad data is fed to the system, the sys‐
          tem’s performance will gradually decline. If it’s a live system, your clients will notice.
          For example, bad data could come from a malfunctioning sensor on a robot, or from
          someone spamming a search engine to try to rank high in search results. To reduce
          this risk, you need to monitor your system closely and promptly switch learning off
          (and possibly revert to a previously working state) if you detect a drop in perfor‐
          mance. You may also want to monitor the input data and react to abnormal data (e.g.,
          using an anomaly detection algorithm).                      
          Instance-Based Versus Model-Based Learning                  
                                                                      
          One more way to categorize Machine Learning systems is by how they generalize.
          Most Machine Learning tasks are about making predictions. This means that given a
          number of training examples, the system needs to be able to make good predictions
          for (generalize to) examples it has never seen before. Having a good performance
          measure on the training data is good, but insufficient; the true goal is to perform well
          on new instances.                                           
          There are two main approaches to generalization: instance-based learning and
          model-based learning.                                       
                                                                      
          Instance-based learning                                     
          Possibly the most trivial form of learning is simply to learn by heart. If you were to
          create a spam filter this way, it would just flag all emails that are identical to emails
          that have already been flagged by users—not the worst solution, but certainly not the
          best.                                                       
                                                                      
                                                                      "|prediction problems; instance-based learning
"                                                                      
                                                                      
                                                                      
                                                                      
          MNIST (loaded and normalized as in Chapter 10), using the SELU activation
          function:                                                   
                                                                      
            stacked_encoder = keras.models.Sequential([               
               keras.layers.Flatten(input_shape=[28, 28]),            
               keras.layers.Dense(100, activation=""selu""),            
               keras.layers.Dense(30, activation=""selu""),             
            ])                                                        
            stacked_decoder = keras.models.Sequential([               
               keras.layers.Dense(100, activation=""selu"", input_shape=[30]),
               keras.layers.Dense(28 * 28, activation=""sigmoid""),     
               keras.layers.Reshape([28, 28])                         
            ])                                                        
            stacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])
            stacked_ae.compile(loss=""binary_crossentropy"",            
                       optimizer=keras.optimizers.SGD(lr=1.5))        
            history = stacked_ae.fit(X_train, X_train, epochs=10,     
                           validation_data=[X_valid, X_valid])        
          Let’s go through this code:                                 
           • Just like earlier, we split the autoencoder model into two submodels: the encoder
            and the decoder.                                          
           • The encoder takes 28 × 28–pixel grayscale images, flattens them so that each
            image is represented as a vector of size 784, then processes these vectors through
            two Dense layers of diminishing sizes (100 units then 30 units), both using the
            SELU activation function (you may want to add LeCun normal initialization as
            well, but the network is not very deep so it won’t make a big difference). For each
            input image, the encoder outputs a vector of size 30.     
           • The decoder takes codings of size 30 (output by the encoder) and processes them
            through two Dense layers of increasing sizes (100 units then 784 units), and it
            reshapes the final vectors into 28 × 28 arrays so the decoder’s outputs have the
            same shape as the encoder’s inputs.                       
           • When compiling the stacked autoencoder, we use the binary cross-entropy loss
            instead of the mean squared error. We are treating the reconstruction task as a
            multilabel binary classification problem: each pixel intensity represents the prob‐
            ability that the pixel should be black. Framing it this way (rather than as a regres‐
            sion problem) tends to make the model converge faster.2   
           • Finally, we train the model using X_train as both the inputs and the targets (and
            similarly, we use X_valid as both the validation inputs and targets).
                                                                      
          2 You might be tempted to use the accuracy metric, but it would not work properly, since this metric expects the
           labels to be either 0 or 1 for each pixel. You can easily work around this problem by creating a custom metric
           that computes the accuracy after rounding the targets and predictions to 0 or 1."|mean squared error
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                             Convergence Rate                         
                                                                      
           When the cost function is convex and its slope does not change abruptly (as is the
           case for the MSE cost function), Batch Gradient Descent with a fixed learning rate
           will eventually converge to the optimal solution, but you may have to wait a while: it
           can take O(1/ϵ) iterations to reach the optimum within a range of ϵ, depending on the
           shape of the cost function. If you divide the tolerance by 10 to have a more precise
           solution, then the algorithm may have to run about 10 times longer.
                                                                      
          Stochastic Gradient Descent                                 
                                                                      
          The main problem with Batch Gradient Descent is the fact that it uses the whole
          training set to compute the gradients at every step, which makes it very slow when
          the training set is large. At the opposite extreme, Stochastic Gradient Descent picks a
          random instance in the training set at every step and computes the gradients based
          only on that single instance. Obviously, working on a single instance at a time makes
          the algorithm much faster because it has very little data to manipulate at every itera‐
          tion. It also makes it possible to train on huge training sets, since only one instance
          needs to be in memory at each iteration (Stochastic GD can be implemented as an
          out-of-core algorithm; see Chapter 1).                      
          On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much
          less regular than Batch Gradient Descent: instead of gently decreasing until it reaches
          the minimum, the cost function will bounce up and down, decreasing only on aver‐
          age. Over time it will end up very close to the minimum, but once it gets there it will
          continue to bounce around, never settling down (see Figure 4-9). So once the algo‐
          rithm stops, the final parameter values are good, but not optimal.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-9. With Stochastic Gradient Descent, each training step is much faster but also
          much more stochastic than when using Batch Gradient Descent 
                                                                      
                                                                      "|Stochastic Gradient Descent (SGD); Stochastic Gradient Descent
"                                                                      
                                                                      
                                                                      
                                                                      
          Equation 15-4 summarizes how to compute the cell’s state at each time step for a sin‐
          gle instance.                                               
                                                                      
            Equation 15-4. GRU computations                           
                                                                      
                   ⊺     ⊺                                            
            z =σ W  x +W  h   +b                                      
             t    xz t  hz t−1  z                                     
                   ⊺     ⊺                                            
            r =σ W  x +W  h   +b                                      
             t    xr t  hr t−1  r                                     
            g = tanh W ⊺ x +W ⊺ r ⊗h +b                               
             t       xg t  hg t   t−1  g                              
            h =z  ⊗h   + 1−z ⊗g                                       
             t   t  t−1     t   t                                     
          Keras provides a keras.layers.GRU layer (based on the keras.layers.GRUCell
          memory cell); using it is just a matter of replacing SimpleRNN or LSTM with GRU.
          LSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet
          while they can tackle much longer sequences than simple RNNs, they still have a
          fairly limited short-term memory, and they have a hard time learning long-term pat‐
          terns in sequences of 100 time steps or more, such as audio samples, long time series,
          or long sentences. One way to solve this is to shorten the input sequences, for exam‐
          ple using 1D convolutional layers.                          
          Using 1D convolutional layers to process sequences          
          In Chapter 14, we saw that a 2D convolutional layer works by sliding several fairly
          small kernels (or filters) across an image, producing multiple 2D feature maps (one
          per kernel). Similarly, a 1D convolutional layer slides several kernels across a
          sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a
          single very short sequential pattern (no longer than the kernel size). If you use 10 ker‐
          nels, then the layer’s output will be composed of 10 1-dimensional sequences (all of
          the same length), or equivalently you can view this output as a single 10-dimensional
          sequence. This means that you can build a neural network composed of a mix of
          recurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a
          1D convolutional layer with a stride of 1 and ""same"" padding, then the output
          sequence will have the same length as the input sequence. But if you use ""valid""
          padding or a stride greater than 1, then the output sequence will be shorter than the
          input sequence, so make sure you adjust the targets accordingly. For example, the fol‐
          lowing model is the same as earlier, except it starts with a 1D convolutional layer that
          downsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is
          larger than the stride, so all inputs will be used to compute the layer’s output, and
          therefore the model can learn to preserve the useful information, dropping only the
          unimportant details. By shortening the sequences, the convolutional layer may help
          the GRU layers detect longer patterns. Note that we must also crop off the first three"|1D convolutional layer; 1D convolutional layers
"                                                                      
                                                                      
                                                                      
                                                                      
          The K-Means algorithm                                       
                                                                      
          So, how does the algorithm work? Well, suppose you were given the centroids. You
          could easily label all the instances in the dataset by assigning each of them to the clus‐
          ter whose centroid is closest. Conversely, if you were given all the instance labels, you
          could easily locate all the centroids by computing the mean of the instances for each
          cluster. But you are given neither the labels nor the centroids, so how can you pro‐
          ceed? Well, just start by placing the centroids randomly (e.g., by picking k instances at
          random and using their locations as centroids). Then label the instances, update the
          centroids, label the instances, update the centroids, and so on until the centroids stop
          moving. The algorithm is guaranteed to converge in a finite number of steps (usually
          quite small); it will not oscillate forever.2               
          You can see the algorithm in action in Figure 9-4: the centroids are initialized ran‐
          domly (top left), then the instances are labeled (top right), then the centroids are
          updated (center left), the instances are relabeled (center right), and so on. As you can
          see, in just three iterations, the algorithm has reached a clustering that seems close to
          optimal.                                                    
                                                                      
                   The computational complexity of the algorithm is generally linear
                   with regard to the number of instances m, the number of clusters k,
                   and the number of dimensions n. However, this is only true when
                   the data has a clustering structure. If it does not, then in the worst-
                   case scenario the complexity can increase exponentially with the
                   number of instances. In practice, this rarely happens, and K-Means
                   is generally one of the fastest clustering algorithms.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          2 That’s because the mean squared distance between the instances and their closest centroid can only go down
           at each step.                                              "|K-Means algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
          rather than the sum. So, the reconstruction loss is 784 times smaller than we need it
          to be. We could define a custom loss to compute the sum rather than the mean, but it
          is simpler to divide the latent loss by 784 (the final loss will be 784 times smaller than
          it should be, but this just means that we should use a larger learning rate).
                                                                      
          Note that we use the RMSprop optimizer, which works well in this case. And finally we
          can train the autoencoder!                                  
            history = variational_ae.fit(X_train, X_train, epochs=50, batch_size=128,
                             validation_data=[X_valid, X_valid])      
          Generating Fashion MNIST Images                             
                                                                      
          Now let’s use this variational autoencoder to generate images that look like fashion
          items. All we need to do is sample random codings from a Gaussian distribution and
          decode them:                                                
            codings = tf.random.normal(shape=[12, codings_size])      
            images = variational_decoder(codings).numpy()             
                                                                      
          Figure 17-13 shows the 12 generated images.                 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-13. Fashion MNIST images generated by the variational autoencoder
                                                                      
          The majority of these images look fairly convincing, if a bit too fuzzy. The rest are not
          great, but don’t be too harsh on the autoencoder—it only had a few minutes to learn!
          Give it a bit more fine-tuning and training time, and those images should look better.
                                                                      
          Variational autoencoders make it possible to perform semantic interpolation: instead
          of interpolating two images at the pixel level (which would look as if the two images
          were overlaid), we can interpolate at the codings level. We first run both images
          through the encoder, then we interpolate the two codings we get, and finally we
          decode the interpolated codings to get the final image. It will look like a regular Fash‐
          ion MNIST image, but it will be an intermediate between the original images. In the
          following code example, we take the 12 codings we just generated, we organize them"|Fashion MNIST dataset; semantic interpolation
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-19. Data parallelism with centralized parameters  
          Whereas the mirrored strategy imposes synchronous weight updates across all GPUs,
          this centralized approach allows either synchronous or asynchronous updates. Let’s
          see the pros and cons of both options.                      
                                                                      
          Synchronous updates. With synchronous updates, the aggregator waits until all gradi‐
          ents are available before it computes the average gradients and passes them to the
          optimizer, which will update the model parameters. Once a replica has finished com‐
          puting its gradients, it must wait for the parameters to be updated before it can pro‐
          ceed to the next mini-batch. The downside is that some devices may be slower than
          others, so all other devices will have to wait for them at every step. Moreover, the
          parameters will be copied to every device almost at the same time (immediately after
          the gradients are applied), which may saturate the parameter servers’ bandwidth.
                                                                      
                   To reduce the waiting time at each step, you could ignore the gradi‐
                   ents from the slowest few replicas (typically ~10%). For example,
                   you could run 20 replicas, but only aggregate the gradients from
                   the fastest 18 replicas at each step, and just ignore the gradients
                   from the last 2. As soon as the parameters are updated, the first 18
                   replicas can start working again immediately, without having to
                   wait for the 2 slowest replicas. This setup is generally described as
                   having 18 replicas plus 2 spare replicas.19        
                                                                      
          19 This name is slightly confusing because it sounds like some replicas are special, doing nothing. In reality, all
           replicas are equivalent: they all work hard to be among the fastest at each training step, and the losers vary at
           every step (unless some devices are really slower than others). However, it does mean that if a server crashes,
           training will continue just fine.                          "|spare replicas; synchronous updates
"                                                                      
                                                                      
                                                                      
                                                                      
            bag_clf = BaggingClassifier(                              
               DecisionTreeClassifier(splitter=""random"", max_leaf_nodes=16),
               n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)
          Extra-Trees                                                 
                                                                      
          When you are growing a tree in a Random Forest, at each node only a random subset
          of the features is considered for splitting (as discussed earlier). It is possible to make
          trees even more random by also using random thresholds for each feature rather than
          searching for the best possible thresholds (like regular Decision Trees do).
                                                                      
          A forest of such extremely random trees is called an Extremely Randomized Trees
          ensemble12 (or Extra-Trees for short). Once again, this technique trades more bias for
          a lower variance. It also makes Extra-Trees much faster to train than regular Random
          Forests, because finding the best possible threshold for each feature at every node is
          one of the most time-consuming tasks of growing a tree.     
          You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier
          class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra
          TreesRegressor class has the same API as the RandomForestRegressor class.
                                                                      
                   It is hard to tell in advance whether a RandomForestClassifier
                   will perform better or worse than an ExtraTreesClassifier. Gen‐
                   erally, the only way to know is to try both and compare them using
                   cross-validation (tuning the hyperparameters using grid search).
                                                                      
          Feature Importance                                          
                                                                      
          Yet another great quality of Random Forests is that they make it easy to measure the
          relative importance of each feature. Scikit-Learn measures a feature’s importance by
          looking at how much the tree nodes that use that feature reduce impurity on average
          (across all trees in the forest). More precisely, it is a weighted average, where each
          node’s weight is equal to the number of training samples that are associated with it
          (see Chapter 6).                                            
          Scikit-Learn computes this score automatically for each feature after training, then it
          scales the results so that the sum of all importances is equal to 1. You can access the
          result using the feature_importances_ variable. For example, the following code
          trains a RandomForestClassifier on the iris dataset (introduced in Chapter 4) and
          outputs each feature’s importance. It seems that the most important features are the
          petal length (44%) and width (42%), while sepal length and width are rather unim‐
          portant in comparison (11% and 2%, respectively):           
                                                                      
                                                                      
          12 Pierre Geurts et al., “Extremely Randomized Trees,” Machine Learning 63, no. 1 (2006): 3–42."|Extra-Trees; feature importance; feature importance scoring; Extremely Randomized Trees ensemble; ExtraTreesClassifier class; Extra-Trees classifier
"                                                                      
                                                                      
                                                                      
                                                                      
          verb, another linear layer will extract just the fact that it is past tense, and so on. Then
          the Scaled Dot-Product Attention layers implement the lookup phase, and finally we
          concatenate all the results and project them back to the original space.
                                                                      
          At the time of this writing, there is no Transformer class or MultiHeadAttention
          class available for TensorFlow 2. However, you can check out TensorFlow’s great tuto‐
          rial for building a Transformer model for language understanding. Moreover, the TF
          Hub team is currently porting several Transformer-based modules to TensorFlow 2,
          and they should be available soon. In the meantime, I hope I have demonstrated that
          it is not that hard to implement a Transformer yourself, and it is certainly a great
          exercise!                                                   
          Recent Innovations in Language Models                       
                                                                      
          The year 2018 has been called the “ImageNet moment for NLP”: progress was
          astounding, with larger and larger LSTM and Transformer-based architectures
          trained on immense datasets. I highly recommend you check out the following
          papers, all published in 2018:                              
                                                                      
           • The ELMo paper24 by Matthew Peters introduced Embeddings from Language
            Models (ELMo): these are contextualized word embeddings learned from the
            internal states of a deep bidirectional language model. For example, the word
            “queen” will not have the same embedding in “Queen of the United Kingdom”
            and in “queen bee.”                                       
           • The ULMFiT paper25 by Jeremy Howard and Sebastian Ruder demonstrated the
            effectiveness of unsupervised pretraining for NLP tasks: the authors trained an
            LSTM language model using self-supervised learning (i.e., generating the labels
            automatically from the data) on a huge text corpus, then they fine-tuned it on
            various tasks. Their model outperformed the state of the art on six text classifica‐
            tion tasks by a large margin (reducing the error rate by 18–24% in most cases).
            Moreover, they showed that by fine-tuning the pretrained model on just 100
            labeled examples, they could achieve the same performance as a model trained
            from scratch on 10,000 examples.                          
           • The GPT paper26 by Alec Radford and other OpenAI researchers also demon‐
            strated the effectiveness of unsupervised pretraining, but this time using a
                                                                      
                                                                      
          24 Matthew Peters et al., “Deep Contextualized Word Representations,” Proceedings of the 2018 Conference of the
           North American Chapter of the Association for Computational Linguistics: Human Language Technologies 1
           (2018): 2227–2237.                                         
          25 Jeremy Howard and Sebastian Ruder, “Universal Language Model Fine-Tuning for Text Classification,” Pro‐
           ceedings of the 56th Annual Meeting of the Association for Computational Linguistics 1 (2018): 328–339.
          26 Alec Radford et al., “Improving Language Understanding by Generative Pre-Training” (2018)."|natural language processing (NLP); attention mechanisms; language models; neural machine translation (NMT)
"                                                                      
                                                                      
                                                                      
                                                                      
          The first step is to get your hands on a GPU. There are two options for this: you can
          either purchase your own GPU(s), or you can use GPU-equipped virtual machines
          on the cloud. Let’s start with the first option.            
                                                                      
          Getting Your Own GPU                                        
                                                                      
          If you choose to purchase a GPU card, then take some time to make the right choice.
          Tim Dettmers wrote an excellent blog post to help you choose, and he updates it reg‐
          ularly: I encourage you to read it carefully. At the time of this writing, TensorFlow
          only supports Nvidia cards with CUDA Compute Capability 3.5+ (as well as Google’s
          TPUs, of course), but it may extend its support to other manufacturers. Moreover,
          although TPUs are currently only available on GCP, it is highly likely that TPU-like
          cards will be available for sale in the near future, and TensorFlow may support them.
          In short, make sure to check TensorFlow’s documentation to see what devices are
          supported at this point.                                    
          If you go for an Nvidia GPU card, you will need to install the appropriate Nvidia
          drivers and several Nvidia libraries.10 These include the Compute Unified Device
          Architecture library (CUDA), which allows developers to use CUDA-enabled GPUs
          for all sorts of computations (not just graphics acceleration), and the CUDA Deep
          Neural Network library (cuDNN), a GPU-accelerated library of primitives for DNNs.
          cuDNN provides optimized implementations of common DNN computations such
          as activation layers, normalization, forward and backward convolutions, and pooling
          (see Chapter 14). It is part of Nvidia’s Deep Learning SDK (note that you’ll need to
          create an Nvidia developer account in order to download it). TensorFlow uses CUDA
          and cuDNN to control the GPU cards and accelerate computations (see
          Figure 19-10).                                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-10. TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs
                                                                      
                                                                      
                                                                      
          10 Please check the docs for detailed and up-to-date installation instructions, as they change quite often."|selecting; CUDA Deep Neural Network library (cuDNN); Nvidia GPU cards; Compute Unified Device Architecture library (CUDA)
"                                                                      
                                                                      
                                                                      
                                                                      
          There is nothing special about the implementation: just train an autoencoder using
          all the training data (labeled plus unlabeled), then reuse its encoder layers to create a
          new neural network (see the exercises at the end of this chapter for an example).
                                                                      
          Next, let’s look at a few techniques for training stacked autoencoders.
          Tying Weights                                               
                                                                      
          When an autoencoder is neatly symmetrical, like the one we just built, a common
          technique is to tie the weights of the decoder layers to the weights of the encoder lay‐
          ers. This halves the number of weights in the model, speeding up training and limit‐
          ing the risk of overfitting. Specifically, if the autoencoder has a total of N layers (not
          counting the input layer), and W represents the connection weights of the Lth layer
                              L                                       
          (e.g., layer 1 is the first hidden layer, layer N/2 is the coding layer, and layer N is the
          output layer), then the decoder layer weights can be defined simply as: W = W ⊺
                                                     N–L+1 L          
          (with L = 1, 2, …, N/2).                                    
          To tie weights between layers using Keras, let’s define a custom layer:
            class DenseTranspose(keras.layers.Layer):                 
               def __init__(self, dense, activation=None, **kwargs):  
                 self.dense = dense                                   
                 self.activation = keras.activations.get(activation)  
                 super().__init__(**kwargs)                           
               def build(self, batch_input_shape):                    
                 self.biases = self.add_weight(name=""bias"", initializer=""zeros"",
                                   shape=[self.dense.input_shape[-1]])
                 super().build(batch_input_shape)                     
               def call(self, inputs):                                
                 z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)
                 return self.activation(z + self.biases)              
          This custom layer acts like a regular Dense layer, but it uses another Dense layer’s
          weights, transposed (setting transpose_b=True is equivalent to transposing the sec‐
          ond argument, but it’s more efficient as it performs the transposition on the fly within
          the matmul() operation). However, it uses its own bias vector. Next, we can build a
          new stacked autoencoder, much like the previous one, but with the decoder’s Dense
          layers tied to the encoder’s Dense layers:                  
            dense_1 = keras.layers.Dense(100, activation=""selu"")      
            dense_2 = keras.layers.Dense(30, activation=""selu"")       
            tied_encoder = keras.models.Sequential([                  
               keras.layers.Flatten(input_shape=[28, 28]),            
               dense_1,                                               
               dense_2                                                
            ])                                                        "|tying weights
"                                                                      
                                                                      
                                                                      
                                                                      
          As you can see, this code matches Figure 14-18 pretty closely. In the constructor, we
          create all the layers we will need: the main layers are the ones on the right side of the
          diagram, and the skip layers are the ones on the left (only needed if the stride is
          greater than 1). Then in the call() method, we make the inputs go through the main
          layers and the skip layers (if any), then we add both outputs and apply the activation
          function.                                                   
                                                                      
          Next, we can build the ResNet-34 using a Sequential model, since it’s really just a
          long sequence of layers (we can treat each residual unit as a single layer now that we
          have the ResidualUnit class):                               
            model = keras.models.Sequential()                         
            model.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3],
                              padding=""same"", use_bias=False))        
            model.add(keras.layers.BatchNormalization())              
            model.add(keras.layers.Activation(""relu""))                
            model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=""same""))
            prev_filters = 64                                         
            for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:
               strides = 1 if filters == prev_filters else 2          
               model.add(ResidualUnit(filters, strides=strides))      
               prev_filters = filters                                 
            model.add(keras.layers.GlobalAvgPool2D())                 
            model.add(keras.layers.Flatten())                         
            model.add(keras.layers.Dense(10, activation=""softmax""))   
          The only slightly tricky part in this code is the loop that adds the ResidualUnit layers
          to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs
          have 128 filters, and so on. We then set the stride to 1 when the number of filters is
          the same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit,
          and finally we update prev_filters.                         
          It is amazing that in fewer than 40 lines of code, we can build the model that won the
          ILSVRC 2015 challenge! This demonstrates both the elegance of the ResNet model
          and the expressiveness of the Keras API. Implementing the other CNN architectures
          is not much harder. However, Keras comes with several of these architectures built in,
          so why not use them instead?                                
          Using Pretrained Models from Keras                          
          In general, you won’t have to implement standard models like GoogLeNet or ResNet
          manually, since pretrained networks are readily available with a single line of code in
          the keras.applications package. For example, you can load the ResNet-50 model,
          pretrained on ImageNet, with the following line of code:    
            model = keras.applications.resnet50.ResNet50(weights=""imagenet"")
                                                                      
                                                                      "|models from Keras; pretrained models from Keras; using pretrained models from
"                                                                      
                                                                      
                                                                      
                                                                      
                 else:                                                
                   return np.c_[X, rooms_per_household, population_per_household]
                                                                      
            attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
            housing_extra_attribs = attr_adder.transform(housing.values)
          In this example the transformer has one hyperparameter, add_bedrooms_per_room,
          set to True by default (it is often helpful to provide sensible defaults). This hyperpara‐
          meter will allow you to easily find out whether adding this attribute helps the
          Machine Learning algorithms or not. More generally, you can add a hyperparameter
          to gate any data preparation step that you are not 100% sure about. The more you
          automate these data preparation steps, the more combinations you can automatically
          try out, making it much more likely that you will find a great combination (and sav‐
          ing you a lot of time).                                     
          Feature Scaling                                             
                                                                      
          One of the most important transformations you need to apply to your data is feature
          scaling. With few exceptions, Machine Learning algorithms don’t perform well when
          the input numerical attributes have very different scales. This is the case for the hous‐
          ing data: the total number of rooms ranges from about 6 to 39,320, while the median
          incomes only range from 0 to 15. Note that scaling the target values is generally not
          required.                                                   
          There are two common ways to get all attributes to have the same scale: min-max
          scaling and standardization.                                
                                                                      
          Min-max scaling (many people call this normalization) is the simplest: values are shif‐
          ted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting
          the min value and dividing by the max minus the min. Scikit-Learn provides a trans‐
          former called MinMaxScaler for this. It has a feature_range hyperparameter that lets
          you change the range if, for some reason, you don’t want 0–1.
          Standardization is different: first it subtracts the mean value (so standardized values
          always have a zero mean), and then it divides by the standard deviation so that the
          resulting distribution has unit variance. Unlike min-max scaling, standardization
          does not bound values to a specific range, which may be a problem for some algo‐
          rithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐
          ever, standardization is much less affected by outliers. For example, suppose a district
          had a median income equal to 100 (by mistake). Min-max scaling would then crush
          all the other values from 0–15 down to 0–0.15, whereas standardization would not be
          much affected. Scikit-Learn provides a transformer called StandardScaler for
          standardization.                                            
                                                                      
                                                                      
                                                                      "|feature scaling; standardization; min-max scaling; normalization
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-11. The famous Breakout game                      
                                                                      
          Installing TF-Agents                                        
                                                                      
          Let’s start by installing TF-Agents. This can be done using pip (as always, if you are
          using a virtual environment, make sure to activate it first; if not, you will need to use
          the --user option, or have administrator rights):           
            $ python3 -m pip install -U tf-agents                     
                                                                      
                   At the time of this writing, TF-Agents is still quite new and
                   improving every day, so the API may change a bit by the time you
                   read this—but the big picture should remain the same, as well as
                   most of the code. If anything breaks, I will update the Jupyter note‐
                   book accordingly, so make sure to check it out.    
                                                                      
          Next, let’s create a TF-Agents environment that will just wrap OpenAI GGym’s Break‐
          out environment. For this, you must first install OpenAI Gym’s Atari dependencies:
            $ python3 -m pip install -U 'gym[atari]'                  
          Among other libraries, this command will install atari-py, which is a Python inter‐
          face for the Arcade Learning Environment (ALE), a framework built on top of the
          Atari 2600 emulator Stella.                                 
                                                                      
          TF-Agents Environments                                      
                                                                      
          If everything went well, you should be able to import TF-Agents and create a Break‐
          out environment:                                            
            >>> from tf_agents.environments import suite_gym          
            >>> env = suite_gym.load(""Breakout-v4"")                   
            >>> env                                                   
            <tf_agents.environments.wrappers.TimeLimit at 0x10c523c18>
                                                                      "|installing; environments
"                                                                      
                                                                      
                                                                      
                                                                      
                parameter. It returns the transformed dataset. This transformation generally
                relies on the learned parameters, as is the case for an imputer. All transform‐
                ers also have a convenience method called fit_transform() that is equiva‐
                lent to calling fit() and then transform() (but sometimes
                fit_transform() is optimized and runs much faster).   
                                                                      
              Predictors                                              
                Finally, some estimators, given a dataset, are capable of making predictions;
                they are called predictors. For example, the LinearRegression model in the
                previous chapter was a predictor: given a country’s GDP per capita, it pre‐
                dicted life satisfaction. A predictor has a predict() method that takes a
                dataset of new instances and returns a dataset of corresponding predictions.
                It also has a score() method that measures the quality of the predictions,
                given a test set (and the corresponding labels, in the case of supervised learn‐
                ing algorithms).18                                    
           Inspection                                                 
              All the estimator’s hyperparameters are accessible directly via public instance
              variables (e.g., imputer.strategy), and all the estimator’s learned parameters are
              accessible via public instance variables with an underscore suffix (e.g.,
              imputer.statistics_).                                   
           Nonproliferation of classes                                
              Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of
              homemade classes. Hyperparameters are just regular Python strings or numbers.
           Composition                                                
              Existing building blocks are reused as much as possible. For example, it is easy to
              create a Pipeline estimator from an arbitrary sequence of transformers followed
              by a final estimator, as we will see.                   
           Sensible defaults                                          
              Scikit-Learn provides reasonable default values for most parameters, making it
              easy to quickly create a baseline working system.       
                                                                      
                                                                      
          Handling Text and Categorical Attributes                    
                                                                      
          So far we have only dealt with numerical attributes, but now let’s look at text
          attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look
          at its value for the first 10 instances:                    
                                                                      
                                                                      
                                                                      
                                                                      
          18 Some predictors also provide methods to measure the confidence of their predictions."|handling text and categorical attributes; predictors
"                                                                      
                                                                      
                                                                      
                                                                      
          example, if you want to analyze satellite images to measure how much total forest
          area there is in a region, color segmentation may be just fine.
                                                                      
          First, use Matplotlib’s imread() function to load the image (see the upper-left image
          in Figure 9-12):                                            
            >>> from matplotlib.image import imread # or `from imageio import imread`
            >>> image = imread(os.path.join(""images"",""unsupervised_learning"",""ladybug.png""))
            >>> image.shape                                           
            (533, 800, 3)                                             
          The image is represented as a 3D array. The first dimension’s size is the height; the
          second is the width; and the third is the number of color channels, in this case red,
          green, and blue (RGB). In other words, for each pixel there is a 3D vector containing
          the intensities of red, green, and blue, each between 0.0 and 1.0 (or between 0 and
          255, if you use imageio.imread()). Some images may have fewer channels, such as
          grayscale images (one channel). And some images may have more channels, such as
          images with an additional alpha channel for transparency or satellite images, which
          often contain channels for many light frequencies (e.g., infrared). The following code
          reshapes the array to get a long list of RGB colors, then it clusters these colors using
          K-Means:                                                    
            X = image.reshape(-1, 3)                                  
            kmeans = KMeans(n_clusters=8).fit(X)                      
            segmented_img = kmeans.cluster_centers_[kmeans.labels_]   
            segmented_img = segmented_img.reshape(image.shape)        
          For example, it may identify a color cluster for all shades of green. Next, for each
          color (e.g., dark green), it looks for the mean color of the pixel’s color cluster. For
          example, all shades of green may be replaced with the same light green color (assum‐
          ing the mean color of the green cluster is light green). Finally, it reshapes this long list
          of colors to get the same shape as the original image. And we’re done!
          This outputs the image shown in the upper right of Figure 9-12. You can experiment
          with various numbers of clusters, as shown in the figure. When you use fewer than
          eight clusters, notice that the ladybug’s flashy red color fails to get a cluster of its own:
          it gets merged with colors from the environment. This is because K-Means prefers
          clusters of similar sizes. The ladybug is small—much smaller than the rest of the
          image—so even though its color is flashy, K-Means fails to dedicate a cluster to it.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|alpha channels
"                                                                      
                                                                      
                                                                      
                                                                      
          will likely be close to 1, and there will be few clusters. Finally, the Wishart distribution
          is used to sample covariance matrices: the parameters d and V control the distribu‐
          tion of cluster shapes.                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-22. Bayesian Gaussian mixture model                
                                                                      
          Prior knowledge about the latent variables z can be encoded in a probability distribu‐
          tion p(z) called the prior. For example, we may have a prior belief that the clusters are
          likely to be few (low concentration), or conversely, that they are likely to be plentiful
          (high concentration). This prior belief about the number of clusters can be adjusted
          using the weight_concentration_prior hyperparameter. Setting it to 0.01 or 10,000
          gives very different clusterings (see Figure 9-23). The more data we have, however,
          the less the priors matter. In fact, to plot diagrams with such large differences, you
          must use very strong priors and little data.                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-23. Using different concentration priors on the same data results in different
          numbers of clusters                                         
                                                                      
                                                                      "|p (prior) distribution
"                                                                      
                                                                      
                                                                      
                                                                      
          strategy, create a MirroredStrategy object, call its scope() method to get a distribu‐
          tion context, and wrap the creation and compilation of your model inside that con‐
          text. Then call the model’s fit() method normally:          
                                                                      
            distribution = tf.distribute.MirroredStrategy()           
            with distribution.scope():                                
               mirrored_model = keras.models.Sequential([...])        
               mirrored_model.compile([...])                          
            batch_size = 100 # must be divisible by the number of replicas
            history = mirrored_model.fit(X_train, y_train, epochs=10) 
                                                                      
          Under the hood, tf.keras is distribution-aware, so in this MirroredStrategy context it
          knows that it must replicate all variables and operations across all available GPU
          devices. Note that the fit() method will automatically split each training batch
          across all the replicas, so it’s important that the batch size be divisible by the number
          of replicas. And that’s all! Training will generally be significantly faster than using a
          single device, and the code change was really minimal.      
          Once you have finished training your model, you can use it to make predictions effi‐
          ciently: call the predict() method, and it will automatically split the batch across all
          replicas, making predictions in parallel (again, the batch size must be divisible by the
          number of replicas). If you call the model’s save() method, it will be saved as a regu‐
          lar model, not as a mirrored model with multiple replicas. So when you load it, it will
          run like a regular model, on a single device (by default GPU 0, or the CPU if there are
          no GPUs). If you want to load a model and run it on all available devices, you must
          call keras.models.load_model() within a distribution context:
            with distribution.scope():                                
               mirrored_model = keras.models.load_model(""my_mnist_model.h5"")
          If you only want to use a subset of all the available GPU devices, you can pass the list
          to the MirroredStrategy’s constructor:                      
                                                                      
            distribution = tf.distribute.MirroredStrategy([""/gpu:0"", ""/gpu:1""])
          By default, the MirroredStrategy class uses the NVIDIA Collective Communications
          Library (NCCL) for the AllReduce mean operation, but you can change it by setting
          the cross_device_ops argument to an instance of the tf.distribute.Hierarchical
          CopyAllReduce class, or an instance of the tf.distribute.ReductionToOneDevice
          class. The default NCCL option is based on the tf.distribute.NcclAllReduce class,
          which is usually faster, but this depends on the number and types of GPUs, so you
          may want to give the alternatives a try.21                  
                                                                      
                                                                      
          21 For more details on AllReduce algorithms, read this great post by Yuichiro Ueno, and this page on scaling
           with NCCL.                                                 "|NVIDIA Collective Communications Library (NCCL)
"                                                                      
                                                                      
                                                                      
                                                                      
           In short, the PDF is a function of x (with θ fixed), while the likelihood function is a
           function of θ (with x fixed). It is important to understand that the likelihood function
           is not a probability distribution: if you integrate a probability distribution over all
           possible values of x, you always get 1; but if you integrate the likelihood function over
           all possible values of θ, the result can be any positive value.
           Given a dataset X, a common task is to try to estimate the most likely values for the
           model parameters. To do this, you must find the values that maximize the likelihood
           function, given X. In this example, if you have observed a single instance x=2.5, the
           maximum likelihood estimate (MLE) of θ is θ=1.5. If a prior probability distribution g
           over θ exists, it is possible to take it into account by maximizing ℒ(θ|x)g(θ) rather
           than just maximizing ℒ(θ|x). This is called maximum a-posteriori (MAP) estimation.
           Since MAP constrains the parameter values, you can think of it as a regularized ver‐
           sion of MLE.                                               
           Notice that maximizing the likelihood function is equivalent to maximizing its loga‐
           rithm (represented in the lower-righthand plot in Figure 9-20). Indeed the logarithm
           is a strictly increasing function, so if θ maximizes the log likelihood, it also maximizes
           the likelihood. It turns out that it is generally easier to maximize the log likelihood.
           For example, if you observed several independent instances x(1) to x(m), you would
           need to find the value of θ that maximizes the product of the individual likelihood
           functions. But it is equivalent, and much simpler, to maximize the sum (not the prod‐
           uct) of the log likelihood functions, thanks to the magic of the logarithm which con‐
           verts products into sums: log(ab)=log(a)+log(b).           
           Once you have estimated θ, the value of θ that maximizes the likelihood function,
           then you are ready to compute L =ℒ θ,  , which is the value used to compute the
           AIC and BIC; you can think of it as a measure of how well the model fits the data.
                                                                      
                                                                      
          To compute the BIC and AIC, call the bic() and aic() methods:
            >>> gm.bic(X)                                             
            8189.74345832983                                          
            >>> gm.aic(X)                                             
            8102.518178214792                                         
          Figure 9-21 shows the BIC for different numbers of clusters k. As you can see, both
          the BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note
          that we could also search for the best value for the covariance_type hyperparameter.
          For example, if it is ""spherical"" rather than ""full"", then the model has significantly
          fewer parameters to learn, but it does not fit the data as well.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|maximum likelihood estimate (MLE); maximum a-posteriori (MAP) estimation
"                                                                      
                                                                      
                                                                      
                                                                      
          If the dataset does not fit in memory, the simplest option is to use the memmap class, as
          we did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch
          at a time to the partial_fit() method, but this will require much more work, since
          you will need to perform multiple initializations and select the best one yourself (see
          the mini-batch K-Means section of the notebook for an example).
                                                                      
          Although the Mini-batch K-Means algorithm is much faster than the regular K-
          Means algorithm, its inertia is generally slightly worse, especially as the number of
          clusters increases. You can see this in Figure 9-6: the plot on the left compares the
          inertias of Mini-batch K-Means and regular K-Means models trained on the previous
          dataset using various numbers of clusters k. The difference between the two curves
          remains fairly constant, but this difference becomes more and more significant as k
          increases, since the inertia becomes smaller and smaller. In the plot on the right, you
          can see that Mini-batch K-Means is much faster than regular K-Means, and this dif‐
          ference increases with k.                                   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-6. Mini-batch K-Means has a higher inertia than K-Means (left) but it is much
          faster (right), especially as k increases                   
                                                                      
          Finding the optimal number of clusters                      
          So far, we have set the number of clusters k to 5 because it was obvious by looking at
          the data that this was the correct number of clusters. But in general, it will not be so
          easy to know how to set k, and the result might be quite bad if you set it to the wrong
          value. As you can see in Figure 9-7, setting k to 3 or 8 results in fairly bad models.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|optimal cluster number
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> from sklearn.ensemble import RandomForestRegressor    
            >>> forest_reg = RandomForestRegressor()                  
            >>> forest_reg.fit(housing_prepared, housing_labels)      
            >>> [...]                                                 
            >>> forest_rmse                                           
            18603.515021376355                                        
            >>> display_scores(forest_rmse_scores)                    
            Scores: [49519.80364233 47461.9115823 50029.02762854 52325.28068953
             49308.39426421 53446.37892622 48634.8036574 47585.73832311
             53490.10699751 50021.5852922 ]                           
            Mean: 50182.303100336096                                  
            Standard deviation: 2097.0810550985693                    
          Wow, this is much better: Random Forests look very promising. However, note that
          the score on the training set is still much lower than on the validation sets, meaning
          that the model is still overfitting the training set. Possible solutions for overfitting are
          to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.
          Before you dive much deeper into Random Forests, however, you should try out
          many other models from various categories of Machine Learning algorithms (e.g.,
          several Support Vector Machines with different kernels, and possibly a neural net‐
          work), without spending too much time tweaking the hyperparameters. The goal is to
          shortlist a few (two to five) promising models.             
                   You should save every model you experiment with so that you can
                   come back easily to any model you want. Make sure you save both
                   the hyperparameters and the trained parameters, as well as the
                   cross-validation scores and perhaps the actual predictions as well.
                   This will allow you to easily compare scores across model types,
                   and compare the types of errors they make. You can easily save
                   Scikit-Learn models by using Python’s pickle module or by using
                   the joblib library, which is more efficient at serializing large
                   NumPy arrays (you can install this library using pip):
                     import joblib                                    
                     joblib.dump(my_model, ""my_model.pkl"")            
                     # and later...                                   
                     my_model_loaded = joblib.load(""my_model.pkl"")    
          Fine-Tune Your Model                                        
          Let’s assume that you now have a shortlist of promising models. You now need to
          fine-tune them. Let’s look at a few ways you can do that.   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|fine-tuning; serializing large arrays; hyperparameter tuning; saving models; model fine-tuning
"                                                                      
                                                                      
                                                                      
                                                                      
           • This will make it possible for you to easily try various transformations and see
            which combination of transformations works best.          
                                                                      
          But first let’s revert to a clean training set (by copying strat_train_set once again).
          Let’s also separate the predictors and the labels, since we don’t necessarily want to
          apply the same transformations to the predictors and the target values (note that
          drop() creates a copy of the data and does not affect strat_train_set):
            housing = strat_train_set.drop(""median_house_value"", axis=1)
            housing_labels = strat_train_set[""median_house_value""].copy()
                                                                      
          Data Cleaning                                               
                                                                      
          Most Machine Learning algorithms cannot work with missing features, so let’s create
          a few functions to take care of them. We saw earlier that the total_bedrooms
          attribute has some missing values, so let’s fix this. You have three options:
           1. Get rid of the corresponding districts.                 
                                                                      
           2. Get rid of the whole attribute.                         
           3. Set the values to some value (zero, the mean, the median, etc.).
                                                                      
          You can accomplish these easily using DataFrame’s dropna(), drop(), and fillna()
          methods:                                                    
            housing.dropna(subset=[""total_bedrooms""]) # option 1      
            housing.drop(""total_bedrooms"", axis=1) # option 2         
            median = housing[""total_bedrooms""].median() # option 3    
            housing[""total_bedrooms""].fillna(median, inplace=True)    
          If you choose option 3, you should compute the median value on the training set and
          use it to fill the missing values in the training set. Don’t forget to save the median
          value that you have computed. You will need it later to replace missing values in the
          test set when you want to evaluate your system, and also once the system goes live to
          replace missing values in new data.                         
          Scikit-Learn provides a handy class to take care of missing values: SimpleImputer.
          Here is how to use it. First, you need to create a SimpleImputer instance, specifying
          that you want to replace each attribute’s missing values with the median of that
          attribute:                                                  
            from sklearn.impute import SimpleImputer                  
                                                                      
            imputer = SimpleImputer(strategy=""median"")                
          Since the median can only be computed on numerical attributes, you need to create a
          copy of the data without the text attribute ocean_proximity:
                                                                      
            housing_num = housing.drop(""ocean_proximity"", axis=1)     "|data cleaning; missing value handling
"                                                                      
                                                                      
                                                                      
                                                                      
          much as immediate rewards. Typical discount factors vary from 0.9 to 0.99. With a
          discount factor of 0.95, rewards 13 steps into the future count roughly for half as
          much as immediate rewards (since 0.9513 ≈ 0.5), while with a discount factor of 0.99,
          rewards 69 steps into the future count for half as much as immediate rewards. In the
          CartPole environment, actions have fairly short-term effects, so choosing a discount
          factor of 0.95 seems reasonable.                            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-6. Computing an action’s return: the sum of discounted future rewards
                                                                      
          Of course, a good action may be followed by several bad actions that cause the pole to
          fall quickly, resulting in the good action getting a low return (similarly, a good actor
          may sometimes star in a terrible movie). However, if we play the game enough times,
          on average good actions will get a higher return than bad ones. We want to estimate
          how much better or worse an action is, compared to the other possible actions, on
          average. This is called the action advantage. For this, we must run many episodes and
          normalize all the action returns (by subtracting the mean and dividing by the stan‐
          dard deviation). After that, we can reasonably assume that actions with a negative
          advantage were bad while actions with a positive advantage were good. Perfect—now
          that we have a way to evaluate each action, we are ready to train our first agent using
          policy gradients. Let’s see how.                            
                                                                      
          Policy Gradients                                            
                                                                      
          As discussed earlier, PG algorithms optimize the parameters of a policy by following
          the gradients toward higher rewards. One popular class of PG algorithms, called
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|action advantage; policy gradients (PG); REINFORCE algorithms; policy gradients
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                            Data Augmentation                         
                                                                      
           Data augmentation artificially increases the size of the training set by generating
           many realistic variants of each training instance. This reduces overfitting, making this
           a regularization technique. The generated instances should be as realistic as possible:
           ideally, given an image from the augmented training set, a human should not be able
           to tell whether it was augmented or not. Simply adding white noise will not help; the
           modifications should be learnable (white noise is not).    
           For example, you can slightly shift, rotate, and resize every picture in the training set
           by various amounts and add the resulting pictures to the training set (see
           Figure 14-12). This forces the model to be more tolerant to variations in the position,
           orientation, and size of the objects in the pictures. For a model that’s more tolerant of
           different lighting conditions, you can similarly generate many images with various
           contrasts. In general, you can also flip the pictures horizontally (except for text, and
           other asymmetrical objects). By combining these transformations, you can greatly
           increase the size of your training set.                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
           Figure 14-12. Generating new training instances from existing ones
                                                                      
                                                                      
          AlexNet also uses a competitive normalization step immediately after the ReLU step
          of layers C1 and C3, called local response normalization (LRN): the most strongly acti‐
          vated neurons inhibit other neurons located at the same position in neighboring fea‐
          ture maps (such competitive activation has been observed in biological neurons).
          This encourages different feature maps to specialize, pushing them apart and forcing"|local response normalization
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
           • It outputs five bounding boxes for each grid cell (instead of just one), and each
            bounding box comes with an objectness score. It also outputs 20 class probabili‐
            ties per grid cell, as it was trained on the PASCAL VOC dataset, which contains
            20 classes. That’s a total of 45 numbers per grid cell: 5 bounding boxes, each with
            4 coordinates, plus 5 objectness scores, plus 20 class probabilities.
           • Instead of predicting the absolute coordinates of the bounding box centers,
            YOLOv3 predicts an offset relative to the coordinates of the grid cell, where (0, 0)
            means the top left of that cell and (1, 1) means the bottom right. For each grid
            cell, YOLOv3 is trained to predict only bounding boxes whose center lies in that
            cell (but the bounding box itself generally extends well beyond the grid cell).
            YOLOv3 applies the logistic activation function to the bounding box coordinates
            to ensure they remain in the 0 to 1 range.                
           • Before training the neural net, YOLOv3 finds five representative bounding box
            dimensions, called anchor boxes (or bounding box priors). It does this by applying
            the K-Means algorithm (see Chapter 9) to the height and width of the training set
            bounding boxes. For example, if the training images contain many pedestrians,
            then one of the anchor boxes will likely have the dimensions of a typical pedes‐
            trian. Then when the neural net predicts five bounding boxes per grid cell, it
            actually predicts how much to rescale each of the anchor boxes. For example,
            suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network
            predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for
            one of the grid cells). This will result in a predicted bounding box of size 150 × 45
            pixels. To be more precise, for each grid cell and each anchor box, the network
            predicts the log of the vertical and horizontal rescaling factors. Having these pri‐
            ors makes the network more likely to predict bounding boxes of the appropriate
            dimensions, and it also speeds up training because it will more quickly learn what
            reasonable bounding boxes look like.                      
           • The network is trained using images of different scales: every few batches during
            training, the network randomly chooses a new image dimension (from 330 × 330
            to 608 × 608 pixels). This allows the network to learn to detect objects at different
            scales. Moreover, it makes it possible to use YOLOv3 at different scales: the
            smaller scale will be less accurate but faster than the larger scale, so you can
            choose the right trade-off for your use case.             
          There are a few more innovations you might be interested in, such as the use of skip
          connections to recover some of the spatial resolution that is lost in the CNN (we will
          discuss this shortly, when we look at semantic segmentation). In the 2016 paper, the
          authors introduce the YOLO9000 model that uses hierarchical classification: the
          model predicts a probability for each node in a visual hierarchy called WordTree. This
          makes it possible for the network to predict with high confidence that an image rep‐
          resents, say, a dog, even though it is unsure what specific type of dog. I encourage you
                                                                      "|WordTrees; bounding box priors; anchor boxes
"                                                                      
                                                                      
                                                                      
                                                                      
          Nonrepresentative Training Data                             
                                                                      
          In order to generalize well, it is crucial that your training data be representative of the
          new cases you want to generalize to. This is true whether you use instance-based
          learning or model-based learning.                           
          For example, the set of countries we used earlier for training the linear model was not
          perfectly representative; a few countries were missing. Figure 1-21 shows what the
          data looks like when you add the missing countries.         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 1-21. A more representative training sample          
                                                                      
          If you train a linear model on this data, you get the solid line, while the old model is
          represented by the dotted line. As you can see, not only does adding a few missing
          countries significantly alter the model, but it makes it clear that such a simple linear
          model is probably never going to work well. It seems that very rich countries are not
          happier than moderately rich countries (in fact, they seem unhappier), and con‐
          versely some poor countries seem happier than many rich countries.
          By using a nonrepresentative training set, we trained a model that is unlikely to make
          accurate predictions, especially for very poor and very rich countries.
                                                                      
          It is crucial to use a training set that is representative of the cases you want to general‐
          ize to. This is often harder than it sounds: if the sample is too small, you will have
          sampling noise (i.e., nonrepresentative data as a result of chance), but even very large
          samples can be nonrepresentative if the sampling method is flawed. This is called
          sampling bias.                                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|nonrepresentative; sampling bias; sampling noise
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-1. The chess memory experiment (left) and a simple autoencoder (right)
                                                                      
          As you can see, an autoencoder typically has the same architecture as a Multi-Layer
          Perceptron (MLP; see Chapter 10), except that the number of neurons in the output
          layer must be equal to the number of inputs. In this example, there is just one hidden
          layer composed of two neurons (the encoder), and one output layer composed of
          three neurons (the decoder). The outputs are often called the reconstructions because
          the autoencoder tries to reconstruct the inputs, and the cost function contains a
          reconstruction loss that penalizes the model when the reconstructions are different
          from the inputs.                                            
          Because the internal representation has a lower dimensionality than the input data (it
          is 2D instead of 3D), the autoencoder is said to be undercomplete. An undercomplete
          autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to
          output a copy of its inputs. It is forced to learn the most important features in the
          input data (and drop the unimportant ones).                 
                                                                      
          Let’s see how to implement a very simple undercomplete autoencoder for dimension‐
          ality reduction.                                            
          Performing PCA with an Undercomplete Linear                 
                                                                      
          Autoencoder                                                 
                                                                      
          If the autoencoder uses only linear activations and the cost function is the mean
          squared error (MSE), then it ends up performing Principal Component Analysis
          (PCA; see Chapter 8).                                       
          The following code builds a simple linear autoencoder to perform PCA on a 3D data‐
          set, projecting it to 2D:                                   
                                                                      
                                                                      "|mean squared error; reconstruction loss; PCA with undercomplete linear autoencoders; undercomplete; linear autoencoders; undercomplete linear autoencoders; reconstructions; undercomplete autoencoders
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                   CHAPTER 9          
                                                                      
                    Unsupervised    Learning   Techniques             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Although most of the applications of Machine Learning today are based on super‐
          vised learning (and as a result, this is where most of the investments go to), the vast
          majority of the available data is unlabeled: we have the input features X, but we do
          not have the labels y. The computer scientist Yann LeCun famously said that “if intel‐
          ligence was a cake, unsupervised learning would be the cake, supervised learning
          would be the icing on the cake, and reinforcement learning would be the cherry on
          the cake.” In other words, there is a huge potential in unsupervised learning that we
          have only barely started to sink our teeth into.            
          Say you want to create a system that will take a few pictures of each item on a manu‐
          facturing production line and detect which items are defective. You can fairly easily
          create a system that will take pictures automatically, and this might give you thou‐
          sands of pictures every day. You can then build a reasonably large dataset in just a few
          weeks. But wait, there are no labels! If you want to train a regular binary classifier that
          will predict whether an item is defective or not, you will need to label every single
          picture as “defective” or “normal.” This will generally require human experts to sit
          down and manually go through all the pictures. This is a long, costly, and tedious
          task, so it will usually only be done on a small subset of the available pictures. As a
          result, the labeled dataset will be quite small, and the classifier’s performance will be
          disappointing. Moreover, every time the company makes any change to its products,
          the whole process will need to be started over from scratch. Wouldn’t it be great if the
          algorithm could just exploit the unlabeled data without needing humans to label
          every picture? Enter unsupervised learning.                 
          In Chapter 8 we looked at the most common unsupervised learning task: dimension‐
          ality reduction. In this chapter we will look at a few more unsupervised learning tasks
          and algorithms:                                             
                                                                      
                                                                      "|unsupervised learning
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 16          
                                                                      
                    Natural   Language    Processing  with            
                                                                      
                                     RNNs   and  Attention            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          When Alan Turing imagined his famous Turing test1 in 1950, his objective was to
          evaluate a machine’s ability to match human intelligence. He could have tested for
          many things, such as the ability to recognize cats in pictures, play chess, compose
          music, or escape a maze, but, interestingly, he chose a linguistic task. More specifi‐
          cally, he devised a chatbot capable of fooling its interlocutor into thinking it was
          human.2 This test does have its weaknesses: a set of hardcoded rules can fool unsus‐
          pecting or naive humans (e.g., the machine could give vague predefined answers in
          response to some keywords; it could pretend that it is joking or drunk, to get a pass
          on its weirdest answers; or it could escape difficult questions by answering them with
          its own questions), and many aspects of human intelligence are utterly ignored (e.g.,
          the ability to interpret nonverbal communication such as facial expressions, or to
          learn a manual task). But the test does highlight the fact that mastering language is
          arguably Homo sapiens’s greatest cognitive ability. Can we build a machine that can
          read and write natural language?                            
          A common approach for natural language tasks is to use recurrent neural networks.
          We will therefore continue to explore RNNs (introduced in Chapter 15), starting with
          a character RNN, trained to predict the next character in a sentence. This will allow us
          to generate some original text, and in the process we will see how to build a Tensor‐
          Flow Dataset on a very long sequence. We will first use a stateless RNN (which learns
                                                                      
          1 Alan Turing, “Computing Machinery and Intelligence,” Mind 49 (1950): 433–460.
          2 Of course, the word chatbot came much later. Turing called his test the imitation game: machine A and human
           B chat with human interrogator C via text messages; the interrogator asks questions to figure out which one is
           the machine (A or B). The machine passes the test if it can fool the interrogator, while the human B must try
           to help the interrogator.                                  "|natural language processing (NLP); Turing test; stateless and stateful; chatbots
"                                                                      
                                                                      
                                                                      
                                                                      
          In practice, there are different techniques to maximize the ELBO. In mean field varia‐
          tional inference, it is necessary to pick the family of distributions q(z; λ) and the prior
          p(z) very carefully to ensure that the equation for the ELBO simplifies to a form that
          can be computed. Unfortunately, there is no general way to do this. Picking the right
          family of distributions and the right prior depends on the task and requires some
          mathematical skills. For example, the distributions and lower-bound equations used
          in Scikit-Learn’s BayesianGaussianMixture class are presented in the documenta‐
          tion. From these equations it is possible to derive update equations for the cluster
          parameters and assignment variables: these are then used very much like in the
          Expectation-Maximization algorithm. In fact, the computational complexity of the
          BayesianGaussianMixture class is similar to that of the GaussianMixture class (but
          generally significantly slower). A simpler approach to maximizing the ELBO is called
          black box stochastic variational inference (BBSVI): at each iteration, a few samples are
          drawn from q, and they are used to estimate the gradients of the ELBO with regard to
          the variational parameters λ, which are then used in a gradient ascent step. This
          approach makes it possible to use Bayesian inference with any kind of model (pro‐
          vided it is differentiable), even deep neural networks; using Bayesian inference with
          deep neural networks is called Bayesian Deep Learning.      
                                                                      
                   If you want to dive deeper into Bayesian statistics, check out the
                   book Bayesian Data Analysis by Andrew Gelman et al. (Chapman
                   & Hall).                                           
                                                                      
                                                                      
          Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try
          to fit a dataset with different shapes, you may have bad surprises. For example, let’s
          see what happens if we use a Bayesian Gaussian mixture model to cluster the moons
          dataset (see Figure 9-24).                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-24. Fitting a Gaussian mixture to nonellipsoidal clusters
                                                                      
                                                                      
                                                                      "|black box stochastic variational inference (BBSVI); mean field variational inference
"                                                                      
                                                                      
                                                                      
                                                                      
           7. Now you need to configure AI Platform (formerly known as ML Engine) so that
            it knows which models and versions you want to use. In the navigation menu,
            scroll down to the Artificial Intelligence section, and click AI Platform → Models.
            Click Activate API (it takes a few minutes), then click “Create model.” Fill in the
            model details (see Figure 19-5) and click Create.         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Figure 19-5. Creating a new model on Google Cloud AI Platform
                                                                      
           8. Now that you have a model on AI Platform, you need to create a model version.
            In the list of models, click the model you just created, then click “Create version”
            and fill in the version details (see Figure 19-6): set the name, description, Python
            version (3.5 or above), framework (TensorFlow), framework version (2.0 if avail‐
            able, or 1.13),6 ML runtime version (2.0, if available or 1.13), machine type
            (choose “Single core CPU” for now), model path on GCS (this is the full path to
            the actual version folder, e.g., gs://my-mnist-model-bucket/my_mnist_model/
            0002/), scaling (choose automatic), and minimum number of TF Serving con‐
            tainers to have running at all times (leave this field empty). Then click Save.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          6 At the time of this writing, TensorFlow version 2 is not available yet on AI Platform, but that’s OK: you can
           use 1.13, and it will run your TF 2 SavedModels just fine. "|ML Engine; AI Platform
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-4. Feeding the previous output word as input at inference time
                                                                      
          OK, now you have the big picture. Still, there are a few more details to handle if you
          implement this model:                                       
                                                                      
           • So far we have assumed that all input sequences (to the encoder and to the
            decoder) have a constant length. But obviously sentence lengths vary. Since regu‐
            lar tensors have fixed shapes, they can only contain sentences of the same length.
            You can use masking to handle this, as discussed earlier. However, if the senten‐
            ces have very different lengths, you can’t just crop them like we did for sentiment
            analysis (because we want full translations, not cropped translations). Instead,
            group sentences into buckets of similar lengths (e.g., a bucket for the 1- to 6-
            word sentences, another for the 7- to 12-word sentences, and so on), using pad‐
            ding for the shorter sequences to ensure all sentences in a bucket have the same
            length (check out the tf.data.experimental.bucket_by_sequence_length()
            function for this). For example, “I drink milk” becomes “<pad> <pad> <pad>
            milk drink I.”                                            
           • We want to ignore any output past the EOS token, so these tokens should not
            contribute to the loss (they must be masked out). For example, if the model out‐
            puts “Je bois du lait <eos> oui,” the loss for the last word should be ignored.
           • When the output vocabulary is large (which is the case here), outputting a proba‐
            bility for each and every possible word would be terribly slow. If the target
            vocabulary contains, say, 50,000 French words, then the decoder would output
            50,000-dimensional vectors, and then computing the softmax function over such
            a large vector would be very computationally intensive. To avoid this, one solu‐
            tion is to look only at the logits output by the model for the correct word and for
            a random sample of incorrect words, then compute an approximation of the loss
            based only on these logits. This sampled softmax technique was introduced in
                                                                      
                                                                      
                                                                      "|sampled softmax technique
"                                                                      
                                                                      
                                                                      
                                                                      
          Here is the code to build this neural network policy using tf.keras:
                                                                      
            import tensorflow as tf                                   
            from tensorflow import keras                              
            n_inputs = 4 # == env.observation_space.shape[0]          
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.Dense(5, activation=""elu"", input_shape=[n_inputs]),
               keras.layers.Dense(1, activation=""sigmoid""),           
            ])                                                        
          After the imports, we use a simple Sequential model to define the policy network.
          The number of inputs is the size of the observation space (which in the case of Cart‐
          Pole is 4), and we have just five hidden units because it’s a simple problem. Finally, we
          want to output a single probability (the probability of going left), so we have a single
          output neuron using the sigmoid activation function. If there were more than two
          possible actions, there would be one output neuron per action, and we would use the
          softmax activation function instead.                        
          OK, we now have a neural network policy that will take observations and output
          action probabilities. But how do we train it?               
          Evaluating Actions: The Credit Assignment Problem           
                                                                      
          If we knew what the best action was at each step, we could train the neural network as
          usual, by minimizing the cross entropy between the estimated probability distribu‐
          tion and the target probability distribution. It would just be regular supervised learn‐
          ing. However, in Reinforcement Learning the only guidance the agent gets is through
          rewards, and rewards are typically sparse and delayed. For example, if the agent man‐
          ages to balance the pole for 100 steps, how can it know which of the 100 actions it
          took were good, and which of them were bad? All it knows is that the pole fell after
          the last action, but surely this last action is not entirely responsible. This is called the
          credit assignment problem: when the agent gets a reward, it is hard for it to know
          which actions should get credited (or blamed) for it. Think of a dog that gets rewar‐
          ded hours after it behaved well; will it understand what it is being rewarded for?
          To tackle this problem, a common strategy is to evaluate an action based on the sum
          of all the rewards that come after it, usually applying a discount factor γ (gamma) at
          each step. This sum of discounted rewards is called the action’s return. Consider the
          example in Figure 18-6). If an agent decides to go right three times in a row and gets
          +10 reward after the first step, 0 after the second step, and finally –50 after the third
          step, then assuming we use a discount factor γ = 0.8, the first action will have a return
          of 10 + γ × 0 + γ2 × (–50) = –22. If the discount factor is close to 0, then future
          rewards won’t count for much compared to immediate rewards. Conversely, if the
          discount factor is close to 1, then rewards far into the future will count almost as
                                                                      "|credit assignment problem; evaluating actions; discount factors; evaluating
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-28. Skip layers recover some spatial resolution from lower layers
                                                                      
          Once again, many GitHub repositories provide TensorFlow implementations of
          semantic segmentation (TensorFlow 1 for now), and you will even find pretrained
          instance segmentation models in the TensorFlow Models project. Instance segmenta‐
          tion is similar to semantic segmentation, but instead of merging all objects of the
          same class into one big lump, each object is distinguished from the others (e.g., it
          identifies each individual bicycle). At present, the instance segmentation models
          available in the TensorFlow Models project are based on the Mask R-CNN architec‐
          ture, which was proposed in a 2017 paper:34 it extends the Faster R-CNN model by
          additionally producing a pixel mask for each bounding box. So not only do you get a
          bounding box around each object, with a set of estimated class probabilities, but you
          also get a pixel mask that locates pixels in the bounding box that belong to the object.
          As you can see, the field of Deep Computer Vision is vast and moving fast, with all
          sorts of architectures popping out every year, all based on convolutional neural net‐
          works. The progress made in just a few years has been astounding, and researchers
          are now focusing on harder and harder problems, such as adversarial learning (which
          attempts to make the network more resistant to images designed to fool it), explaina‐
          bility (understanding why the network makes a specific classification), realistic image
          generation (which we will come back to in Chapter 17), and single-shot learning (a sys‐
          tem that can recognize an object after it has seen it just once). Some even explore
          completely novel architectures, such as Geoffrey Hinton’s capsule networks35 (I pre‐
          sented them in a couple of videos, with the corresponding code in a notebook). Now
          on to the next chapter, where we will look at how to process sequential data such as
          time series using recurrent neural networks and convolutional neural networks.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          34 Kaiming He et al., “Mask R-CNN,” arXiv preprint arXiv:1703.06870 (2017).
          35 Geoffrey Hinton et al., “Matrix Capsules with EM Routing,” Proceedings of the International Conference on
           Learning Representations (2018).                           "|instance segmentation; adversarial learning; Mask R-CNN; image generation; single-shot learning
"                                                                      
                                                                      
                                                                      
                                                                      
            Next sentence prediction (NSP)                            
               The model is trained to predict whether two sentences are consecutive or
               not. For example, it should predict that “The dog sleeps” and “It snores
               loudly” are consecutive sentences, while “The dog sleeps” and “The Earth
               orbits the Sun” are not consecutive. This is a challenging task, and it signifi‐
               cantly improves the performance of the model when it is fine-tuned on tasks
               such as question answering or entailment.              
                                                                      
          As you can see, the main innovations in 2018 and 2019 have been better subword
          tokenization, shifting from LSTMs to Transformers, and pretraining universal lan‐
          guage models using self-supervised learning, then fine-tuning them with very few
          architectural changes (or none at all). Things are moving fast; no one can say what
          architectures will prevail next year. Today, it’s clearly Transformers, but tomorrow it
          might be CNNs (e.g., check out the 2018 paper30 by Maha Elbayad et al., where the
          researchers use masked 2D convolutional layers for sequence-to-sequence tasks). Or
          it might even be RNNs, if they make a surprise comeback (e.g., check out the 2018
          paper31 by Shuai Li et al. that shows that by making neurons independent of each
          other in a given RNN layer, it is possible to train much deeper RNNs capable of learn‐
          ing much longer sequences).                                 
          In the next chapter we will discuss how to learn deep representations in an unsuper‐
          vised way using autoencoders, and we will use generative adversarial networks
          (GANs) to produce images and more!                          
                                                                      
          Exercises                                                   
                                                                      
           1. What are the pros and cons of using a stateful RNN versus a stateless RNN?
                                                                      
           2. Why do people use Encoder–Decoder RNNs rather than plain sequence-to-
            sequence RNNs for automatic translation?                  
           3. How can you deal with variable-length input sequences? What about variable-
            length output sequences?                                  
           4. What is beam search and why would you use it? What tool can you use to imple‐
            ment it?                                                  
           5. What is an attention mechanism? How does it help?       
                                                                      
                                                                      
                                                                      
                                                                      
          30 Maha Elbayad et al., “Pervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Pre‐
           diction,” arXiv preprint arXiv:1808.03867 (2018).          
          31 Shuai Li et al., “Independently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN,”
           Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018): 5457–5466."|next sentence prediction (NSP)
"                                                                      
                                                                      
                                                                      
                                                                      
          components, preserving a large part of the dataset’s variance. As a result, the 2D pro‐
          jection looks very much like the original 3D dataset.       
                                                                      
          To project the training set onto the hyperplane and obtain a reduced dataset X of
                                                        d-proj        
          dimensionality d, compute the matrix multiplication of the training set matrix X by
          the matrix W , defined as the matrix containing the first d columns of V, as shown in
                  d                                                   
          Equation 8-2.                                               
            Equation 8-2. Projecting the training set down to d dimensions
            X   =XW                                                   
             d‐proj d                                                 
          The following Python code projects the training set onto the plane defined by the first
          two principal components:                                   
            W2 = Vt.T[:, :2]                                          
            X2D = X_centered.dot(W2)                                  
          There you have it! You now know how to reduce the dimensionality of any dataset
          down to any number of dimensions, while preserving as much variance as possible.
          Using Scikit-Learn                                          
                                                                      
          Scikit-Learn’s PCA class uses SVD decomposition to implement PCA, just like we did
          earlier in this chapter. The following code applies PCA to reduce the dimensionality
          of the dataset down to two dimensions (note that it automatically takes care of center‐
          ing the data):                                              
            from sklearn.decomposition import PCA                     
                                                                      
            pca = PCA(n_components = 2)                               
            X2D = pca.fit_transform(X)                                
          After fitting the PCA transformer to the dataset, its components_ attribute holds the
          transpose of W (e.g., the unit vector that defines the first principal component is
                   d                                                  
          equal to pca.components_.T[:, 0]).                          
          Explained Variance Ratio                                    
                                                                      
          Another useful piece of information is the explained variance ratio of each principal
          component, available via the explained_variance_ratio_ variable. The ratio indi‐
          cates the proportion of the dataset’s variance that lies along each principal compo‐
          nent. For example, let’s look at the explained variance ratios of the first two
          components of the 3D dataset represented in Figure 8-2:     
            >>> pca.explained_variance_ratio_                         
            array([0.84248607, 0.14631839])                           
                                                                      "|using Scikit-Learn; reducing dimensionality; PCAng; explained variance ratio
"                                                                      
                                                                      
                                                                      
                                                                      
          As you can see in Figure 7-4, predictors can all be trained in parallel, via different
          CPU cores or even different servers. Similarly, predictions can be made in parallel.
          This is one of the reasons bagging and pasting are such popular methods: they scale
          very well.                                                  
                                                                      
          Bagging and Pasting in Scikit-Learn                         
                                                                      
          Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas
          sifier class (or BaggingRegressor for regression). The following code trains an
          ensemble of 500 Decision Tree classifiers:5 each is trained on 100 training instances
          randomly sampled from the training set with replacement (this is an example of bag‐
          ging, but if you want to use pasting instead, just set bootstrap=False). The n_jobs
          parameter tells Scikit-Learn the number of CPU cores to use for training and predic‐
          tions (–1 tells Scikit-Learn to use all available cores):   
            from sklearn.ensemble import BaggingClassifier            
            from sklearn.tree import DecisionTreeClassifier           
            bag_clf = BaggingClassifier(                              
               DecisionTreeClassifier(), n_estimators=500,            
               max_samples=100, bootstrap=True, n_jobs=-1)            
            bag_clf.fit(X_train, y_train)                             
            y_pred = bag_clf.predict(X_test)                          
                                                                      
                   The BaggingClassifier automatically performs soft voting
                   instead of hard voting if the base classifier can estimate class proba‐
                   bilities (i.e., if it has a predict_proba() method), which is the case
                   with Decision Tree classifiers.                    
                                                                      
          Figure 7-5 compares the decision boundary of a single Decision Tree with the deci‐
          sion boundary of a bagging ensemble of 500 trees (from the preceding code), both
          trained on the moons dataset. As you can see, the ensemble’s predictions will likely
          generalize much better than the single Decision Tree’s predictions: the ensemble has a
          comparable bias but a smaller variance (it makes roughly the same number of errors
          on the training set, but the decision boundary is less irregular).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          5 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances
           to sample is equal to the size of the training set times max_samples."|in Scikit-Learn; bagging and pasting
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-8. Explained variance as a function of the number of dimensions
                                                                      
          PCA for Compression                                         
                                                                      
          After dimensionality reduction, the training set takes up much less space. As an
          example, try applying PCA to the MNIST dataset while preserving 95% of its var‐
          iance. You should find that each instance will have just over 150 features, instead of
          the original 784 features. So, while most of the variance is preserved, the dataset is
          now less than 20% of its original size! This is a reasonable compression ratio, and you
          can see how this size reduction can speed up a classification algorithm (such as an
          SVM classifier) tremendously.                               
          It is also possible to decompress the reduced dataset back to 784 dimensions by
          applying the inverse transformation of the PCA projection. This won’t give you back
          the original data, since the projection lost a bit of information (within the 5% var‐
          iance that was dropped), but it will likely be close to the original data. The mean
          squared distance between the original data and the reconstructed data (compressed
          and then decompressed) is called the reconstruction error.  
          The following code compresses the MNIST dataset down to 154 dimensions, then
          uses the inverse_transform() method to decompress it back to 784 dimensions:
            pca = PCA(n_components = 154)                             
            X_reduced = pca.fit_transform(X_train)                    
            X_recovered = pca.inverse_transform(X_reduced)            
          Figure 8-9 shows a few digits from the original training set (on the left), and the cor‐
          responding digits after compression and decompression. You can see that there is a
          slight image quality loss, but the digits are still mostly intact.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|decompression; reconstruction error; compression; decompressing; for compression; compressing
"                                                                      
                                                                      
                                                                      
                                                                      
          Deep Q-Learning Variants                                    
                                                                      
          Let’s look at a few variants of the Deep Q-Learning algorithm that can stabilize and
          speed up training.                                          
                                                                      
          Fixed Q-Value Targets                                       
                                                                      
          In the basic Deep Q-Learning algorithm, the model is used both to make predictions
          and to set its own targets. This can lead to a situation analogous to a dog chasing its
          own tail. This feedback loop can make the network unstable: it can diverge, oscillate,
          freeze, and so on. To solve this problem, in their 2013 paper the DeepMind research‐
          ers used two DQNs instead of one: the first is the online model, which learns at each
          step and is used to move the agent around, and the other is the target model used only
          to define the targets. The target model is just a clone of the online model:
            target = keras.models.clone_model(model)                  
            target.set_weights(model.get_weights())                   
          Then, in the training_step() function, we just need to change one line to use the
          target model instead of the online model when computing the Q-Values of the next
          states:                                                     
            next_Q_values = target.predict(next_states)               
                                                                      
          Finally, in the training loop, we must copy the weights of the online model to the tar‐
          get model, at regular intervals (e.g., every 50 episodes):  
            if episode % 50 == 0:                                     
               target.set_weights(model.get_weights())                
          Since the target model is updated much less often than the online model, the Q-Value
          targets are more stable, the feedback loop we discussed earlier is dampened, and its
          effects are less severe. This approach was one of the DeepMind researchers’ main
          contributions in their 2013 paper, allowing agents to learn to play Atari games from
          raw pixels. To stabilize training, they used a tiny learning rate of 0.00025, they upda‐
          ted the target model only every 10,000 steps (instead of the 50 in the previous code
          example), and they used a very large replay buffer of 1 million experiences. They
          decreased epsilon very slowly, from 1 to 0.1 in 1 million steps, and they let the algo‐
          rithm run for 50 million steps.                             
          Later in this chapter, we will use the TF-Agents library to train a DQN agent to play
          Breakout using these hyperparameters, but before we get there, let’s take a look at
          another DQN variant that managed to beat the state of the art once more.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|fixed Q-Value targets; online model; target model; Deep Q-Learning
"                                                                      
                                                                      
                                                                      
                                                                      
                   With Stochastic and Mini-batch Gradient Descent, the curves are
                   not so smooth, and it may be hard to know whether you have
                   reached the minimum or not. One solution is to stop only after the
                   validation error has been above the minimum for some time (when
                   you are confident that the model will not do any better), then roll
                   back the model parameters to the point where the validation error
                   was at a minimum.                                  
                                                                      
          Here is a basic implementation of early stopping:           
            from sklearn.base import clone                            
                                                                      
            # prepare the data                                        
            poly_scaler = Pipeline([                                  
                 (""poly_features"", PolynomialFeatures(degree=90, include_bias=False)),
                 (""std_scaler"", StandardScaler())                     
               ])                                                     
            X_train_poly_scaled = poly_scaler.fit_transform(X_train)  
            X_val_poly_scaled = poly_scaler.transform(X_val)          
            sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
                          penalty=None, learning_rate=""constant"", eta0=0.0005)
            minimum_val_error = float(""inf"")                          
            best_epoch = None                                         
            best_model = None                                         
            for epoch in range(1000):                                 
               sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off
               y_val_predict = sgd_reg.predict(X_val_poly_scaled)     
               val_error = mean_squared_error(y_val, y_val_predict)   
               if val_error < minimum_val_error:                      
                 minimum_val_error = val_error                        
                 best_epoch = epoch                                   
                 best_model = clone(sgd_reg)                          
          Note that with warm_start=True, when the fit() method is called it continues train‐
          ing where it left off, instead of restarting from scratch.  
          Logistic Regression                                         
          As we discussed in Chapter 1, some regression algorithms can be used for classifica‐
          tion (and vice versa). Logistic Regression (also called Logit Regression) is commonly
          used to estimate the probability that an instance belongs to a particular class (e.g.,
          what is the probability that this email is spam?). If the estimated probability is greater
          than 50%, then the model predicts that the instance belongs to that class (called the
          positive class, labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to
          the negative class, labeled “0”). This makes it a binary classifier.
                                                                      "|Logistic Regression; regularized linear models
"                                                                      
                                                                      
                                                                      
                                                                      
           • The driver then passes the action to the environment, which returns the next
            time step.                                                
                                                                      
           • Finally, the driver creates a trajectory object to represent this transition and
            broadcasts it to all the observers.                       
          Some policies, such as RNN policies, are stateful: they choose an action based on both
          the given time step and their own internal state. Stateful policies return their own
          state in the action step, along with the chosen action. The driver will then pass this
          state back to the policy at the next time step. Moreover, the driver saves the policy
          state to the trajectory (in the policy_info field), so it ends up in the replay buffer.
          This is essential when training a stateful policy: when the agent samples a trajectory, it
          must set the policy’s state to the state it was in at the time of the sampled time step.
                                                                      
          Also, as discussed earlier, the environment may be a batched environment, in which
          case the driver passes a batched time step to the policy (i.e., a time step object contain‐
          ing a batch of observations, a batch of step types, a batch of rewards, and a batch of
          discounts, all four batches of the same size). The driver also passes a batch of previous
          policy states. The policy then returns a batched action step containing a batch of
          actions and a batch of policy states. Finally, the driver creates a batched trajectory (i.e.,
          a trajectory containing a batch of step types, a batch of observations, a batch of
          actions, a batch of rewards, and more generally a batch for each trajectory attribute,
          with all batches of the same size).                         
          There are two main driver classes: DynamicStepDriver and DynamicEpisodeDriver.
          The first one collects experiences for a given number of steps, while the second col‐
          lects experiences for a given number of episodes. We want to collect experiences for
          four steps for each training iteration (as was done in the 2015 DQN paper), so let’s
          create a DynamicStepDriver:                                 
            from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver
                                                                      
            collect_driver = DynamicStepDriver(                       
               tf_env,                                                
               agent.collect_policy,                                  
               observers=[replay_buffer_observer] + training_metrics, 
               num_steps=update_period) # collect 4 steps for each training iteration
          We give it the environment to play with, the agent’s collect policy, a list of observers
          (including the replay buffer observer and the training metrics), and finally the num‐
          ber of steps to run (in this case, four). We could now run it by calling its run()
          method, but it’s best to warm up the replay buffer with experiences collected using a
          purely random policy. For this, we can use the RandomTFPolicy class and create a sec‐
          ond driver that will run this policy for 20,000 steps (which is equivalent to 80,000
          simulator frames, as was done in the 2015 DQN paper). We can use our ShowPro
          gress observer to display the progress:                     "|batched time step; batched trajectory; batched action step
"                                                                      
                                                                      
                                                                      
                                                                      
          That wasn’t too hard! Let’s look at the resulting theta:    
                                                                      
            >>> theta                                                 
            array([[4.21509616],                                      
                [2.77011339]])                                        
          Hey, that’s exactly what the Normal Equation found! Gradient Descent worked per‐
          fectly. But what if you had used a different learning rate eta? Figure 4-8 shows the
          first 10 steps of Gradient Descent using three different learning rates (the dashed line
          represents the starting point).                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-8. Gradient Descent with various learning rates    
                                                                      
          On the left, the learning rate is too low: the algorithm will eventually reach the solu‐
          tion, but it will take a long time. In the middle, the learning rate looks pretty good: in
          just a few iterations, it has already converged to the solution. On the right, the learn‐
          ing rate is too high: the algorithm diverges, jumping all over the place and actually
          getting further and further away from the solution at every step.
          To find a good learning rate, you can use grid search (see Chapter 2). However, you
          may want to limit the number of iterations so that grid search can eliminate models
          that take too long to converge.                             
                                                                      
          You may wonder how to set the number of iterations. If it is too low, you will still be
          far away from the optimal solution when the algorithm stops; but if it is too high, you
          will waste time while the model parameters do not change anymore. A simple solu‐
          tion is to set a very large number of iterations but to interrupt the algorithm when the
          gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny
          number ϵ (called the tolerance)—because this happens when Gradient Descent has
          (almost) reached the minimum.                               
                                                                      
                                                                      
                                                                      
                                                                      "|tolerance
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-16. Images generated by the GAN after one epoch of training
          The Difficulties of Training GANs                           
                                                                      
          During training, the generator and the discriminator constantly try to outsmart each
          other, in a zero-sum game. As training advances, the game may end up in a state that
          game theorists call a Nash equilibrium, named after the mathematician John Nash:
          this is when no player would be better off changing their own strategy, assuming the
          other players do not change theirs. For example, a Nash equilibrium is reached when
          everyone drives on the left side of the road: no driver would be better off being the
          only one to switch sides. Of course, there is a second possible Nash equilibrium:
          when everyone drives on the right side of the road. Different initial states and dynam‐
          ics may lead to one equilibrium or the other. In this example, there is a single optimal
          strategy once an equilibrium is reached (i.e., driving on the same side as everyone
          else), but a Nash equilibrium can involve multiple competing strategies (e.g., a preda‐
          tor chases its prey, the prey tries to escape, and neither would be better off changing
          their strategy).                                            
          So how does this apply to GANs? Well, the authors of the paper demonstrated that a
          GAN can only reach a single Nash equilibrium: that’s when the generator produces
          perfectly realistic images, and the discriminator is forced to guess (50% real, 50%
          fake). This fact is very encouraging: it would seem that you just need to train the
          GAN for long enough, and it will eventually reach this equilibrium, giving you a per‐
          fect generator. Unfortunately, it’s not that simple: nothing guarantees that the equili‐
          brium will ever be reached.                                 
                                                                      
                                                                      
                                                                      
                                                                      "|Nash equilibrium; difficulties of training
"                                                                      
                                                                      
                                                                      
                                                                      
           Similarly, you can compute Madrid – Spain + France, and the result is close to Paris,
           which seems to show that the notion of capital city was also encoded in the
           embeddings.                                                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
           Figure 13-5. Word embeddings of similar words tend to be close, and some axes seem to
           encode meaningful concepts                                 
           Unfortunately, word embeddings sometimes capture our worst biases. For example,
           although they correctly learn that Man is to King as Woman is to Queen, they also
           seem to learn that Man is to Doctor as Woman is to Nurse: quite a sexist bias! To be
           fair, this particular example is probably exaggerated, as was pointed out in a 2019
           paper10 by Malvina Nissim et al. Nevertheless, ensuring fairness in Deep Learning
           algorithms is an important and active research topic.      
                                                                      
                                                                      
          Let’s look at how we could implement embeddings manually, to understand how they
          work (then we will use a simple Keras layer instead). First, we need to create an
          embedding matrix containing each category’s embedding, initialized randomly; it will
          have one row per category and per oov bucket, and one column per embedding
          dimension:                                                  
            embedding_dim = 2                                         
            embed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])
            embedding_matrix = tf.Variable(embed_init)                
                                                                      
                                                                      
                                                                      
                                                                      
          10 Malvina Nissim et al., “Fair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor,” arXiv pre‐
           print arXiv:1905.09866 (2019).                             "|embedding matrix
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Equation 5-8. Second-degree polynomial mapping            
                                                                      
                        2                                             
                       x                                              
                        1                                             
                  x                                                   
                  1                                                   
            ϕ x =ϕ  =  2x x                                           
                        1 2                                           
                  x                                                   
                  2                                                   
                        2                                             
                       x                                              
                        2                                             
          Notice that the transformed vector is 3D instead of 2D. Now let’s look at what hap‐
          pens to a couple of 2D vectors, a and b, if we apply this second-degree polynomial
          mapping and then compute the dot product7 of the transformed vectors (See Equa‐
          tion 5-9).                                                  
            Equation 5-9. Kernel trick for a second-degree polynomial mapping
                        2 ⊺   2                                       
                       a    b                                         
                       1     1                                        
            ϕ a ⊺ ϕ b = 2a a 2b b =a 2 b 2 +2a b a b +a 2 b 2         
                        1 2   1 2 1 1   1 1 2 2 2 2                   
                        2     2                                       
                       a    b                                         
                       2     2                                        
                               a ⊺ b 2                                
                            2   1  1   ⊺ 2                            
                    = a b +a b =     = a b                            
                      1 1 2 2  a  b                                   
                                2  2                                  
          How about that? The dot product of the transformed vectors is equal to the square of
          the dot product of the original vectors: ϕ(a)⊺ ϕ(b) = (a⊺ b)2.
          Here is the key insight: if you apply the transformation ϕ to all training instances,
          then the dual problem (see Equation 5-6) will contain the dot product ϕ(x(i))⊺ ϕ(x(j)).
          But if ϕ is the second-degree polynomial transformation defined in Equation 5-8,
          then you can replace this dot product of transformed vectors simply by xi⊺ x j 2 . So,
          you don’t need to transform the training instances at all; just replace the dot product
          by its square in Equation 5-6. The result will be strictly the same as if you had gone
          through the trouble of transforming the training set then fitting a linear SVM algo‐
          rithm, but this trick makes the whole process much more computationally efficient.
          The function K(a, b) = (a⊺ b)2 is a second-degree polynomial kernel. In Machine
          Learning, a kernel is a function capable of computing the dot product ϕ(a)⊺ ϕ(b),
          7 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a · b. However, in
           Machine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the
           dot product is achieved by computing a⊺b. To remain consistent with the rest of the book, we will use this
           notation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value."|polynomial kernels; kernels
"                                                                      
                                                                      
                                                                      
                                                                      
          decide which hyperparameter values to use during the next trial? Well, AI Platform
          just monitors the output directory (specified via --job-dir) for any event file (intro‐
          duced in Chapter 10) containing summaries for a metric named ""accuracy"" (or
          whatever metric name is specified as the hyperparameterMetricTag), and it reads
          those values. So your training code simply has to use the TensorBoard() callback
          (which you will want to do anyway for monitoring), and you’re good to go!
                                                                      
          Once the job is finished, all the hyperparameter values used in each trial and the
          resulting accuracy will be available in the job’s output (available via the AI Platform →
          Jobs page).                                                 
                                                                      
                   AI Platform jobs can also be used to efficiently execute your model
                   on large amounts of data: each worker can read part of the data
                   from GCS, make predictions, and save them to GCS.  
                                                                      
                                                                      
          Now you have all the tools and knowledge you need to create state-of-the-art neural
          net architectures and train them at scale using various distribution strategies, on your
          own infrastructure or on the cloud—and you can even perform powerful Bayesian
          optimization to fine-tune the hyperparameters!              
                                                                      
          Exercises                                                   
                                                                      
           1. What does a SavedModel contain? How do you inspect its content?
           2. When should you use TF Serving? What are its main features? What are some
            tools you can use to deploy it?                           
           3. How do you deploy a model across multiple TF Serving instances?
                                                                      
           4. When should you use the gRPC API rather than the REST API to query a model
            served by TF Serving?                                     
           5. What are the different ways TFLite reduces a model’s size to make it run on a
            mobile or embedded device?                                
           6. What is quantization-aware training, and why would you need it?
           7. What are model parallelism and data parallelism? Why is the latter generally
            recommended?                                              
                                                                      
           8. When training a model across multiple servers, what distribution strategies can
            you use? How do you choose which one to use?              
           9. Train a model (any model you like) and deploy it to TF Serving or Google Cloud
            AI Platform. Write the client code to query it using the REST API or the gRPC
                                                                      
                                                                      "|training models across multiple devices; training across multiple devices
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                     The Unreasonable Effectiveness of Data           
                                                                      
           In a famous paper published in 2001, Microsoft researchers Michele Banko and Eric
           Brill showed that very different Machine Learning algorithms, including fairly simple
           ones, performed almost identically well on a complex problem of natural language
           disambiguation8 once they were given enough data (as you can see in Figure 1-20).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
           Figure 1-20. The importance of data versus algorithms9     
                                                                      
           As the authors put it, “these results suggest that we may want to reconsider the trade-
           off between spending time and money on algorithm development versus spending it
           on corpus development.”                                    
           The idea that data matters more than algorithms for complex problems was further
           popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness
           of Data”, published in 2009.10 It should be noted, however, that small- and medium-
           sized datasets are still very common, and it is not always easy or cheap to get extra
           training data—so don’t abandon algorithms just yet.        
                                                                      
                                                                      
                                                                      
                                                                      
          8 For example, knowing whether to write “to,” “two,” or “too,” depending on the context.
          9 Figure reproduced with permission from Michele Banko and Eric Brill, “Scaling to Very Very Large Corpora
           for Natural Language Disambiguation,” Proceedings of the 39th Annual Meeting of the Association for Compu‐
           tational Linguistics (2001): 26–33.                        
          10 Peter Norvig et al., “The Unreasonable Effectiveness of Data,” IEEE Intelligent Systems 24, no. 2 (2009): 8–12."|importance of data over; importance of over algorithms; corpus development
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-21. AIC and BIC for different numbers of clusters k
                                                                      
          Bayesian Gaussian Mixture Models                            
                                                                      
          Rather than manually searching for the optimal number of clusters, you can use the
          BayesianGaussianMixture class, which is capable of giving weights equal (or close)
          to zero to unnecessary clusters. Set the number of clusters n_components to a value
          that you have good reason to believe is greater than the optimal number of clusters
          (this assumes some minimal knowledge about the problem at hand), and the algo‐
          rithm will eliminate the unnecessary clusters automatically. For example, let’s set the
          number of clusters to 10 and see what happens:              
            >>> from sklearn.mixture import BayesianGaussianMixture   
            >>> bgm = BayesianGaussianMixture(n_components=10, n_init=10)
            >>> bgm.fit(X)                                            
            >>> np.round(bgm.weights_, 2)                             
            array([0.4 , 0.21, 0.4 , 0. , 0. , 0. , 0. , 0. , 0. , 0. ])
          Perfect: the algorithm automatically detected that only three clusters are needed, and
          the resulting clusters are almost identical to the ones in Figure 9-17.
          In this model, the cluster parameters (including the weights, means, and covariance
          matrices) are not treated as fixed model parameters anymore, but as latent random
          variables, like the cluster assignments (see Figure 9-22). So z now includes both the
          cluster parameters and the cluster assignments.             
          The Beta distribution is commonly used to model random variables whose values lie
          within a fixed range. In this case, the range is from 0 to 1. The Stick-Breaking Process
          (SBP) is best explained through an example: suppose Φ=[0.3, 0.6, 0.5,…], then 30% of
          the instances will be assigned to cluster 0, then 60% of the remaining instances will be
          assigned to cluster 1, then 50% of the remaining instances will be assigned to cluster
          2, and so on. This process is a good model for datasets where new instances are more
          likely to join large clusters than small clusters (e.g., people are more likely to move to
          larger cities). If the concentration α is high, then Φ values will likely be close to 0, and
          the SBP generate many clusters. Conversely, if the concentration is low, then Φ values
                                                                      "|Bayesian Gaussian Mixture models
"                                                                      
                                                                      
                                                                      
                                                                      
          As you can see in Figure 3-7, the RandomForestClassifier’s ROC curve looks much
          better than the SGDClassifier’s: it comes much closer to the top-left corner. As a
          result, its ROC AUC score is also significantly better:     
                                                                      
            >>> roc_auc_score(y_train_5, y_scores_forest)             
            0.9983436731328145                                        
          Try measuring the precision and recall scores: you should find 99.0% precision and
          86.6% recall. Not too bad!                                  
          You now know how to train binary classifiers, choose the appropriate metric for your
          task, evaluate your classifiers using cross-validation, select the precision/recall trade-
          off that fits your needs, and use ROC curves and ROC AUC scores to compare vari‐
          ous models. Now let’s try to detect more than just the 5s.  
                                                                      
          Multiclass Classification                                   
                                                                      
          Whereas binary classifiers distinguish between two classes, multiclass classifiers (also
          called multinomial classifiers) can distinguish between more than two classes.
          Some algorithms (such as SGD classifiers, Random Forest classifiers, and naive Bayes
          classifiers) are capable of handling multiple classes natively. Others (such as Logistic
          Regression or Support Vector Machine classifiers) are strictly binary classifiers. How‐
          ever, there are various strategies that you can use to perform multiclass classification
          with multiple binary classifiers.                           
                                                                      
          One way to create a system that can classify the digit images into 10 classes (from 0 to
          9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-
          detector, and so on). Then when you want to classify an image, you get the decision
          score from each classifier for that image and you select the class whose classifier out‐
          puts the highest score. This is called the one-versus-the-rest (OvR) strategy (also called
          one-versus-all).                                            
          Another strategy is to train a binary classifier for every pair of digits: one to distin‐
          guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.
          This is called the one-versus-one (OvO) strategy. If there are N classes, you need to
          train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45
          binary classifiers! When you want to classify an image, you have to run the image
          through all 45 classifiers and see which class wins the most duels. The main advan‐
          tage of OvO is that each classifier only needs to be trained on the part of the training
          set for the two classes that it must distinguish.           
          Some algorithms (such as Support Vector Machine classifiers) scale poorly with the
          size of the training set. For these algorithms OvO is preferred because it is faster to
          train many classifiers on small training sets than to train few classifiers on large train‐
          ing sets. For most binary classification algorithms, however, OvR is preferred.
                                                                      "|performance measures; multinomial classifiers; one-versus-all (OvA) strategy; one-versus-one (OvO) strategy; multiclass classification; one-versus-the-rest (OvR) strategy
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> softmax_reg.predict([[5, 2]])                         
            array([2])                                                
            >>> softmax_reg.predict_proba([[5, 2]])                   
            array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]]) 
          Figure 4-25 shows the resulting decision boundaries, represented by the background
          colors. Notice that the decision boundaries between any two classes are linear. The
          figure also shows the probabilities for the Iris versicolor class, represented by the
          curved lines (e.g., the line labeled with 0.450 represents the 45% probability bound‐
          ary). Notice that the model can predict a class that has an estimated probability below
          50%. For example, at the point where all decision boundaries meet, all classes have an
          equal estimated probability of 33%.                         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-25. Softmax Regression decision boundaries         
                                                                      
          Exercises                                                   
                                                                      
                                                                      
           1. Which Linear Regression training algorithm can you use if you have a training
            set with millions of features?                            
           2. Suppose the features in your training set have very different scales. Which algo‐
            rithms might suffer from this, and how? What can you do about it?
           3. Can Gradient Descent get stuck in a local minimum when training a Logistic
            Regression model?                                         
           4. Do all Gradient Descent algorithms lead to the same model, provided you let
            them run long enough?                                     
                                                                      
           5. Suppose you use Batch Gradient Descent and you plot the validation error at
            every epoch. If you notice that the validation error consistently goes up, what is
            likely going on? How can you fix this?                    
           6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐
            dation error goes up?                                     
                                                                      "|Logistic Regression; Softmax Regression
"                                                                      
                                                                      
                                                                      
                                                                      
          by the activation function. Finally, it returns the outputs twice (once as the outputs,
          and once as the new hidden states). To use this custom cell, all we need to do is create
          a keras.layers.RNN layer, passing it a cell instance:       
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,
                         input_shape=[None, 1]),                      
               keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),
               keras.layers.TimeDistributed(keras.layers.Dense(10))   
            ])                                                        
          Similarly, you could create a custom cell to apply dropout between each time step. But
          there’s a simpler way: all recurrent layers (except for keras.layers.RNN) and all cells
          provided by Keras have a dropout hyperparameter and a recurrent_dropout hyper‐
          parameter: the former defines the dropout rate to apply to the inputs (at each time
          step), and the latter defines the dropout rate for the hidden states (also at each time
          step). No need to create a custom cell to apply dropout at each time step in an RNN.
          With these techniques, you can alleviate the unstable gradients problem and train an
          RNN much more efficiently. Now let’s look at how to deal with the short-term mem‐
          ory problem.                                                
          Tackling the Short-Term Memory Problem                      
                                                                      
          Due to the transformations that the data goes through when traversing an RNN,
          some information is lost at each time step. After a while, the RNN’s state contains vir‐
          tually no trace of the first inputs. This can be a showstopper. Imagine Dory the fish6
          trying to translate a long sentence; by the time she’s finished reading it, she has no
          clue how it started. To tackle this problem, various types of cells with long-term
          memory have been introduced. They have proven so successful that the basic cells are
          not used much anymore. Let’s first look at the most popular of these long-term mem‐
          ory cells: the LSTM cell.                                   
          LSTM cells                                                  
                                                                      
          The Long Short-Term Memory (LSTM) cell was proposed in 19977 by Sepp Hochreiter
          and Jürgen Schmidhuber and gradually improved over the years by several research‐
          ers, such as Alex Graves, Haşim Sak,8 and Wojciech Zaremba.9 If you consider the
                                                                      
                                                                      
          6 A character from the animated movies Finding Nemo and Finding Dory who has short-term memory loss.
          7 Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997):
           1735–1780.                                                 
          8 Haşim Sak et al., “Long Short-Term Memory Based Recurrent Neural Network Architectures for Large
           Vocabulary Speech Recognition,” arXiv preprint arXiv:1402.1128 (2014).
          9 Wojciech Zaremba et al., “Recurrent Neural Network Regularization,” arXiv preprint arXiv:1409.2329 (2014)."|long sequences short-term memory problems; Long Short-Term Memory (LSTM) cell; short-term memory problems
"                                                                      
                                                                      
                                                                      
                                                                      
                   This algorithm is an example of Dynamic Programming, which
                   breaks down a complex problem into tractable subproblems that
                   can be tackled iteratively.                        
                                                                      
                                                                      
          Knowing the optimal state values can be useful, in particular to evaluate a policy, but
          it does not give us the optimal policy for the agent. Luckily, Bellman found a very
          similar algorithm to estimate the optimal state-action values, generally called Q-
          Values (Quality Values). The optimal Q-Value of the state-action pair (s, a), noted
          Q*(s, a), is the sum of discounted future rewards the agent can expect on average
          after it reaches the state s and chooses action a, but before it sees the outcome of this
          action, assuming it acts optimally after that action.       
                                                                      
          Here is how it works: once again, you start by initializing all the Q-Value estimates to
          zero, then you update them using the Q-Value Iteration algorithm (see Equation
          18-3).                                                      
                                                                      
            Equation 18-3. Q-Value Iteration algorithm                
            Q   s,a ∑T s,a,s′ R s,a,s′ +γ· max Q s′,a′ for all s′a    
             k+1    s′             a′  k                              
                                                                      
          Once you have the optimal Q-Values, defining the optimal policy, noted π*(s), is triv‐
          ial: when the agent is in state s, it should choose the action with the highest Q-Value
          for that state: π* s = argmax Q* s,a .                      
                         a                                            
          Let’s apply this algorithm to the MDP represented in Figure 18-8. First, we need to
          define the MDP:                                             
            transition_probabilities = [ # shape=[s, a, s']           
                 [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], 
                 [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]],            
                 [None, [0.8, 0.1, 0.1], None]]                       
            rewards = [ # shape=[s, a, s']                            
                 [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],                 
                 [[0, 0, 0], [0, 0, 0], [0, 0, -50]],                 
                 [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]                 
            possible_actions = [[0, 1, 2], [0, 2], [1]]               
          For example, to know the transition probability from s to s after playing action a ,
                                          2  0             1          
          we will look up transition_probabilities[2][1][0] (which is 0.8). Similarly, to
          get the corresponding reward, we will look up rewards[2][1][0] (which is +40).
          And to get the list of possible actions in s , we will look up possible_actions[2] (in
                                  2                                   
          this case, only action a is possible). Next, we must initialize all the Q-Values to 0
                        1                                             
          (except for the the impossible actions, for which we set the Q-Values to –∞):"|Q-Value Iteration; Q-Values; Dynamic Programming; state-action values
"                                                                      
                                                                      
                                                                      
                                                                      
          rate is not too large and you wait long enough). The partial derivatives of the cost
          function with regard to the jth model parameter θ are given by Equation 4-18.
                                      j                               
            Equation 4-18. Logistic cost function partial derivatives 
                                                                      
                    m                                                 
             ∂ J θ = 1 ∑ σ θ ⊺ x i −y i x i                           
            ∂θ    m            j                                      
              j    i=1                                                
          This equation looks very much like Equation 4-5: for each instance it computes the
          prediction error and multiplies it by the jth feature value, and then it computes the
          average over all training instances. Once you have the gradient vector containing all
          the partial derivatives, you can use it in the Batch Gradient Descent algorithm. That’s
          it: you now know how to train a Logistic Regression model. For Stochastic GD you
          would take one instance at a time, and for Mini-batch GD you would use a mini-
          batch at a time.                                            
          Decision Boundaries                                         
          Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that
          contains the sepal and petal length and width of 150 iris flowers of three different
          species: Iris setosa, Iris versicolor, and Iris virginica (see Figure 4-22).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-22. Flowers of three iris plant species14          
                                                                      
                                                                      
                                                                      
                                                                      
          14 Photos reproduced from the corresponding Wikipedia pages. Iris virginica photo by Frank Mayfield (Creative
           Commons BY-SA 2.0), Iris versicolor photo by D. Gordon E. Robertson (Creative Commons BY-SA 3.0), Iris
           setosa photo public domain.                                "|decision boundaries; iris dataset
