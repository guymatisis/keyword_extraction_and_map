With Stochastic and Mini-batch Gradient Descent, the curves are
not so smooth, and it may be hard to know whether you have
reached the minimum or not. One solution is to stop only after the
validation error has been above the minimum for some time (when
you are confident that the model will not do any better), then roll
back the model parameters to the point where the validation error
was at a minimum.
Here is a basic implementation of early stopping:
<b>from</b> <b>sklearn.base</b> <b>import</b> clone
<i>#</i> <i>prepare</i> <i>the</i> <i>data</i>
poly_scaler = Pipeline([
("poly_features", PolynomialFeatures(degree=90, include_bias=False)),
("std_scaler", StandardScaler())
])
X_train_poly_scaled = poly_scaler.fit_transform(X_train)
X_val_poly_scaled = poly_scaler.transform(X_val)
sgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True,
penalty=None, learning_rate="constant", eta0=0.0005)
minimum_val_error = float("inf")
best_epoch = None
best_model = None
<b>for</b> epoch <b>in</b> range(1000):
sgd_reg.fit(X_train_poly_scaled, y_train) <i>#</i> <i>continues</i> <i>where</i> <i>it</i> <i>left</i> <i>off</i>
y_val_predict = sgd_reg.predict(X_val_poly_scaled)
val_error = mean_squared_error(y_val, y_val_predict)
<b>if</b> val_error < minimum_val_error:
minimum_val_error = val_error
best_epoch = epoch
best_model = clone(sgd_reg)
warm_start=True fit()
Note that with , when the method is called it continues train‐
ing where it left off, instead of restarting from scratch.
<header><largefont><b>Logistic</b></largefont> <largefont><b>Regression</b></largefont></header>
As we discussed in Chapter 1, some regression algorithms can be used for classifica‐
tion (and vice versa). <i>Logistic</i> <i>Regression</i> (also called <i>Logit</i> <i>Regression)</i> is commonly
used to estimate the probability that an instance belongs to a particular class (e.g.,
what is the probability that this email is spam?). If the estimated probability is greater
than 50%, then the model predicts that the instance belongs to that class (called the
<i>positive</i> <i>class,</i> labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to
the <i>negative</i> <i>class,</i> labeled “0”). This makes it a binary classifier.