<header><largefont><b>Deep</b></largefont> <largefont><b>Q-Learning</b></largefont> <largefont><b>Variants</b></largefont></header>
Let’s look at a few variants of the Deep Q-Learning algorithm that can stabilize and
speed up training.
<header><largefont><b>Fixed</b></largefont> <largefont><b>Q-Value</b></largefont> <largefont><b>Targets</b></largefont></header>
In the basic Deep Q-Learning algorithm, the model is used both to make predictions
and to set its own targets. This can lead to a situation analogous to a dog chasing its
own tail. This feedback loop can make the network unstable: it can diverge, oscillate,
freeze, and so on. To solve this problem, in their 2013 paper the DeepMind research‐
ers used two DQNs instead of one: the first is the <i>online</i> <i>model,</i> which learns at each
step and is used to move the agent around, and the other is the <i>target</i> <i>model</i> used only
to define the targets. The target model is just a clone of the online model:
target = keras.models.clone_model(model)
target.set_weights(model.get_weights())
training_step()
Then, in the function, we just need to change one line to use the
target model instead of the online model when computing the Q-Values of the next
states:
next_Q_values = target.predict(next_states)
Finally, in the training loop, we must copy the weights of the online model to the tar‐
get model, at regular intervals (e.g., every 50 episodes):
<b>if</b> episode % 50 == 0:
target.set_weights(model.get_weights())
Since the target model is updated much less often than the online model, the Q-Value
targets are more stable, the feedback loop we discussed earlier is dampened, and its
effects are less severe. This approach was one of the DeepMind researchers’ main
contributions in their 2013 paper, allowing agents to learn to play Atari games from
raw pixels. To stabilize training, they used a tiny learning rate of 0.00025, they upda‐
ted the target model only every 10,000 steps (instead of the 50 in the previous code
example), and they used a very large replay buffer of 1 million experiences. They
decreased epsilon very slowly, from 1 to 0.1 in 1 million steps, and they let the algo‐
rithm run for 50 million steps.
Later in this chapter, we will use the TF-Agents library to train a DQN agent to play
<i>Breakout</i> using these hyperparameters, but before we get there, let’s take a look at
another DQN variant that managed to beat the state of the art once more.