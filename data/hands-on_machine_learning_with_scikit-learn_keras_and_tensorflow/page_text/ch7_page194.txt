As you can see in Figure 7-4, predictors can all be trained in parallel, via different
CPU cores or even different servers. Similarly, predictions can be made in parallel.
This is one of the reasons bagging and pasting are such popular methods: they scale
very well.
<header><largefont><b>Bagging</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Pasting</b></largefont> <largefont><b>in</b></largefont> <largefont><b>Scikit-Learn</b></largefont></header>
BaggingClas
Scikit-Learn offers a simple API for both bagging and pasting with the
sifier BaggingRegressor
class (or for regression). The following code trains an
ensemble of 500 Decision Tree classifiers: 5 each is trained on 100 training instances
randomly sampled from the training set with replacement (this is an example of bag‐
bootstrap=False n_jobs
ging, but if you want to use pasting instead, just set ). The
parameter tells Scikit-Learn the number of CPU cores to use for training and predic‐
tions ( –1 tells Scikit-Learn to use all available cores):
<b>from</b> <b>sklearn.ensemble</b> <b>import</b> BaggingClassifier
<b>from</b> <b>sklearn.tree</b> <b>import</b> DecisionTreeClassifier
bag_clf = BaggingClassifier(
DecisionTreeClassifier(), n_estimators=500,
max_samples=100, bootstrap=True, n_jobs=-1)
bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
BaggingClassifier
The automatically performs soft voting
instead of hard voting if the base classifier can estimate class proba‐
predict_proba()
bilities (i.e., if it has a method), which is the case
with Decision Tree classifiers.
Figure 7-5 compares the decision boundary of a single Decision Tree with the deci‐
sion boundary of a bagging ensemble of 500 trees (from the preceding code), both
trained on the moons dataset. As you can see, the ensemble’s predictions will likely
generalize much better than the single Decision Tree’s predictions: the ensemble has a
comparable bias but a smaller variance (it makes roughly the same number of errors
on the training set, but the decision boundary is less irregular).
5 max_samplescanalternativelybesettoafloatbetween0.0and1.0,inwhichcasethemaxnumberofinstances
max_samples
tosampleisequaltothesizeofthetrainingsettimes .