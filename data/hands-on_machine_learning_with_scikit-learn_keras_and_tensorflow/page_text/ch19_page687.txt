                                                                      
                                                                      
                                                                      
                                                                      
          floats, but not too far off, so the accuracy loss is usually acceptable). To avoid recom‐
          puting them all the time, the recovered floats are cached, so there is no reduction of
          RAM usage. And there is no reduction either in compute speed.
                                                                      
          The most effective way to reduce latency and power consumption is to also quantize
          the activations so that the computations can be done entirely with integers, without
          the need for any floating-point operations. Even when using the same bit-width (e.g.,
          32-bit integers instead of 32-bit floats), integer computations use less CPU cycles,
          consume less energy, and produce less heat. And if you also reduce the bit-width (e.g.,
          down to 8-bit integers), you can get huge speedups. Moreover, some neural network
          accelerator devices (such as the Edge TPU) can only process integers, so full quanti‐
          zation of both weights and activations is compulsory. This can be done post-training;
          it requires a calibration step to find the maximum absolute value of the activations, so
          you need to provide a representative sample of training data to TFLite (it does not
          need to be huge), and it will process the data through the model and measure the
          activation statistics required for quantization (this step is typically fast).
          The main problem with quantization is that it loses a bit of accuracy: it is equivalent
          to adding noise to the weights and activations. If the accuracy drop is too severe, then
          you may need to use quantization-aware training. This means adding fake quantiza‐
          tion operations to the model so it can learn to ignore the quantization noise during
          training; the final weights will then be more robust to quantization. Moreover, the
          calibration step can be taken care of automatically during training, which simplifies
          the whole process.                                          
          I have explained the core concepts of TFLite, but going all the way to coding a mobile
          app or an embedded program would require a whole other book. Fortunately, one
          exists: if you want to learn more about building TensorFlow applications for mobile
          and embedded devices, check out the O’Reilly book TinyML: Machine Learning with
          TensorFlow on Arduino and Ultra-Low Power Micro-Controllers, by Pete Warden (who
          leads the TFLite team) and Daniel Situnayake.               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      