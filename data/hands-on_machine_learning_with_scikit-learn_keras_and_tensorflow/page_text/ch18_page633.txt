                                                                      
                                                                      
                                                                      
                                                                      
          Alternatively, rather than relying only on chance for exploration, another approach is
          to encourage the exploration policy to try actions that it has not tried much before.
          This can be implemented as a bonus added to the Q-Value estimates, as shown in
          Equation 18-6.                                              
                                                                      
            Equation 18-6. Q-Learning using an exploration function   
                                                                      
            Q s,a r+γ·max f Q s′,a′ ,N s′,a′                          
                α     a′                                              
                                                                      
          In this equation:                                           
                                                                      
           • N(s′, a′) counts the number of times the action a′ was chosen in state s′.
           • f(Q, N) is an exploration function, such as f(Q, N) = Q + κ/(1 + N), where κ is a
            curiosity hyperparameter that measures how much the agent is attracted to the
            unknown.                                                  
                                                                      
          Approximate Q-Learning and Deep Q-Learning                  
                                                                      
          The main problem with Q-Learning is that it does not scale well to large (or even
          medium) MDPs with many states and actions. For example, suppose you wanted to
          use Q-Learning to train an agent to play Ms. Pac-Man (see Figure 18-1). There are
          about 150 pellets that Ms. Pac-Man can eat, each of which can be present or absent
          (i.e., already eaten). So, the number of possible states is greater than 2150 ≈ 1045. And if
          you add all the possible combinations of positions for all the ghosts and Ms. Pac-
          Man, the number of possible states becomes larger than the number of atoms in our
          planet, so there’s absolutely no way you can keep track of an estimate for every single
          Q-Value.                                                    
          The solution is to find a function Q (s, a) that approximates the Q-Value of any state-
                               θ                                      
          action pair (s, a) using a manageable number of parameters (given by the parameter
          vector θ). This is called Approximate Q-Learning. For years it was recommended to
          use linear combinations of handcrafted features extracted from the state (e.g., dis‐
          tance of the closest ghosts, their directions, and so on) to estimate Q-Values, but in
          2013, DeepMind showed that using deep neural networks can work much better,
          especially for complex problems, and it does not require any feature engineering. A
          DNN used to estimate Q-Values is called a Deep Q-Network (DQN), and using a
          DQN for Approximate Q-Learning is called Deep Q-Learning.   
          Now, how can we train a DQN? Well, consider the approximate Q-Value computed
          by the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want
          this approximate Q-Value to be as close as possible to the reward r that we actually
          observe after playing action a in state s, plus the discounted value of playing optimally