                                                                      
                                                                      
                                                                      
                                                                      
          Reducing r increases the amount of regularization and helps reduce overfitting. Max-
          norm regularization can also help alleviate the unstable gradients problems (if you
          are not using Batch Normalization).                         
                                                                      
          To implement max-norm regularization in Keras, set the kernel_constraint argu‐
          ment of each hidden layer to a max_norm() constraint with the appropriate max value,
          like this:                                                  
            keras.layers.Dense(100, activation="elu", kernel_initializer="he_normal",
                       kernel_constraint=keras.constraints.max_norm(1.))
          After each training iteration, the model’s fit() method will call the object returned
          by max_norm(), passing it the layer’s weights and getting rescaled weights in return,
          which then replace the layer’s weights. As you’ll see in Chapter 12, you can define
          your own custom constraint function if necessary and use it as the kernel_con
          straint. You can also constrain the bias terms by setting the bias_constraint
          argument.                                                   
                                                                      
          The max_norm() function has an axis argument that defaults to 0. A Dense layer usu‐
          ally has weights of shape [number of inputs, number of neurons], so using axis=0
          means that the max-norm constraint will apply independently to each neuron’s
          weight vector. If you want to use max-norm with convolutional layers (see Chap‐
          ter 14), make sure to set the max_norm() constraint’s axis argument appropriately
          (usually axis=[0, 1, 2]).                                   
          Summary  and Practical Guidelines                           
                                                                      
          In this chapter we have covered a wide range of techniques, and you may be wonder‐
          ing which ones you should use. This depends on the task, and there is no clear con‐
          sensus yet, but I have found the configuration in Table 11-3 to work fine in most
          cases, without requiring much hyperparameter tuning. That said, please do not con‐
          sider these defaults as hard rules!                         
                                                                      
          Table 11-3. Default DNN configuration                       
                                                                      
          Hyperparameter Default value                                
          Kernel initializer He initialization                        
          Activation function ELU                                     
          Normalization None if shallow; Batch Norm if deep           
          Regularization Early stopping (+ℓ reg. if needed)           
                           2                                          
          Optimizer Momentum optimization (or RMSProp or Nadam)       
          Learning rate schedule 1cycle                               
                                                                      
                                                                      