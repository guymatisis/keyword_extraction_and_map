                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 16-7. Visual attention: an input image (left) and the model’s focus before produc‐
          ing the word “frisbee” (right)18                            
                                                                      
                                                                      
                              Explainability                          
           One extra benefit of attention mechanisms is that they make it easier to understand
           what led the model to produce its output. This is called explainability. It can be espe‐
           cially useful when the model makes a mistake: for example, if an image of a dog walk‐
           ing in the snow is labeled as “a wolf walking in the snow,” then you can go back and
           check what the model focused on when it output the word “wolf.” You may find that it
           was paying attention not only to the dog, but also to the snow, hinting at a possible
           explanation: perhaps the way the model learned to distinguish dogs from wolves is by
           checking whether or not there’s a lot of snow around. You can then fix this by training
           the model with more images of wolves without snow, and dogs with snow. This exam‐
           ple comes from a great 2016 paper19 by Marco Tulio Ribeiro et al. that uses a different
           approach to explainability: learning an interpretable model locally around a classi‐
           fier’s prediction.                                         
           In some applications, explainability is not just a tool to debug a model; it can be a
           legal requirement (think of a system deciding whether or not it should grant you a
           loan).                                                     
                                                                      
                                                                      
                                                                      
                                                                      
          18 This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors.
          19 Marco Tulio Ribeiro et al., “‘Why Should I Trust You?’: Explaining the Predictions of Any Classifier,” Proceed‐
           ings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):
           1135–1144.                                                 