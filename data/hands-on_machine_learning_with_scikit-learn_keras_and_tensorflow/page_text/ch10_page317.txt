<header><largefont><b>Using</b></largefont> <largefont><b>TensorBoard</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Visualization</b></largefont></header>
TensorBoard is a great interactive visualization tool that you can use to view the
learning curves during training, compare learning curves between multiple runs, vis‐
ualize the computation graph, analyze training statistics, view images generated by
your model, visualize complex multidimensional data projected down to 3D and
automatically clustered for you, and more! This tool is installed automatically when
you install TensorFlow, so you already have it.
To use it, you must modify your program so that it outputs the data you want to visu‐
alize to special binary log files called <i>event</i> <i>files.</i> Each binary data record is called a
<i>summary.</i> The TensorBoard server will monitor the log directory, and it will automat‐
ically pick up the changes and update the visualizations: this allows you to visualize
live data (with a short delay), such as the learning curves during training. In general,
you want to point the TensorBoard server to a root log directory and configure your
program so that it writes to a different subdirectory every time it runs. This way, the
same TensorBoard server instance will allow you to visualize and compare data from
multiple runs of your program, without getting everything mixed up.
Let’s start by defining the root log directory we will use for our TensorBoard logs,
plus a small function that will generate a subdirectory path based on the current date
and time so that it’s different at every run. You may want to include extra information
in the log directory name, such as hyperparameter values that you are testing, to
make it easier to know what you are looking at in TensorBoard:
<b>import</b> <b>os</b>
root_logdir = os.path.join(os.curdir, "my_logs")
<b>def</b> get_run_logdir():
<b>import</b> <b>time</b>
run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")
<b>return</b> os.path.join(root_logdir, run_id)
run_logdir = get_run_logdir() <i>#</i> <i>e.g.,</i> <i>'./my_logs/run_2019_06_07-15_15_22'</i>
The good news is that Keras provides a nice TensorBoard() callback:
[...] <i>#</i> <i>Build</i> <i>and</i> <i>compile</i> <i>your</i> <i>model</i>
tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)
history = model.fit(X_train, y_train, epochs=30,
validation_data=(X_valid, y_valid),
callbacks=[tensorboard_cb])
And that’s all there is to it! It could hardly be easier to use. If you run this code, the
TensorBoard()
callback will take care of creating the log directory for you (along
with its parent directories if needed), and during training it will create event files and
write summaries to them. After running the program a second time (perhaps