                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Using TensorBoard for Visualization                         
                                                                      
          TensorBoard is a great interactive visualization tool that you can use to view the
          learning curves during training, compare learning curves between multiple runs, vis‐
          ualize the computation graph, analyze training statistics, view images generated by
          your model, visualize complex multidimensional data projected down to 3D and
          automatically clustered for you, and more! This tool is installed automatically when
          you install TensorFlow, so you already have it.             
          To use it, you must modify your program so that it outputs the data you want to visu‐
          alize to special binary log files called event files. Each binary data record is called a
          summary. The TensorBoard server will monitor the log directory, and it will automat‐
          ically pick up the changes and update the visualizations: this allows you to visualize
          live data (with a short delay), such as the learning curves during training. In general,
          you want to point the TensorBoard server to a root log directory and configure your
          program so that it writes to a different subdirectory every time it runs. This way, the
          same TensorBoard server instance will allow you to visualize and compare data from
          multiple runs of your program, without getting everything mixed up.
          Let’s start by defining the root log directory we will use for our TensorBoard logs,
          plus a small function that will generate a subdirectory path based on the current date
          and time so that it’s different at every run. You may want to include extra information
          in the log directory name, such as hyperparameter values that you are testing, to
          make it easier to know what you are looking at in TensorBoard:
                                                                      
            import os                                                 
            root_logdir = os.path.join(os.curdir, "my_logs")          
            def get_run_logdir():                                     
               import time                                            
               run_id = time.strftime("run_%Y_%m_%d-%H_%M_%S")        
               return os.path.join(root_logdir, run_id)               
            run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'
          The good news is that Keras provides a nice TensorBoard() callback:
                                                                      
            [...] # Build and compile your model                      
            tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)  
            history = model.fit(X_train, y_train, epochs=30,          
                        validation_data=(X_valid, y_valid),           
                        callbacks=[tensorboard_cb])                   
          And that’s all there is to it! It could hardly be easier to use. If you run this code, the
          TensorBoard() callback will take care of creating the log directory for you (along
          with its parent directories if needed), and during training it will create event files and
          write summaries to them. After running the program a second time (perhaps