                                                                      
                                                                      
                                                                      
                                                                      
            array([[11., 12., 13.],                                   
                [14., 15., 16.]], dtype=float32)>                     
            >>> tf.square(t)                                          
            <tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=    
            array([[ 1., 4., 9.],                                     
                [16., 25., 36.]], dtype=float32)>                     
            >>> t @ tf.transpose(t)                                   
            <tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=    
            array([[14., 32.],                                        
                [32., 77.]], dtype=float32)>                          
          Note that writing t + 10 is equivalent to calling tf.add(t, 10) (indeed, Python calls
          the magic method t.__add__(10), which just calls tf.add(t, 10)). Other operators
          like - and * are also supported. The @ operator was added in Python 3.5, for matrix
          multiplication: it is equivalent to calling the tf.matmul() function.
          You will find all the basic math operations you need (tf.add(), tf.multiply(),
          tf.square(), tf.exp(), tf.sqrt(), etc.) and most operations that you can find in
          NumPy (e.g., tf.reshape(), tf.squeeze(), tf.tile()). Some functions have a dif‐
          ferent name than in NumPy; for instance, tf.reduce_mean(), tf.reduce_sum(),
          tf.reduce_max(), and tf.math.log() are the equivalent of np.mean(), np.sum(),
          np.max() and np.log(). When the name differs, there is often a good reason for it.
          For example, in TensorFlow you must write tf.transpose(t); you cannot just write
          t.T like in NumPy. The reason is that the tf.transpose() function does not do
          exactly the same thing as NumPy’s T attribute: in TensorFlow, a new tensor is created
          with its own copy of the transposed data, while in NumPy, t.T is just a transposed
          view on the same data. Similarly, the tf.reduce_sum() operation is named this way
          because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that does
          not guarantee the order in which the elements are added: because 32-bit floats have
          limited precision, the result may change ever so slightly every time you call this oper‐
          ation. The same is true of tf.reduce_mean() (but of course tf.reduce_max() is
          deterministic).                                             
                   Many functions and classes have aliases. For example, tf.add()
                   and tf.math.add() are the same function. This allows TensorFlow
                   to have concise names for the most common operations4 while pre‐
                   serving well-organized packages.                   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          4 A notable exception is tf.math.log(), which is commonly used but doesn’t have a tf.log() alias (as it might
           be confused with logging).                                 