RandomForestClassifier
As you can see in Figure 3-7, the ’s ROC curve looks much
better than the SGDClassifier ’s: it comes much closer to the top-left corner. As a
result, its ROC AUC score is also significantly better:
<b>>>></b> roc_auc_score(y_train_5, y_scores_forest)
0.9983436731328145
Try measuring the precision and recall scores: you should find 99.0% precision and
86.6% recall. Not too bad!
You now know how to train binary classifiers, choose the appropriate metric for your
task, evaluate your classifiers using cross-validation, select the precision/recall trade-
off that fits your needs, and use ROC curves and ROC AUC scores to compare vari‐
ous models. Now let’s try to detect more than just the 5s.
<header><largefont><b>Multiclass</b></largefont> <largefont><b>Classification</b></largefont></header>
Whereas binary classifiers distinguish between two classes, <i>multiclass</i> <i>classifiers</i> (also
called <i>multinomial</i> <i>classifiers)</i> can distinguish between more than two classes.
Some algorithms (such as SGD classifiers, Random Forest classifiers, and naive Bayes
classifiers) are capable of handling multiple classes natively. Others (such as Logistic
Regression or Support Vector Machine classifiers) are strictly binary classifiers. How‐
ever, there are various strategies that you can use to perform multiclass classification
with multiple binary classifiers.
One way to create a system that can classify the digit images into 10 classes (from 0 to
9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-
detector, and so on). Then when you want to classify an image, you get the decision
score from each classifier for that image and you select the class whose classifier out‐
puts the highest score. This is called the <i>one-versus-the-rest</i> (OvR) strategy (also called
<i>one-versus-all).</i>
Another strategy is to train a binary classifier for every pair of digits: one to distin‐
guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.
This is called the <i>one-versus-one</i> (OvO) strategy. If there are <i>N</i> classes, you need to
train <i>N</i> × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45
binary classifiers! When you want to classify an image, you have to run the image
through all 45 classifiers and see which class wins the most duels. The main advan‐
tage of OvO is that each classifier only needs to be trained on the part of the training
set for the two classes that it must distinguish.
Some algorithms (such as Support Vector Machine classifiers) scale poorly with the
size of the training set. For these algorithms OvO is preferred because it is faster to
train many classifiers on small training sets than to train few classifiers on large train‐
ing sets. For most binary classification algorithms, however, OvR is preferred.