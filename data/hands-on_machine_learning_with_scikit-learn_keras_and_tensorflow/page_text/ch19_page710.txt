MirroredStrategy scope()
strategy, create a object, call its method to get a distribu‐
tion context, and wrap the creation and compilation of your model inside that con‐
text. Then call the model’s fit() method normally:
distribution = tf.distribute.MirroredStrategy()
<b>with</b> distribution.scope():
mirrored_model = keras.models.Sequential([...])
mirrored_model.compile([...])
batch_size = 100 <i>#</i> <i>must</i> <i>be</i> <i>divisible</i> <i>by</i> <i>the</i> <i>number</i> <i>of</i> <i>replicas</i>
history = mirrored_model.fit(X_train, y_train, epochs=10)
Under the hood, tf.keras is distribution-aware, so in this MirroredStrategy context it
knows that it must replicate all variables and operations across all available GPU
fit()
devices. Note that the method will automatically split each training batch
across all the replicas, so it’s important that the batch size be divisible by the number
of replicas. And that’s all! Training will generally be significantly faster than using a
single device, and the code change was really minimal.
Once you have finished training your model, you can use it to make predictions effi‐
predict()
ciently: call the method, and it will automatically split the batch across all
replicas, making predictions in parallel (again, the batch size must be divisible by the
number of replicas). If you call the model’s save() method, it will be saved as a regu‐
lar model, <i>not</i> as a mirrored model with multiple replicas. So when you load it, it will
run like a regular model, on a single device (by default GPU 0, or the CPU if there are
no GPUs). If you want to load a model and run it on all available devices, you must
call keras.models.load_model() within a distribution context:
<b>with</b> distribution.scope():
mirrored_model = keras.models.load_model("my_mnist_model.h5")
If you only want to use a subset of all the available GPU devices, you can pass the list
MirroredStrategy
to the ’s constructor:
distribution = tf.distribute.MirroredStrategy(["/gpu:0", "/gpu:1"])
MirroredStrategy
By default, the class uses the <i>NVIDIA</i> <i>Collective</i> <i>Communications</i>
<i>Library</i> (NCCL) for the AllReduce mean operation, but you can change it by setting
the cross_device_ops argument to an instance of the tf.distribute.Hierarchical
CopyAllReduce tf.distribute.ReductionToOneDevice
class, or an instance of the
class. The default NCCL option is based on the tf.distribute.NcclAllReduce class,
which is usually faster, but this depends on the number and types of GPUs, so you
may want to give the alternatives a try.21
21 FormoredetailsonAllReducealgorithms,readthisgreatpostbyYuichiroUeno,andthispageonscaling
withNCCL.