                                                                      
                                                                      
                                                                      
                                                                      
          Unsupervised Pretraining Using Stacked Autoencoders         
                                                                      
          As we discussed in Chapter 11, if you are tackling a complex supervised task but you
          do not have a lot of labeled training data, one solution is to find a neural network that
          performs a similar task and reuse its lower layers. This makes it possible to train a
          high-performance model using little training data because your neural network won’t
          have to learn all the low-level features; it will just reuse the feature detectors learned
          by the existing network.                                    
          Similarly, if you have a large dataset but most of it is unlabeled, you can first train a
          stacked autoencoder using all the data, then reuse the lower layers to create a neural
          network for your actual task and train it using the labeled data. For example,
          Figure 17-6 shows how to use a stacked autoencoder to perform unsupervised pre‐
          training for a classification neural network. When training the classifier, if you really
          don’t have much labeled training data, you may want to freeze the pretrained layers
          (at least the lower ones).                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 17-6. Unsupervised pretraining using autoencoders    
                                                                      
                   Having plenty of unlabeled data and little labeled data is common.
                   Building a large unlabeled dataset is often cheap (e.g., a simple
                   script can download millions of images off the internet), but label‐
                   ing those images (e.g., classifying them as cute or not) can usually
                   be done reliably only by humans. Labeling instances is time-
                   consuming and costly, so it’s normal to have only a few thousand
                   human-labeled instances.                           