                                                                      
                                                                      
                                                                      
                                                                      
          for it to overfit the data. A simple way to regularize a polynomial model is to reduce
          the number of polynomial degrees.                           
                                                                      
          For a linear model, regularization is typically achieved by constraining the weights of
          the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,
          which implement three different ways to constrain the weights.
          Ridge Regression                                            
                                                                      
          Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin‐
          ear Regression: a regularization term equal to α∑n θ2 is added to the cost function.
                                      i=1 i                           
          This forces the learning algorithm to not only fit the data but also keep the model
          weights as small as possible. Note that the regularization term should only be added
          to the cost function during training. Once the model is trained, you want to use the
          unregularized performance measure to evaluate the model’s performance.
                   It is quite common for the cost function used during training to be
                   different from the performance measure used for testing. Apart
                   from regularization, another reason they might be different is that a
                   good training cost function should have optimization-friendly
                   derivatives, while the performance measure used for testing should
                   be as close as possible to the final objective. For example, classifiers
                   are often trained using a cost function such as the log loss (dis‐
                   cussed in a moment) but evaluated using precision/recall.
                                                                      
          The hyperparameter α controls how much you want to regularize the model. If α = 0,
          then Ridge Regression is just Linear Regression. If α is very large, then all weights end
          up very close to zero and the result is a flat line going through the data’s mean. Equa‐
          tion 4-8 presents the Ridge Regression cost function.9      
                                                                      
            Equation 4-8. Ridge Regression cost function              
                      1 n  2                                          
            J θ =MSE θ +α ∑ θ                                         
                      2 i=1 i                                         
          Note that the bias term θ is not regularized (the sum starts at i = 1, not 0). If we
                         0                                            
          define w as the vector of feature weights (θ to θ ), then the regularization term is
                                    1  n                              
                                                                      
                                                                      
          9 It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this
           notation throughout the rest of this book. The context will make it clear which cost function is being dis‐
           cussed.                                                    