Keras does not include a depthwise max pooling layer, but TensorFlow’s low-level
Deep Learning API does: just use the tf.nn.max_pool() function, and specify the
kernel size and strides as 4-tuples (i.e., tuples of size 4). The first three values of each
should be 1: this indicates that the kernel size and stride along the batch, height, and
width dimensions should be 1. The last value should be whatever kernel size and
stride you want along the depth dimension—for example, 3 (this must be a divisor of
the input depth; it will not work if the previous layer outputs 20 feature maps, since
20 is not a multiple of 3):
output = tf.nn.max_pool(images,
ksize=(1, 1, 1, 3),
strides=(1, 1, 1, 3),
padding="valid")
If you want to include this as a layer in your Keras models, wrap it in a Lambda layer
(or create a custom Keras layer):
depth_pool = keras.layers.Lambda(
<b>lambda</b> X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),
padding="valid"))
One last type of pooling layer that you will often see in modern architectures is the
<i>global</i> <i>average</i> <i>pooling</i> <i>layer.</i> It works very differently: all it does is compute the mean
of each entire feature map (it’s like an average pooling layer using a pooling kernel
with the same spatial dimensions as the inputs). This means that it just outputs a sin‐
gle number per feature map and per instance. Although this is of course extremely
destructive (most of the information in the feature map is lost), it can be useful as the
output layer, as we will see later in this chapter. To create such a layer, simply use the
keras.layers.GlobalAvgPool2D class:
global_avg_pool = keras.layers.GlobalAvgPool2D()
It’s equivalent to this simple Lambda layer, which computes the mean over the spatial
dimensions (height and width):
global_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))
Now you know all the building blocks to create convolutional neural networks. Let’s
see how to assemble them.
<header><largefont><b>CNN</b></largefont> <largefont><b>Architectures</b></largefont></header>
Typical CNN architectures stack a few convolutional layers (each one generally fol‐
lowed by a ReLU layer), then a pooling layer, then another few convolutional layers
(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller
as it progresses through the network, but it also typically gets deeper and deeper (i.e.,
with more feature maps), thanks to the convolutional layers (see Figure 14-11). At the
top of the stack, a regular feedforward neural network is added, composed of a few