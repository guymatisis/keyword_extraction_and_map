                                                                      
                                                                      
                                                                      
                                                                      
          Policy Search                                               
                                                                      
          The algorithm a software agent uses to determine its actions is called its policy. The
          policy could be a neural network taking observations as inputs and outputting the
          action to take (see Figure 18-2).                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-2. Reinforcement Learning using a neural network policy
                                                                      
          The policy can be any algorithm you can think of, and it does not have to be deter‐
          ministic. In fact, in some cases it does not even have to observe the environment! For
          example, consider a robotic vacuum cleaner whose reward is the amount of dust it
          picks up in 30 minutes. Its policy could be to move forward with some probability p
          every second, or randomly rotate left or right with probability 1 – p. The rotation
          angle would be a random angle between –r and +r. Since this policy involves some
          randomness, it is called a stochastic policy. The robot will have an erratic trajectory,
          which guarantees that it will eventually get to any place it can reach and pick up all
          the dust. The question is, how much dust will it pick up in 30 minutes?
          How would you train such a robot? There are just two policy parameters you can
          tweak: the probability p and the angle range r. One possible learning algorithm could
          be to try out many different values for these parameters, and pick the combination
          that performs best (see Figure 18-3). This is an example of policy search, in this case
          using a brute force approach. When the policy space is too large (which is generally
          the case), finding a good set of parameters this way is like searching for a needle in a
          gigantic haystack.                                          
                                                                      
          Another way to explore the policy space is to use genetic algorithms. For example, you
          could randomly create a first generation of 100 policies and try them out, then “kill”
          the 80 worst policies6 and make the 20 survivors produce 4 offspring each. An
                                                                      
                                                                      
                                                                      
                                                                      
          6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the “gene
           pool.”                                                     