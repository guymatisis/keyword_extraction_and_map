<i>Equation</i> <i>4-6.</i> <i>Gradient</i> <i>vector</i> <i>of</i> <i>the</i> <i>cost</i> <i>function</i>
∂
MSE <b>θ</b>
∂θ
0
∂
MSE <b>θ</b>
2
∇ ∂θ ⊺
MSE <b>θ</b> = 1 = <b>X</b> <b>Xθ</b> − <b>y</b>
<b>θ</b>
<i>m</i>
⋮
∂
MSE <b>θ</b>
∂θ
<i>n</i>
Notice that this formula involves calculations over the full training
set <b>X,</b> at each Gradient Descent step! This is why the algorithm is
called <i>Batch</i> <i>Gradient</i> <i>Descent:</i> it uses the whole batch of training
data at every step (actually, <i>Full</i> <i>Gradient</i> <i>Descent</i> would probably
be a better name). As a result it is terribly slow on very large train‐
ing sets (but we will see much faster Gradient Descent algorithms
shortly). However, Gradient Descent scales well with the number of
features; training a Linear Regression model when there are hun‐
dreds of thousands of features is much faster using Gradient
Descent than using the Normal Equation or SVD decomposition.
Once you have the gradient vector, which points uphill, just go in the opposite direc‐
∇
tion to go downhill. This means subtracting MSE(θ) from <b>θ.</b> This is where the
<b>θ</b>
learning rate <i>η</i> comes into play: 5 multiply the gradient vector by <i>η</i> to determine the
size of the downhill step (Equation 4-7).
<i>Equation</i> <i>4-7.</i> <i>Gradient</i> <i>Descent</i> <i>step</i>
nextstep ∇
<b>θ</b> = <b>θ</b> − <i>η</i> MSE <b>θ</b>
<b>θ</b>
Let’s look at a quick implementation of this algorithm:
eta = 0.1 <i>#</i> <i>learning</i> <i>rate</i>
n_iterations = 1000
m = 100
theta = np.random.randn(2,1) <i>#</i> <i>random</i> <i>initialization</i>
<b>for</b> iteration <b>in</b> range(n_iterations):
gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
theta = theta - eta * gradients
5 Eta(η)istheseventhletteroftheGreekalphabet.