<i>bagging1</i>
pling is performed <i>with</i> replacement, this method is called (short for <i>boot‐</i>
<i>strap</i> <i>aggregating2).</i> When sampling is performed <i>without</i> replacement, it is called
<i>pasting.</i> 3
In other words, both bagging and pasting allow training instances to be sampled sev‐
eral times across multiple predictors, but only bagging allows training instances to be
sampled several times for the same predictor. This sampling and training process is
represented in Figure 7-4.
<i>Figure</i> <i>7-4.</i> <i>Bagging</i> <i>and</i> <i>pasting</i> <i>involves</i> <i>training</i> <i>several</i> <i>predictors</i> <i>on</i> <i>different</i> <i>random</i>
<i>samples</i> <i>of</i> <i>the</i> <i>training</i> <i>set</i>
Once all predictors are trained, the ensemble can make a prediction for a new
instance by simply aggregating the predictions of all predictors. The aggregation
function is typically the <i>statistical</i> <i>mode</i> (i.e., the most frequent prediction, just like a
hard voting classifier) for classification, or the average for regression. Each individual
predictor has a higher bias than if it were trained on the original training set, but
variance.4
aggregation reduces both bias and Generally, the net result is that the
ensemble has a similar bias but a lower variance than a single predictor trained on the
original training set.
1 LeoBreiman,“BaggingPredictors,”MachineLearning24,no.2(1996):123–140.
2 Instatistics,resamplingwithreplacementiscalledbootstrapping.
3 LeoBreiman,“PastingSmallVotesforClassificationinLargeDatabasesandOn-Line,”MachineLearning36,
no.1–2(1999):85–103.
4 BiasandvariancewereintroducedinChapter4.