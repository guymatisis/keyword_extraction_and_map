layer is applied independently at each time step and that the model will output a
sequence, not just a single vector.
All outputs are needed during training, but only the output at the last time step is
useful for predictions and for evaluation. So although we will rely on the MSE over all
the outputs for training, we will use a custom metric for evaluation, to only compute
the MSE over the output at the last time step:
<b>def</b> last_time_step_mse(Y_true, Y_pred):
<b>return</b> keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])
optimizer = keras.optimizers.Adam(lr=0.01)
model.compile(loss="mse", optimizer=optimizer, metrics=[last_time_step_mse])
We get a validation MSE of about 0.006, which is 25% better than the previous model.
You can combine this approach with the first one: just predict the next 10 values
using this RNN, then concatenate these values to the input time series and use the
model again to predict the next 10 values, and repeat the process as many times as
needed. With this approach, you can generate arbitrarily long sequences. It may not
be very accurate for long-term predictions, but it may be just fine if your goal is to
generate original music or text, as we will see in Chapter 16.
When forecasting time series, it is often useful to have some error
bars along with your predictions. For this, an efficient technique is
MC Dropout, introduced in Chapter 11: add an MC Dropout layer
within each memory cell, dropping part of the inputs and hidden
states. After training, to forecast a new time series, use the model
many times and compute the mean and standard deviation of the
predictions at each time step.
Simple RNNs can be quite good at forecasting time series or handling other kinds of
sequences, but they do not perform as well on long time series or sequences. Let’s dis‐
cuss why and see what we can do about it.
<header><largefont><b>Handling</b></largefont> <largefont><b>Long</b></largefont> <largefont><b>Sequences</b></largefont></header>
To train an RNN on long sequences, we must run it over many time steps, making the
unrolled RNN a very deep network. Just like any deep neural network it may suffer
from the unstable gradients problem, discussed in Chapter 11: it may take forever to
train, or training may be unstable. Moreover, when an RNN processes a long
sequence, it will gradually forget the first inputs in the sequence. Let’s look at both
these problems, starting with the unstable gradients problem.