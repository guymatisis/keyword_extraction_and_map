<i>Figure</i> <i>16-6.</i> <i>Neural</i> <i>machine</i> <i>translation</i> <i>using</i> <i>an</i> <i>Encoder–Decoder</i> <i>network</i> <i>with</i> <i>an</i>
<i>attention</i> <i>model</i>
But where do these <i>α</i> weights come from? It’s actually pretty simple: they are gener‐
(t,i)
ated by a type of small neural network called an <i>alignment</i> <i>model</i> (or an <i>attention</i>
<i>layer),</i> which is trained jointly with the rest of the Encoder–Decoder model. This
alignment model is illustrated on the righthand side of Figure 16-6. It starts with a
Dense
time-distributed layer 15 with a single neuron, which receives as input all the
encoder outputs, concatenated with the decoder’s previous hidden state (e.g., <b>h</b> ).
(2)
This layer outputs a score (or energy) for each encoder output (e.g., <i>e</i> ): this score
(3,2)
measures how well each output is aligned with the decoder’s previous hidden state.
Finally, all the scores go through a softmax layer to get a final weight for each encoder
output (e.g., <i>α</i> ). All the weights for a given decoder time step add up to 1 (since the
(3,2)
softmax layer is not time-distributed). This particular attention mechanism is called
<i>Bahdanau</i> <i>attention</i> (named after the paper’s first author). Since it concatenates the
encoder output with the decoder’s previous hidden state, it is sometimes called <i>con‐</i>
<i>catenative</i> <i>attention</i> (or <i>additive</i> <i>attention).</i>
Dense Dense
15 Recallthatatime-distributed layerisequivalenttoaregular layerthatyouapplyindependently
ateachtimestep(onlymuchfaster).