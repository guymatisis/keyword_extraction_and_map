                                                                      
                                                                      
                                                                      
                                                                      
          possible by subnetworks called inception modules,14 which allow GoogLeNet to use
          parameters much more efficiently than previous architectures: GoogLeNet actually
          has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).
                                                                      
          Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +
          1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and "same" padding. The input
          signal is first copied and fed to four different layers. All convolutional layers use the
          ReLU activation function. Note that the second set of convolutional layers uses differ‐
          ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different
          scales. Also note that every single layer uses a stride of 1 and "same" padding (even
          the max pooling layer), so their outputs all have the same height and width as their
          inputs. This makes it possible to concatenate all the outputs along the depth dimen‐
          sion in the final depth concatenation layer (i.e., stack the feature maps from all four
          top convolutional layers). This concatenation layer can be implemented in Tensor‐
          Flow using the tf.concat() operation, with axis=3 (the axis is the depth).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-13. Inception module                              
                                                                      
          You may wonder why inception modules have convolutional layers with 1 × 1 ker‐
          nels. Surely these layers cannot capture any features because they look at only one
          pixel at a time? In fact, the layers serve three purposes:  
           • Although they cannot capture spatial patterns, they can capture patterns along
            the depth dimension.                                      
           • They are configured to output fewer feature maps than their inputs, so they serve
            as bottleneck layers, meaning they reduce dimensionality. This cuts the computa‐
                                                                      
                                                                      
                                                                      
          14 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams;
           hence the name of these modules.                           