<i>modules,14</i>
possible by subnetworks called <i>inception</i> which allow GoogLeNet to use
parameters much more efficiently than previous architectures: GoogLeNet actually
has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million).
Figure 14-13 shows the architecture of an inception module. The notation “3 × 3 +
"same"
1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and padding. The input
signal is first copied and fed to four different layers. All convolutional layers use the
ReLU activation function. Note that the second set of convolutional layers uses differ‐
ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different
"same"
scales. Also note that every single layer uses a stride of 1 and padding (even
the max pooling layer), so their outputs all have the same height and width as their
inputs. This makes it possible to concatenate all the outputs along the depth dimen‐
sion in the final <i>depth</i> <i>concatenation</i> <i>layer</i> (i.e., stack the feature maps from all four
top convolutional layers). This concatenation layer can be implemented in Tensor‐
tf.concat() axis=3
Flow using the operation, with (the axis is the depth).
<i>Figure</i> <i>14-13.</i> <i>Inception</i> <i>module</i>
You may wonder why inception modules have convolutional layers with 1 × 1 ker‐
nels. Surely these layers cannot capture any features because they look at only one
pixel at a time? In fact, the layers serve three purposes:
• Although they cannot capture spatial patterns, they can capture patterns along
the depth dimension.
• They are configured to output fewer feature maps than their inputs, so they serve
as <i>bottleneck</i> <i>layers,</i> meaning they reduce dimensionality. This cuts the computa‐
14 Inthe2010movieInception,thecharacterskeepgoingdeeperanddeeperintomultiplelayersofdreams;
hencethenameofthesemodules.