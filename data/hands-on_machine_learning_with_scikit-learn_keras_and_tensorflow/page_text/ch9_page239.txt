<i>Figure</i> <i>9-2.</i> <i>An</i> <i>unlabeled</i> <i>dataset</i> <i>composed</i> <i>of</i> <i>five</i> <i>blobs</i> <i>of</i> <i>instances</i>
Let’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and
assign each instance to the closest blob:
<b>from</b> <b>sklearn.cluster</b> <b>import</b> KMeans
k = 5
kmeans = KMeans(n_clusters=k)
y_pred = kmeans.fit_predict(X)
Note that you have to specify the number of clusters <i>k</i> that the algorithm must find.
In this example, it is pretty obvious from looking at the data that <i>k</i> should be set to 5,
but in general it is not that easy. We will discuss this shortly.
Each instance was assigned to one of the five clusters. In the context of clustering, an
instance’s <i>label</i> is the index of the cluster that this instance gets assigned to by the
algorithm: this is not to be confused with the class labels in classification (remember
KMeans
that clustering is an unsupervised learning task). The instance preserves a
copy of the labels of the instances it was trained on, available via the labels_ instance
variable:
<b>>>></b> y_pred
array([4, 0, 1, ..., 2, 1, 0], dtype=int32)
<b>>>></b> y_pred <b>is</b> kmeans.labels_
True
We can also take a look at the five centroids that the algorithm found:
<b>>>></b> kmeans.cluster_centers_
array([[-2.80389616, 1.80117999],
[ 0.20876306, 2.25551336],
[-2.79290307, 2.79641063],
[-1.46679593, 2.28585348],
[-2.80037642, 1.30082566]])