<i>Figure</i> <i>17-12.</i> <i>Variational</i> <i>autoencoder</i> <i>(left)</i> <i>and</i> <i>an</i> <i>instance</i> <i>going</i> <i>through</i> <i>it</i> <i>(right)</i>
As you can see in the diagram, although the inputs may have a very convoluted distri‐
bution, a variational autoencoder tends to produce codings that look as though they
were sampled from a simple Gaussian distribution: 8 during training, the cost function
(discussed next) pushes the codings to gradually migrate within the coding space
(also called the <i>latent</i> <i>space)</i> to end up looking like a cloud of Gaussian points. One
great consequence is that after training a variational autoencoder, you can very easily
generate a new instance: just sample a random coding from the Gaussian distribu‐
tion, decode it, and voilà!
Now, let’s look at the cost function. It is composed of two parts. The first is the usual
reconstruction loss that pushes the autoencoder to reproduce its inputs (we can use
cross entropy for this, as discussed earlier). The second is the <i>latent</i> <i>loss</i> that pushes
the autoencoder to have codings that look as though they were sampled from a simple
Gaussian distribution: it is the KL divergence between the target distribution (i.e., the
Gaussian distribution) and the actual distribution of the codings. The math is a bit
more complex than with the sparse autoencoder, in particular because of the Gaus‐
sian noise, which limits the amount of information that can be transmitted to the
coding layer (thus pushing the autoencoder to learn useful features). Luckily, the
8 Variationalautoencodersareactuallymoregeneral;thecodingsarenotlimitedtoGaussiandistributions.