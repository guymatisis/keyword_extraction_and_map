to count the number of episodes, the number of steps taken, and most importantly
the average return per episode and the average episode length:
<b>from</b> <b>tf_agents.metrics</b> <b>import</b> tf_metrics
train_metrics = [
tf_metrics.NumberOfEpisodes(),
tf_metrics.EnvironmentSteps(),
tf_metrics.AverageReturnMetric(),
tf_metrics.AverageEpisodeLengthMetric(),
]
Discounting the rewards makes sense for training or to implement
a policy, as it makes it possible to balance the importance of imme‐
diate rewards with future rewards. However, once an episode is
over, we can evaluate how good it was overalls by summing the
AverageReturnMetric
<i>undiscounted</i> rewards. For this reason, the
computes the sum of undiscounted rewards for each episode, and it
keeps track of the streaming mean of these sums over all the epi‐
sodes it encounters.
At any time, you can get the value of each of these metrics by calling its result()
train_metrics[0].result()
method (e.g., ). Alternatively, you can log all metrics by
calling log_metrics(train_metrics) (this function is located in the
tf_agents.eval.metric_utils
package):
<b>>>></b> <b>from</b> <b>tf_agents.eval.metric_utils</b> <b>import</b> log_metrics
<b>>>></b> <b>import</b> <b>logging</b>
<b>>>></b> logging.get_logger().set_level(logging.INFO)
<b>>>></b> log_metrics(train_metrics)
[...]
NumberOfEpisodes = 0
EnvironmentSteps = 0
AverageReturn = 0.0
AverageEpisodeLength = 0.0
Next, let’s create the collect driver.
<header><largefont><b>Creating</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Collect</b></largefont> <largefont><b>Driver</b></largefont></header>
As we explored in Figure 18-13, a driver is an object that explores an environment
using a given policy, collects experiences, and broadcasts them to some observers. At
each step, the following things happen:
• The driver passes the current time step to the collect policy, which uses this time
step to choose an action and returns an <i>action</i> <i>step</i> object containing the action.