                                                                      
                                                                      
                                                                      
                                                                      
                   It is possible to use Gradient Boosting with other cost functions.
                   This is controlled by the loss hyperparameter (see Scikit-Learn’s
                   documentation for more details).                   
                                                                      
                                                                      
          It is worth noting that an optimized implementation of Gradient Boosting is available
          in the popular Python library XGBoost, which stands for Extreme Gradient Boosting.
          This package was initially developed by Tianqi Chen as part of the Distributed (Deep)
          Machine Learning Community (DMLC), and it aims to be extremely fast, scalable,
          and portable. In fact, XGBoost is often an important component of the winning
          entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:
                                                                      
            import xgboost                                            
            xgb_reg = xgboost.XGBRegressor()                          
            xgb_reg.fit(X_train, y_train)                             
            y_pred = xgb_reg.predict(X_val)                           
          XGBoost also offers several nice features, such as automatically taking care of early
          stopping:                                                   
            xgb_reg.fit(X_train, y_train,                             
                   eval_set=[(X_val, y_val)], early_stopping_rounds=2)
            y_pred = xgb_reg.predict(X_val)                           
          You should definitely check it out!                         
                                                                      
          Stacking                                                    
                                                                      
          The last Ensemble method we will discuss in this chapter is called stacking (short for
          stacked generalization).18 It is based on a simple idea: instead of using trivial functions
          (such as hard voting) to aggregate the predictions of all predictors in an ensemble,
          why don’t we train a model to perform this aggregation? Figure 7-12 shows such an
          ensemble performing a regression task on a new instance. Each of the bottom three
          predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor
          (called a blender, or a meta learner) takes these predictions as inputs and makes the
          final prediction (3.0).                                     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          18 David H. Wolpert, “Stacked Generalization,” Neural Networks 5, no. 2 (1992): 241–259.