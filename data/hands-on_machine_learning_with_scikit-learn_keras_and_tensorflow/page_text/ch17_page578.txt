tied_decoder = keras.models.Sequential([
DenseTranspose(dense_2, activation="selu"),
DenseTranspose(dense_1, activation="sigmoid"),
keras.layers.Reshape([28, 28])
])
tied_ae = keras.models.Sequential([tied_encoder, tied_decoder])
This model achieves a very slightly lower reconstruction error than the previous
model, with almost half the number of parameters.
<header><largefont><b>Training</b></largefont> <largefont><b>One</b></largefont> <largefont><b>Autoencoder</b></largefont> <largefont><b>at</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Time</b></largefont></header>
Rather than training the whole stacked autoencoder in one go like we just did, it is
possible to train one shallow autoencoder at a time, then stack all of them into a sin‐
gle stacked autoencoder (hence the name), as shown in Figure 17-7. This technique is
not used as much these days, but you may still run into papers that talk about “greedy
layerwise training,” so it’s good to know what it means.
<i>Figure</i> <i>17-7.</i> <i>Training</i> <i>one</i> <i>autoencoder</i> <i>at</i> <i>a</i> <i>time</i>
During the first phase of training, the first autoencoder learns to reconstruct the
inputs. Then we encode the whole training set using this first autoencoder, and this
gives us a new (compressed) training set. We then train a second autoencoder on this
new dataset. This is the second phase of training. Finally, we build a big sandwich
using all these autoencoders, as shown in Figure 17-7 (i.e., we first stack the hidden
layers of each autoencoder, then the output layers in reverse order). This gives us the
final stacked autoencoder (see the “Training One Autoencoder at a Time” section in
the notebook for an implementation). We could easily train more autoencoders this
way, building a very deep stacked autoencoder.