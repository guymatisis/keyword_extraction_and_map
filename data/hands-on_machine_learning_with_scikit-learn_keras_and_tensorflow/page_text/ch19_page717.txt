                                                                      
                                                                      
                                                                      
                                                                      
          decide which hyperparameter values to use during the next trial? Well, AI Platform
          just monitors the output directory (specified via --job-dir) for any event file (intro‐
          duced in Chapter 10) containing summaries for a metric named "accuracy" (or
          whatever metric name is specified as the hyperparameterMetricTag), and it reads
          those values. So your training code simply has to use the TensorBoard() callback
          (which you will want to do anyway for monitoring), and you’re good to go!
                                                                      
          Once the job is finished, all the hyperparameter values used in each trial and the
          resulting accuracy will be available in the job’s output (available via the AI Platform →
          Jobs page).                                                 
                                                                      
                   AI Platform jobs can also be used to efficiently execute your model
                   on large amounts of data: each worker can read part of the data
                   from GCS, make predictions, and save them to GCS.  
                                                                      
                                                                      
          Now you have all the tools and knowledge you need to create state-of-the-art neural
          net architectures and train them at scale using various distribution strategies, on your
          own infrastructure or on the cloud—and you can even perform powerful Bayesian
          optimization to fine-tune the hyperparameters!              
                                                                      
          Exercises                                                   
                                                                      
           1. What does a SavedModel contain? How do you inspect its content?
           2. When should you use TF Serving? What are its main features? What are some
            tools you can use to deploy it?                           
           3. How do you deploy a model across multiple TF Serving instances?
                                                                      
           4. When should you use the gRPC API rather than the REST API to query a model
            served by TF Serving?                                     
           5. What are the different ways TFLite reduces a model’s size to make it run on a
            mobile or embedded device?                                
           6. What is quantization-aware training, and why would you need it?
           7. What are model parallelism and data parallelism? Why is the latter generally
            recommended?                                              
                                                                      
           8. When training a model across multiple servers, what distribution strategies can
            you use? How do you choose which one to use?              
           9. Train a model (any model you like) and deploy it to TF Serving or Google Cloud
            AI Platform. Write the client code to query it using the REST API or the gRPC
                                                                      
                                                                      