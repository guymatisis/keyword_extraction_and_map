level API for building, training, evaluating, and running neural networks. But don’t
be fooled by its simplicity: it is expressive and flexible enough to let you build a wide
variety of neural network architectures. In fact, it will probably be sufficient for most
of your use cases. And should you ever need extra flexibility, you can always write
custom Keras components using its lower-level API, as we will see in Chapter 12.
But first, let’s go back in time to see how artificial neural networks came to be!
<header><largefont><b>From</b></largefont> <largefont><b>Biological</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Artificial</b></largefont> <largefont><b>Neurons</b></largefont></header>
Surprisingly, ANNs have been around for quite a while: they were first introduced
back in 1943 by the neurophysiologist Warren McCulloch and the mathematician
Walter Pitts. In their landmark paper2 “A Logical Calculus of Ideas Immanent in
Nervous Activity,” McCulloch and Pitts presented a simplified computational model
of how biological neurons might work together in animal brains to perform complex
computations using <i>propositional</i> <i>logic.</i> This was the first artificial neural network
architecture. Since then many other architectures have been invented, as we will see.
The early successes of ANNs led to the widespread belief that we would soon be con‐
versing with truly intelligent machines. When it became clear in the 1960s that this
promise would go unfulfilled (at least for quite a while), funding flew elsewhere, and
ANNs entered a long winter. In the early 1980s, new architectures were invented and
better training techniques were developed, sparking a revival of interest in <i>connec‐</i>
<i>tionism</i> (the study of neural networks). But progress was slow, and by the 1990s other
powerful Machine Learning techniques were invented, such as Support Vector
Machines (see Chapter 5). These techniques seemed to offer better results and stron‐
ger theoretical foundations than ANNs, so once again the study of neural networks
was put on hold.
We are now witnessing yet another wave of interest in ANNs. Will this wave die out
like the previous ones did? Well, here are a few good reasons to believe that this time
is different and that the renewed interest in ANNs will have a much more profound
impact on our lives:
• There is now a huge quantity of data available to train neural networks, and
ANNs frequently outperform other ML techniques on very large and complex
problems.
• The tremendous increase in computing power since the 1990s now makes it pos‐
sible to train large neural networks in a reasonable amount of time. This is in
part due to Moore’s law (the number of components in integrated circuits has
2 WarrenS.McCullochandWalterPitts,“ALogicalCalculusoftheIdeasImmanentinNervousActivity,”The
<i>BulletinofMathematicalBiology5,no.4(1943):115–113.</i>