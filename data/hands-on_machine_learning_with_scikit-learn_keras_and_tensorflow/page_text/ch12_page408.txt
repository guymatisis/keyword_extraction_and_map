tf__sum_squares() for
function, and it generates the function. In this function, the
loop is replaced by the definition of the loop_body() function (containing the body
for for_stmt()
of the original loop), followed by a call to the function. This call will
build the appropriate tf.while_loop() operation in the computation graph.
<i>Figure</i> <i>12-4.</i> <i>How</i> <i>TensorFlow</i> <i>generates</i> <i>graphs</i> <i>using</i> <i>AutoGraph</i> <i>and</i> <i>tracing</i>
Next, TensorFlow calls this “upgraded” function, but instead of passing the argument,
it passes a <i>symbolic</i> <i>tensor—a</i> tensor without any actual value, only a name, a data
sum_squares(tf.constant(10))
type, and a shape. For example, if you call , then the
tf__sum_squares() function will be called with a symbolic tensor of type int32 and
shape []. The function will run in <i>graph</i> <i>mode,</i> meaning that each TensorFlow opera‐
tion will add a node in the graph to represent itself and its output tensor(s) (as
opposed to the regular mode, called <i>eager</i> <i>execution,</i> or <i>eager</i> <i>mode).</i> In graph mode,
TF operations do not perform any computations. This should feel familiar if you
know TensorFlow 1, as graph mode was the default mode. In Figure 12-4, you can see
tf__sum_squares()
the function being called with a symbolic tensor as its argument
(in this case, an int32 tensor of shape []) and the final graph being generated during
tracing. The nodes represent operations, and the arrows represent tensors (both the
generated function and the graph are simplified).