By default, all variables and all operations will be placed on the first GPU
(named /gpu:0 ), except for variables and operations that don’t have a GPU kernel: 14
these are placed on the CPU (named /cpu:0 ). A tensor or variable’s device attribute
on:15
tells you which device it was placed
<b>>>></b> a = tf.Variable(42.0)
<b>>>></b> a.device
'/job:localhost/replica:0/task:0/device:GPU:0'
<b>>>></b> b = tf.Variable(42)
<b>>>></b> b.device
'/job:localhost/replica:0/task:0/device:CPU:0'
You can safely ignore the prefix /job:localhost/replica:0/task:0 for now (it
allows you to place operations on other machines when using a TensorFlow cluster;
we will talk about jobs, replicas, and tasks later in this chapter). As you can see, the
first variable was placed on GPU 0, which is the default device. However, the second
variable was placed on the CPU: this is because there are no GPU kernels for integer
variables (or for operations involving integer tensors), so TensorFlow fell back to the
CPU.
If you want to place an operation on a different device than the default one, use a
tf.device() context:
<b>>>></b> <b>with</b> tf.device("/cpu:0"):
<b>...</b> c = tf.Variable(42.0)
<b>...</b>
<b>>>></b> c.device
'/job:localhost/replica:0/task:0/device:CPU:0'
/cpu:0
The CPU is always treated as a single device ( ), even if your
machine has multiple CPU cores. Any operation placed on the
CPU may run in parallel across multiple cores if it has a multi‐
threaded kernel.
If you explicitly try to place an operation or variable on a device that does not exist or
for which there is no kernel, then you will get an exception. However, in some cases
you may prefer to fall back to the CPU; for example, if your program may run both
on CPU-only machines and on GPU machines, you may want TensorFlow to ignore
your tf.device("/gpu:*") on CPU-only machines. To do this, you can call tf.con
fig.set_soft_device_placement(True)
just after importing TensorFlow: when a
14 AswesawinChapter12,akernelisavariableoroperation’simplementationforaspecificdatatypeand
float32tf.matmul()
devicetype.Forexample,thereisaGPUkernelforthe operation,butthereisnoGPU
kernelfor int32tf.matmul() (onlyaCPUkernel).
tf.debugging.set_log_device_placement(True)
15 Youcanalsouse tologalldeviceplacements.