corresponding leaf node is the depth-2 left node, so the Decision Tree should output
the following probabilities: 0% for <i>Iris</i> <i>setosa</i> (0/54), 90.7% for <i>Iris</i> <i>versicolor</i> (49/54),
and 9.3% for <i>Iris</i> <i>virginica</i> (5/54). And if you ask it to predict the class, it should out‐
put <i>Iris</i> <i>versicolor</i> (class 1) because it has the highest probability. Let’s check this:
<b>>>></b> tree_clf.predict_proba([[5, 1.5]])
array([[0. , 0.90740741, 0.09259259]])
<b>>>></b> tree_clf.predict([[5, 1.5]])
array([1])
Perfect! Notice that the estimated probabilities would be identical anywhere else in
the bottom-right rectangle of Figure 6-2—for example, if the petals were 6 cm long
and 1.5 cm wide (even though it seems obvious that it would most likely be an <i>Iris</i>
<i>virginica</i> in this case).
<header><largefont><b>The</b></largefont> <largefont><b>CART</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Algorithm</b></largefont></header>
Scikit-Learn uses the <i>Classification</i> <i>and</i> <i>Regression</i> <i>Tree</i> (CART) algorithm to train
Decision Trees (also called “growing” trees). The algorithm works by first splitting the
training set into two subsets using a single feature <i>k</i> and a threshold <i>t</i> (e.g., “petal
<i>k</i>
length ≤ 2.45 cm”). How does it choose <i>k</i> and <i>t</i> ? It searches for the pair (k, <i>t</i> ) that
<i>k</i> <i>k</i>
produces the purest subsets (weighted by their size). Equation 6-2 gives the cost func‐
tion that the algorithm tries to minimize.
<i>Equation</i> <i>6-2.</i> <i>CART</i> <i>cost</i> <i>function</i> <i>for</i> <i>classification</i>
<i>m</i> <i>m</i>
left right
<i>J</i> <i>k,t</i> = <i>G</i> + <i>G</i>
<i>k</i> <i>m</i> left <i>m</i> right
<i>G</i> measures the impurity of the left/right subset,
left/right
where
<i>m</i> is the number of instances in the left/right subset.
left/right
Once the CART algorithm has successfully split the training set in two, it splits the
subsets using the same logic, then the sub-subsets, and so on, recursively. It stops
max_depth
recursing once it reaches the maximum depth (defined by the hyperpara‐
meter), or if it cannot find a split that will reduce impurity. A few other hyperparame‐
ters (described in a moment) control additional stopping conditions
min_samples_split min_samples_leaf min_weight_fraction_leaf
( , , , and
max_leaf_nodes ).