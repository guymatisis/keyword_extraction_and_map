<i>Figure</i> <i>5-1.</i> <i>Large</i> <i>margin</i> <i>classification</i>
Notice that adding more training instances “off the street” will not affect the decision
boundary at all: it is fully determined (or “supported”) by the instances located on the
edge of the street. These instances are called the <i>support</i> <i>vectors</i> (they are circled in
Figure 5-1).
<i>Figure</i> <i>5-2.</i> <i>Sensitivity</i> <i>to</i> <i>feature</i> <i>scales</i>
SVMs are sensitive to the feature scales, as you can see in
Figure 5-2: in the left plot, the vertical scale is much larger than the
horizontal scale, so the widest possible street is close to horizontal.
StandardScaler
After feature scaling (e.g., using Scikit-Learn’s ),
the decision boundary in the right plot looks much better.
<header><largefont><b>Soft</b></largefont> <largefont><b>Margin</b></largefont> <largefont><b>Classification</b></largefont></header>
If we strictly impose that all instances must be off the street and on the right side, this
is called <i>hard</i> <i>margin</i> <i>classification.</i> There are two main issues with hard margin clas‐
sification. First, it only works if the data is linearly separable. Second, it is sensitive to
outliers. Figure 5-3 shows the iris dataset with just one additional outlier: on the left,
it is impossible to find a hard margin; on the right, the decision boundary ends up
very different from the one we saw in Figure 5-1 without the outlier, and it will prob‐
ably not generalize as well.