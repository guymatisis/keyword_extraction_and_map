                                                                      
                                                                      
                                                                      
                                                                      
          information to make good predictions, or that the model is not powerful enough. As
          we saw in the previous chapter, the main ways to fix underfitting are to select a more
          powerful model, to feed the training algorithm with better features, or to reduce the
          constraints on the model. This model is not regularized, which rules out the last
          option. You could try to add more features (e.g., the log of the population), but first
          let’s try a more complex model to see how it does.          
                                                                      
          Let’s train a DecisionTreeRegressor. This is a powerful model, capable of finding
          complex nonlinear relationships in the data (Decision Trees are presented in more
          detail in Chapter 6). The code should look familiar by now: 
            from sklearn.tree import DecisionTreeRegressor            
            tree_reg = DecisionTreeRegressor()                        
            tree_reg.fit(housing_prepared, housing_labels)            
                                                                      
          Now that the model is trained, let’s evaluate it on the training set:
            >>> housing_predictions = tree_reg.predict(housing_prepared)
            >>> tree_mse = mean_squared_error(housing_labels, housing_predictions)
            >>> tree_rmse = np.sqrt(tree_mse)                         
            >>> tree_rmse                                             
            0.0                                                       
          Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,
          it is much more likely that the model has badly overfit the data. How can you be sure?
          As we saw earlier, you don’t want to touch the test set until you are ready to launch a
          model you are confident about, so you need to use part of the training set for training
          and part of it for model validation.                        
          Better Evaluation Using Cross-Validation                    
                                                                      
          One way to evaluate the Decision Tree model would be to use the
          train_test_split() function to split the training set into a smaller training set and a
          validation set, then train your models against the smaller training set and evaluate
          them against the validation set. It’s a bit of work, but nothing too difficult, and it
          would work fairly well.                                     
          A great alternative is to use Scikit-Learn’s K-fold cross-validation feature. The follow‐
          ing code randomly splits the training set into 10 distinct subsets called folds, then it
          trains and evaluates the Decision Tree model 10 times, picking a different fold for
          evaluation every time and training on the other 9 folds. The result is an array con‐
          taining the 10 evaluation scores:                           
            from sklearn.model_selection import cross_val_score       
            scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                           scoring="neg_mean_squared_error", cv=10)   
            tree_rmse_scores = np.sqrt(-scores)                       
                                                                      