<header><largefont><b>ResNet</b></largefont></header>
Kaiming He et al. won the ILSVRC 2015 challenge using a <i>Residual</i> <i>Network</i> (or
<i>ResNet),</i> 16 that delivered an astounding top-five error rate under 3.6%. The winning
variant used an extremely deep CNN composed of 152 layers (other variants had 34,
50, and 101 layers). It confirmed the general trend: models are getting deeper and
deeper, with fewer and fewer parameters. The key to being able to train such a deep
network is to use <i>skip</i> <i>connections</i> (also called <i>shortcut</i> <i>connections):</i> the signal feeding
into a layer is also added to the output of a layer located a bit higher up the stack. Let’s
see why this is useful.
When training a neural network, the goal is to make it model a target function <i>h(x).</i>
If you add the input <b>x</b> to the output of the network (i.e., you add a skip connection),
then the network will be forced to model <i>f(x)</i> = <i>h(x)</i> – <b>x</b> rather than <i>h(x).</i> This is
called <i>residual</i> <i>learning</i> (see Figure 14-15).
<i>Figure</i> <i>14-15.</i> <i>Residual</i> <i>learning</i>
When you initialize a regular neural network, its weights are close to zero, so the net‐
work just outputs values close to zero. If you add a skip connection, the resulting net‐
work just outputs a copy of its inputs; in other words, it initially models the identity
function. If the target function is fairly close to the identity function (which is often
the case), this will speed up training considerably.
Moreover, if you add many skip connections, the network can start making progress
even if several layers have not started learning yet (see Figure 14-16). Thanks to skip
connections, the signal can easily make its way across the whole network. The deep
residual network can be seen as a stack of <i>residual</i> <i>units</i> (RUs), where each residual
unit is a small neural network with a skip connection.
16 KaimingHeetal.,“DeepResidualLearningforImageRecognition,”arXivpreprintarXiv:1512:03385(2015).