distances and the indices of the <i>k</i> nearest neighbors in the training set (two matrices,
each with <i>k</i> columns):
<b>>>></b> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)
<b>>>></b> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]
<b>>>></b> y_pred[y_dist > 0.2] = -1
<b>>>></b> y_pred.ravel()
array([-1, 0, 1, -1])
<i>Figure</i> <i>9-15.</i> <i>Decision</i> <i>boundary</i> <i>between</i> <i>two</i> <i>clusters</i>
In short, DBSCAN is a very simple yet powerful algorithm capable of identifying any
number of clusters of any shape. It is robust to outliers, and it has just two hyperpara‐
eps min_samples
meters ( and ). If the density varies significantly across the clusters,
however, it can be impossible for it to capture all the clusters properly. Its computa‐
tional complexity is roughly <i>O(m</i> log <i>m),</i> making it pretty close to linear with regard
to the number of instances, but Scikit-Learn’s implementation can require up to
<i>O(m2)</i> eps
memory if is large.
You may also want to try <i>Hierarchical</i> <i>DBSCAN</i> (HDBSCAN),
which is implemented in the scikit-learn-contrib project.
<header><largefont><b>Other</b></largefont> <largefont><b>Clustering</b></largefont> <largefont><b>Algorithms</b></largefont></header>
Scikit-Learn implements several more clustering algorithms that you should take a
look at. We cannot cover them all in detail here, but here is a brief overview:
<i>Agglomerative</i> <i>clustering</i>
A hierarchy of clusters is built from the bottom up. Think of many tiny bubbles
floating on water and gradually attaching to each other until there’s one big group
of bubbles. Similarly, at each iteration, agglomerative clustering connects the
nearest pair of clusters (starting with individual instances). If you drew a tree