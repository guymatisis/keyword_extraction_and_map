<i>Figure</i> <i>15-4.</i> <i>Seq-to-seq</i> <i>(top</i> <i>left),</i> <i>seq-to-vector</i> <i>(top</i> <i>right),</i> <i>vector-to-seq</i> <i>(bottom</i> <i>left),</i>
<i>and</i> <i>Encoder–Decoder</i> <i>(bottom</i> <i>right)</i> <i>networks</i>
Sounds promising, but how do you train a recurrent neural network?
<header><largefont><b>Training</b></largefont> <largefont><b>RNNs</b></largefont></header>
To train an RNN, the trick is to unroll it through time (like we just did) and then
simply use regular backpropagation (see Figure 15-5). This strategy is called <i>backpro‐</i>
<i>pagation</i> <i>through</i> <i>time</i> (BPTT).
Just like in regular backpropagation, there is a first forward pass through the unrolled
network (represented by the dashed arrows). Then the output sequence is evaluated
using a cost function <i>C(Y</i> , <b>Y</b> , …Y ) (where <i>T</i> is the max time step). Note that this
(0) (1) (T)
cost function may ignore some outputs, as shown in Figure 15-5 (for example, in a
sequence-to-vector RNN, all outputs are ignored except for the very last one). The
gradients of that cost function are then propagated backward through the unrolled
network (represented by the solid arrows). Finally the model parameters are updated
using the gradients computed during BPTT. Note that the gradients flow backward
through all the outputs used by the cost function, not just through the final output
(for example, in Figure 15-5 the cost function is computed using the last three out‐
puts of the network, <b>Y</b> , <b>Y</b> , and <b>Y</b> , so gradients flow through these three outputs,
(2) (3) (4)