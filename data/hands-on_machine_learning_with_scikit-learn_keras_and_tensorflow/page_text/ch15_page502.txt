                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 15-4. Seq-to-seq (top left), seq-to-vector (top right), vector-to-seq (bottom left),
          and Encoder–Decoder (bottom right) networks                 
                                                                      
          Sounds promising, but how do you train a recurrent neural network?
                                                                      
          Training RNNs                                               
                                                                      
          To train an RNN, the trick is to unroll it through time (like we just did) and then
          simply use regular backpropagation (see Figure 15-5). This strategy is called backpro‐
          pagation through time (BPTT).                               
          Just like in regular backpropagation, there is a first forward pass through the unrolled
          network (represented by the dashed arrows). Then the output sequence is evaluated
          using a cost function C(Y , Y , …Y ) (where T is the max time step). Note that this
                         (0) (1) (T)                                  
          cost function may ignore some outputs, as shown in Figure 15-5 (for example, in a
          sequence-to-vector RNN, all outputs are ignored except for the very last one). The
          gradients of that cost function are then propagated backward through the unrolled
          network (represented by the solid arrows). Finally the model parameters are updated
          using the gradients computed during BPTT. Note that the gradients flow backward
          through all the outputs used by the cost function, not just through the final output
          (for example, in Figure 15-5 the cost function is computed using the last three out‐
          puts of the network, Y , Y , and Y , so gradients flow through these three outputs,
                       (2) (3) (4)                                    