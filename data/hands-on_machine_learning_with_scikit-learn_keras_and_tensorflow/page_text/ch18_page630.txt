                                                                      
                                                                      
                                                                      
                                                                      
          only partial knowledge of the MDP. In general we assume that the agent initially
          knows only the possible states and actions, and nothing more. The agent uses an
          exploration policy—for example, a purely random policy—to explore the MDP, and as
          it progresses, the TD Learning algorithm updates the estimates of the state values
          based on the transitions and rewards that are actually observed (see Equation 18-4).
                                                                      
            Equation 18-4. TD Learning algorithm                      
                                                                      
            V   s  1−α V s +α r+γ·V s′                                
             k+1       k        k                                     
            or, equivalently:                                         
            V   s  V s +α·δ s,r,s′                                    
             k+1    k    k                                            
            with δ s,r,s′ =r+γ·V s′ −V s                              
               k          k    k                                      
          In this equation:                                           
           • α is the learning rate (e.g., 0.01).                     
           • r + γ · V(s′) is called the TD target.                   
                 k                                                    
           • δ (s, r, s′) is called the TD error.                     
             k                                                        
          A more concise way of writing the first form of this equation is to use the notation
          a  b, which means a ← (1 – α) · a + α ·b. So, the first line of Equation 18-4 can
           α           k+1      k   k                                 
          be rewritten like this: V s r+γ·V s′ .                      
                          α                                           
                   TD Learning has many similarities with Stochastic Gradient
                   Descent, in particular the fact that it handles one sample at a time.
                   Moreover, just like Stochastic GD, it can only truly converge if you
                   gradually reduce the learning rate (otherwise it will keep bouncing
                   around the optimum Q-Values).                      
          For each state s, this algorithm simply keeps track of a running average of the imme‐
          diate rewards the agent gets upon leaving that state, plus the rewards it expects to get
          later (assuming it acts optimally).                         
          Q-Learning                                                  
                                                                      
          Similarly, the Q-Learning algorithm is an adaptation of the Q-Value Iteration algo‐
          rithm to the situation where the transition probabilities and the rewards are initially
          unknown (see Equation 18-5). Q-Learning works by watching an agent play (e.g.,
          randomly) and gradually improving its estimates of the Q-Values. Once it has
                                                                      
                                                                      