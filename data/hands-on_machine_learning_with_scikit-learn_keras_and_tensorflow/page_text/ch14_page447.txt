                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 14-1. Biological neurons in the visual cortex respond to specific patterns in small
          regions of the visual field called receptive fields; as the visual signal makes its way
          through consecutive brain modules, neurons respond to more complex patterns in larger
          receptive fields.                                           
                                                                      
          These studies of the visual cortex inspired the neocognitron,4 introduced in 1980,
          which gradually evolved into what we now call convolutional neural networks. An
          important milestone was a 1998 paper5 by Yann LeCun et al. that introduced the
          famous LeNet-5 architecture, widely used by banks to recognize handwritten check
          numbers. This architecture has some building blocks that you already know, such as
          fully connected layers and sigmoid activation functions, but it also introduces two
          new building blocks: convolutional layers and pooling layers. Let’s look at them now.
                                                                      
                   Why not simply use a deep neural network with fully connected
                   layers for image recognition tasks? Unfortunately, although this
                   works fine for small images (e.g., MNIST), it breaks down for
                   larger images because of the huge number of parameters it
                   requires. For example, a 100 × 100–pixel image has 10,000 pixels,
                   and if the first layer has just 1,000 neurons (which already severely
                   restricts the amount of information transmitted to the next layer),
                   this means a total of 10 million connections. And that’s just the first
                   layer. CNNs solve this problem using partially connected layers and
                   weight sharing.                                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          4 Kunihiko Fukushima, “Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern
           Recognition Unaffected by Shift in Position,” Biological Cybernetics 36 (1980): 193–202.
          5 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE 86,
           no. 11 (1998): 2278–2324.                                  