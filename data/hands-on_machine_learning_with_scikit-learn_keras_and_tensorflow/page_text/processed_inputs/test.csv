text|keyphrases
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                   CHAPTER 6          
                                                                      
                                           Decision   Trees           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Like SVMs, Decision Trees are versatile Machine Learning algorithms that can per‐
          form both classification and regression tasks, and even multioutput tasks. They are
          powerful algorithms, capable of fitting complex datasets. For example, in Chapter 2
          you trained a DecisionTreeRegressor model on the California housing dataset, fit‐
          ting it perfectly (actually, overfitting it).               
          Decision Trees are also the fundamental components of Random Forests (see Chap‐
          ter 7), which are among the most powerful Machine Learning algorithms available
          today.                                                      
                                                                      
          In this chapter we will start by discussing how to train, visualize, and make predic‐
          tions with Decision Trees. Then we will go through the CART training algorithm
          used by Scikit-Learn, and we will discuss how to regularize trees and use them for
          regression tasks. Finally, we will discuss some of the limitations of Decision Trees.
          Training and Visualizing a Decision Tree                    
                                                                      
          To understand Decision Trees, let’s build one and take a look at how it makes predic‐
          tions. The following code trains a DecisionTreeClassifier on the iris dataset (see
          Chapter 4):                                                 
                                                                      
            from sklearn.datasets import load_iris                    
            from sklearn.tree import DecisionTreeClassifier           
            iris = load_iris()                                        
            X = iris.data[:, 2:] # petal length and width             
            y = iris.target                                           
            tree_clf = DecisionTreeClassifier(max_depth=2)            
            tree_clf.fit(X, y)                                        
                                                                      "|training and visualizing; Decision Trees
"                                                                      
                                                                      
                                                                      
                                                                      
          You can visualize the trained Decision Tree by first using the export_graphviz()
          method to output a graph definition file called iris_tree.dot:
                                                                      
            from sklearn.tree import export_graphviz                  
            export_graphviz(                                          
                 tree_clf,                                            
                 out_file=image_path(""iris_tree.dot""),                
                 feature_names=iris.feature_names[2:],                
                 class_names=iris.target_names,                       
                 rounded=True,                                        
                 filled=True                                          
               )                                                      
          Then you can use the dot command-line tool from the Graphviz package to convert
          this .dot file to a variety of formats, such as PDF or PNG.1 This command line con‐
          verts the .dot file to a .png image file:                   
            $ dot -Tpng iris_tree.dot -o iris_tree.png                
          Your first Decision Tree looks like Figure 6-1.             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 6-1. Iris Decision Tree                              
                                                                      
          Making Predictions                                          
                                                                      
          Let’s see how the tree represented in Figure 6-1 makes predictions. Suppose you find
          an iris flower and you want to classify it. You start at the root node (depth 0, at the
          top): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,
          then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf
                                                                      
                                                                      
                                                                      
          1 Graphviz is an open source graph visualization software package, available at http://www.graphviz.org/."|leaf nodes; making predictions; root nodes
"                                                                      
                                                                      
                                                                      
                                                                      
          node (i.e., it does not have any child nodes), so it does not ask any questions: simply
          look at the predicted class for that node, and the Decision Tree predicts that your
          flower is an Iris setosa (class=setosa).                    
                                                                      
          Now suppose you find another flower, and this time the petal length is greater than
          2.45 cm. You must move down to the root’s right child node (depth 1, right), which is
          not a leaf node, so the node asks another question: is the petal width smaller than
          1.75 cm? If it is, then your flower is most likely an Iris versicolor (depth 2, left). If not,
          it is likely an Iris virginica (depth 2, right). It’s really that simple.
                                                                      
                   One of the many qualities of Decision Trees is that they require
                   very little data preparation. In fact, they don’t require feature scal‐
                   ing or centering at all.                           
                                                                      
                                                                      
          A node’s samples attribute counts how many training instances it applies to. For
          example, 100 training instances have a petal length greater than 2.45 cm (depth 1,
          right), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left). A
          node’s value attribute tells you how many training instances of each class this node
          applies to: for example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor,
          and 45 Iris virginica. Finally, a node’s gini attribute measures its impurity: a node is
          “pure” (gini=0) if all training instances it applies to belong to the same class. For
          example, since the depth-1 left node applies only to Iris setosa training instances, it is
          pure and its gini score is 0. Equation 6-1 shows how the training algorithm com‐
          putes the gini score G of the ith node. The depth-2 left node has a gini score equal to
                       i                                              
          1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168.                   
            Equation 6-1. Gini impurity                               
                  n                                                   
            G =1− ∑ p 2                                               
             i      i,k                                               
                 k=1                                                  
          In this equation:                                           
           • p is the ratio of class k instances among the training instances in the ith node.
             i,k                                                      
                                                                      
                   Scikit-Learn uses the CART algorithm, which produces only binary
                   trees: nonleaf nodes always have two children (i.e., questions only
                   have yes/no answers). However, other algorithms such as ID3 can
                   produce Decision Trees with nodes that have more than two
                   children.                                          
                                                                      "|Classification and Regression Tree (CART); impurity; binary trees; CART training algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
          Figure 6-2 shows this Decision Tree’s decision boundaries. The thick vertical line rep‐
          resents the decision boundary of the root node (depth 0): petal length = 2.45 cm.
          Since the lefthand area is pure (only Iris setosa), it cannot be split any further. How‐
          ever, the righthand area is impure, so the depth-1 right node splits it at petal width =
          1.75 cm (represented by the dashed line). Since max_depth was set to 2, the Decision
          Tree stops right there. If you set max_depth to 3, then the two depth-2 nodes would
          each add another decision boundary (represented by the dotted lines).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 6-2. Decision Tree decision boundaries               
                                                                      
                  Model Interpretation: White Box Versus Black Box    
                                                                      
           Decision Trees are intuitive, and their decisions are easy to interpret. Such models are
           often called white box models. In contrast, as we will see, Random Forests or neural
           networks are generally considered black box models. They make great predictions,
           and you can easily check the calculations that they performed to make these predic‐
           tions; nevertheless, it is usually hard to explain in simple terms why the predictions
           were made. For example, if a neural network says that a particular person appears on
           a picture, it is hard to know what contributed to this prediction: did the model recog‐
           nize that person’s eyes? Their mouth? Their nose? Their shoes? Or even the couch
           that they were sitting on? Conversely, Decision Trees provide nice, simple classifica‐
           tion rules that can even be applied manually if need be (e.g., for flower classification).
                                                                      
          Estimating Class Probabilities                              
                                                                      
          A Decision Tree can also estimate the probability that an instance belongs to a partic‐
          ular class k. First it traverses the tree to find the leaf node for this instance, and then it
          returns the ratio of training instances of class k in this node. For example, suppose
          you have found a flower whose petals are 5 cm long and 1.5 cm wide. The"|estimating class probabilities; white versus black box; white box models; black box models
"                                                                      
                                                                      
                                                                      
                                                                      
          corresponding leaf node is the depth-2 left node, so the Decision Tree should output
          the following probabilities: 0% for Iris setosa (0/54), 90.7% for Iris versicolor (49/54),
          and 9.3% for Iris virginica (5/54). And if you ask it to predict the class, it should out‐
          put Iris versicolor (class 1) because it has the highest probability. Let’s check this:
                                                                      
            >>> tree_clf.predict_proba([[5, 1.5]])                    
            array([[0. , 0.90740741, 0.09259259]])                    
            >>> tree_clf.predict([[5, 1.5]])                          
            array([1])                                                
          Perfect! Notice that the estimated probabilities would be identical anywhere else in
          the bottom-right rectangle of Figure 6-2—for example, if the petals were 6 cm long
          and 1.5 cm wide (even though it seems obvious that it would most likely be an Iris
          virginica in this case).                                    
          The CART Training Algorithm                                 
                                                                      
          Scikit-Learn uses the Classification and Regression Tree (CART) algorithm to train
          Decision Trees (also called “growing” trees). The algorithm works by first splitting the
          training set into two subsets using a single feature k and a threshold t (e.g., “petal
                                                    k                 
          length ≤ 2.45 cm”). How does it choose k and t? It searches for the pair (k, t) that
                                      k                 k             
          produces the purest subsets (weighted by their size). Equation 6-2 gives the cost func‐
          tion that the algorithm tries to minimize.                  
            Equation 6-2. CART cost function for classification       
                  m      m                                            
                   left   right                                       
             J k,t = G +    G                                         
               k   m  left m right                                    
                 G    measures the impurity of the left/right subset, 
                  left/right                                          
             where                                                    
                 m     is the number of instances in the left/right subset.
                  left/right                                          
          Once the CART algorithm has successfully split the training set in two, it splits the
          subsets using the same logic, then the sub-subsets, and so on, recursively. It stops
          recursing once it reaches the maximum depth (defined by the max_depth hyperpara‐
          meter), or if it cannot find a split that will reduce impurity. A few other hyperparame‐
          ters (described in a moment) control additional stopping conditions
          (min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and
          max_leaf_nodes).                                            
                                                                      
                                                                      "|Classification and Regression Tree (CART); CART training algorithm
"                                                                      
                                                                      
                                                                      
                                                                      
                   As you can see, the CART algorithm is a greedy algorithm: it greed‐
                   ily searches for an optimum split at the top level, then repeats the
                   process at each subsequent level. It does not check whether or not
                   the split will lead to the lowest possible impurity several levels
                   down. A greedy algorithm often produces a solution that’s reasona‐
                   bly good but not guaranteed to be optimal.         
                   Unfortunately, finding the optimal tree is known to be an NP-
                   Complete problem:2 it requires O(exp(m)) time, making the prob‐
                   lem intractable even for small training sets. This is why we must
                   settle for a “reasonably good” solution.           
          Computational Complexity                                    
                                                                      
                                                                      
          Making predictions requires traversing the Decision Tree from the root to a leaf.
          Decision Trees generally are approximately balanced, so traversing the Decision Tree
          requires going through roughly O(log (m)) nodes.3 Since each node only requires
                                 2                                    
          checking the value of one feature, the overall prediction complexity is O(log (m)),
                                                        2             
          independent of the number of features. So predictions are very fast, even when deal‐
          ing with large training sets.                               
          The training algorithm compares all features (or less if max_features is set) on all
          samples at each node. Comparing all features on all samples at each node results in a
          training complexity of O(n × m log (m)). For small training sets (less than a few thou‐
                              2                                       
          sand instances), Scikit-Learn can speed up training by presorting the data (set pre
          sort=True), but doing that slows down training considerably for larger training sets.
          Gini Impurity or Entropy?                                   
          By default, the Gini impurity measure is used, but you can select the entropy impurity
          measure instead by setting the criterion hyperparameter to ""entropy"". The concept
          of entropy originated in thermodynamics as a measure of molecular disorder:
          entropy approaches zero when molecules are still and well ordered. Entropy later
          spread to a wide variety of domains, including Shannon’s information theory, where it
          measures the average information content of a message:4 entropy is zero when all
          messages are identical. In Machine Learning, entropy is frequently used as an
                                                                      
          2 P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can
           be verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced
           in polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐
           tion is whether or not P = NP. If P ≠ NP (which seems likely), then no polynomial algorithm will ever be
           found for any NP-Complete problem (except perhaps on a quantum computer).
          3 log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).
          4 A reduction of entropy is often called an information gain."|presorting data with; impurity; Shannon's information theory; greedy algorithms; NP-Complete problem; computational complexity; Gini impurity versus entropy; Gini impurity measure; information theory; entropy impurity measure
"                                                                      
                                                                      
                                                                      
                                                                      
          impurity measure: a set’s entropy is zero when it contains instances of only one class.
          Equation 6-3 shows the definition of the entropy of the ith node. For example, the
          depth-2 left node in Figure 6-1 has an entropy equal to –(49/54) log (49/54) – (5/54)
                                                 2                    
          log (5/54) ≈ 0.445.                                         
            2                                                         
            Equation 6-3. Entropy                                     
                  n                                                   
            H = − ∑  p log p                                          
             i       i,k 2 i,k                                        
                 k=1                                                  
                 p ≠0                                                 
                 i,k                                                  
          So, should you use Gini impurity or entropy? The truth is, most of the time it does
          not make a big difference: they lead to similar trees. Gini impurity is slightly faster to
          compute, so it is a good default. However, when they differ, Gini impurity tends to
          isolate the most frequent class in its own branch of the tree, while entropy tends to
          produce slightly more balanced trees.5                      
          Regularization Hyperparameters                              
          Decision Trees make very few assumptions about the training data (as opposed to lin‐
          ear models, which assume that the data is linear, for example). If left unconstrained,
          the tree structure will adapt itself to the training data, fitting it very closely—indeed,
          most likely overfitting it. Such a model is often called a nonparametric model, not
          because it does not have any parameters (it often has a lot) but because the number of
          parameters is not determined prior to training, so the model structure is free to stick
          closely to the data. In contrast, a parametric model, such as a linear model, has a pre‐
          determined number of parameters, so its degree of freedom is limited, reducing the
          risk of overfitting (but increasing the risk of underfitting).
          To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom
          during training. As you know by now, this is called regularization. The regularization
          hyperparameters depend on the algorithm used, but generally you can at least restrict
          the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the
          max_depth hyperparameter (the default value is None, which means unlimited).
          Reducing max_depth will regularize the model and thus reduce the risk of overfitting.
          The DecisionTreeClassifier class has a few other parameters that similarly restrict
          the shape of the Decision Tree: min_samples_split (the minimum number of sam‐
          ples a node must have before it can be split), min_samples_leaf (the minimum num‐
          ber of samples a leaf node must have), min_weight_fraction_leaf (same as
                                                                      
                                                                      
                                                                      
          5 See Sebastian Raschka’s interesting analysis for more details."|regularization hyperparameters; max_depth hyperparameter; nonparametric models; parametric models; hyperparameters for Decision Trees; parametric versus nonparametric
"                                                                      
                                                                      
                                                                      
                                                                      
          min_samples_leaf but expressed as a fraction of the total number of weighted
          instances), max_leaf_nodes (the maximum number of leaf nodes), and max_features
          (the maximum number of features that are evaluated for splitting at each node).
          Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize
          the model.                                                  
                                                                      
                   Other algorithms work by first training the Decision Tree without
                   restrictions, then pruning (deleting) unnecessary nodes. A node
                   whose children are all leaf nodes is considered unnecessary if the
                   purity improvement it provides is not statistically significant. Stan‐
                   dard statistical tests, such as the χ2 test (chi-squared test), are used
                   to estimate the probability that the improvement is purely the
                   result of chance (which is called the null hypothesis). If this proba‐
                   bility, called the p-value, is higher than a given threshold (typically
                   5%, controlled by a hyperparameter), then the node is considered
                   unnecessary and its children are deleted. The pruning continues
                   until all unnecessary nodes have been pruned.      
          Figure 6-3 shows two Decision Trees trained on the moons dataset (introduced in
          Chapter 5). On the left the Decision Tree is trained with the default hyperparameters
          (i.e., no restrictions), and on the right it’s trained with min_samples_leaf=4. It is
          quite obvious that the model on the left is overfitting, and the model on the right will
          probably generalize better.                                 
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 6-3. Regularization using min_samples_leaf           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|pruning; chi-squared test; p-value; null hypothesis; statistical significance
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Regression                                                  
                                                                      
          Decision Trees are also capable of performing regression tasks. Let’s build a regres‐
          sion tree using Scikit-Learn’s DecisionTreeRegressor class, training it on a noisy
          quadratic dataset with max_depth=2:                         
                                                                      
            from sklearn.tree import DecisionTreeRegressor            
            tree_reg = DecisionTreeRegressor(max_depth=2)             
            tree_reg.fit(X, y)                                        
          The resulting tree is represented in Figure 6-4.            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 6-4. A Decision Tree for regression                  
                                                                      
          This tree looks very similar to the classification tree you built earlier. The main differ‐
          ence is that instead of predicting a class in each node, it predicts a value. For example,
          suppose you want to make a prediction for a new instance with x = 0.6. You traverse
                                                1                     
          the tree starting at the root, and you eventually reach the leaf node that predicts
          value=0.111. This prediction is the average target value of the 110 training instances
          associated with this leaf node, and it results in a mean squared error equal to 0.015
          over these 110 instances.                                   
          This model’s predictions are represented on the left in Figure 6-5. If you set
          max_depth=3, you get the predictions represented on the right. Notice how the pre‐
          dicted value for each region is always the average target value of the instances in that
          region. The algorithm splits each region in a way that makes most training instances
          as close as possible to that predicted value.               
                                                                      "|mean squared error; DecisionTreeRegressor class; regression tasks; Decision Trees
"                                                                      
                                                                      
                                                                      
                                                                      
          Instability                                                 
                                                                      
          Hopefully by now you are convinced that Decision Trees have a lot going for them:
          they are simple to understand and interpret, easy to use, versatile, and powerful.
          However, they do have a few limitations. First, as you may have noticed, Decision
          Trees love orthogonal decision boundaries (all splits are perpendicular to an axis),
          which makes them sensitive to training set rotation. For example, Figure 6-7 shows a
          simple linearly separable dataset: on the left, a Decision Tree can split it easily, while
          on the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐
          sarily convoluted. Although both Decision Trees fit the training set perfectly, it is very
          likely that the model on the right will not generalize well. One way to limit this prob‐
          lem is to use Principal Component Analysis (see Chapter 8), which often results in a
          better orientation of the training data.                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 6-7. Sensitivity to training set rotation            
          More generally, the main issue with Decision Trees is that they are very sensitive to
          small variations in the training data. For example, if you just remove the widest Iris
          versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)
          and train a new Decision Tree, you may get the model represented in Figure 6-8. As
          you can see, it looks very different from the previous Decision Tree (Figure 6-2).
          Actually, since the training algorithm used by Scikit-Learn is stochastic,6 you may
          get very different models even on the same training data (unless you set the
          random_state hyperparameter).                               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          6 It randomly selects the set of features to evaluate at each node."|instability; instability drawbacks; random_state hyperparameter; training set rotation
"                                                                      
                                                                      
                                                                      
                                                                      
             c. Use grid search with cross-validation (with the help of the GridSearchCV
              class) to find good hyperparameter values for a DecisionTreeClassifier.
              Hint: try various values for max_leaf_nodes.            
                                                                      
            d. Train it on the full training set using these hyperparameters, and measure
              your model’s performance on the test set. You should get roughly 85% to 87%
              accuracy.                                               
           8. Grow a forest by following these steps:                 
             a. Continuing the previous exercise, generate 1,000 subsets of the training set,
              each containing 100 instances selected randomly. Hint: you can use Scikit-
              Learn’s ShuffleSplit class for this.                    
            b. Train one Decision Tree on each subset, using the best hyperparameter values
              found in the previous exercise. Evaluate these 1,000 Decision Trees on the test
              set. Since they were trained on smaller sets, these Decision Trees will likely
              perform worse than the first Decision Tree, achieving only about 80%
              accuracy.                                               
                                                                      
             c. Now comes the magic. For each test set instance, generate the predictions of
              the 1,000 Decision Trees, and keep only the most frequent prediction (you can
              use SciPy’s mode() function for this). This approach gives you majority-vote
              predictions over the test set.                          
            d. Evaluate these predictions on the test set: you should obtain a slightly higher
              accuracy than your first model (about 0.5 to 1.5% higher). Congratulations,
              you have trained a Random Forest classifier!            
          Solutions to these exercises are available in Appendix A.   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|majority-vote predictions
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 10          
                                                                      
            Introduction   to Artificial Neural  Networks             
                                                                      
                                               with  Keras            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Birds inspired us to fly, burdock plants inspired Velcro, and nature has inspired
          countless more inventions. It seems only logical, then, to look at the brain’s architec‐
          ture for inspiration on how to build an intelligent machine. This is the logic that
          sparked artificial neural networks (ANNs): an ANN is a Machine Learning model
          inspired by the networks of biological neurons found in our brains. However,
          although planes were inspired by birds, they don’t have to flap their wings. Similarly,
          ANNs have gradually become quite different from their biological cousins. Some
          researchers even argue that we should drop the biological analogy altogether (e.g., by
          saying “units” rather than “neurons”), lest we restrict our creativity to biologically
          plausible systems.1                                         
          ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala‐
          ble, making them ideal to tackle large and highly complex Machine Learning tasks
          such as classifying billions of images (e.g., Google Images), powering speech recogni‐
          tion services (e.g., Apple’s Siri), recommending the best videos to watch to hundreds
          of millions of users every day (e.g., YouTube), or learning to beat the world champion
          at the game of Go (DeepMind’s AlphaGo).                     
          The first part of this chapter introduces artificial neural networks, starting with a
          quick tour of the very first ANN architectures and leading up to Multilayer Percep‐
          trons (MLPs), which are heavily used today (other architectures will be explored in
          the next chapters). In the second part, we will look at how to implement neural net‐
          works using the popular Keras API. This is a beautifully designed and simple high-
                                                                      
                                                                      
          1 You can get the best of both worlds by being open to biological inspirations without being afraid to create
           biologically unrealistic models, as long as they work well."|anomaly detection
"                                                                      
                                                                      
                                                                      
                                                                      
          level API for building, training, evaluating, and running neural networks. But don’t
          be fooled by its simplicity: it is expressive and flexible enough to let you build a wide
          variety of neural network architectures. In fact, it will probably be sufficient for most
          of your use cases. And should you ever need extra flexibility, you can always write
          custom Keras components using its lower-level API, as we will see in Chapter 12.
                                                                      
          But first, let’s go back in time to see how artificial neural networks came to be!
          From Biological to Artificial Neurons                       
                                                                      
          Surprisingly, ANNs have been around for quite a while: they were first introduced
          back in 1943 by the neurophysiologist Warren McCulloch and the mathematician
          Walter Pitts. In their landmark paper2 “A Logical Calculus of Ideas Immanent in
          Nervous Activity,” McCulloch and Pitts presented a simplified computational model
          of how biological neurons might work together in animal brains to perform complex
          computations using propositional logic. This was the first artificial neural network
          architecture. Since then many other architectures have been invented, as we will see.
                                                                      
          The early successes of ANNs led to the widespread belief that we would soon be con‐
          versing with truly intelligent machines. When it became clear in the 1960s that this
          promise would go unfulfilled (at least for quite a while), funding flew elsewhere, and
          ANNs entered a long winter. In the early 1980s, new architectures were invented and
          better training techniques were developed, sparking a revival of interest in connec‐
          tionism (the study of neural networks). But progress was slow, and by the 1990s other
          powerful Machine Learning techniques were invented, such as Support Vector
          Machines (see Chapter 5). These techniques seemed to offer better results and stron‐
          ger theoretical foundations than ANNs, so once again the study of neural networks
          was put on hold.                                            
          We are now witnessing yet another wave of interest in ANNs. Will this wave die out
          like the previous ones did? Well, here are a few good reasons to believe that this time
          is different and that the renewed interest in ANNs will have a much more profound
          impact on our lives:                                        
                                                                      
           • There is now a huge quantity of data available to train neural networks, and
            ANNs frequently outperform other ML techniques on very large and complex
            problems.                                                 
           • The tremendous increase in computing power since the 1990s now makes it pos‐
            sible to train large neural networks in a reasonable amount of time. This is in
            part due to Moore’s law (the number of components in integrated circuits has
                                                                      
                                                                      
          2 Warren S. McCulloch and Walter Pitts, “A Logical Calculus of the Ideas Immanent in Nervous Activity,” The
           Bulletin of Mathematical Biology 5, no. 4 (1943): 115–113. "|propositional logic; connectionism; from biological to artificial neurons; biological neurons; from biological to artificial
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-1. Biological neuron4                             
                                                                      
          Thus, individual biological neurons seem to behave in a rather simple way, but they
          are organized in a vast network of billions, with each neuron typically connected to
          thousands of other neurons. Highly complex computations can be performed by a
          network of fairly simple neurons, much like a complex anthill can emerge from the
          combined efforts of simple ants. The architecture of biological neural networks
          (BNNs)5 is still the subject of active research, but some parts of the brain have been
          mapped, and it seems that neurons are often organized in consecutive layers, espe‐
          cially in the cerebral cortex (i.e., the outer layer of your brain), as shown in
          Figure 10-2.                                                
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          4 Image by Bruce Blaus (Creative Commons 3.0). Reproduced from https://en.wikipedia.org/wiki/Neuron.
          5 In the context of Machine Learning, the phrase “neural networks” generally refers to ANNs, not BNNs."|biological neural networks (BNN)
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-2. Multiple layers in a biological neural network (human cortex)6
                                                                      
          Logical Computations with Neurons                           
          McCulloch and Pitts proposed a very simple model of the biological neuron, which
          later became known as an artificial neuron: it has one or more binary (on/off) inputs
          and one binary output. The artificial neuron activates its output when more than a
          certain number of its inputs are active. In their paper, they showed that even with
          such a simplified model it is possible to build a network of artificial neurons that
          computes any logical proposition you want. To see how such a network works, let’s
          build a few ANNs that perform various logical computations (see Figure 10-3),
          assuming that a neuron is activated when at least two of its inputs are active.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-3. ANNs performing simple logical computations    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          6 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe
           dia.org/wiki/Cerebral_cortex.                              "|logical computations; logical computations with; artificial neurons
"                                                                      
                                                                      
                                                                      
                                                                      
          Let’s see what these networks do:                           
                                                                      
           • The first network on the left is the identity function: if neuron A is activated,
            then neuron C gets activated as well (since it receives two input signals from neu‐
            ron A); but if neuron A is off, then neuron C is off as well.
           • The second network performs a logical AND: neuron C is activated only when
            both neurons A and B are activated (a single input signal is not enough to acti‐
            vate neuron C).                                           
           • The third network performs a logical OR: neuron C gets activated if either neu‐
            ron A or neuron B is activated (or both).                 
                                                                      
           • Finally, if we suppose that an input connection can inhibit the neuron’s activity
            (which is the case with biological neurons), then the fourth network computes a
            slightly more complex logical proposition: neuron C is activated only if neuron A
            is active and neuron B is off. If neuron A is active all the time, then you get a
            logical NOT: neuron C is active when neuron B is off, and vice versa.
          You can imagine how these networks can be combined to compute complex logical
          expressions (see the exercises at the end of the chapter for an example).
                                                                      
          The Perceptron                                              
                                                                      
          The Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank
          Rosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called
          a threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). The inputs
          and output are numbers (instead of binary on/off values), and each input connection
          is associated with a weight. The TLU computes a weighted sum of its inputs (z = w x
                                                          1 1         
          + w x + ⋯ + w x = x⊺ w), then applies a step function to that sum and outputs the
            2 2     n n                                               
          result: h (x) = step(z), where z = x⊺ w.                    
               w                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-4. Threshold logic unit: an artificial neuron which computes a weighted sum
          of its inputs then applies a step function                  
                                                                      
                                                                      "|threshold logic unit (TLU); step function; Perceptron
"                                                                      
                                                                      
                                                                      
                                                                      
          The most common step function used in Perceptrons is the Heaviside step function
          (see Equation 10-1). Sometimes the sign function is used instead.
                                                                      
            Equation 10-1. Common step functions used in Perceptrons (assuming threshold =
            0)                                                        
                                    −1 if z<0                         
                     0 if z<0                                         
            heaviside z =     sgn z = 0 if z=0                        
                     1 if z≥0                                         
                                    +1 if z>0                         
          A single TLU can be used for simple linear binary classification. It computes a linear
          combination of the inputs, and if the result exceeds a threshold, it outputs the posi‐
          tive class. Otherwise it outputs the negative class (just like a Logistic Regression or
          linear SVM classifier). You could, for example, use a single TLU to classify iris flowers
          based on petal length and width (also adding an extra bias feature x = 1, just like we
                                                  0                   
          did in previous chapters). Training a TLU in this case means finding the right values
          for w , w , and w (the training algorithm is discussed shortly).
             0 1    2                                                 
          A Perceptron is simply composed of a single layer of TLUs,7 with each TLU connected
          to all the inputs. When all the neurons in a layer are connected to every neuron in the
          previous layer (i.e., its input neurons), the layer is called a fully connected layer, or a
          dense layer. The inputs of the Perceptron are fed to special passthrough neurons
          called input neurons: they output whatever input they are fed. All the input neurons
          form the input layer. Moreover, an extra bias feature is generally added (x = 1): it is
                                                     0                
          typically represented using a special type of neuron called a bias neuron, which out‐
          puts 1 all the time. A Perceptron with two inputs and three outputs is represented in
          Figure 10-5. This Perceptron can classify instances simultaneously into three different
          binary classes, which makes it a multioutput classifier.    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          7 The name Perceptron is sometimes used to mean a tiny network with a single TLU."|input neurons; bias neurons; dense layer; Heaviside step function; dense (fully connected) layer; fully connected layer
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-5. Architecture of a Perceptron with two input neurons, one bias neuron, and
          three output neurons                                        
                                                                      
          Thanks to the magic of linear algebra, Equation 10-2 makes it possible to efficiently
          compute the outputs of a layer of artificial neurons for several instances at once.
                                                                      
            Equation 10-2. Computing the outputs of a fully connected layer
            h  X =ϕ XW+b                                              
             W,b                                                      
          In this equation:                                           
                                                                      
           • As always, X represents the matrix of input features. It has one row per instance
            and one column per feature.                               
           • The weight matrix W contains all the connection weights except for the ones
            from the bias neuron. It has one row per input neuron and one column per artifi‐
            cial neuron in the layer.                                 
                                                                      
           • The bias vector b contains all the connection weights between the bias neuron
            and the artificial neurons. It has one bias term per artificial neuron.
           • The function ϕ is called the activation function: when the artificial neurons are
            TLUs, it is a step function (but we will discuss other activation functions shortly).
                                                                      
          So, how is a Perceptron trained? The Perceptron training algorithm proposed by
          Rosenblatt was largely inspired by Hebb’s rule. In his 1949 book The Organization of
          Behavior (Wiley), Donald Hebb suggested that when a biological neuron triggers
          another neuron often, the connection between these two neurons grows stronger. Sie‐
          grid Löwel later summarized Hebb’s idea in the catchy phrase, “Cells that fire
          together, wire together”; that is, the connection weight between two neurons tends to
          increase when they fire simultaneously. This rule later became known as Hebb’s rule
          (or Hebbian learning). Perceptrons are trained using a variant of this rule that takes
          into account the error made by the network when it makes a prediction; the"|Hebb's rule
"                                                                      
                                                                      
                                                                      
                                                                      
          Perceptron learning rule reinforces connections that help reduce the error. More
          specifically, the Perceptron is fed one training instance at a time, and for each
          instance it makes its predictions. For every output neuron that produced a wrong
          prediction, it reinforces the connection weights from the inputs that would have con‐
          tributed to the correct prediction. The rule is shown in Equation 10-3.
                                                                      
            Equation 10-3. Perceptron learning rule (weight update)   
                                                                      
              next step                                               
            w      =w  +η y −y x                                      
             i,j      i,j j  j i                                      
          In this equation:                                           
           • w is the connection weight between the ith input neuron and the jth output
             i, j                                                     
            neuron.                                                   
           • x is the ith input value of the current training instance.
             i                                                        
           • y is the output of the jth output neuron for the current training instance.
             j                                                        
           • y is the target output of the jth output neuron for the current training instance.
             j                                                        
           • η is the learning rate.                                  
          The decision boundary of each output neuron is linear, so Perceptrons are incapable
          of learning complex patterns (just like Logistic Regression classifiers). However, if the
          training instances are linearly separable, Rosenblatt demonstrated that this algorithm
          would converge to a solution.8 This is called the Perceptron convergence theorem.
          Scikit-Learn provides a Perceptron class that implements a single-TLU network. It
          can be used pretty much as you would expect—for example, on the iris dataset (intro‐
          duced in Chapter 4):                                        
            import numpy as np                                        
            from sklearn.datasets import load_iris                    
            from sklearn.linear_model import Perceptron               
            iris = load_iris()                                        
            X = iris.data[:, (2, 3)] # petal length, petal width      
            y = (iris.target == 0).astype(np.int) # Iris setosa?      
            per_clf = Perceptron()                                    
            per_clf.fit(X, y)                                         
            y_pred = per_clf.predict([[2, 0.5]])                      
                                                                      
                                                                      
          8 Note that this solution is not unique: when data points are linearly separable, there is an infinity of hyper‐
           planes that can separate them.                             "|Hebbian learning; Perceptron convergence theorem; Perceptron class
"                                                                      
                                                                      
                                                                      
                                                                      
          You may have noticed that the Perceptron learning algorithm strongly resembles Sto‐
          chastic Gradient Descent. In fact, Scikit-Learn’s Perceptron class is equivalent to
          using an SGDClassifier with the following hyperparameters: loss=""perceptron"",
          learning_rate=""constant"", eta0=1 (the learning rate), and penalty=None (no
          regularization).                                            
                                                                      
          Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class
          probability; rather, they make predictions based on a hard threshold. This is one rea‐
          son to prefer Logistic Regression over Perceptrons.         
          In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert highligh‐
          ted a number of serious weaknesses of Perceptrons—in particular, the fact that they
          are incapable of solving some trivial problems (e.g., the Exclusive OR (XOR) classifi‐
          cation problem; see the left side of Figure 10-6). This is true of any other linear classi‐
          fication model (such as Logistic Regression classifiers), but researchers had expected
          much more from Perceptrons, and some were so disappointed that they dropped
          neural networks altogether in favor of higher-level problems such as logic, problem
          solving, and search.                                        
          It turns out that some of the limitations of Perceptrons can be eliminated by stacking
          multiple Perceptrons. The resulting ANN is called a Multilayer Perceptron (MLP). An
          MLP can solve the XOR problem, as you can verify by computing the output of the
          MLP represented on the right side of Figure 10-6: with inputs (0, 0) or (1, 1), the net‐
          work outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. All connections have a
          weight equal to 1, except the four connections where the weight is shown. Try verify‐
          ing that this network indeed solves the XOR problem!        
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-6. XOR classification problem and an MLP that solves it
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|Exclusive OR (XOR) classification problem; Perceptron
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          The Multilayer Perceptron and Backpropagation               
                                                                      
          An MLP is composed of one (passthrough) input layer, one or more layers of TLUs,
          called hidden layers, and one final layer of TLUs called the output layer (see
          Figure 10-7). The layers close to the input layer are usually called the lower layers, and
          the ones close to the outputs are usually called the upper layers. Every layer except the
          output layer includes a bias neuron and is fully connected to the next layer.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-7. Architecture of a Multilayer Perceptron with two inputs, one hidden layer of
          four neurons, and three output neurons (the bias neurons are shown here, but usually
          they are implicit)                                          
                                                                      
                   The signal flows only in one direction (from the inputs to the out‐
                   puts), so this architecture is an example of a feedforward neural net‐
                   work (FNN).                                        
                                                                      
                                                                      
          When an ANN contains a deep stack of hidden layers,9 it is called a deep neural net‐
          work (DNN). The field of Deep Learning studies DNNs, and more generally models
          containing deep stacks of computations. Even so, many people talk about Deep
          Learning whenever neural networks are involved (even shallow ones).
                                                                      
          For many years researchers struggled to find a way to train MLPs, without success.
          But in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a
                                                                      
                                                                      
                                                                      
          9 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see
           ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy."|hidden layers in MLPs; input layer; hidden layer; backpropagation; feedforward neural networks (FNNs); deep neural networks (DNNs); output layers; input layers; output layer
"                                                                      
                                                                      
                                                                      
                                                                      
          groundbreaking paper10 that introduced the backpropagation training algorithm,
          which is still used today. In short, it is Gradient Descent (introduced in Chapter 4)
          using an efficient technique for computing the gradients automatically:11 in just two
          passes through the network (one forward, one backward), the backpropagation algo‐
          rithm is able to compute the gradient of the network’s error with regard to every sin‐
          gle model parameter. In other words, it can find out how each connection weight and
          each bias term should be tweaked in order to reduce the error. Once it has these gra‐
          dients, it just performs a regular Gradient Descent step, and the whole process is
          repeated until the network converges to the solution.       
                                                                      
                   Automatically computing gradients is called automatic differentia‐
                   tion, or autodiff. There are various autodiff techniques, with differ‐
                   ent pros and cons. The one used by backpropagation is called
                   reverse-mode autodiff. It is fast and precise, and is well suited when
                   the function to differentiate has many variables (e.g., connection
                   weights) and few outputs (e.g., one loss). If you want to learn more
                   about autodiff, check out Appendix D.              
                                                                      
          Let’s run through this algorithm in a bit more detail:      
           • It handles one mini-batch at a time (for example, containing 32 instances each),
            and it goes through the full training set multiple times. Each pass is called an
            epoch.                                                    
           • Each mini-batch is passed to the network’s input layer, which sends it to the first
            hidden layer. The algorithm then computes the output of all the neurons in this
            layer (for every instance in the mini-batch). The result is passed on to the next
            layer, its output is computed and passed to the next layer, and so on until we get
            the output of the last layer, the output layer. This is the forward pass: it is exactly
            like making predictions, except all intermediate results are preserved since they
            are needed for the backward pass.                         
                                                                      
           • Next, the algorithm measures the network’s output error (i.e., it uses a loss func‐
            tion that compares the desired output and the actual output of the network, and
            returns some measure of the error).                       
           • Then it computes how much each output connection contributed to the error.
            This is done analytically by applying the chain rule (perhaps the most fundamen‐
            tal rule in calculus), which makes this step fast and precise.
                                                                      
                                                                      
          10 David Rumelhart et al. “Learning Internal Representations by Error Propagation,” (Defense Technical Infor‐
           mation Center technical report, September 1985).           
          11 This technique was actually independently invented several times by various researchers in different fields,
           starting with Paul Werbos in 1974.                         "|reverse-mode autodiff; epochs; forward pass; automatic differentiation (autodiff); chain rule
"                                                                      
                                                                      
                                                                      
                                                                      
           • The algorithm then measures how much of these error contributions came from
            each connection in the layer below, again using the chain rule, working backward
            until the algorithm reaches the input layer. As explained earlier, this reverse pass
            efficiently measures the error gradient across all the connection weights in the
            network by propagating the error gradient backward through the network (hence
            the name of the algorithm).                               
           • Finally, the algorithm performs a Gradient Descent step to tweak all the connec‐
            tion weights in the network, using the error gradients it just computed.
                                                                      
          This algorithm is so important that it’s worth summarizing it again: for each training
          instance, the backpropagation algorithm first makes a prediction (forward pass) and
          measures the error, then goes through each layer in reverse to measure the error con‐
          tribution from each connection (reverse pass), and finally tweaks the connection
          weights to reduce the error (Gradient Descent step).        
                                                                      
                   It is important to initialize all the hidden layers’ connection weights
                   randomly, or else training will fail. For example, if you initialize all
                   weights and biases to zero, then all neurons in a given layer will be
                   perfectly identical, and thus backpropagation will affect them in
                   exactly the same way, so they will remain identical. In other words,
                   despite having hundreds of neurons per layer, your model will act
                   as if it had only one neuron per layer: it won’t be too smart. If
                   instead you randomly initialize the weights, you break the symme‐
                   try and allow backpropagation to train a diverse team of neurons.
          In order for this algorithm to work properly, its authors made a key change to the
          MLP’s architecture: they replaced the step function with the logistic (sigmoid) func‐
          tion, σ(z) = 1 / (1 + exp(–z)). This was essential because the step function contains
          only flat segments, so there is no gradient to work with (Gradient Descent cannot
          move on a flat surface), while the logistic function has a well-defined nonzero deriva‐
          tive everywhere, allowing Gradient Descent to make some progress at every step. In
          fact, the backpropagation algorithm works well with many other activation functions,
          not just the logistic function. Here are two other popular choices:
                                                                      
          The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1       
            Just like the logistic function, this activation function is S-shaped, continuous,
            and differentiable, but its output value ranges from –1 to 1 (instead of 0 to 1 in
            the case of the logistic function). That range tends to make each layer’s output
            more or less centered around 0 at the beginning of training, which often helps
            speed up convergence.                                     
                                                                      
                                                                      
                                                                      "|hyperbolic tangent function (tanh); symmetry, breaking in backpropagation; hyperbolic tangent (tanh); break the symmetry
"                                                                      
                                                                      
                                                                      
                                                                      
          The Rectified Linear Unit function: ReLU(z) = max(0, z)     
            The ReLU function is continuous but unfortunately not differentiable at z = 0
            (the slope changes abruptly, which can make Gradient Descent bounce around),
            and its derivative is 0 for z < 0. In practice, however, it works very well and has
            the advantage of being fast to compute, so it has become the default.12 Most
            importantly, the fact that it does not have a maximum output value helps reduce
            some issues during Gradient Descent (we will come back to this in Chapter 11).
                                                                      
          These popular activation functions and their derivatives are represented in
          Figure 10-8. But wait! Why do we need activation functions in the first place? Well, if
          you chain several linear transformations, all you get is a linear transformation. For
          example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions
          gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t
          have some nonlinearity between layers, then even a deep stack of layers is equivalent
          to a single layer, and you can’t solve very complex problems with that. Conversely, a
          large enough DNN with nonlinear activations can theoretically approximate any con‐
          tinuous function.                                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-8. Activation functions and their derivatives     
                                                                      
          OK! You know where neural nets came from, what their architecture is, and how to
          compute their outputs. You’ve also learned about the backpropagation algorithm. But
          what exactly can you do with them?                          
                                                                      
          Regression MLPs                                             
          First, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,
          the price of a house, given many of its features), then you just need a single output
          neuron: its output is the predicted value. For multivariate regression (i.e., to predict
                                                                      
                                                                      
          12 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck
           to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is
           one of the cases where the biological analogy was misleading."|Rectified Linear Unit function (ReLU); regression MLPs; ReLU (Rectified Linear Unit function); backpropagation
"                                                                      
                                                                      
                                                                      
                                                                      
          multiple values at once), you need one output neuron per output dimension. For
          example, to locate the center of an object in an image, you need to predict 2D coordi‐
          nates, so you need two output neurons. If you also want to place a bounding box
          around the object, then you need two more numbers: the width and the height of the
          object. So, you end up with four output neurons.            
                                                                      
          In general, when building an MLP for regression, you do not want to use any activa‐
          tion function for the output neurons, so they are free to output any range of values. If
          you want to guarantee that the output will always be positive, then you can use the
          ReLU activation function in the output layer. Alternatively, you can use the softplus
          activation function, which is a smooth variant of ReLU: softplus(z) = log(1 + exp(z)).
          It is close to 0 when z is negative, and close to z when z is positive. Finally, if you want
          to guarantee that the predictions will fall within a given range of values, then you can
          use the logistic function or the hyperbolic tangent, and then scale the labels to the
          appropriate range: 0 to 1 for the logistic function and –1 to 1 for the hyperbolic
          tangent.                                                    
          The loss function to use during training is typically the mean squared error, but if you
          have a lot of outliers in the training set, you may prefer to use the mean absolute
          error instead. Alternatively, you can use the Huber loss, which is a combination of
          both.                                                       
                                                                      
                   The Huber loss is quadratic when the error is smaller than a thres‐
                   hold δ (typically 1) but linear when the error is larger than δ. The
                   linear part makes it less sensitive to outliers than the mean squared
                   error, and the quadratic part allows it to converge faster and be
                   more precise than the mean absolute error.         
          Table 10-1 summarizes the typical architecture of a regression MLP.
                                                                      
          Table 10-1. Typical regression MLP architecture             
                                                                      
          Hyperparameter Typical value                                
          # input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)
          # hidden layers Depends on the problem, but typically 1 to 5
          # neurons per hidden layer Depends on the problem, but typically 10 to 100
          # output neurons 1 per prediction dimension                 
          Hidden activation ReLU (or SELU, see Chapter 11)            
          Output activation None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs)
          Loss function MSE or MAE/Huber (if outliers)                
                                                                      
                                                                      
                                                                      
                                                                      "|mean squared error; softplus activation function; Logistic (sigmoid) function; softplus; Rectified Linear Unit function (ReLU); mean absolute error (MAE); ReLU (Rectified Linear Unit function); sigmoid (Logistic) activation function; Logistic (sigmoid); Huber loss
"                                                                      
                                                                      
                                                                      
                                                                      
          Classification MLPs                                         
                                                                      
          MLPs can also be used for classification tasks. For a binary classification problem,
          you just need a single output neuron using the logistic activation function: the output
          will be a number between 0 and 1, which you can interpret as the estimated probabil‐
          ity of the positive class. The estimated probability of the negative class is equal to one
          minus that number.                                          
          MLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For
          example, you could have an email classification system that predicts whether each
          incoming email is ham or spam, and simultaneously predicts whether it is an urgent
          or nonurgent email. In this case, you would need two output neurons, both using the
          logistic activation function: the first would output the probability that the email is
          spam, and the second would output the probability that it is urgent. More generally,
          you would dedicate one output neuron for each positive class. Note that the output
          probabilities do not necessarily add up to 1. This lets the model output any combina‐
          tion of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and per‐
          haps even urgent spam (although that would probably be an error).
          If each instance can belong only to a single class, out of three or more possible classes
          (e.g., classes 0 through 9 for digit image classification), then you need to have one
          output neuron per class, and you should use the softmax activation function for the
          whole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4)
          will ensure that all the estimated probabilities are between 0 and 1 and that they add
          up to 1 (which is required if the classes are exclusive). This is called multiclass
          classification.                                             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-9. A modern MLP (including ReLU and softmax) for classification
                                                                      
                                                                      "|softmax; classification MLPs; Logistic (sigmoid) function; sigmoid (Logistic) activation function; softmax function
"                                                                      
                                                                      
                                                                      
                                                                      
          Regarding the loss function, since we are predicting probability distributions, the
          cross-entropy loss (also called the log loss, see Chapter 4) is generally a good choice.
                                                                      
          Table 10-2 summarizes the typical architecture of a classification MLP.
                                                                      
          Table 10-2. Typical classification MLP architecture         
          Hyperparameter Binary classification Multilabel binary classification Multiclass classification
          Input and hidden layers Same as regression Same as regression Same as regression
          # output neurons 1  1 per label  1 per class                
          Output layer activation Logistic Logistic Softmax           
          Loss function Cross entropy Cross entropy Cross entropy     
                                                                      
                   Before we go on, I recommend you go through exercise 1 at the
                   end of this chapter. You will play with various neural network
                   architectures and visualize their outputs using the TensorFlow Play‐
                   ground. This will be very useful to better understand MLPs, includ‐
                   ing the effects of all the hyperparameters (number of layers and
                   neurons, activation functions, and more).          
                                                                      
          Now you have all the concepts you need to start implementing MLPs with Keras!
                                                                      
          Implementing MLPs with Keras                                
                                                                      
          Keras is a high-level Deep Learning API that allows you to easily build, train, evalu‐
          ate, and execute all sorts of neural networks. Its documentation (or specification) is
          available at https://keras.io/. The reference implementation, also called Keras, was
          developed by François Chollet as part of a research project13 and was released as an
          open source project in March 2015. It quickly gained popularity, owing to its ease of
          use, flexibility, and beautiful design. To perform the heavy computations required by
          neural networks, this reference implementation relies on a computation backend. At
          present, you can choose from three popular open source Deep Learning libraries:
          TensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano. Therefore, to avoid
          any confusion, we will refer to this reference implementation as multibackend Keras.
          Since late 2016, other implementations have been released. You can now run Keras on
          Apache MXNet, Apple’s Core ML, JavaScript or TypeScript (to run Keras code in a
          web browser), and PlaidML (which can run on all sorts of GPU devices, not just Nvi‐
          dia). Moreover, TensorFlow itself now comes bundled with its own Keras implemen‐
          tation, tf.keras. It only supports TensorFlow as the backend, but it has the advantage
          of offering some very useful extra features (see Figure 10-10): for example, it supports
                                                                      
                                                                      
          13 Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System)."|cross-entropy loss (log loss); implementing MLPs with Keras; from biological to artificial neurons; implementing MLPs with; multibackend Keras; Microsoft Cognitive Toolkit (CNTK); tf.keras; from biological to artificial; Theano; TensorFlow Playground
"                                                                      
                                                                      
                                                                      
                                                                      
          TensorFlow’s Data API, which makes it easy to load and preprocess data efficiently.
          For this reason, we will use tf.keras in this book. However, in this chapter we will not
          use any of the TensorFlow-specific features, so the code should run fine on other
          Keras implementations as well (at least in Python), with only minor modifications,
          such as changing the imports.                               
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-10. Two implementations of the Keras API: multibackend Keras (left) and
          tf.keras (right)                                            
                                                                      
          The most popular Deep Learning library, after Keras and TensorFlow, is Facebook’s
          PyTorch library. The good news is that its API is quite similar to Keras’s (in part
          because both APIs were inspired by Scikit-Learn and Chainer), so once you know
          Keras, it is not difficult to switch to PyTorch, if you ever want to. PyTorch’s popularity
          grew exponentially in 2018, largely thanks to its simplicity and excellent documenta‐
          tion, which were not TensorFlow 1.x’s main strengths. However, TensorFlow 2 is
          arguably just as simple as PyTorch, as it has adopted Keras as its official high-level
          API and its developers have greatly simplified and cleaned up the rest of the API. The
          documentation has also been completely reorganized, and it is much easier to find
          what you need now. Similarly, PyTorch’s main weaknesses (e.g., limited portability
          and no computation graph analysis) have been largely addressed in PyTorch 1.0.
          Healthy competition is beneficial to everyone.              
          All right, it’s time to code! As tf.keras is bundled with TensorFlow, let’s start by instal‐
          ling TensorFlow.                                            
          Installing TensorFlow 2                                     
                                                                      
          Assuming you installed Jupyter and Scikit-Learn by following the installation instruc‐
          tions in Chapter 2, use pip to install TensorFlow. If you created an isolated environ‐
          ment using virtualenv, you first need to activate it:       
                                                                      
                                                                      
                                                                      "|PyTorch library and; installing; PyTorch library
"                                                                      
                                                                      
                                                                      
                                                                      
            $ cd $ML_PATH     # Your ML working directory (e.g., $HOME/ml)
            $ source my_env/bin/activate # on Linux or macOS          
            $ .\my_env\Scripts\activate # on Windows                  
          Next, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐
          trator rights, or to add the --user option):                
            $ python3 -m pip install -U tensorflow                    
                                                                      
                   For GPU support, at the time of this writing you need to install
                   tensorflow-gpu instead of tensorflow, but the TensorFlow team
                   is working on having a single library that will support both CPU-
                   only and GPU-equipped systems. You will still need to install extra
                   libraries for GPU support (see https://tensorflow.org/install for
                   more details). We will look at GPUs in more depth in Chapter 19.
                                                                      
          To test your installation, open a Python shell or a Jupyter notebook, then import
          TensorFlow and tf.keras and print their versions:           
            >>> import tensorflow as tf                               
            >>> from tensorflow import keras                          
            >>> tf.__version__                                        
            '2.0.0'                                                   
            >>> keras.__version__                                     
            '2.2.4-tf'                                                
          The second version is the version of the Keras API implemented by tf.keras. Note that
          it ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus
          some extra TensorFlow-specific features.                    
          Now let’s use tf.keras! We’ll start by building a simple image classifier.
                                                                      
          Building an Image Classifier Using the Sequential API       
          First, we need to load a dataset. In this chapter we will tackle Fashion MNIST, which
          is a drop-in replacement of MNIST (introduced in Chapter 3). It has the exact same
          format as MNIST (70,000 grayscale images of 28 × 28 pixels each, with 10 classes),
          but the images represent fashion items rather than handwritten digits, so each class is
          more diverse, and the problem turns out to be significantly more challenging than
          MNIST. For example, a simple linear model reaches about 92% accuracy on MNIST,
          but only about 83% on Fashion MNIST.                        
                                                                      
          Using Keras to load the dataset                             
                                                                      
          Keras provides some utility functions to fetch and load common datasets, including
          MNIST, Fashion MNIST, and the California housing dataset we used in Chapter 2.
          Let’s load Fashion MNIST:                                   
                                                                      "|loading datasets with; using Sequential API; image classifiers using Sequential APIs; Fashion MNIST dataset; image classifiers
"                                                                      
                                                                      
                                                                      
                                                                      
          Creating the model using the Sequential API                 
                                                                      
          Now let’s build the neural network! Here is a classification MLP with two hidden
          layers:                                                     
            model = keras.models.Sequential()                         
            model.add(keras.layers.Flatten(input_shape=[28, 28]))     
            model.add(keras.layers.Dense(300, activation=""relu""))     
            model.add(keras.layers.Dense(100, activation=""relu""))     
            model.add(keras.layers.Dense(10, activation=""softmax""))   
          Let’s go through this code line by line:                    
           • The first line creates a Sequential model. This is the simplest kind of Keras
            model for neural networks that are just composed of a single stack of layers con‐
            nected sequentially. This is called the Sequential API.   
           • Next, we build the first layer and add it to the model. It is a Flatten layer whose
            role is to convert each input image into a 1D array: if it receives input data X, it
            computes X.reshape(-1, 1). This layer does not have any parameters; it is just
            there to do some simple preprocessing. Since it is the first layer in the model, you
            should specify the input_shape, which doesn’t include the batch size, only the
            shape of the instances. Alternatively, you could add a keras.layers.InputLayer
            as the first layer, setting input_shape=[28,28].          
                                                                      
           • Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activa‐
            tion function. Each Dense layer manages its own weight matrix, containing all the
            connection weights between the neurons and their inputs. It also manages a vec‐
            tor of bias terms (one per neuron). When it receives some input data, it computes
            Equation 10-2.                                            
           • Then we add a second Dense hidden layer with 100 neurons, also using the ReLU
            activation function.                                      
           • Finally, we add a Dense output layer with 10 neurons (one per class), using the
            softmax activation function (because the classes are exclusive).
                                                                      
                                                                      
                   Specifying activation=""relu"" is equivalent to specifying activa
                   tion=keras.activations.relu. Other activation functions are
                   available in the keras.activations package, we will use many of
                   them in this book. See https://keras.io/activations/ for the full list.
                                                                      
          Instead of adding the layers one by one as we just did, you can pass a list of layers
          when creating the Sequential model:                         
                                                                      
                                                                      
                                                                      "|softmax; softmax function
"                                                                      
                                                                      
                                                                      
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.Flatten(input_shape=[28, 28]),            
               keras.layers.Dense(300, activation=""relu""),            
               keras.layers.Dense(100, activation=""relu""),            
               keras.layers.Dense(10, activation=""softmax"")           
            ])                                                        
                       Using Code Examples from keras.io              
                                                                      
           Code examples documented on keras.io will work fine with tf.keras, but you need to
           change the imports. For example, consider this keras.io code:
             from keras.layers import Dense                           
             output_layer = Dense(10)                                 
           You must change the imports like this:                     
                                                                      
             from tensorflow.keras.layers import Dense                
             output_layer = Dense(10)                                 
           Or simply use full paths, if you prefer:                   
             from tensorflow import keras                             
             output_layer = keras.layers.Dense(10)                    
           This approach is more verbose, but I use it in this book so you can easily see which
           packages to use, and to avoid confusion between standard classes and custom classes.
           In production code, I prefer the previous approach. Many people also use from ten
           sorflow.keras import layers followed by layers.Dense(10).  
                                                                      
                                                                      
          The model’s summary() method displays all the model’s layers,14 including each layer’s
          name (which is automatically generated unless you set it when creating the layer), its
          output shape (None means the batch size can be anything), and its number of parame‐
          ters. The summary ends with the total number of parameters, including trainable and
          non-trainable parameters. Here we only have trainable parameters (we will see exam‐
          ples of non-trainable parameters in Chapter 11):            
            >>> model.summary()                                       
            Model: ""sequential""                                       
            _________________________________________________________________
            Layer (type)     Output Shape    Param #                  
            =================================================================
            flatten (Flatten) (None, 784)    0                        
            _________________________________________________________________
            dense (Dense)    (None, 300)     235500                   
            _________________________________________________________________
                                                                      
          14 You can use keras.utils.plot_model() to generate an image of your model."|using code examples from keras.io
"                                                                      
                                                                      
                                                                      
                                                                      
          weights) or bias_initializer when creating the layer. We will discuss initializers
          further in Chapter 11, but if you want the full list, see https://keras.io/initializers/.
                                                                      
                   The shape of the weight matrix depends on the number of inputs.
                   This is why it is recommended to specify the input_shape when
                   creating the first layer in a Sequential model. However, if you do
                   not specify the input shape, it’s OK: Keras will simply wait until it
                   knows the input shape before it actually builds the model. This will
                   happen either when you feed it actual data (e.g., during training),
                   or when you call its build() method. Until the model is really
                   built, the layers will not have any weights, and you will not be able
                   to do certain things (such as print the model summary or save the
                   model). So, if you know the input shape when creating the model,
                   it is best to specify it.                          
          Compiling the model                                         
                                                                      
          After a model is created, you must call its compile() method to specify the loss func‐
          tion and the optimizer to use. Optionally, you can specify a list of extra metrics to
          compute during training and evaluation:                     
            model.compile(loss=""sparse_categorical_crossentropy"",     
                    optimizer=""sgd"",                                  
                    metrics=[""accuracy""])                             
                                                                      
                   Using loss=""sparse_categorical_crossentropy"" is equivalent to
                   using loss=keras.losses.sparse_categorical_crossentropy.
                   Similarly, specifying optimizer=""sgd"" is equivalent to specifying
                   optimizer=keras.optimizers.SGD(), and metrics=[""accuracy""]
                   is equivalent to metrics=[keras.metrics.sparse_categori
                   cal_accuracy] (when using this loss). We will use many other los‐
                   ses, optimizers, and metrics in this book; for the full lists, see
                   https://keras.io/losses, https://keras.io/optimizers, and https://
                   keras.io/metrics.                                  
          This code requires some explanation. First, we use the ""sparse_categorical_cross
          entropy"" loss because we have sparse labels (i.e., for each instance, there is just a tar‐
          get class index, from 0 to 9 in this case), and the classes are exclusive. If instead we
          had one target probability per class for each instance (such as one-hot vectors, e.g.
          [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would
          need to use the ""categorical_crossentropy"" loss instead. If we were doing binary
          classification (with one or more binary labels), then we would use the ""sigmoid"" (i.e.,
          logistic) activation function in the output layer instead of the ""softmax"" activation
          function, and we would use the ""binary_crossentropy"" loss.  
                                                                      "|Logistic (sigmoid) function; Logistic (sigmoid); sigmoid (Logistic) activation function
"                                                                      
                                                                      
                                                                      
                                                                      
          As you can see, for each instance the model estimates one probability per class, from
          class 0 to class 9. For example, for the first image it estimates that the probability of
          class 9 (ankle boot) is 96%, the probability of class 5 (sandal) is 3%, the probability of
          class 7 (sneaker) is 1%, and the probabilities of the other classes are negligible. In
          other words, it “believes” the first image is footwear, most likely ankle boots but pos‐
          sibly sandals or sneakers. If you only care about the class with the highest estimated
          probability (even if that probability is quite low), then you can use the pre
          dict_classes() method instead:                              
                                                                      
            >>> y_pred = model.predict_classes(X_new)                 
            >>> y_pred                                                
            array([9, 2, 1])                                          
            >>> np.array(class_names)[y_pred]                         
            array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')
          Here, the classifier actually classified all three images correctly (these images are
          shown in Figure 10-13):                                     
            >>> y_new = y_test[:3]                                    
            >>> y_new                                                 
            array([9, 2, 1])                                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-13. Correctly classified Fashion MNIST images     
                                                                      
          Now you know how to use the Sequential API to build, train, evaluate, and use a clas‐
          sification MLP. But what about regression?                  
                                                                      
          Building a Regression MLP Using the Sequential API          
                                                                      
          Let’s switch to the California housing problem and tackle it using a regression neural
          network. For simplicity, we will use Scikit-Learn’s fetch_california_housing()
          function to load the data. This dataset is simpler than the one we used in Chapter 2,
          since it contains only numerical features (there is no ocean_proximity feature), and
          there is no missing value. After loading the data, we split it into a training set, a vali‐
          dation set, and a test set, and we scale all the features:  
            from sklearn.datasets import fetch_california_housing     
            from sklearn.model_selection import train_test_split      
            from sklearn.preprocessing import StandardScaler          "|using Sequential API; image classifiers using Sequential APIs; regression MLPs using Sequential API; regression MLP; image classifiers
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            housing = fetch_california_housing()                      
                                                                      
            X_train_full, X_test, y_train_full, y_test = train_test_split(
               housing.data, housing.target)                          
            X_train, X_valid, y_train, y_valid = train_test_split(    
               X_train_full, y_train_full)                            
            scaler = StandardScaler()                                 
            X_train = scaler.fit_transform(X_train)                   
            X_valid = scaler.transform(X_valid)                       
            X_test = scaler.transform(X_test)                         
          Using the Sequential API to build, train, evaluate, and use a regression MLP to make
          predictions is quite similar to what we did for classification. The main differences are
          the fact that the output layer has a single neuron (since we only want to predict a sin‐
          gle value) and uses no activation function, and the loss function is the mean squared
          error. Since the dataset is quite noisy, we just use a single hidden layer with fewer
          neurons than before, to avoid overfitting:                  
            model = keras.models.Sequential([                         
               keras.layers.Dense(30, activation=""relu"", input_shape=X_train.shape[1:]),
               keras.layers.Dense(1)                                  
            ])                                                        
            model.compile(loss=""mean_squared_error"", optimizer=""sgd"") 
            history = model.fit(X_train, y_train, epochs=20,          
                        validation_data=(X_valid, y_valid))           
            mse_test = model.evaluate(X_test, y_test)                 
            X_new = X_test[:3] # pretend these are new instances      
            y_pred = model.predict(X_new)                             
          As you can see, the Sequential API is quite easy to use. However, although Sequen
          tial models are extremely common, it is sometimes useful to build neural networks
          with more complex topologies, or with multiple inputs or outputs. For this purpose,
          Keras offers the Functional API.                            
          Building Complex Models Using the Functional API            
          One example of a nonsequential neural network is a Wide & Deep neural network.
          This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng
          et al.16 It connects all or part of the inputs directly to the output layer, as shown in
          Figure 10-14. This architecture makes it possible for the neural network to learn both
          deep patterns (using the deep path) and simple rules (through the short path).17 In
          contrast, a regular MLP forces all the data to flow through the full stack of layers;
                                                                      
          16 Heng-Tze Cheng et al., “Wide & Deep Learning for Recommender Systems,” Proceedings of the First Workshop
           on Deep Learning for Recommender Systems (2016): 7–10.     
          17 The short path can also be used to provide manually engineered features to the neural network."|mean squared error; Functional API; Wide & Deep neural networks; nonsequential neural networks; complex using Functional API
"                                                                      
                                                                      
                                                                      
                                                                      
          The code is self-explanatory. You should name at least the most important layers,
          especially when the model gets a bit complex like this. Note that we specified
          inputs=[input_A, input_B] when creating the model. Now we can compile the
          model as usual, but when we call the fit() method, instead of passing a single input
          matrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per
          input.19 The same is true for X_valid, and also for X_test and X_new when you call
          evaluate() or predict():                                    
                                                                      
            model.compile(loss=""mse"", optimizer=keras.optimizers.SGD(lr=1e-3))
            X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]     
            X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]     
            X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]         
            X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]             
            history = model.fit((X_train_A, X_train_B), y_train, epochs=20,
                        validation_data=((X_valid_A, X_valid_B), y_valid))
            mse_test = model.evaluate((X_test_A, X_test_B), y_test)   
            y_pred = model.predict((X_new_A, X_new_B))                
          There are many use cases in which you may want to have multiple outputs:
                                                                      
           • The task may demand it. For instance, you may want to locate and classify the
            main object in a picture. This is both a regression task (finding the coordinates of
            the object’s center, as well as its width and height) and a classification task.
           • Similarly, you may have multiple independent tasks based on the same data. Sure,
            you could train one neural network per task, but in many cases you will get better
            results on all tasks by training a single neural network with one output per task.
            This is because the neural network can learn features in the data that are useful
            across tasks. For example, you could perform multitask classification on pictures
            of faces, using one output to classify the person’s facial expression (smiling, sur‐
            prised, etc.) and another output to identify whether they are wearing glasses or
            not.                                                      
           • Another use case is as a regularization technique (i.e., a training constraint whose
            objective is to reduce overfitting and thus improve the model’s ability to general‐
            ize). For example, you may want to add some auxiliary outputs in a neural net‐
            work architecture (see Figure 10-16) to ensure that the underlying part of the
            network learns something useful on its own, without relying on the rest of the
            network.                                                  
                                                                      
                                                                      
                                                                      
          19 Alternatively, you can pass a dictionary mapping the input names to the input values, like {""wide_input"":
           X_train_A, ""deep_input"": X_train_B}. This is especially useful when there are many inputs, to avoid get‐
           ting the order wrong.                                      "|image classification; multiple outputs; multitask classification
"                                                                      
                                                                      
                                                                      
                                                                      
          When we evaluate the model, Keras will return the total loss, as well as all the individ‐
          ual losses:                                                 
                                                                      
            total_loss, main_loss, aux_loss = model.evaluate(         
               [X_test_A, X_test_B], [y_test, y_test])                
          Similarly, the predict() method will return predictions for each output:
            y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])
          As you can see, you can build any sort of architecture you want quite easily with the
          Functional API. Let’s look at one last way you can build Keras models.
                                                                      
          Using the Subclassing API to Build Dynamic Models           
                                                                      
          Both the Sequential API and the Functional API are declarative: you start by declar‐
          ing which layers you want to use and how they should be connected, and only then
          can you start feeding the model some data for training or inference. This has many
          advantages: the model can easily be saved, cloned, and shared; its structure can be
          displayed and analyzed; the framework can infer shapes and check types, so errors
          can be caught early (i.e., before any data ever goes through the model). It’s also fairly
          easy to debug, since the whole model is a static graph of layers. But the flip side is just
          that: it’s static. Some models involve loops, varying shapes, conditional branching,
          and other dynamic behaviors. For such cases, or simply if you prefer a more impera‐
          tive programming style, the Subclassing API is for you.     
          Simply subclass the Model class, create the layers you need in the constructor, and use
          them to perform the computations you want in the call() method. For example, cre‐
          ating an instance of the following WideAndDeepModel class gives us an equivalent
          model to the one we just built with the Functional API. You can then compile it, eval‐
          uate it, and use it to make predictions, exactly like we just did:
            class WideAndDeepModel(keras.Model):                      
               def __init__(self, units=30, activation=""relu"", **kwargs):
                 super().__init__(**kwargs) # handles standard args (e.g., name)
                 self.hidden1 = keras.layers.Dense(units, activation=activation)
                 self.hidden2 = keras.layers.Dense(units, activation=activation)
                 self.main_output = keras.layers.Dense(1)             
                 self.aux_output = keras.layers.Dense(1)              
               def call(self, inputs):                                
                 input_A, input_B = inputs                            
                 hidden1 = self.hidden1(input_B)                      
                 hidden2 = self.hidden2(hidden1)                      
                 concat = keras.layers.concatenate([input_A, hidden2])
                 main_output = self.main_output(concat)               
                 aux_output = self.aux_output(hidden2)                
                 return main_output, aux_output                       
            model = WideAndDeepModel()                                "|Functional API; dynamic using Subclassing API; Subclassing API; dynamic models; complex using Functional API
"                                                                      
                                                                      
                                                                      
                                                                      
          This example looks very much like the Functional API, except we do not need to cre‐
          ate the inputs; we just use the input argument to the call() method, and we separate
          the creation of the layers21 in the constructor from their usage in the call() method.
          The big difference is that you can do pretty much anything you want in the call()
          method: for loops, if statements, low-level TensorFlow operations—your imagina‐
          tion is the limit (see Chapter 12)! This makes it a great API for researchers experi‐
          menting with new ideas.                                     
                                                                      
          This extra flexibility does come at a cost: your model’s architecture is hidden within
          the call() method, so Keras cannot easily inspect it; it cannot save or clone it; and
          when you call the summary() method, you only get a list of layers, without any infor‐
          mation on how they are connected to each other. Moreover, Keras cannot check types
          and shapes ahead of time, and it is easier to make mistakes. So unless you really need
          that extra flexibility, you should probably stick to the Sequential API or the Func‐
          tional API.                                                 
                                                                      
                   Keras models can be used just like regular layers, so you can easily
                   combine them to build complex architectures.       
                                                                      
                                                                      
                                                                      
          Now that you know how to build and train neural nets using Keras, you will want to
          save them!                                                  
          Saving and Restoring a Model                                
                                                                      
          When using the Sequential API or the Functional API, saving a trained Keras model
          is as simple as it gets:                                    
                                                                      
            model = keras.models.Sequential([...]) # or keras.Model([...])
            model.compile([...])                                      
            model.fit([...])                                          
            model.save(""my_keras_model.h5"")                           
          Keras will use the HDF5 format to save both the model’s architecture (including every
          layer’s hyperparameters) and the values of all the model parameters for every layer
          (e.g., connection weights and biases). It also saves the optimizer (including its hyper‐
          parameters and any state it may have). In Chapter 19, we will see how to save a
          tf.keras model using TensorFlow’s SavedModel format instead.
                                                                      
                                                                      
                                                                      
          21 Keras models have an output attribute, so we cannot use that name for the main output layer, which is why
           we renamed it to main_output.                              "|restoring models; saving and restoring models; saving and restoring; complex architectures; HDF5 format
"                                                                      
                                                                      
                                                                      
                                                                      
          You will typically have a script that trains a model and saves it, and one or more
          scripts (or web services) that load the model and use it to make predictions. Loading
          the model is just as easy:                                  
                                                                      
            model = keras.models.load_model(""my_keras_model.h5"")      
                                                                      
                   This will work when using the Sequential API or the Functional
                   API, but unfortunately not when using model subclassing. You can
                   use save_weights() and load_weights() to at least save and
                   restore the model parameters, but you will need to save and restore
                   everything else yourself.                          
          But what if training lasts several hours? This is quite common, especially when train‐
          ing on large datasets. In this case, you should not only save your model at the end of
          training, but also save checkpoints at regular intervals during training, to avoid losing
          everything if your computer crashes. But how can you tell the fit() method to save
          checkpoints? Use callbacks.                                 
                                                                      
          Using Callbacks                                             
                                                                      
          The fit() method accepts a callbacks argument that lets you specify a list of objects
          that Keras will call at the start and end of training, at the start and end of each epoch,
          and even before and after processing each batch. For example, the ModelCheckpoint
          callback saves checkpoints of your model at regular intervals during training, by
          default at the end of each epoch:                           
            [...] # build and compile the model                       
            checkpoint_cb = keras.callbacks.ModelCheckpoint(""my_keras_model.h5"")
            history = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])
          Moreover, if you use a validation set during training, you can set
          save_best_only=True when creating the ModelCheckpoint. In this case, it will only
          save your model when its performance on the validation set is the best so far. This
          way, you do not need to worry about training for too long and overfitting the training
          set: simply restore the last model saved after training, and this will be the best model
          on the validation set. The following code is a simple way to implement early stopping
          (introduced in Chapter 4):                                  
            checkpoint_cb = keras.callbacks.ModelCheckpoint(""my_keras_model.h5"",
                                        save_best_only=True)          
            history = model.fit(X_train, y_train, epochs=10,          
                        validation_data=(X_valid, y_valid),           
                        callbacks=[checkpoint_cb])                    
            model = keras.models.load_model(""my_keras_model.h5"") # roll back to best model
          Another way to implement early stopping is to simply use the EarlyStopping call‐
          back. It will interrupt training when it measures no progress on the validation set for"|using callbacks; callbacks
"                                                                      
                                                                      
                                                                      
                                                                      
          a number of epochs (defined by the patience argument), and it will optionally roll
          back to the best model. You can combine both callbacks to save checkpoints of your
          model (in case your computer crashes) and interrupt training early when there is no
          more progress (to avoid wasting time and resources):        
                                                                      
            early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,
                                          restore_best_weights=True)  
            history = model.fit(X_train, y_train, epochs=100,         
                        validation_data=(X_valid, y_valid),           
                        callbacks=[checkpoint_cb, early_stopping_cb]) 
          The number of epochs can be set to a large value since training will stop automati‐
          cally when there is no more progress. In this case, there is no need to restore the best
          model saved because the EarlyStopping callback will keep track of the best weights
          and restore them for you at the end of training.            
                   There are many other callbacks available in the keras.callbacks
                   package.                                           
                                                                      
                                                                      
                                                                      
          If you need extra control, you can easily write your own custom callbacks. As an
          example of how to do that, the following custom callback will display the ratio
          between the validation loss and the training loss during training (e.g., to detect over‐
          fitting):                                                   
            class PrintValTrainRatioCallback(keras.callbacks.Callback):
               def on_epoch_end(self, epoch, logs):                   
                 print(""\nval/train: {:.2f}"".format(logs[""val_loss""] / logs[""loss""]))
          As you might expect, you can implement on_train_begin(), on_train_end(),
          on_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end(). Call‐
          backs can also be used during evaluation and predictions, should you ever need them
          (e.g., for debugging). For evaluation, you should implement on_test_begin(),
          on_test_end(), on_test_batch_begin(), or on_test_batch_end() (called by evalu
          ate()), and for prediction you should implement on_predict_begin(), on_pre
          dict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by
          predict()).                                                 
                                                                      
          Now let’s take a look at one more tool you should definitely have in your toolbox
          when using tf.keras: TensorBoard.                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|keras.callbacks package
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Using TensorBoard for Visualization                         
                                                                      
          TensorBoard is a great interactive visualization tool that you can use to view the
          learning curves during training, compare learning curves between multiple runs, vis‐
          ualize the computation graph, analyze training statistics, view images generated by
          your model, visualize complex multidimensional data projected down to 3D and
          automatically clustered for you, and more! This tool is installed automatically when
          you install TensorFlow, so you already have it.             
          To use it, you must modify your program so that it outputs the data you want to visu‐
          alize to special binary log files called event files. Each binary data record is called a
          summary. The TensorBoard server will monitor the log directory, and it will automat‐
          ically pick up the changes and update the visualizations: this allows you to visualize
          live data (with a short delay), such as the learning curves during training. In general,
          you want to point the TensorBoard server to a root log directory and configure your
          program so that it writes to a different subdirectory every time it runs. This way, the
          same TensorBoard server instance will allow you to visualize and compare data from
          multiple runs of your program, without getting everything mixed up.
          Let’s start by defining the root log directory we will use for our TensorBoard logs,
          plus a small function that will generate a subdirectory path based on the current date
          and time so that it’s different at every run. You may want to include extra information
          in the log directory name, such as hyperparameter values that you are testing, to
          make it easier to know what you are looking at in TensorBoard:
                                                                      
            import os                                                 
            root_logdir = os.path.join(os.curdir, ""my_logs"")          
            def get_run_logdir():                                     
               import time                                            
               run_id = time.strftime(""run_%Y_%m_%d-%H_%M_%S"")        
               return os.path.join(root_logdir, run_id)               
            run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'
          The good news is that Keras provides a nice TensorBoard() callback:
                                                                      
            [...] # Build and compile your model                      
            tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)  
            history = model.fit(X_train, y_train, epochs=30,          
                        validation_data=(X_valid, y_valid),           
                        callbacks=[tensorboard_cb])                   
          And that’s all there is to it! It could hardly be easier to use. If you run this code, the
          TensorBoard() callback will take care of creating the log directory for you (along
          with its parent directories if needed), and during training it will create event files and
          write summaries to them. After running the program a second time (perhaps"|event files; summaries (TensorFlow); TensorBoard; using TensorBoard for visualization
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-17. Visualizing learning curves with TensorBoard  
                                                                      
          You can also visualize the whole graph, the learned weights (projected to 3D), or the
          profiling traces. The TensorBoard() callback has options to log extra data too, such
          as embeddings (see Chapter 13).                             
                                                                      
          Additionally, TensorFlow offers a lower-level API in the tf.summary package. The
          following code creates a SummaryWriter using the create_file_writer() function,
          and it uses this writer as a context to log scalars, histograms, images, audio, and text,
          all of which can then be visualized using TensorBoard (give it a try!):
            test_logdir = get_run_logdir()                            
            writer = tf.summary.create_file_writer(test_logdir)       
            with writer.as_default():                                 
               for step in range(1, 1000 + 1):                        
                 tf.summary.scalar(""my_scalar"", np.sin(step / 10), step=step)
                 data = (np.random.randn(100) + 2) * step / 100 # some random data
                 tf.summary.histogram(""my_hist"", data, buckets=50, step=step)
                 images = np.random.rand(2, 32, 32, 3) # random 32×32 RGB images
                 tf.summary.image(""my_images"", images * step / 1000, step=step)
                 texts = [""The step is "" + str(step), ""Its square is "" + str(step**2)]
                 tf.summary.text(""my_text"", texts, step=step)         
                 sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)
                 audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])
                 tf.summary.audio(""my_audio"", audio, sample_rate=48000, step=step)"|tf.summary package
"                                                                      
                                                                      
                                                                      
                                                                      
          This is actually a useful visualization tool to have, even beyond TensorFlow or Deep
          Learning.                                                   
                                                                      
          Let’s summarize what you’ve learned so far in this chapter: we saw where neural nets
          came from, what an MLP is and how you can use it for classification and regression,
          how to use tf.keras’s Sequential API to build MLPs, and how to use the Functional
          API or the Subclassing API to build more complex model architectures. You learned
          how to save and restore a model and how to use callbacks for checkpointing, early
          stopping, and more. Finally, you learned how to use TensorBoard for visualization.
          You can already go ahead and use neural networks to tackle many problems! How‐
          ever, you may wonder how to choose the number of hidden layers, the number of
          neurons in the network, and all the other hyperparameters. Let’s look at this now.
          Fine-Tuning Neural Network Hyperparameters                  
                                                                      
          The flexibility of neural networks is also one of their main drawbacks: there are many
          hyperparameters to tweak. Not only can you use any imaginable network architec‐
          ture, but even in a simple MLP you can change the number of layers, the number of
          neurons per layer, the type of activation function to use in each layer, the weight initi‐
          alization logic, and much more. How do you know what combination of hyperpara‐
          meters is the best for your task?                           
                                                                      
          One option is to simply try many combinations of hyperparameters and see which
          one works best on the validation set (or use K-fold cross-validation). For example, we
          can use GridSearchCV or RandomizedSearchCV to explore the hyperparameter space,
          as we did in Chapter 2. To do this, we need to wrap our Keras models in objects that
          mimic regular Scikit-Learn regressors. The first step is to create a function that will
          build and compile a Keras model, given a set of hyperparameters:
            def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):
               model = keras.models.Sequential()                      
               model.add(keras.layers.InputLayer(input_shape=input_shape))
               for layer in range(n_hidden):                          
                 model.add(keras.layers.Dense(n_neurons, activation=""relu""))
               model.add(keras.layers.Dense(1))                       
               optimizer = keras.optimizers.SGD(lr=learning_rate)     
               model.compile(loss=""mse"", optimizer=optimizer)         
               return model                                           
          This function creates a simple Sequential model for univariate regression (only one
          output neuron), with the given input shape and the given number of hidden layers
          and neurons, and it compiles it using an SGD optimizer configured with the specified
          learning rate. It is good practice to provide reasonable defaults to as many hyperpara‐
          meters as you can, as Scikit-Learn does.                    
          Next, let’s create a KerasRegressor based on this build_model() function:"|fine-tuning hyperparameters; fine-tuning for neural networks; implementing MLPs with; implementing MLPs with Keras
"                                                                      
                                                                      
                                                                      
                                                                      
            >>> rnd_search_cv.best_params_                            
            {'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}
            >>> rnd_search_cv.best_score_                             
            -0.3189529188278931                                       
            >>> model = rnd_search_cv.best_estimator_.model           
          You can now save this model, evaluate it on the test set, and, if you are satisfied with
          its performance, deploy it to production. Using randomized search is not too hard,
          and it works well for many fairly simple problems. When training is slow, however
          (e.g., for more complex problems with larger datasets), this approach will only
          explore a tiny portion of the hyperparameter space. You can partially alleviate this
          problem by assisting the search process manually: first run a quick random search
          using wide ranges of hyperparameter values, then run another search using smaller
          ranges of values centered on the best ones found during the first run, and so on. This
          approach will hopefully zoom in on a good set of hyperparameters. However, it’s very
          time consuming, and probably not the best use of your time. 
          Fortunately, there are many techniques to explore a search space much more effi‐
          ciently than randomly. Their core idea is simple: when a region of the space turns out
          to be good, it should be explored more. Such techniques take care of the “zooming”
          process for you and lead to much better solutions in much less time. Here are some
          Python libraries you can use to optimize hyperparameters:   
          Hyperopt                                                    
            A popular library for optimizing over all sorts of complex search spaces (includ‐
            ing real values, such as the learning rate, and discrete values, such as the number
            of layers).                                               
                                                                      
          Hyperas, kopt, or Talos                                     
            Useful libraries for optimizing hyperparameters for Keras models (the first two
            are based on Hyperopt).                                   
          Keras Tuner                                                 
            An easy-to-use hyperparameter optimization library by Google for Keras models,
            with a hosted service for visualization and analysis.     
                                                                      
          Scikit-Optimize (skopt)                                     
            A general-purpose optimization library. The BayesSearchCV class performs
            Bayesian optimization using an interface similar to GridSearchCV.
          Spearmint                                                   
            A Bayesian optimization library.                          
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|Scikit-Optimize; Spearmint library; kopt library; Keras Tuner; Talos library; Python libraries for optimization; Hyperopt; Hyperas
"                                                                      
                                                                      
                                                                      
                                                                      
          Hyperband                                                   
            A fast hyperparameter tuning library based on the recent Hyperband paper22 by
            Lisha Li et al.                                           
                                                                      
          Sklearn-Deap                                                
            A hyperparameter optimization library based on evolutionary algorithms, with a
            GridSearchCV-like interface.                              
          Moreover, many companies offer services for hyperparameter optimization. We’ll dis‐
          cuss Google Cloud AI Platform’s hyperparameter tuning service in Chapter 19. Other
          options include services by Arimo and SigOpt, and CallDesk’s Oscar.
                                                                      
          Hyperparameter tuning is still an active area of research, and evolutionary algorithms
          are making a comeback. For example, check out DeepMind’s excellent 2017 paper,23
          where the authors jointly optimize a population of models and their hyperparame‐
          ters. Google has also used an evolutionary approach, not just to search for hyperpara‐
          meters but also to look for the best neural network architecture for the problem; their
          AutoML suite is already available as a cloud service. Perhaps the days of building neu‐
          ral networks manually will soon be over? Check out Google’s post on this topic. In
          fact, evolutionary algorithms have been used successfully to train individual neural
          networks, replacing the ubiquitous Gradient Descent! For an example, see the 2017
          post by Uber where the authors introduce their Deep Neuroevolution technique.
          But despite all this exciting progress and all these tools and services, it still helps to
          have an idea of what values are reasonable for each hyperparameter so that you can
          build a quick prototype and restrict the search space. The following sections provide
          guidelines for choosing the number of hidden layers and neurons in an MLP and for
          selecting good values for some of the main hyperparameters. 
          Number of Hidden Layers                                     
                                                                      
          For many problems, you can begin with a single hidden layer and get reasonable
          results. An MLP with just one hidden layer can theoretically model even the most
          complex functions, provided it has enough neurons. But for complex problems, deep
          networks have a much higher parameter efficiency than shallow ones: they can model
          complex functions using exponentially fewer neurons than shallow nets, allowing
          them to reach much better performance with the same amount of training data.
          To understand why, suppose you are asked to draw a forest using some drawing soft‐
          ware, but you are forbidden to copy and paste anything. It would take an enormous
                                                                      
                                                                      
          22 Lisha Li et al., “Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,” Journal of
           Machine Learning Research 18 (April 2018): 1–52.           
          23 Max Jaderberg et al., “Population Based Training of Neural Networks,” arXiv preprint arXiv:1711.09846
           (2017).                                                    "|AutoML; Hyperband; Sklearn-Deap; parameter efficiency; Deep Neuroevolution
"                                                                      
                                                                      
                                                                      
                                                                      
          amount of time: you would have to draw each tree individually, branch by branch,
          leaf by leaf. If you could instead draw one leaf, copy and paste it to draw a branch,
          then copy and paste that branch to create a tree, and finally copy and paste this tree to
          make a forest, you would be finished in no time. Real-world data is often structured
          in such a hierarchical way, and deep neural networks automatically take advantage of
          this fact: lower hidden layers model low-level structures (e.g., line segments of vari‐
          ous shapes and orientations), intermediate hidden layers combine these low-level
          structures to model intermediate-level structures (e.g., squares, circles), and the high‐
          est hidden layers and the output layer combine these intermediate structures to
          model high-level structures (e.g., faces).                  
          Not only does this hierarchical architecture help DNNs converge faster to a good sol‐
          ution, but it also improves their ability to generalize to new datasets. For example, if
          you have already trained a model to recognize faces in pictures and you now want to
          train a new neural network to recognize hairstyles, you can kickstart the training by
          reusing the lower layers of the first network. Instead of randomly initializing the
          weights and biases of the first few layers of the new neural network, you can initialize
          them to the values of the weights and biases of the lower layers of the first network.
          This way the network will not have to learn from scratch all the low-level structures
          that occur in most pictures; it will only have to learn the higher-level structures (e.g.,
          hairstyles). This is called transfer learning.              
                                                                      
          In summary, for many problems you can start with just one or two hidden layers and
          the neural network will work just fine. For instance, you can easily reach above 97%
          accuracy on the MNIST dataset using just one hidden layer with a few hundred neu‐
          rons, and above 98% accuracy using two hidden layers with the same total number of
          neurons, in roughly the same amount of training time. For more complex problems,
          you can ramp up the number of hidden layers until you start overfitting the training
          set. Very complex tasks, such as large image classification or speech recognition, typi‐
          cally require networks with dozens of layers (or even hundreds, but not fully connec‐
          ted ones, as we will see in Chapter 14), and they need a huge amount of training data.
          You will rarely have to train such networks from scratch: it is much more common to
          reuse parts of a pretrained state-of-the-art network that performs a similar task.
          Training will then be a lot faster and require much less data (we will discuss this in
          Chapter 11).                                                
          Number of Neurons per Hidden Layer                          
                                                                      
          The number of neurons in the input and output layers is determined by the type of
          input and output your task requires. For example, the MNIST task requires 28 × 28 =
          784 input neurons and 10 output neurons.                    
          As for the hidden layers, it used to be common to size them to form a pyramid, with
          fewer and fewer neurons at each layer—the rationale being that many low-level fea‐
                                                                      "|per hidden layer; neurons per hidden layer; transfer learning
"                                                                      
                                                                      
                                                                      
                                                                      
          tures can coalesce into far fewer high-level features. A typical neural network for
          MNIST might have 3 hidden layers, the first with 300 neurons, the second with 200,
          and the third with 100. However, this practice has been largely abandoned because it
          seems that using the same number of neurons in all hidden layers performs just as
          well in most cases, or even better; plus, there is only one hyperparameter to tune,
          instead of one per layer. That said, depending on the dataset, it can sometimes help to
          make the first hidden layer bigger than the others.         
                                                                      
          Just like the number of layers, you can try increasing the number of neurons gradu‐
          ally until the network starts overfitting. But in practice, it’s often simpler and more
          efficient to pick a model with more layers and neurons than you actually need, then
          use early stopping and other regularization techniques to prevent it from overfitting.
          Vincent Vanhoucke, a scientist at Google, has dubbed this the “stretch pants”
          approach: instead of wasting time looking for pants that perfectly match your size,
          just use large stretch pants that will shrink down to the right size. With this approach,
          you avoid bottleneck layers that could ruin your model. On the flip side, if a layer has
          too few neurons, it will not have enough representational power to preserve all the
          useful information from the inputs (e.g., a layer with two neurons can only output 2D
          data, so if it processes 3D data, some information will be lost). No matter how big and
          powerful the rest of the network is, that information will never be recovered.
                   In general you will get more bang for your buck by increasing the
                   number of layers instead of the number of neurons per layer.
                                                                      
                                                                      
                                                                      
          Learning Rate, Batch Size, and Other Hyperparameters        
                                                                      
          The numbers of hidden layers and neurons are not the only hyperparameters you can
          tweak in an MLP. Here are some of the most important ones, as well as tips on how to
          set them:                                                   
          Learning rate                                               
            The learning rate is arguably the most important hyperparameter. In general, the
            optimal learning rate is about half of the maximum learning rate (i.e., the learn‐
            ing rate above which the training algorithm diverges, as we saw in Chapter 4).
            One way to find a good learning rate is to train the model for a few hundred iter‐
            ations, starting with a very low learning rate (e.g., 10-5) and gradually increasing
            it up to a very large value (e.g., 10). This is done by multiplying the learning rate
            by a constant factor at each iteration (e.g., by exp(log(106)/500) to go from 10-5 to
            10 in 500 iterations). If you plot the loss as a function of the learning rate (using a
            log scale for the learning rate), you should see it dropping at first. But after a
            while, the learning rate will be too large, so the loss will shoot back up: the opti‐
                                                                      "|learning rate; batch size
"                                                                      
                                                                      
                                                                      
                                                                      
          Number of iterations                                        
            In most cases, the number of training iterations does not actually need to be
            tweaked: just use early stopping instead.                 
                                                                      
                   The optimal learning rate depends on the other hyperparameters—
                   especially the batch size—so if you modify any hyperparameter,
                   make sure to update the learning rate as well.     
                                                                      
                                                                      
          For more best practices regarding tuning neural network hyperparameters, check out
          the excellent 2018 paper27 by Leslie Smith.                 
                                                                      
          This concludes our introduction to artificial neural networks and their implementa‐
          tion with Keras. In the next few chapters, we will discuss techniques to train very
          deep nets. We will also explore how to customize models using TensorFlow’s lower-
          level API and how to load and preprocess data efficiently using the Data API. And we
          will dive into other popular neural network architectures: convolutional neural net‐
          works for image processing, recurrent neural networks for sequential data, autoen‐
          coders for representation learning, and generative adversarial networks to model and
          generate data.28                                            
          Exercises                                                   
                                                                      
                                                                      
           1. The TensorFlow Playground is a handy neural network simulator built by the
            TensorFlow team. In this exercise, you will train several binary classifiers in just a
            few clicks, and tweak the model’s architecture and its hyperparameters to gain
            some intuition on how neural networks work and what their hyperparameters
            do. Take some time to explore the following:              
             a. The patterns learned by a neural net. Try training the default neural network
              by clicking the Run button (top left). Notice how it quickly finds a good solu‐
              tion for the classification task. The neurons in the first hidden layer have
              learned simple patterns, while the neurons in the second hidden layer have
              learned to combine the simple patterns of the first hidden layer into more
              complex patterns. In general, the more layers there are, the more complex the
              patterns can be.                                        
            b. Activation functions. Try replacing the tanh activation function with a ReLU
              activation function, and train the network again. Notice that it finds a solution
                                                                      
                                                                      
          27 Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch
           Size, Momentum, and Weight Decay,” arXiv preprint arXiv:1803.09820 (2018).
          28 A few extra ANN architectures are presented in Appendix E."|fine-tuning hyperparameters; fine-tuning for neural networks
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 11          
                                                                      
                        Training  Deep  Neural   Networks             
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          In Chapter 10 we introduced artificial neural networks and trained our first deep
          neural networks. But they were shallow nets, with just a few hidden layers. What if
          you need to tackle a complex problem, such as detecting hundreds of types of objects
          in high-resolution images? You may need to train a much deeper DNN, perhaps with
          10 layers or many more, each containing hundreds of neurons, linked by hundreds of
          thousands of connections. Training a deep DNN isn’t a walk in the park. Here are
          some of the problems you could run into:                    
           • You may be faced with the tricky vanishing gradients problem or the related
            exploding gradients problem. This is when the gradients grow smaller and
            smaller, or larger and larger, when flowing backward through the DNN during
            training. Both of these problems make lower layers very hard to train.
                                                                      
           • You might not have enough training data for such a large network, or it might be
            too costly to label.                                      
           • Training may be extremely slow.                          
           • A model with millions of parameters would severely risk overfitting the training
            set, especially if there are not enough training instances or if they are too noisy.
                                                                      
          In this chapter we will go through each of these problems and present techniques to
          solve them. We will start by exploring the vanishing and exploding gradients prob‐
          lems and some of their most popular solutions. Next, we will look at transfer learning
          and unsupervised pretraining, which can help you tackle complex tasks even when
          you have little labeled data. Then we will discuss various optimizers that can speed up
          training large models tremendously. Finally, we will go through a few popular regula‐
          rization techniques for large neural networks.              
          With these tools, you will be able to train very deep nets. Welcome to Deep Learning!
                                                                      "|deep neural networks (DNNs)
"                                                                      
                                                                      
                                                                      
                                                                      
          The Vanishing/Exploding Gradients Problems                  
                                                                      
          As we discussed in Chapter 10, the backpropagation algorithm works by going from
          the output layer to the input layer, propagating the error gradient along the way. Once
          the algorithm has computed the gradient of the cost function with regard to each
          parameter in the network, it uses these gradients to update each parameter with a
          Gradient Descent step.                                      
                                                                      
          Unfortunately, gradients often get smaller and smaller as the algorithm progresses
          down to the lower layers. As a result, the Gradient Descent update leaves the lower
          layers’ connection weights virtually unchanged, and training never converges to a
          good solution. We call this the vanishing gradients problem. In some cases, the oppo‐
          site can happen: the gradients can grow bigger and bigger until layers get insanely
          large weight updates and the algorithm diverges. This is the exploding gradients prob‐
          lem, which surfaces in recurrent neural networks (see Chapter 15). More generally,
          deep neural networks suffer from unstable gradients; different layers may learn at
          widely different speeds.                                    
          This unfortunate behavior was empirically observed long ago, and it was one of the
          reasons deep neural networks were mostly abandoned in the early 2000s. It wasn’t
          clear what caused the gradients to be so unstable when training a DNN, but some
          light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio.1 The authors
          found a few suspects, including the combination of the popular logistic sigmoid acti‐
          vation function and the weight initialization technique that was most popular at the
          time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In
          short, they showed that with this activation function and this initialization scheme,
          the variance of the outputs of each layer is much greater than the variance of its
          inputs. Going forward in the network, the variance keeps increasing after each layer
          until the activation function saturates at the top layers. This saturation is actually
          made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyper‐
          bolic tangent function has a mean of 0 and behaves slightly better than the logistic
          function in deep networks).                                 
          Looking at the logistic activation function (see Figure 11-1), you can see that when
          inputs become large (negative or positive), the function saturates at 0 or 1, with a
          derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually
          no gradient to propagate back through the network; and what little gradient exists
          keeps getting diluted as backpropagation progresses down through the top layers, so
          there is really nothing left for the lower layers.          
                                                                      
                                                                      
                                                                      
          1 Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training Deep Feedforward Neural Net‐
           works,” Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010): 249–256."|exploding gradients problem; Logistic (sigmoid) function; vanishing/exploding gradients problems; Logistic (sigmoid); sigmoid (Logistic) activation function
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-1. Logistic activation function saturation        
                                                                      
          Glorot and He Initialization                                
          In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable
          gradients problem. They point out that we need the signal to flow properly in both
          directions: in the forward direction when making predictions, and in the reverse
          direction when backpropagating gradients. We don’t want the signal to die out, nor
          do we want it to explode and saturate. For the signal to flow properly, the authors
          argue that we need the variance of the outputs of each layer to be equal to the var‐
          iance of its inputs,2 and we need the gradients to have equal variance before and after
          flowing through a layer in the reverse direction (please check out the paper if you are
          interested in the mathematical details). It is actually not possible to guarantee both
          unless the layer has an equal number of inputs and neurons (these numbers are called
          the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compro‐
          mise that has proven to work very well in practice: the connection weights of each
          layer must be initialized randomly as described in Equation 11-1, where fan = (fan
                                                      avg  in         
          + fan )/2. This initialization strategy is called Xavier initialization or Glorot initiali‐
             out                                                      
          zation, after the paper’s first author.                     
                                                                      
                                                                      
                                                                      
                                                                      
          2 Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but
           if you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐
           ing. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come
           out loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude
           as it came in.                                             "|Glorot and He initialization; Xavier initialization; He initialization; fan-in/fan-out numbers
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
            Equation 11-1. Glorot initialization (when using the logistic activation function)
                                                                      
                                       2  1                           
            Normal distribution with mean 0 and variance σ =          
                                         fan                          
                                           avg                        
                                             3                        
            Or a uniform distribution between −r and +r, with r=      
                                            fan                       
                                              avg                     
          If you replace fan with fan in Equation 11-1, you get an initialization strategy that
                    avg    in                                         
          Yann LeCun proposed in the 1990s. He called it LeCun initialization. Genevieve Orr
          and Klaus-Robert Müller even recommended it in their 1998 book Neural Networks:
          Tricks of the Trade (Springer). LeCun initialization is equivalent to Glorot initializa‐
          tion when fan = fan . It took over a decade for researchers to realize how important
                  in  out                                             
          this trick is. Using Glorot initialization can speed up training considerably, and it is
          one of the tricks that led to the success of Deep Learning. 
          Some papers3 have provided similar strategies for different activation functions.
          These strategies differ only by the scale of the variance and whether they use fan or
                                                         avg          
          fan , as shown in Table 11-1 (for the uniform distribution, just compute r = 3σ2).
            in                                                        
          The initialization strategy for the ReLU activation function (and its variants, includ‐
          ing the ELU activation described shortly) is sometimes called He initialization, after
          the paper’s first author. The SELU activation function will be explained later in this
          chapter. It should be used with LeCun initialization (preferably with a normal distri‐
          bution, as we will see).                                    
          Table 11-1. Initialization parameters for each type of activation function
          Initialization Activation functions σ² (Normal)             
          Glorot None, tanh, logistic, softmax 1 / fan                
                               avg                                    
          He     ReLU and variants 2 / fan                            
                               in                                     
          LeCun  SELU        1 / fan                                  
                               in                                     
          By default, Keras uses Glorot initialization with a uniform distribution. When creat‐
          ing a layer, you can change this to He initialization by setting kernel_initial
          izer=""he_uniform"" or kernel_initializer=""he_normal"" like this:
            keras.layers.Dense(10, activation=""relu"", kernel_initializer=""he_normal"")
          If you want He initialization with a uniform distribution but based on fan rather
                                                      avg             
          than fan , you can use the VarianceScaling initializer like this:
               in                                                     
          3 E.g., Kaiming He et al., “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
           Classification,” Proceedings of the 2015 IEEE International Conference on Computer Vision (2015): 1026–1034."|Scaled Exponential Linear Unit (SELU); LeCun initialization; Scaled Exponential Linear Unit (SELU) function
"                                                                      
                                                                      
                                                                      
                                                                      
            he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',
                                         distribution='uniform')      
            keras.layers.Dense(10, activation=""sigmoid"", kernel_initializer=he_avg_init)
          Nonsaturating Activation Functions                          
                                                                      
          One of the insights in the 2010 paper by Glorot and Bengio was that the problems
          with unstable gradients were in part due to a poor choice of activation function. Until
          then most people had assumed that if Mother Nature had chosen to use roughly sig‐
          moid activation functions in biological neurons, they must be an excellent choice. But
          it turns out that other activation functions behave much better in deep neural net‐
          works—in particular, the ReLU activation function, mostly because it does not satu‐
          rate for positive values (and because it is fast to compute).
          Unfortunately, the ReLU activation function is not perfect. It suffers from a problem
          known as the dying ReLUs: during training, some neurons effectively “die,” meaning
          they stop outputting anything other than 0. In some cases, you may find that half of
          your network’s neurons are dead, especially if you used a large learning rate. A neu‐
          ron dies when its weights get tweaked in such a way that the weighted sum of its
          inputs are negative for all instances in the training set. When this happens, it just
          keeps outputting zeros, and Gradient Descent does not affect it anymore because the
          gradient of the ReLU function is zero when its input is negative.4
                                                                      
          To solve this problem, you may want to use a variant of the ReLU function, such as
          the leaky ReLU. This function is defined as LeakyReLU (z) = max(αz, z) (see
                                             α                        
          Figure 11-2). The hyperparameter α defines how much the function “leaks”: it is the
          slope of the function for z < 0 and is typically set to 0.01. This small slope ensures that
          leaky ReLUs never die; they can go into a long coma, but they have a chance to even‐
          tually wake up. A 2015 paper5 compared several variants of the ReLU activation func‐
          tion, and one of its conclusions was that the leaky variants always outperformed the
          strict ReLU activation function. In fact, setting α = 0.2 (a huge leak) seemed to result
          in better performance than α = 0.01 (a small leak). The paper also evaluated the
          randomized leaky ReLU (RReLU), where α is picked randomly in a given range during
          training and is fixed to an average value during testing. RReLU also performed fairly
          well and seemed to act as a regularizer (reducing the risk of overfitting the training
          set). Finally, the paper evaluated the parametric leaky ReLU (PReLU), where α is
          authorized to be learned during training (instead of being a hyperparameter, it
          becomes a parameter that can be modified by backpropagation like any other param‐
          4 Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: Gradient Descent
           may indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s
           inputs is positive again.                                  
          5 Bing Xu et al., “Empirical Evaluation of Rectified Activations in Convolutional Network,” arXiv preprint
           arXiv:1505.00853 (2015).                                   "|randomized leaky ReLU (RReLU); dying ReLUs problem; nonsaturating activation functions; leaky ReLU function; nonsaturating; parametric leaky ReLU (PReLU)
"                                                                      
                                                                      
                                                                      
                                                                      
          eter). PReLU was reported to strongly outperform ReLU on large image datasets, but
          on smaller datasets it runs the risk of overfitting the training set.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-2. Leaky ReLU: like ReLU, but with a small slope for negative values
                                                                      
          Last but not least, a 2015 paper by Djork-Arné Clevert et al.6 proposed a new activa‐
          tion function called the exponential linear unit (ELU) that outperformed all the ReLU
          variants in the authors’ experiments: training time was reduced, and the neural net‐
          work performed better on the test set. Figure 11-3 graphs the function, and Equation
          11-2 shows its definition.                                  
            Equation 11-2. ELU activation function                    
                                                                      
                   α exp z −1 if z<0                                  
            ELU z =                                                   
               α   z        if z≥0                                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-3. ELU activation function                        
                                                                      
                                                                      
          6 Djork-Arné Clevert et al., “Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),”
           Proceedings of the International Conference on Learning Representations (2016)."|exponential linear unit (ELU); ELU (exponential linear unit)
"                                                                      
                                                                      
                                                                      
                                                                      
          The ELU activation function looks a lot like the ReLU function, with a few major
          differences:                                                
                                                                      
           • It takes on negative values when z < 0, which allows the unit to have an average
            output closer to 0 and helps alleviate the vanishing gradients problem. The
            hyperparameter α defines the value that the ELU function approaches when z is a
            large negative number. It is usually set to 1, but you can tweak it like any other
            hyperparameter.                                           
           • It has a nonzero gradient for z < 0, which avoids the dead neurons problem.
           • If α is equal to 1 then the function is smooth everywhere, including around z = 0,
            which helps speed up Gradient Descent since it does not bounce as much to the
            left and right of z = 0.                                  
                                                                      
          The main drawback of the ELU activation function is that it is slower to compute
          than the ReLU function and its variants (due to the use of the exponential function).
          Its faster convergence rate during training compensates for that slow computation,
          but still, at test time an ELU network will be slower than a ReLU network.
          Then, a 2017 paper7 by Günter Klambauer et al. introduced the Scaled ELU (SELU)
          activation function: as its name suggests, it is a scaled variant of the ELU activation
          function. The authors showed that if you build a neural network composed exclu‐
          sively of a stack of dense layers, and if all hidden layers use the SELU activation func‐
          tion, then the network will self-normalize: the output of each layer will tend to
          preserve a mean of 0 and standard deviation of 1 during training, which solves the
          vanishing/exploding gradients problem. As a result, the SELU activation function
          often significantly outperforms other activation functions for such neural nets (espe‐
          cially deep ones). There are, however, a few conditions for self-normalization to hap‐
          pen (see the paper for the mathematical justification):     
                                                                      
           • The input features must be standardized (mean 0 and standard deviation 1).
           • Every hidden layer’s weights must be initialized with LeCun normal initialization.
            In Keras, this means setting kernel_initializer=""lecun_normal"".
           • The network’s architecture must be sequential. Unfortunately, if you try to use
            SELU in nonsequential architectures, such as recurrent networks (see Chap‐
            ter 15) or networks with skip connections (i.e., connections that skip layers, such
            as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will
            not necessarily outperform other activation functions.    
                                                                      
                                                                      
                                                                      
                                                                      
          7 Günter Klambauer et al., “Self-Normalizing Neural Networks,” Proceedings of the 31st International Conference
           on Neural Information Processing Systems (2017): 972–981.  "|Scaled Exponential Linear Unit (SELU); self-normalization; skip connections; Scaled Exponential Linear Unit (SELU) function
"                                                                      
                                                                      
                                                                      
                                                                      
           • The paper only guarantees self-normalization if all layers are dense, but some
            researchers have noted that the SELU activation function can improve perfor‐
            mance in convolutional neural nets as well (see Chapter 14).
                                                                      
                                                                      
                   So, which activation function should you use for the hidden layers
                   of your deep neural networks? Although your mileage will vary, in
                   general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh
                   > logistic. If the network’s architecture prevents it from self-
                   normalizing, then ELU may perform better than SELU (since SELU
                   is not smooth at z = 0). If you care a lot about runtime latency, then
                   you may prefer leaky ReLU. If you don’t want to tweak yet another
                   hyperparameter, you may use the default α values used by Keras
                   (e.g., 0.3 for leaky ReLU). If you have spare time and computing
                   power, you can use cross-validation to evaluate other activation
                   functions, such as RReLU if your network is overfitting or PReLU
                   if you have a huge training set. That said, because ReLU is the most
                   used activation function (by far), many libraries and hardware
                   accelerators provide ReLU-specific optimizations; therefore, if
                   speed is your priority, ReLU might still be the best choice.
          To use the leaky ReLU activation function, create a LeakyReLU layer and add it to your
          model just after the layer you want to apply it to:         
            model = keras.models.Sequential([                         
               [...]                                                  
               keras.layers.Dense(10, kernel_initializer=""he_normal""),
               keras.layers.LeakyReLU(alpha=0.2),                     
               [...]                                                  
            ])                                                        
          For PReLU, replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no offi‐
          cial implementation of RReLU in Keras, but you can fairly easily implement your own
          (to learn how to do that, see the exercises at the end of Chapter 12).
          For SELU activation, set activation=""selu"" and kernel_initializer=""lecun_nor
          mal"" when creating a layer:                                 
            layer = keras.layers.Dense(10, activation=""selu"",         
                            kernel_initializer=""lecun_normal"")        
          Batch Normalization                                         
                                                                      
          Although using He initialization along with ELU (or any variant of ReLU) can signifi‐
          cantly reduce the danger of the vanishing/exploding gradients problems at the begin‐
          ning of training, it doesn’t guarantee that they won’t come back during training.
                                                                      
                                                                      "|Scaled Exponential Linear Unit (SELU); exponential linear unit (ELU); Scaled Exponential Linear Unit (SELU) function; ELU (exponential linear unit)
"                                                                      
                                                                      
                                                                      
                                                                      
          In a 2015 paper,8 Sergey Ioffe and Christian Szegedy proposed a technique called
          Batch Normalization (BN) that addresses these problems. The technique consists of
          adding an operation in the model just before or after the activation function of each
          hidden layer. This operation simply zero-centers and normalizes each input, then
          scales and shifts the result using two new parameter vectors per layer: one for scaling,
          the other for shifting. In other words, the operation lets the model learn the optimal
          scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as
          the very first layer of your neural network, you do not need to standardize your train‐
          ing set (e.g., using a StandardScaler); the BN layer will do it for you (well, approxi‐
          mately, since it only looks at one batch at a time, and it can also rescale and shift each
          input feature).                                             
          In order to zero-center and normalize the inputs, the algorithm needs to estimate
          each input’s mean and standard deviation. It does so by evaluating the mean and stan‐
          dard deviation of the input over the current mini-batch (hence the name “Batch Nor‐
          malization”). The whole operation is summarized step by step in Equation 11-3.
                                                                      
            Equation 11-3. Batch Normalization algorithm              
                                                                      
                     m                                                
                      B                                               
            1. μ = 1 ∑ x i                                            
                B  m                                                  
                    Bi=1                                              
                     m                                                
                      B                                               
            2. σ 2 = 1 ∑ x i −μ 2                                     
                B  m        B                                         
                    Bi=1                                              
                    i                                                 
                   x −μ                                               
                 i     B                                              
            3. x  =                                                   
                     2                                                
                    σ +ε                                              
                     B                                                
            4. z i =γ⊗x i +β                                          
          In this algorithm:                                          
           • μ is the vector of input means, evaluated over the whole mini-batch B (it con‐
             B                                                        
            tains one mean per input).                                
           • σ is the vector of input standard deviations, also evaluated over the whole mini-
             B                                                        
            batch (it contains one standard deviation per input).     
           • m is the number of instances in the mini-batch.          
              B                                                       
           • x(i) is the vector of zero-centered and normalized inputs for instance i.
          8 Sergey Ioffe and Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing
           Internal Covariate Shift,” Proceedings of the 32nd International Conference on Machine Learning (2015): 448–
           456.                                                       "|normalization; Batch Normalization (BN)
"                                                                      
                                                                      
                                                                      
                                                                      
           • γ is the output scale parameter vector for the layer (it contains one scale parame‐
            ter per input).                                           
                                                                      
           • ⊗ represents element-wise multiplication (each input is multiplied by its corre‐
            sponding output scale parameter).                         
           • β is the output shift (offset) parameter vector for the layer (it contains one offset
            parameter per input). Each input is offset by its corresponding shift parameter.
           • ε is a tiny number that avoids division by zero (typically 10–5). This is called a
            smoothing term.                                           
           • z(i) is the output of the BN operation. It is a rescaled and shifted version of the
            inputs.                                                   
                                                                      
          So during training, BN standardizes its inputs, then rescales and offsets them. Good!
          What about at test time? Well, it’s not that simple. Indeed, we may need to make pre‐
          dictions for individual instances rather than for batches of instances: in this case, we
          will have no way to compute each input’s mean and standard deviation. Moreover,
          even if we do have a batch of instances, it may be too small, or the instances may not
          be independent and identically distributed, so computing statistics over the batch
          instances would be unreliable. One solution could be to wait until the end of training,
          then run the whole training set through the neural network and compute the mean
          and standard deviation of each input of the BN layer. These “final” input means and
          standard deviations could then be used instead of the batch input means and stan‐
          dard deviations when making predictions. However, most implementations of Batch
          Normalization estimate these final statistics during training by using a moving aver‐
          age of the layer’s input means and standard deviations. This is what Keras does auto‐
          matically when you use the BatchNormalization layer. To sum up, four parameter
          vectors are learned in each batch-normalized layer: γ (the output scale vector) and β
          (the output offset vector) are learned through regular backpropagation, and μ (the
          final input mean vector) and σ (the final input standard deviation vector) are estima‐
          ted using an exponential moving average. Note that μ and σ are estimated during
          training, but they are used only after training (to replace the batch input means and
          standard deviations in Equation 11-3).                      
          Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all
          the deep neural networks they experimented with, leading to a huge improvement in
          the ImageNet classification task (ImageNet is a large database of images classified into
          many classes, commonly used to evaluate computer vision systems). The vanishing
          gradients problem was strongly reduced, to the point that they could use saturating
          activation functions such as the tanh and even the logistic activation function. The
          networks were also much less sensitive to the weight initialization. The authors were
          able to use much larger learning rates, significantly speeding up the learning process.
          Specifically, they note that:                               
                                                                      "|smoothing term
"                                                                      
                                                                      
                                                                      
                                                                      
            Applied to a state-of-the-art image classification model, Batch Normalization achieves
            the same accuracy with 14 times fewer training steps, and beats the original model by a
            significant margin. […] Using an ensemble of batch-normalized networks, we improve
            upon the best published result on ImageNet classification: reaching 4.9% top-5 valida‐
            tion error (and 4.8% test error), exceeding the accuracy of human raters.
          Finally, like a gift that keeps on giving, Batch Normalization acts like a regularizer,
          reducing the need for other regularization techniques (such as dropout, described
          later in this chapter).                                     
          Batch Normalization does, however, add some complexity to the model (although it
          can remove the need for normalizing the input data, as we discussed earlier). More‐
          over, there is a runtime penalty: the neural network makes slower predictions due to
          the extra computations required at each layer. Fortunately, it’s often possible to fuse
          the BN layer with the previous layer, after training, thereby avoiding the runtime pen‐
          alty. This is done by updating the previous layer’s weights and biases so that it directly
          produces outputs of the appropriate scale and offset. For example, if the previous
          layer computes XW + b, then the BN layer will compute γ⊗(XW + b – μ)/σ + β
          (ignoring the smoothing term ε in the denominator). If we define W′ = γ⊗W/σ and b
          ′ = γ⊗(b – μ)/σ + β, the equation simplifies to XW′ + b′. So if we replace the previous
          layer’s weights and biases (W and b) with the updated weights and biases (W′ and b′),
          we can get rid of the BN layer (TFLite’s optimizer does this automatically; see Chap‐
          ter 19).                                                    
                                                                      
                   You may find that training is rather slow, because each epoch takes
                   much more time when you use Batch Normalization. This is usu‐
                   ally counterbalanced by the fact that convergence is much faster
                   with BN, so it will take fewer epochs to reach the same perfor‐
                   mance. All in all, wall time will usually be shorter (this is the time
                   measured by the clock on your wall).               
          Implementing Batch Normalization with Keras                 
                                                                      
          As with most things with Keras, implementing Batch Normalization is simple and
          intuitive. Just add a BatchNormalization layer before or after each hidden layer’s
          activation function, and optionally add a BN layer as well as the first layer in your
          model. For example, this model applies BN after every hidden layer and as the first
          layer in the model (after flattening the input images):     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|wall time; implementing Batch Normalization with
"                                                                      
                                                                      
                                                                      
                                                                      
          achieving state-of-the-art performance on complex image classification tasks. As this
          is bleeding-edge research, however, you may want to wait for additional research to
          confirm this finding before you drop Batch Normalization.   
                                                                      
          Gradient Clipping                                           
                                                                      
          Another popular technique to mitigate the exploding gradients problem is to clip the
          gradients during backpropagation so that they never exceed some threshold. This is
          called Gradient Clipping.12 This technique is most often used in recurrent neural net‐
          works, as Batch Normalization is tricky to use in RNNs, as we will see in Chapter 15.
          For other types of networks, BN is usually sufficient.      
          In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or
          clipnorm argument when creating an optimizer, like this:    
            optimizer = keras.optimizers.SGD(clipvalue=1.0)           
            model.compile(loss=""mse"", optimizer=optimizer)            
                                                                      
          This optimizer will clip every component of the gradient vector to a value between
          –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each
          and every trainable parameter) will be clipped between –1.0 and 1.0. The threshold is
          a hyperparameter you can tune. Note that it may change the orientation of the gradi‐
          ent vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly
          in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0],
          which points roughly in the diagonal between the two axes. In practice, this approach
          works well. If you want to ensure that Gradient Clipping does not change the direc‐
          tion of the gradient vector, you should clip by norm by setting clipnorm instead of
          clipvalue. This will clip the whole gradient if its ℓ norm is greater than the thres‐
                                        2                             
          hold you picked. For example, if you set clipnorm=1.0, then the vector [0.9, 100.0]
          will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost elim‐
          inating the first component. If you observe that the gradients explode during training
          (you can track the size of the gradients using TensorBoard), you may want to try both
          clipping by value and clipping by norm, with different thresholds, and see which
          option performs best on the validation set.                 
          Reusing Pretrained Layers                                   
          It is generally not a good idea to train a very large DNN from scratch: instead, you
          should always try to find an existing neural network that accomplishes a similar task
          to the one you are trying to tackle (we will discuss how to find them in Chapter 14),
          then reuse the lower layers of this network. This technique is called transfer learning.
                                                                      
                                                                      
          12 Razvan Pascanu et al., “On the Difficulty of Training Recurrent Neural Networks,” Proceedings of the 30th
           International Conference on Machine Learning (2013): 1310–1318."|reusing pretrained layers; reusing pretrained; gradient clipping; vanishing/exploding gradients problems; transfer learning
"                                                                      
                                                                      
                                                                      
                                                                      
                   The more similar the tasks are, the more layers you want to reuse
                   (starting with the lower layers). For very similar tasks, try keeping
                   all the hidden layers and just replacing the output layer.
                                                                      
                                                                      
          Try freezing all the reused layers first (i.e., make their weights non-trainable so that
          Gradient Descent won’t modify them), then train your model and see how it per‐
          forms. Then try unfreezing one or two of the top hidden layers to let backpropaga‐
          tion tweak them and see if performance improves. The more training data you have,
          the more layers you can unfreeze. It is also useful to reduce the learning rate when
          you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.
                                                                      
          If you still cannot get good performance, and you have little training data, try drop‐
          ping the top hidden layer(s) and freezing all the remaining hidden layers again. You
          can iterate until you find the right number of layers to reuse. If you have plenty of
          training data, you may try replacing the top hidden layers instead of dropping them,
          and even adding more hidden layers.                         
          Transfer Learning with Keras                                
                                                                      
          Let’s look at an example. Suppose the Fashion MNIST dataset only contained eight
          classes—for example, all the classes except for sandal and shirt. Someone built and
          trained a Keras model on that set and got reasonably good performance (>90% accu‐
          racy). Let’s call this model A. You now want to tackle a different task: you have images
          of sandals and shirts, and you want to train a binary classifier (positive=shirt,
          negative=sandal). Your dataset is quite small; you only have 200 labeled images.
          When you train a new model for this task (let’s call it model B) with the same archi‐
          tecture as model A, it performs reasonably well (97.2% accuracy). But since it’s a
          much easier task (there are just two classes), you were hoping for more. While drink‐
          ing your morning coffee, you realize that your task is quite similar to task A, so per‐
          haps transfer learning can help? Let’s find out!            
          First, you need to load model A and create a new model based on that model’s layers.
          Let’s reuse all the layers except for the output layer:     
            model_A = keras.models.load_model(""my_model_A.h5"")        
            model_B_on_A = keras.models.Sequential(model_A.layers[:-1])
            model_B_on_A.add(keras.layers.Dense(1, activation=""sigmoid""))
          Note that model_A and model_B_on_A now share some layers. When you train
          model_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone
          model_A before you reuse its layers. To do this, you clone model A’s architecture with
          clone_model(), then copy its weights (since clone_model() does not clone the
          weights):                                                   
                                                                      
                                                                      "|transfer learning with
"                                                                      
                                                                      
                                                                      
                                                                      
          paper just looks too positive, you should be suspicious: perhaps the flashy new tech‐
          nique does not actually help much (in fact, it may even degrade performance), but the
          authors tried many variants and reported only the best results (which may be due to
          sheer luck), without mentioning how many failures they encountered on the way.
          Most of the time, this is not malicious at all, but it is part of the reason so many
          results in science can never be reproduced.                 
                                                                      
          Why did I cheat? It turns out that transfer learning does not work very well with
          small dense networks, presumably because small networks learn few patterns, and
          dense networks learn very specific patterns, which are unlikely to be useful in other
          tasks. Transfer learning works best with deep convolutional neural networks, which
          tend to learn feature detectors that are much more general (especially in the lower
          layers). We will revisit transfer learning in Chapter 14, using the techniques we just
          discussed (and this time there will be no cheating, I promise!).
          Unsupervised Pretraining                                    
                                                                      
          Suppose you want to tackle a complex task for which you don’t have much labeled
          training data, but unfortunately you cannot find a model trained on a similar task.
          Don’t lose hope! First, you should try to gather more labeled training data, but if you
          can’t, you may still be able to perform unsupervised pretraining (see Figure 11-5).
          Indeed, it is often cheap to gather unlabeled training examples, but expensive to label
          them. If you can gather plenty of unlabeled training data, you can try to use it to train
          an unsupervised model, such as an autoencoder or a generative adversarial network
          (see Chapter 17). Then you can reuse the lower layers of the autoencoder or the lower
          layers of the GAN’s discriminator, add the output layer for your task on top, and fine-
          tune the final network using supervised learning (i.e., with the labeled training
          examples).                                                  
          It is this technique that Geoffrey Hinton and his team used in 2006 and which led to
          the revival of neural networks and the success of Deep Learning. Until 2010, unsuper‐
          vised pretraining—typically with restricted Boltzmann machines (RBMs; see Appen‐
          dix E)—was the norm for deep nets, and only after the vanishing gradients problem
          was alleviated did it become much more common to train DNNs purely using super‐
          vised learning. Unsupervised pretraining (today typically using autoencoders or
          GANs rather than RBMs) is still a good option when you have a complex task to
          solve, no similar model you can reuse, and little labeled training data but plenty of
          unlabeled training data.                                    
          Note that in the early days of Deep Learning it was difficult to train deep models, so
          people would use a technique called greedy layer-wise pretraining (depicted in
          Figure 11-5). They would first train an unsupervised model with a single layer, typi‐
          cally an RBM, then they would freeze that layer and add another one on top of it,
          then train the model again (effectively just training the new layer), then freeze the
                                                                      "|greedy layer-wise pretraining; restricted Boltzmann machines (RBMs); unsupervised pretraining
"                                                                      
                                                                      
                                                                      
                                                                      
          new layer and add another layer on top of it, train the model again, and so on. Nowa‐
          days, things are much simpler: people generally train the full unsupervised model in
          one shot (i.e., in Figure 11-5, just start directly at step three) and use autoencoders or
          GANs rather than RBMs.                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-5. In unsupervised training, a model is trained on the unlabeled data (or on
          all the data) using an unsupervised learning technique, then it is fine-tuned for the final
          task on the labeled data using a supervised learning technique; the unsupervised part
          may train one layer at a time as shown here, or it may train the full model directly
                                                                      
          Pretraining on an Auxiliary Task                            
                                                                      
          If you do not have much labeled training data, one last option is to train a first neural
          network on an auxiliary task for which you can easily obtain or generate labeled
          training data, then reuse the lower layers of that network for your actual task. The
          first neural network’s lower layers will learn feature detectors that will likely be reusa‐
          ble by the second neural network.                           
          For example, if you want to build a system to recognize faces, you may only have a
          few pictures of each individual—clearly not enough to train a good classifier. Gather‐
          ing hundreds of pictures of each person would not be practical. You could, however,
          gather a lot of pictures of random people on the web and train a first neural network
          to detect whether or not two different pictures feature the same person. Such a
                                                                      
                                                                      "|on auxiliary tasks
"                                                                      
                                                                      
                                                                      
                                                                      
          network would learn good feature detectors for faces, so reusing its lower layers
          would allow you to train a good face classifier that uses little training data.
                                                                      
          For natural language processing (NLP) applications, you can download a corpus of
          millions of text documents and automatically generate labeled data from it. For exam‐
          ple, you could randomly mask out some words and train a model to predict what the
          missing words are (e.g., it should predict that the missing word in the sentence “What
          ___ you saying?” is probably “are” or “were”). If you can train a model to reach good
          performance on this task, then it will already know quite a lot about language, and
          you can certainly reuse it for your actual task and fine-tune it on your labeled data
          (we will discuss more pretraining tasks in Chapter 15).     
                                                                      
                   Self-supervised learning is when you automatically generate the
                   labels from the data itself, then you train a model on the resulting
                   “labeled” dataset using supervised learning techniques. Since this
                   approach requires no human labeling whatsoever, it is best classi‐
                   fied as a form of unsupervised learning.           
          Faster Optimizers                                           
                                                                      
          Training a very large deep neural network can be painfully slow. So far we have seen
          four ways to speed up training (and reach a better solution): applying a good initiali‐
          zation strategy for the connection weights, using a good activation function, using
          Batch Normalization, and reusing parts of a pretrained network (possibly built on an
          auxiliary task or using unsupervised learning). Another huge speed boost comes from
          using a faster optimizer than the regular Gradient Descent optimizer. In this section
          we will present the most popular algorithms: momentum optimization, Nesterov
          Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam
          optimization.                                               
                                                                      
          Momentum Optimization                                       
          Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start
          out slowly, but it will quickly pick up momentum until it eventually reaches terminal
          velocity (if there is some friction or air resistance). This is the very simple idea behind
          momentum optimization, proposed by Boris Polyak in 1964.13 In contrast, regular
          Gradient Descent will simply take small, regular steps down the slope, so the algo‐
          rithm will take much more time to reach the bottom.         
                                                                      
                                                                      
                                                                      
                                                                      
          13 Boris T. Polyak, “Some Methods of Speeding Up the Convergence of Iteration Methods,” USSR Computational
           Mathematics and Mathematical Physics 4, no. 5 (1964): 1–17."|momentum optimization; reusing pretrained layers; reusing pretrained; creating faster; faster optimizers; self-supervised learning
"                                                                      
                                                                      
                                                                      
                                                                      
          Recall that Gradient Descent updates the weights θ by directly subtracting the gradi‐
          ent of the cost function J(θ) with regard to the weights (∇ J(θ)) multiplied by the
                                              θ                       
          learning rate η. The equation is: θ ← θ – η∇ J(θ). It does not care about what the ear‐
                                    θ                                 
          lier gradients were. If the local gradient is tiny, it goes very slowly.
          Momentum optimization cares a great deal about what previous gradients were: at
          each iteration, it subtracts the local gradient from the momentum vector m (multi‐
          plied by the learning rate η), and it updates the weights by adding this momentum
          vector (see Equation 11-4). In other words, the gradient is used for acceleration, not
          for speed. To simulate some sort of friction mechanism and prevent the momentum
          from growing too large, the algorithm introduces a new hyperparameter β, called the
          momentum, which must be set between 0 (high friction) and 1 (no friction). A typical
          momentum value is 0.9.                                      
            Equation 11-4. Momentum algorithm                         
            1. m   βm−η∇ J θ                                          
                        θ                                             
            2. θ  θ+m                                                 
          You can easily verify that if the gradient remains constant, the terminal velocity (i.e.,
          the maximum size of the weight updates) is equal to that gradient multiplied by the
          learning rate η multiplied by 1/(1–β) (ignoring the sign). For example, if β = 0.9, then
          the terminal velocity is equal to 10 times the gradient times the learning rate, so
          momentum optimization ends up going 10 times faster than Gradient Descent! This
          allows momentum optimization to escape from plateaus much faster than Gradient
          Descent. We saw in Chapter 4 that when the inputs have very different scales, the cost
          function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes
          down the steep slope quite fast, but then it takes a very long time to go down the val‐
          ley. In contrast, momentum optimization will roll down the valley faster and faster
          until it reaches the bottom (the optimum). In deep neural networks that don’t use
          Batch Normalization, the upper layers will often end up having inputs with very dif‐
          ferent scales, so using momentum optimization helps a lot. It can also help roll past
          local optima.                                               
                                                                      
                   Due to the momentum, the optimizer may overshoot a bit, then
                   come back, overshoot again, and oscillate like this many times
                   before stabilizing at the minimum. This is one of the reasons it’s
                   good to have a bit of friction in the system: it gets rid of these oscil‐
                   lations and thus speeds up convergence.            
                                                                      
          Implementing momentum optimization in Keras is a no-brainer: just use the SGD
          optimizer and set its momentum hyperparameter, then lie back and profit!
                                                                      "|momentum vector
"                                                                      
                                                                      
                                                                      
                                                                      
            optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)  
                                                                      
          The one drawback of momentum optimization is that it adds yet another hyperpara‐
          meter to tune. However, the momentum value of 0.9 usually works well in practice
          and almost always goes faster than regular Gradient Descent.
          Nesterov Accelerated Gradient                               
                                                                      
          One small variant to momentum optimization, proposed by Yurii Nesterov in 1983,14
          is almost always faster than vanilla momentum optimization. The Nesterov Acceler‐
          ated Gradient (NAG) method, also known as Nesterov momentum optimization, meas‐
          ures the gradient of the cost function not at the local position θ but slightly ahead in
          the direction of the momentum, at θ + βm (see Equation 11-5).
                                                                      
            Equation 11-5. Nesterov Accelerated Gradient algorithm    
            1. m   βm−η∇ J θ+βm                                       
                        θ                                             
            2. θ  θ+m                                                 
                                                                      
          This small tweak works because in general the momentum vector will be pointing in
          the right direction (i.e., toward the optimum), so it will be slightly more accurate to
          use the gradient measured a bit farther in that direction rather than the gradient at
          the original position, as you can see in Figure 11-6 (where ∇ represents the gradient
                                             1                        
          of the cost function measured at the starting point θ, and ∇ represents the gradient
                                             2                        
          at the point located at θ + βm).                            
          As you can see, the Nesterov update ends up slightly closer to the optimum. After a
          while, these small improvements add up and NAG ends up being significantly faster
          than regular momentum optimization. Moreover, note that when the momentum
          pushes the weights across a valley, ∇ continues to push farther across the valley,
                                 1                                    
          while ∇ pushes back toward the bottom of the valley. This helps reduce oscillations
               2                                                      
          and thus NAG converges faster.                              
          NAG is generally faster than regular momentum optimization. To use it, simply set
          nesterov=True when creating the SGD optimizer:              
            optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)
                                                                      
                                                                      
                                                                      
          14 Yurii Nesterov, “A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence
           O(1/k2),” Doklady AN USSR 269 (1983): 543–547.             "|Nesterov Accelerated Gradient (NAG); Nesterov momentum optimization
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-6. Regular versus Nesterov momentum optimization: the former applies the
          gradients computed before the momentum step, while the latter applies the gradients
          computed after                                              
                                                                      
          AdaGrad                                                     
                                                                      
          Consider the elongated bowl problem again: Gradient Descent starts by quickly going
          down the steepest slope, which does not point straight toward the global optimum,
          then it very slowly goes down to the bottom of the valley. It would be nice if the algo‐
          rithm could correct its direction earlier to point a bit more toward the global opti‐
          mum. The AdaGrad algorithm15 achieves this correction by scaling down the gradient
          vector along the steepest dimensions (see Equation 11-6).   
                                                                      
            Equation 11-6. AdaGrad algorithm                          
            1. s  s+∇ J θ ⊗∇ J θ                                      
                     θ     θ                                          
            2. θ  θ−η∇ J θ ⊘ s+ε                                      
                      θ                                               
          The first step accumulates the square of the gradients into the vector s (recall that the
          ⊗ symbol represents the element-wise multiplication). This vectorized form is equiv‐
          alent to computing s ← s + (∂ J(θ) / ∂ θ)2 for each element s of the vector s; in other
                      i  i       i           i                        
          words, each s accumulates the squares of the partial derivative of the cost function
                  i                                                   
          with regard to parameter θ. If the cost function is steep along the ith dimension, then
                          i                                           
          s will get larger and larger at each iteration.             
          i                                                           
          The second step is almost identical to Gradient Descent, but with one big difference:
          the gradient vector is scaled down by a factor of s+ε (the ⊘ symbol represents the
          15 John Duchi et al., “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” Journal
           of Machine Learning Research 12 (2011): 2121–2159.         "|AdaGrad
"                                                                      
                                                                      
                                                                      
                                                                      
          element-wise division, and ε is a smoothing term to avoid division by zero, typically
          set to 10–10). This vectorized form is equivalent to simultaneously computing
          θ  θ −η∂J θ /∂θ/ s +ε for all parameters θ.                 
           i  i       i i             i                               
          In short, this algorithm decays the learning rate, but it does so faster for steep dimen‐
          sions than for dimensions with gentler slopes. This is called an adaptive learning rate.
          It helps point the resulting updates more directly toward the global optimum (see
          Figure 11-7). One additional benefit is that it requires much less tuning of the learn‐
          ing rate hyperparameter η.                                  
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-7. AdaGrad versus Gradient Descent: the former can correct its direction ear‐
          lier to point to the optimum                                
                                                                      
          AdaGrad frequently performs well for simple quadratic problems, but it often stops
          too early when training neural networks. The learning rate gets scaled down so much
          that the algorithm ends up stopping entirely before reaching the global optimum. So
          even though Keras has an Adagrad optimizer, you should not use it to train deep neu‐
          ral networks (it may be efficient for simpler tasks such as Linear Regression, though).
          Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate
          optimizers.                                                 
                                                                      
          RMSProp                                                     
          As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never con‐
          verging to the global optimum. The RMSProp algorithm16 fixes this by accumulating
          only the gradients from the most recent iterations (as opposed to all the gradients
                                                                      
                                                                      
                                                                      
          16 This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey Hin‐
           ton in his Coursera class on neural networks (slides: https://homl.info/57; video: https://homl.info/58). Amus‐
           ingly, since the authors did not write a paper to describe the algorithm, researchers often cite “slide 29 in
           lecture 6” in their papers.                                "|adaptive learning rate; RMSProp
"                                                                      
                                                                      
                                                                      
                                                                      
          since the beginning of training). It does so by using exponential decay in the first step
          (see Equation 11-7).                                        
                                                                      
            Equation 11-7. RMSProp algorithm                          
                                                                      
            1. s  βs+ 1−β ∇ J θ ⊗∇ J θ                                
                         θ     θ                                      
            2. θ  θ−η∇ J θ ⊘ s+ε                                      
                      θ                                               
          The decay rate β is typically set to 0.9. Yes, it is once again a new hyperparameter, but
          this default value often works well, so you may not need to tune it at all.
          As you might expect, Keras has an RMSprop optimizer:        
            optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)   
          Note that the rho argument corresponds to β in Equation 11-7. Except on very simple
          problems, this optimizer almost always performs much better than AdaGrad. In fact,
          it was the preferred optimization algorithm of many researchers until Adam optimi‐
          zation came around.                                         
                                                                      
          Adam and Nadam Optimization                                 
                                                                      
          Adam,17 which stands for adaptive moment estimation, combines the ideas of momen‐
          tum optimization and RMSProp: just like momentum optimization, it keeps track of
          an exponentially decaying average of past gradients; and just like RMSProp, it keeps
          track of an exponentially decaying average of past squared gradients (see Equation
          11-8).18                                                    
                                                                      
            Equation 11-8. Adam algorithm                             
            1. m   β m− 1−β ∇ J θ                                     
                   1      1 θ                                         
            2. s  β s+ 1−β ∇ J θ ⊗∇ J θ                               
                   2    2  θ    θ                                     
                    m                                                 
            3. m                                                      
                      t                                               
                   1−β                                                
                      1                                               
                    s                                                 
            4.  s                                                     
                      t                                               
                  1−β                                                 
                     2                                                
            5. θ  θ+ηm⊘  s +ε                                         
          17 Diederik P. Kingma and Jimmy Ba, “Adam: A Method for Stochastic Optimization,” arXiv preprint arXiv:
           1412.6980 (2014).                                          
          18 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the
           first moment while the variance is often called the second moment, hence the name of the algorithm."|Adam and Nadam optimization; adaptive moment estimation
"                                                                      
                                                                      
                                                                      
                                                                      
            and in general Adam performs better. So, this is just one more optimizer you can
            try if you experience problems with Adam on some task.    
                                                                      
          Nadam                                                       
            Nadam optimization is Adam optimization plus the Nesterov trick, so it will
            often converge slightly faster than Adam. In his report introducing this techni‐
            que,19 the researcher Timothy Dozat compares many different optimizers on vari‐
            ous tasks and finds that Nadam generally outperforms Adam but is sometimes
            outperformed by RMSProp.                                  
                                                                      
                   Adaptive optimization methods (including RMSProp, Adam, and
                   Nadam optimization) are often great, converging fast to a good sol‐
                   ution. However, a 2017 paper20 by Ashia C. Wilson et al. showed
                   that they can lead to solutions that generalize poorly on some data‐
                   sets. So when you are disappointed by your model’s performance,
                   try using plain Nesterov Accelerated Gradient instead: your dataset
                   may just be allergic to adaptive gradients. Also check out the latest
                   research, because it’s moving fast.                
          All the optimization techniques discussed so far only rely on the first-order partial
          derivatives (Jacobians). The optimization literature also contains amazing algorithms
          based on the second-order partial derivatives (the Hessians, which are the partial
          derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply
          to deep neural networks because there are n2 Hessians per output (where n is the
          number of parameters), as opposed to just n Jacobians per output. Since DNNs typi‐
          cally have tens of thousands of parameters, the second-order optimization algorithms
          often don’t even fit in memory, and even when they do, computing the Hessians is
          just too slow.                                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          19 Timothy Dozat, “Incorporating Nesterov Momentum into Adam” (2016).
          20 Ashia C. Wilson et al., “The Marginal Value of Adaptive Gradient Methods in Machine Learning,” Advances in
           Neural Information Processing Systems 30 (2017): 4148–4158."|first- and second-order partial derivatives; second-order partial derivatives (Hessians); first-order partial derivatives (Jacobians)
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                           Training Sparse Models                     
                                                                      
           All the optimization algorithms just presented produce dense models, meaning that
           most parameters will be nonzero. If you need a blazingly fast model at runtime, or if
           you need it to take up less memory, you may prefer to end up with a sparse model
           instead.                                                   
           One easy way to achieve this is to train the model as usual, then get rid of the tiny
           weights (set them to zero). Note that this will typically not lead to a very sparse
           model, and it may degrade the model’s performance.         
           A better option is to apply strong ℓ regularization during training (we will see how
                               1                                      
           later in this chapter), as it pushes the optimizer to zero out as many weights as it can
           (as discussed in “Lasso Regression” on page 137 in Chapter 4).
           If these techniques remain insufficient, check out the TensorFlow Model Optimiza‐
           tion Toolkit (TF-MOT), which provides a pruning API capable of iteratively remov‐
           ing connections during training based on their magnitude.  
                                                                      
          Table 11-2 compares all the optimizers we’ve discussed so far (* is bad, ** is average,
          and *** is good).                                           
          Table 11-2. Optimizer comparison                            
                                                                      
          Class               Convergence speed Convergence quality   
          SGD                 *        ***                            
          SGD(momentum=...)   **       ***                            
          SGD(momentum=..., nesterov=True) ** ***                     
          Adagrad             ***      * (stops too early)            
          RMSprop             ***      ** or ***                      
          Adam                ***      ** or ***                      
                                                                      
          Nadam               ***      ** or ***                      
          AdaMax              ***      ** or ***                      
          Learning Rate Scheduling                                    
                                                                      
          Finding a good learning rate is very important. If you set it much too high, training
          may diverge (as we discussed in “Gradient Descent” on page 118). If you set it too
          low, training will eventually converge to the optimum, but it will take a very long
          time. If you set it slightly too high, it will make progress very quickly at first, but it
          will end up dancing around the optimum, never really settling down. If you have a
          limited computing budget, you may have to interrupt training before it has converged
          properly, yielding a suboptimal solution (see Figure 11-8). 
                                                                      "|sparse models; TensorFlow Model Optimization Toolkit (TF-MOT); training sparse models; learning rate scheduling
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 11-8. Learning curves for various learning rates η   
                                                                      
          As we discussed in Chapter 10, you can find a good learning rate by training the
          model for a few hundred iterations, exponentially increasing the learning rate from a
          very small value to a very large value, and then looking at the learning curve and
          picking a learning rate slightly lower than the one at which the learning curve starts
          shooting back up. You can then reinitialize your model and train it with that learning
          rate.                                                       
          But you can do better than a constant learning rate: if you start with a large learning
          rate and then reduce it once training stops making fast progress, you can reach a
          good solution faster than with the optimal constant learning rate. There are many dif‐
          ferent strategies to reduce the learning rate during training. It can also be beneficial to
          start with a low learning rate, increase it, then drop it again. These strategies are
          called learning schedules (we briefly introduced this concept in Chapter 4). These are
          the most commonly used learning schedules:                  
          Power scheduling                                            
            Set the learning rate to a function of the iteration number t: η(t) = η / (1 + t/s)c.
                                                    0                 
            The initial learning rate η , the power c (typically set to 1), and the steps s are
                            0                                         
            hyperparameters. The learning rate drops at each step. After s steps, it is down to
            η / 2. After s more steps, it is down to η / 3, then it goes down to η / 4, then η /
             0                      0               0     0           
            5, and so on. As you can see, this schedule first drops quickly, then more and
            more slowly. Of course, power scheduling requires tuning η and s (and possibly
                                               0                      
            c).                                                       
          Exponential scheduling                                      
            Set the learning rate to η(t) = η 0.1t/s. The learning rate will gradually drop by a
                               0                                      
            factor of 10 every s steps. While power scheduling reduces the learning rate more
            and more slowly, exponential scheduling keeps slashing it by a factor of 10 every
            s steps.                                                  "|learning schedules; power scheduling; exponential scheduling
"                                                                      
                                                                      
                                                                      
                                                                      
          Piecewise constant scheduling                               
            Use a constant learning rate for a number of epochs (e.g., η = 0.1 for 5 epochs),
                                               0                      
            then a smaller learning rate for another number of epochs (e.g., η = 0.001 for 50
                                                   1                  
            epochs), and so on. Although this solution can work very well, it requires fid‐
            dling around to figure out the right sequence of learning rates and how long to
            use each of them.                                         
          Performance scheduling                                      
            Measure the validation error every N steps (just like for early stopping), and
            reduce the learning rate by a factor of λ when the error stops dropping.
          1cycle scheduling                                           
            Contrary to the other approaches, 1cycle (introduced in a 2018 paper21 by Leslie
            Smith) starts by increasing the initial learning rate η , growing linearly up to η
                                            0              1          
            halfway through training. Then it decreases the learning rate linearly down to η
                                                           0          
            again during the second half of training, finishing the last few epochs by drop‐
            ping the rate down by several orders of magnitude (still linearly). The maximum
            learning rate η is chosen using the same approach we used to find the optimal
                     1                                                
            learning rate, and the initial learning rate η is chosen to be roughly 10 times
                                       0                              
            lower. When using a momentum, we start with a high momentum first (e.g.,
            0.95), then drop it down to a lower momentum during the first half of training
            (e.g., down to 0.85, linearly), and then bring it back up to the maximum value
            (e.g., 0.95) during the second half of training, finishing the last few epochs with
            that maximum value. Smith did many experiments showing that this approach
            was often able to speed up training considerably and reach better performance.
            For example, on the popular CIFAR10 image dataset, this approach reached
            91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800
            epochs through a standard approach (with the same neural network
            architecture).                                            
          A 2013 paper22 by Andrew Senior et al. compared the performance of some of the
          most popular learning schedules when using momentum optimization to train deep
          neural networks for speech recognition. The authors concluded that, in this setting,
          both performance scheduling and exponential scheduling performed well. They
          favored exponential scheduling because it was easy to tune and it converged slightly
          faster to the optimal solution (they also mentioned that it was easier to implement
          21 Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters: Part 1—Learning Rate, Batch
           Size, Momentum, and Weight Decay,” arXiv preprint arXiv:1803.09820 (2018).
          22 Andrew Senior et al., “An Empirical Study of Learning Rates in Deep Neural Networks for Speech Recogni‐
           tion,” Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (2013):
           6724–6728.                                                 "|1cycle scheduling; performance scheduling; piecewise constant scheduling
"                                                                      
                                                                      
                                                                      
                                                                      
          This implementation relies on the optimizer’s initial learning rate (contrary to the
          previous implementation), so make sure to set it appropriately.
                                                                      
          When you save a model, the optimizer and its learning rate get saved along with it.
          This means that with this new schedule function, you could just load a trained model
          and continue training where it left off, no problem. Things are not so simple if your
          schedule function uses the epoch argument, however: the epoch does not get saved,
          and it gets reset to 0 every time you call the fit() method. If you were to continue
          training a model where it left off, this could lead to a very large learning rate, which
          would likely damage your model’s weights. One solution is to manually set the fit()
          method’s initial_epoch argument so the epoch starts at the right value.
          For piecewise constant scheduling, you can use a schedule function like the following
          one (as earlier, you can define a more general function if you want; see the “Piecewise
          Constant Scheduling” section of the notebook for an example), then create a Lear
          ningRateScheduler callback with this function and pass it to the fit() method, just
          like we did for exponential scheduling:                     
            def piecewise_constant_fn(epoch):                         
               if epoch < 5:                                          
                 return 0.01                                          
               elif epoch < 15:                                       
                 return 0.005                                         
               else:                                                  
                 return 0.001                                         
          For performance scheduling, use the ReduceLROnPlateau callback. For example, if
          you pass the following callback to the fit() method, it will multiply the learning rate
          by 0.5 whenever the best validation loss does not improve for five consecutive epochs
          (other options are available; please check the documentation for more details):
            lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
          Lastly, tf.keras offers an alternative way to implement learning rate scheduling: define
          the learning rate using one of the schedules available in keras.optimizers.sched
          ules, then pass this learning rate to any optimizer. This approach updates the learn‐
          ing rate at each step rather than at each epoch. For example, here is how to implement
          the same exponential schedule as the exponential_decay_fn() function we defined
          earlier:                                                    
            s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)
            learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)
            optimizer = keras.optimizers.SGD(learning_rate)           
          This is nice and simple, plus when you save the model, the learning rate and its
          schedule (including its state) get saved as well. This approach, however, is not part of
          the Keras API; it is specific to tf.keras.                  
                                                                      
                                                                      "|implementing learning rate scheduling; tf.keras
"                                                                      
                                                                      
                                                                      
                                                                      
          As for the 1cycle approach, the implementation poses no particular difficulty: just
          create a custom callback that modifies the learning rate at each iteration (you can
          update the optimizer’s learning rate by changing self.model.optimizer.lr). See the
          “1Cycle scheduling” section of the notebook for an example. 
                                                                      
          To sum up, exponential decay, performance scheduling, and 1cycle can considerably
          speed up convergence, so give them a try!                   
          Avoiding Overfitting Through Regularization                 
                                                                      
            With four parameters I can fit an elephant and with five I can make him wiggle his
            trunk.                                                    
                                                                      
              —John von Neumann, cited by Enrico Fermi in Nature 427  
          With thousands of parameters, you can fit the whole zoo. Deep neural networks typi‐
          cally have tens of thousands of parameters, sometimes even millions. This gives them
          an incredible amount of freedom and means they can fit a huge variety of complex
          datasets. But this great flexibility also makes the network prone to overfitting the
          training set. We need regularization.                       
          We already implemented one of the best regularization techniques in Chapter 10:
          early stopping. Moreover, even though Batch Normalization was designed to solve
          the unstable gradients problems, it also acts like a pretty good regularizer. In this sec‐
          tion we will examine other popular regularization techniques for neural networks: ℓ
                                                           1          
          and ℓ regularization, dropout, and max-norm regularization. 
             2                                                        
          ℓ and ℓ Regularization                                      
           1    2                                                     
          Just like you did in Chapter 4 for simple linear models, you can use ℓ regularization
                                                   2                  
          to constrain a neural network’s connection weights, and/or ℓ regularization if you
                                              1                       
          want a sparse model (with many weights equal to 0). Here is how to apply ℓ regulari‐
                                                      2               
          zation to a Keras layer’s connection weights, using a regularization factor of 0.01:
            layer = keras.layers.Dense(100, activation=""elu"",         
                            kernel_initializer=""he_normal"",           
                            kernel_regularizer=keras.regularizers.l2(0.01))
          The l2() function returns a regularizer that will be called at each step during training
          to compute the regularization loss. This is then added to the final loss. As you might
          expect, you can just use keras.regularizers.l1() if you want ℓ regularization; if
                                                 1                    
          you want both ℓ and ℓ regularization, use keras.regularizers.l1_l2() (specifying
                   1   2                                              
          both regularization factors).                               
          Since you will typically want to apply the same regularizer to all layers in your net‐
          work, as well as using the same activation function and the same initialization strat‐
          egy in all hidden layers, you may find yourself repeating the same arguments. This"|avoiding through regularization; overfitting; avoiding overfitting through; faster optimizers
"                                                                      
                                                                      
                                                                      
                                                                      
          makes the code ugly and error-prone. To avoid this, you can try refactoring your code
          to use loops. Another option is to use Python’s functools.partial() function,
          which lets you create a thin wrapper for any callable, with some default argument
          values:                                                     
                                                                      
            from functools import partial                             
            RegularizedDense = partial(keras.layers.Dense,            
                            activation=""elu"",                         
                            kernel_initializer=""he_normal"",           
                            kernel_regularizer=keras.regularizers.l2(0.01))
            model = keras.models.Sequential([                         
               keras.layers.Flatten(input_shape=[28, 28]),            
               RegularizedDense(300),                                 
               RegularizedDense(100),                                 
               RegularizedDense(10, activation=""softmax"",             
                         kernel_initializer=""glorot_uniform"")         
            ])                                                        
          Dropout                                                     
          Dropout is one of the most popular regularization techniques for deep neural net‐
          works. It was proposed in a paper23 by Geoffrey Hinton in 2012 and further detailed
          in a 2014 paper24 by Nitish Srivastava et al., and it has proven to be highly successful:
          even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding
          dropout. This may not sound like a lot, but when a model already has 95% accuracy,
          getting a 2% accuracy boost means dropping the error rate by almost 40% (going
          from 5% error to roughly 3%).                               
          It is a fairly simple algorithm: at every training step, every neuron (including the
          input neurons, but always excluding the output neurons) has a probability p of being
          temporarily “dropped out,” meaning it will be entirely ignored during this training
          step, but it may be active during the next step (see Figure 11-9). The hyperparameter
          p is called the dropout rate, and it is typically set between 10% and 50%: closer to 20–
          30% in recurrent neural nets (see Chapter 15), and closer to 40–50% in convolutional
          neural networks (see Chapter 14). After training, neurons don’t get dropped any‐
          more. And that’s all (except for a technical detail we will discuss momentarily).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          23 Geoffrey E. Hinton et al., “Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors,”
           arXiv preprint arXiv:1207.0580 (2012).                     
          24 Nitish Srivastava et al., “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” Journal of
           Machine Learning Research 15 (2014): 1929–1958.            "|dropout
"                                                                      
                                                                      
                                                                      
                                                                      
                   In practice, you can usually apply dropout only to the neurons in
                   the top one to three layers (excluding the output layer).
                                                                      
                                                                      
                                                                      
          There is one small but important technical detail. Suppose p = 50%, in which case
          during testing a neuron would be connected to twice as many input neurons as it
          would be (on average) during training. To compensate for this fact, we need to multi‐
          ply each neuron’s input connection weights by 0.5 after training. If we don’t, each
          neuron will get a total input signal roughly twice as large as what the network was
          trained on and will be unlikely to perform well. More generally, we need to multiply
          each input connection weight by the keep probability (1 – p) after training. Alterna‐
          tively, we can divide each neuron’s output by the keep probability during training
          (these alternatives are not perfectly equivalent, but they work equally well).
                                                                      
          To implement dropout using Keras, you can use the keras.layers.Dropout layer.
          During training, it randomly drops some inputs (setting them to 0) and divides the
          remaining inputs by the keep probability. After training, it does nothing at all; it just
          passes the inputs to the next layer. The following code applies dropout regularization
          before every Dense layer, using a dropout rate of 0.2:      
            model = keras.models.Sequential([                         
               keras.layers.Flatten(input_shape=[28, 28]),            
               keras.layers.Dropout(rate=0.2),                        
               keras.layers.Dense(300, activation=""elu"", kernel_initializer=""he_normal""),
               keras.layers.Dropout(rate=0.2),                        
               keras.layers.Dense(100, activation=""elu"", kernel_initializer=""he_normal""),
               keras.layers.Dropout(rate=0.2),                        
               keras.layers.Dense(10, activation=""softmax"")           
            ])                                                        
                   Since dropout is only active during training, comparing the train‐
                   ing loss and the validation loss can be misleading. In particular, a
                   model may be overfitting the training set and yet have similar
                   training and validation losses. So make sure to evaluate the training
                   loss without dropout (e.g., after training).       
          If you observe that the model is overfitting, you can increase the dropout rate. Con‐
          versely, you should try decreasing the dropout rate if the model underfits the training
          set. It can also help to increase the dropout rate for large layers, and reduce it for
          small ones. Moreover, many state-of-the-art architectures only use dropout after the
          last hidden layer, so you may want to try this if full dropout is too strong.
                                                                      
                                                                      
                                                                      "|implementing dropout; keep probability
"                                                                      
                                                                      
                                                                      
                                                                      
          Dropout does tend to significantly slow down convergence, but it usually results in a
          much better model when tuned properly. So, it is generally well worth the extra time
          and effort.                                                 
                                                                      
                   If you want to regularize a self-normalizing network based on the
                   SELU activation function (as discussed earlier), you should use
                   alpha dropout: this is a variant of dropout that preserves the mean
                   and standard deviation of its inputs (it was introduced in the same
                   paper as SELU, as regular dropout would break self-normalization).
                                                                      
          Monte Carlo (MC) Dropout                                    
                                                                      
          In 2016, a paper25 by Yarin Gal and Zoubin Ghahramani added a few more good rea‐
          sons to use dropout:                                        
           • First, the paper established a profound connection between dropout networks
            (i.e., neural networks containing a Dropout layer before every weight layer) and
            approximate Bayesian inference,26 giving dropout a solid mathematical justifica‐
            tion.                                                     
                                                                      
           • Second, the authors introduced a powerful technique called MC Dropout, which
            can boost the performance of any trained dropout model without having to
            retrain it or even modify it at all, provides a much better measure of the model’s
            uncertainty, and is also amazingly simple to implement.   
          If this all sounds like a “one weird trick” advertisement, then take a look at the follow‐
          ing code. It is the full implementation of MC Dropout, boosting the dropout model
          we trained earlier without retraining it:                   
            y_probas = np.stack([model(X_test_scaled, training=True)  
                         for sample in range(100)])                   
            y_proba = y_probas.mean(axis=0)                           
          We just make 100 predictions over the test set, setting training=True to ensure that
          the Dropout layer is active, and stack the predictions. Since dropout is active, all the
          predictions will be different. Recall that predict() returns a matrix with one row per
          instance and one column per class. Because there are 10,000 instances in the test set
          and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so
          y_probas is an array of shape [100, 10000, 10]. Once we average over the first
                                                                      
                                                                      
                                                                      
          25 Yarin Gal and Zoubin Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty
           in Deep Learning,” Proceedings of the 33rd International Conference on Machine Learning (2016): 1050–1059.
          26 Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian
           inference in a specific type of probabilistic model called a Deep Gaussian Process."|Scaled Exponential Linear Unit (SELU); Monte Carlo (MC) dropout; Scaled Exponential Linear Unit (SELU) function
"                                                                      
                                                                      
                                                                      
                                                                      
                   The number of Monte Carlo samples you use (100 in this example)
                   is a hyperparameter you can tweak. The higher it is, the more accu‐
                   rate the predictions and their uncertainty estimates will be. How‐
                   ever, if you double it, inference time will also be doubled.
                   Moreover, above a certain number of samples, you will notice little
                   improvement. So your job is to find the right trade-off between
                   latency and accuracy, depending on your application.
                                                                      
          If your model contains other layers that behave in a special way during training (such
          as BatchNormalization layers), then you should not force training mode like we just
          did. Instead, you should replace the Dropout layers with the following MCDropout
          class:27                                                    
            class MCDropout(keras.layers.Dropout):                    
               def call(self, inputs):                                
                 return super().call(inputs, training=True)           
          Here, we just subclass the Dropout layer and override the call() method to force its
          training argument to True (see Chapter 12). Similarly, you could define an MCAlpha
          Dropout class by subclassing AlphaDropout instead. If you are creating a model from
          scratch, it’s just a matter of using MCDropout rather than Dropout. But if you have a
          model that was already trained using Dropout, you need to create a new model that’s
          identical to the existing model except that it replaces the Dropout layers with MCDrop
          out, then copy the existing model’s weights to your new model.
          In short, MC Dropout is a fantastic technique that boosts dropout models and pro‐
          vides better uncertainty estimates. And of course, since it is just regular dropout dur‐
          ing training, it also acts like a regularizer.              
                                                                      
          Max-Norm Regularization                                     
                                                                      
          Another regularization technique that is popular for neural networks is called max-
          norm regularization: for each neuron, it constrains the weights w of the incoming
          connections such that ∥ w ∥ ≤ r, where r is the max-norm hyperparameter and ∥ · ∥
                          2                                2          
          is the ℓ norm.                                              
              2                                                       
          Max-norm regularization does not add a regularization loss term to the overall loss
          function. Instead, it is typically implemented by computing ∥w∥ after each training
                                                2                     
          step and rescaling w if needed (w ← w r/‖ w ‖ ).            
                                    2                                 
          27 This MCDropout class will work with all Keras APIs, including the Sequential API. If you only care about the
           Functional API or the Subclassing API, you do not have to create an MCDropout class; you can create a regular
           Dropout layer and call it with training=True.              "|max-norm regularization
"                                                                      
                                                                      
                                                                      
                                                                      
          Reducing r increases the amount of regularization and helps reduce overfitting. Max-
          norm regularization can also help alleviate the unstable gradients problems (if you
          are not using Batch Normalization).                         
                                                                      
          To implement max-norm regularization in Keras, set the kernel_constraint argu‐
          ment of each hidden layer to a max_norm() constraint with the appropriate max value,
          like this:                                                  
            keras.layers.Dense(100, activation=""elu"", kernel_initializer=""he_normal"",
                       kernel_constraint=keras.constraints.max_norm(1.))
          After each training iteration, the model’s fit() method will call the object returned
          by max_norm(), passing it the layer’s weights and getting rescaled weights in return,
          which then replace the layer’s weights. As you’ll see in Chapter 12, you can define
          your own custom constraint function if necessary and use it as the kernel_con
          straint. You can also constrain the bias terms by setting the bias_constraint
          argument.                                                   
                                                                      
          The max_norm() function has an axis argument that defaults to 0. A Dense layer usu‐
          ally has weights of shape [number of inputs, number of neurons], so using axis=0
          means that the max-norm constraint will apply independently to each neuron’s
          weight vector. If you want to use max-norm with convolutional layers (see Chap‐
          ter 14), make sure to set the max_norm() constraint’s axis argument appropriately
          (usually axis=[0, 1, 2]).                                   
          Summary  and Practical Guidelines                           
                                                                      
          In this chapter we have covered a wide range of techniques, and you may be wonder‐
          ing which ones you should use. This depends on the task, and there is no clear con‐
          sensus yet, but I have found the configuration in Table 11-3 to work fine in most
          cases, without requiring much hyperparameter tuning. That said, please do not con‐
          sider these defaults as hard rules!                         
                                                                      
          Table 11-3. Default DNN configuration                       
                                                                      
          Hyperparameter Default value                                
          Kernel initializer He initialization                        
          Activation function ELU                                     
          Normalization None if shallow; Batch Norm if deep           
          Regularization Early stopping (+ℓ reg. if needed)           
                           2                                          
          Optimizer Momentum optimization (or RMSProp or Nadam)       
          Learning rate schedule 1cycle                               
                                                                      
                                                                      "|avoiding overfitting through; overfitting; default configuration; avoiding through regularization
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                  CHAPTER 12          
                                                                      
                           Custom   Models   and  Training            
                                                                      
                                         with TensorFlow              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Up until now, we’ve used only TensorFlow’s high-level API, tf.keras, but it already got
          us pretty far: we built various neural network architectures, including regression and
          classification nets, Wide & Deep nets, and self-normalizing nets, using all sorts of
          techniques, such as Batch Normalization, dropout, and learning rate schedules. In
          fact, 95% of the use cases you will encounter will not require anything other than
          tf.keras (and tf.data; see Chapter 13). But now it’s time to dive deeper into TensorFlow
          and take a look at its lower-level Python API. This will be useful when you need extra
          control to write custom loss functions, custom metrics, layers, models, initializers,
          regularizers, weight constraints, and more. You may even need to fully control the
          training loop itself, for example to apply special transformations or constraints to the
          gradients (beyond just clipping them) or to use multiple optimizers for different parts
          of the network. We will cover all these cases in this chapter, and we will also look at
          how you can boost your custom models and training algorithms using TensorFlow’s
          automatic graph generation feature. But first, let’s take a quick tour of TensorFlow.
                   TensorFlow 2.0 (beta) was released in June 2019, making Tensor‐
                   Flow much easier to use. The first edition of this book used TF 1,
                   while this edition uses TF 2.                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|TensorFlow, custom models and training; versions covered
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          A Quick Tour of TensorFlow                                  
                                                                      
          As you know, TensorFlow is a powerful library for numerical computation, particu‐
          larly well suited and fine-tuned for large-scale Machine Learning (but you could use
          it for anything else that requires heavy computations). It was developed by the Google
          Brain team and it powers many of Google’s large-scale services, such as Google Cloud
          Speech, Google Photos, and Google Search. It was open sourced in November 2015,
          and it is now the most popular Deep Learning library (in terms of citations in papers,
          adoption in companies, stars on GitHub, etc.). Countless projects use TensorFlow for
          all sorts of Machine Learning tasks, such as image classification, natural language
          processing, recommender systems, and time series forecasting.
          So what does TensorFlow offer? Here’s a summary:            
                                                                      
           • Its core is very similar to NumPy, but with GPU support. 
           • It supports distributed computing (across multiple devices and servers).
                                                                      
           • It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐
            tations for speed and memory usage. It works by extracting the computation
            graph from a Python function, then optimizing it (e.g., by pruning unused
            nodes), and finally running it efficiently (e.g., by automatically running inde‐
            pendent operations in parallel).                          
           • Computation graphs can be exported to a portable format, so you can train a
            TensorFlow model in one environment (e.g., using Python on Linux) and run it
            in another (e.g., using Java on an Android device).       
           • It implements autodiff (see Chapter 10 and Appendix D) and provides some
            excellent optimizers, such as RMSProp and Nadam (see Chapter 11), so you can
            easily minimize all sorts of loss functions.              
                                                                      
          TensorFlow offers many more features built on top of these core features: the most
          important is of course tf.keras,1 but it also has data loading and preprocessing ops
          (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops
          (tf.signal), and more (see Figure 12-1 for an overview of TensorFlow’s Python
          API).                                                       
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          1 TensorFlow includes another Deep Learning API called the Estimators API, but the TensorFlow team recom‐
           mends using tf.keras instead.                              "|computation graphs; features; just-in-time (JIT) compiler; benefits, xvi
"                                                                      
                                                                      
                                                                      
                                                                      
                   We will cover many of the packages and functions of the Tensor‐
                   Flow API, but it’s impossible to cover them all, so you should really
                   take some time to browse through the API; you will find that it is
                   quite rich and well documented.                    
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 12-1. TensorFlow’s Python API                        
                                                                      
          At the lowest level, each TensorFlow operation (op for short) is implemented using
          highly efficient C++ code.2 Many operations have multiple implementations called
          kernels: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or
          even TPUs (tensor processing units). As you may know, GPUs can dramatically speed
          up computations by splitting them into many smaller chunks and running them in
          parallel across many GPU threads. TPUs are even faster: they are custom ASIC chips
          built specifically for Deep Learning operations3 (we will discuss how to use Tensor‐
          Flow with GPUs or TPUs in Chapter 19).                      
          TensorFlow’s architecture is shown in Figure 12-2. Most of the time your code will
          use the high-level APIs (especially tf.keras and tf.data); but when you need more flex‐
          ibility, you will use the lower-level Python API, handling tensors directly. Note that
                                                                      
                                                                      
          2 If you ever need to (but you probably won’t), you can write your own operations using the C++ API.
          3 To learn more about TPUs and how they work, check out https://homl.info/tpus."|TensorFlow, basics of architecture; TPUs (tensor processing units); kernels
"                                                                      
                                                                      
                                                                      
                                                                      
          APIs for other languages are also available. In any case, TensorFlow’s execution
          engine will take care of running the operations efficiently, even across multiple devi‐
          ces and machines if you tell it to.                         
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 12-2. TensorFlow’s architecture                      
                                                                      
          TensorFlow runs not only on Windows, Linux, and macOS, but also on mobile devi‐
          ces (using TensorFlow Lite), including both iOS and Android (see Chapter 19). If you
          do not want to use the Python API, there are C++, Java, Go, and Swift APIs. There is
          even a JavaScript implementation called TensorFlow.js that makes it possible to run
          your models directly in your browser.                       
                                                                      
          There’s more to TensorFlow than the library. TensorFlow is at the center of an exten‐
          sive ecosystem of libraries. First, there’s TensorBoard for visualization (see Chap‐
          ter 10). Next, there’s TensorFlow Extended (TFX), which is a set of libraries built by
          Google to productionize TensorFlow projects: it includes tools for data validation,
          preprocessing, model analysis, and serving (with TF Serving; see Chapter 19). Goo‐
          gle’s TensorFlow Hub provides a way to easily download and reuse pretrained neural
          networks. You can also get many neural network architectures, some of them pre‐
          trained, in TensorFlow’s model garden. Check out the TensorFlow Resources and
          https://github.com/jtoy/awesome-tensorflow for more TensorFlow-based projects. You
          will find hundreds of TensorFlow projects on GitHub, so it is often easy to find exist‐
          ing code for whatever you are trying to do.                 
                   More and more ML papers are released along with their implemen‐
                   tations, and sometimes even with pretrained models. Check out
                   https://paperswithcode.com/ to easily find them.   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|operating system compatibility; locating papers on; TensorFlow Hub; library ecosystem; TensorFlow Lite
"                                                                      
                                                                      
                                                                      
                                                                      
          Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐
          opers, as well as a large community contributing to improving it. To ask technical
          questions, you should use http://stackoverflow.com/ and tag your question with ten‐
          sorflow and python. You can file bugs and feature requests through GitHub. For gen‐
          eral discussions, join the Google group.                    
                                                                      
          OK, it’s time to start coding!                              
          Using TensorFlow like NumPy                                 
                                                                      
          TensorFlow’s API revolves around tensors, which flow from operation to operation—
          hence the name TensorFlow. A tensor is very similar to a NumPy ndarray: it is usu‐
          ally a multidimensional array, but it can also hold a scalar (a simple value, such as 42).
          These tensors will be important when we create custom cost functions, custom met‐
          rics, custom layers, and more, so let’s see how to create and manipulate them.
                                                                      
          Tensors and Operations                                      
                                                                      
          You can create a tensor with tf.constant(). For example, here is a tensor represent‐
          ing a matrix with two rows and three columns of floats:     
            >>> tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix    
            <tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=     
            array([[1., 2., 3.],                                      
                [4., 5., 6.]], dtype=float32)>                        
            >>> tf.constant(42) # scalar                              
            <tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>        
          Just like an ndarray, a tf.Tensor has a shape and a data type (dtype):
            >>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])         
            >>> t.shape                                               
            TensorShape([2, 3])                                       
            >>> t.dtype                                               
            tf.float32                                                
          Indexing works much like in NumPy:                          
            >>> t[:, 1:]                                              
            <tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=     
            array([[2., 3.],                                          
                [5., 6.]], dtype=float32)>                            
            >>> t[..., 1, tf.newaxis]                                 
            <tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=    
            array([[2.],                                              
                [5.]], dtype=float32)>                                
          Most importantly, all sorts of tensor operations are available:
            >>> t + 10                                                
            <tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=    "|tensors; community support; getting help; using TensorFlow like
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                            Keras’ Low-Level API                      
                                                                      
           The Keras API has its own low-level API, located in keras.backend. It includes func‐
           tions like square(), exp(), and sqrt(). In tf.keras, these functions generally just call
           the corresponding TensorFlow operations. If you want to write code that will be
           portable to other Keras implementations, you should use these Keras functions. How‐
           ever, they only cover a subset of all functions available in TensorFlow, so in this book
           we will use the TensorFlow operations directly. Here is as simple example using
           keras.backend, which is commonly named K for short:        
             >>> from tensorflow import keras                         
             >>> K = keras.backend                                    
             >>> K.square(K.transpose(t)) + 10                        
             <tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=   
             array([[11., 26.],                                       
                  [14., 35.],                                         
                  [19., 46.]], dtype=float32)>                        
          Tensors and NumPy                                           
                                                                      
          Tensors play nice with NumPy: you can create a tensor from a NumPy array, and vice
          versa. You can even apply TensorFlow operations to NumPy arrays and NumPy oper‐
          ations to tensors:                                          
            >>> a = np.array([2., 4., 5.])                            
            >>> tf.constant(a)                                        
            <tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>
            >>> t.numpy() # or np.array(t)                            
            array([[1., 2., 3.],                                      
                [4., 5., 6.]], dtype=float32)                         
            >>> tf.square(a)                                          
            <tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>
            >>> np.square(t)                                          
            array([[ 1., 4., 9.],                                     
                [16., 25., 36.]], dtype=float32)                      
                   Notice that NumPy uses 64-bit precision by default, while Tensor‐
                   Flow uses 32-bit. This is because 32-bit precision is generally more
                   than enough for neural networks, plus it runs faster and uses less
                   RAM. So when you create a tensor from a NumPy array, make sure
                   to set dtype=tf.float32.                           
          Type Conversions                                            
          Type conversions can significantly hurt performance, and they can easily go unno‐
          ticed when they are done automatically. To avoid this, TensorFlow does not perform
                                                                      "|type conversions; low-level API
"                                                                      
                                                                      
                                                                      
                                                                      
          any type conversions automatically: it just raises an exception if you try to execute an
          operation on tensors with incompatible types. For example, you cannot add a float
          tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:
                                                                      
            >>> tf.constant(2.) + tf.constant(40)                     
            Traceback[...]InvalidArgumentError[...]expected to be a float[...]
            >>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)  
            Traceback[...]InvalidArgumentError[...]expected to be a double[...]
          This may be a bit annoying at first, but remember that it’s for a good cause! And of
          course you can use tf.cast() when you really need to convert types:
            >>> t2 = tf.constant(40., dtype=tf.float64)               
            >>> tf.constant(2.0) + tf.cast(t2, tf.float32)            
            <tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>  
          Variables                                                   
                                                                      
          The tf.Tensor values we’ve seen so far are immutable: you cannot modify them. This
          means that we cannot use regular tensors to implement weights in a neural network,
          since they need to be tweaked by backpropagation. Plus, other parameters may also
          need to change over time (e.g., a momentum optimizer keeps track of past gradients).
          What we need is a tf.Variable:                              
            >>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])         
            >>> v                                                     
            <tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=
            array([[1., 2., 3.],                                      
                [4., 5., 6.]], dtype=float32)>                        
          A tf.Variable acts much like a tf.Tensor: you can perform the same operations
          with it, it plays nicely with NumPy as well, and it is just as picky with types. But it can
          also be modified in place using the assign() method (or assign_add() or
          assign_sub(), which increment or decrement the variable by the given value). You
          can also modify individual cells (or slices), by using the cell’s (or slice’s) assign()
          method (direct item assignment will not work) or by using the scatter_update() or
          scatter_nd_update() methods:                                
            v.assign(2 * v) # => [[2., 4., 6.], [8., 10., 12.]]       
            v[0, 1].assign(42) # => [[2., 42., 6.], [8., 10., 12.]]   
            v[:, 2].assign([0., 1.]) # => [[2., 42., 0.], [8., 10., 1.]]
            v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.])
                            # => [[100., 42., 0.], [8., 10., 200.]]   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|variables
"                                                                      
                                                                      
                                                                      
                                                                      
                   In practice you will rarely have to create variables manually, since
                   Keras provides an add_weight() method that will take care of it for
                   you, as we will see. Moreover, model parameters will generally be
                   updated directly by the optimizers, so you will rarely need to
                   update variables manually.                         
                                                                      
          Other Data Structures                                       
                                                                      
          TensorFlow supports several other data structures, including the following (please see
          the “Tensors and Operations” section in the notebook or Appendix F for more
          details):                                                   
          Sparse tensors (tf.SparseTensor)                            
            Efficiently represent tensors containing mostly zeros. The tf.sparse package
            contains operations for sparse tensors.                   
                                                                      
          Tensor arrays (tf.TensorArray)                              
            Are lists of tensors. They have a fixed size by default but can optionally be made
            dynamic. All tensors they contain must have the same shape and data type.
          Ragged tensors (tf.RaggedTensor)                            
            Represent static lists of lists of tensors, where every tensor has the same shape
            and data type. The tf.ragged package contains operations for ragged tensors.
                                                                      
          String tensors                                              
            Are regular tensors of type tf.string. These represent byte strings, not Unicode
            strings, so if you create a string tensor using a Unicode string (e.g., a regular
            Python 3 string like ""café""), then it will get encoded to UTF-8 automatically
            (e.g., b""caf\xc3\xa9""). Alternatively, you can represent Unicode strings using
            tensors of type tf.int32, where each item represents a Unicode code point (e.g.,
            [99, 97, 102, 233]). The tf.strings package (with an s) contains ops for byte
            strings and Unicode strings (and to convert one into the other). It’s important to
            note that a tf.string is atomic, meaning that its length does not appear in the
            tensor’s shape. Once you convert it to a Unicode tensor (i.e., a tensor of type
            tf.int32 holding Unicode code points), the length appears in the shape.
          Sets                                                        
            Are represented as regular tensors (or sparse tensors). For example, tf.con
            stant([[1, 2], [3, 4]]) represents the two sets {1, 2} and {3, 4}. More gener‐
            ally, each set is represented by a vector in the tensor’s last axis. You can
            manipulate sets using operations from the tf.sets package.
                                                                      
          Queues                                                      
            Store tensors across multiple steps. TensorFlow offers various kinds of queues:
            simple First In, First Out (FIFO) queues (FIFOQueue), queues that can prioritize"|sparse tensors; ragged tensors; tensor arrays; sets; string tensors
"                                                                      
                                                                      
                                                                      
                                                                      
            some items (PriorityQueue), shuffle their items (RandomShuffleQueue), and
            batch items of different shapes by padding (PaddingFIFOQueue). These classes are
            all in the tf.queue package.                              
                                                                      
          With tensors, operations, variables, and various data structures at your disposal, you
          are now ready to customize your models and training algorithms!
                                                                      
          Customizing Models and Training Algorithms                  
                                                                      
          Let’s start by creating a custom loss function, which is a simple and common use case.
          Custom Loss Functions                                       
                                                                      
          Suppose you want to train a regression model, but your training set is a bit noisy. Of
          course, you start by trying to clean up your dataset by removing or fixing the outliers,
          but that turns out to be insufficient; the dataset is still noisy. Which loss function
          should you use? The mean squared error might penalize large errors too much and
          cause your model to be imprecise. The mean absolute error would not penalize outli‐
          ers as much, but training might take a while to converge, and the trained model
          might not be very precise. This is probably a good time to use the Huber loss (intro‐
          duced in Chapter 10) instead of the good old MSE. The Huber loss is not currently
          part of the official Keras API, but it is available in tf.keras (just use an instance of the
          keras.losses.Huber class). But let’s pretend it’s not there: implementing it is easy as
          pie! Just create a function that takes the labels and predictions as arguments, and use
          TensorFlow operations to compute every instance’s loss:     
            def huber_fn(y_true, y_pred):                             
               error = y_true - y_pred                                
               is_small_error = tf.abs(error) < 1                     
               squared_loss = tf.square(error) / 2                    
               linear_loss = tf.abs(error) - 0.5                      
               return tf.where(is_small_error, squared_loss, linear_loss)
                   For better performance, you should use a vectorized implementa‐
                   tion, as in this example. Moreover, if you want to benefit from Ten‐
                   sorFlow’s graph features, you should use only TensorFlow
                   operations.                                        
                                                                      
          It is also preferable to return a tensor containing one loss per instance, rather than
          returning the mean loss. This way, Keras can apply class weights or sample weights
          when requested (see Chapter 10).                            
                                                                      
                                                                      
                                                                      
                                                                      "|mean squared error; loss functions; using TensorFlow like; custom with TensorFlow; queues; First In, First Out (FIFO) queues; Huber loss
"                                                                      
                                                                      
                                                                      
                                                                      
          Now you can use this loss when you compile the Keras model, then train your model:
                                                                      
            model.compile(loss=huber_fn, optimizer=""nadam"")           
            model.fit(X_train, y_train, [...])                        
          And that’s it! For each batch during training, Keras will call the huber_fn() function
          to compute the loss and use it to perform a Gradient Descent step. Moreover, it will
          keep track of the total loss since the beginning of the epoch, and it will display the
          mean loss.                                                  
          But what happens to this custom loss when you save the model?
                                                                      
          Saving and Loading Models That Contain Custom Components    
                                                                      
          Saving a model containing a custom loss function works fine, as Keras saves the name
          of the function. Whenever you load it, you’ll need to provide a dictionary that maps
          the function name to the actual function. More generally, when you load a model
          containing custom objects, you need to map the names to the objects:
            model = keras.models.load_model(""my_model_with_a_custom_loss.h5"",
                               custom_objects={""huber_fn"": huber_fn}) 
          With the current implementation, any error between –1 and 1 is considered “small.”
          But what if you want a different threshold? One solution is to create a function that
          creates a configured loss function:                         
            def create_huber(threshold=1.0):                          
               def huber_fn(y_true, y_pred):                          
                 error = y_true - y_pred                              
                 is_small_error = tf.abs(error) < threshold           
                 squared_loss = tf.square(error) / 2                  
                 linear_loss = threshold * tf.abs(error) - threshold**2 / 2
                 return tf.where(is_small_error, squared_loss, linear_loss)
               return huber_fn                                        
            model.compile(loss=create_huber(2.0), optimizer=""nadam"")  
          Unfortunately, when you save the model, the threshold will not be saved. This means
          that you will have to specify the threshold value when loading the model (note that
          the name to use is ""huber_fn"", which is the name of the function you gave Keras, not
          the name of the function that created it):                  
            model = keras.models.load_model(""my_model_with_a_custom_loss_threshold_2.h5"",
                               custom_objects={""huber_fn"": create_huber(2.0)})
          You can solve this by creating a subclass of the keras.losses.Loss class, and then
          implementing its get_config() method:                       
                                                                      
                                                                      
                                                                      
                                                                      "|saving and loading
"                                                                      
                                                                      
                                                                      
                                                                      
          When you save the model, the threshold will be saved along with it; and when you
          load the model, you just need to map the class name to the class itself:
                                                                      
            model = keras.models.load_model(""my_model_with_a_custom_loss_class.h5"",
                               custom_objects={""HuberLoss"": HuberLoss})
          When you save a model, Keras calls the loss instance’s get_config() method and
          saves the config as JSON in the HDF5 file. When you load the model, it calls the
          from_config() class method on the HuberLoss class: this method is implemented by
          the base class (Loss) and creates an instance of the class, passing **config to the
          constructor.                                                
          That’s it for losses! That wasn’t too hard, was it? Just as simple are custom activation
          functions, initializers, regularizers, and constraints. Let’s look at these now.
                                                                      
          Custom Activation Functions, Initializers, Regularizers, and
          Constraints                                                 
                                                                      
          Most Keras functionalities, such as losses, regularizers, constraints, initializers, met‐
          rics, activation functions, layers, and even full models, can be customized in very
          much the same way. Most of the time, you will just need to write a simple function
          with the appropriate inputs and outputs. Here are examples of a custom activation
          function (equivalent to keras.activations.softplus() or tf.nn.softplus()), a
          custom Glorot initializer (equivalent to keras.initializers.glorot_normal()), a
          custom ℓ regularizer (equivalent to keras.regularizers.l1(0.01)), and a custom
               1                                                      
          constraint that ensures weights are all positive (equivalent to keras.con
          straints.nonneg() or tf.nn.relu()):                         
            def my_softplus(z): # return value is just tf.nn.softplus(z)
               return tf.math.log(tf.exp(z) + 1.0)                    
            def my_glorot_initializer(shape, dtype=tf.float32):       
               stddev = tf.sqrt(2. / (shape[0] + shape[1]))           
               return tf.random.normal(shape, stddev=stddev, dtype=dtype)
            def my_l1_regularizer(weights):                           
               return tf.reduce_sum(tf.abs(0.01 * weights))           
            def my_positive_weights(weights): # return value is just tf.nn.relu(weights)
               return tf.where(weights < 0., tf.zeros_like(weights), weights)
          As you can see, the arguments depend on the type of custom function. These custom
          functions can then be used normally; for example:           
            layer = keras.layers.Dense(30, activation=my_softplus,    
                            kernel_initializer=my_glorot_initializer, 
                            kernel_regularizer=my_l1_regularizer,     
                            kernel_constraint=my_positive_weights)    
                                                                      "|activation functions, initializers, regularizers, and constraints
"                                                                      
                                                                      
                                                                      
                                                                      
          The activation function will be applied to the output of this Dense layer, and its result
          will be passed on to the next layer. The layer’s weights will be initialized using the
          value returned by the initializer. At each training step the weights will be passed to the
          regularization function to compute the regularization loss, which will be added to the
          main loss to get the final loss used for training. Finally, the constraint function will be
          called after each training step, and the layer’s weights will be replaced by the con‐
          strained weights.                                           
                                                                      
          If a function has hyperparameters that need to be saved along with the model, then
          you will want to subclass the appropriate class, such as keras.regularizers.Regular
          izer, keras.constraints.Constraint, keras.initializers.Initializer, or
          keras.layers.Layer (for any layer, including activation functions). Much like we did
          for the custom loss, here is a simple class for ℓ regularization that saves its factor
                                      1                               
          hyperparameter (this time we do not need to call the parent constructor or the
          get_config() method, as they are not defined by the parent class):
            class MyL1Regularizer(keras.regularizers.Regularizer):    
               def __init__(self, factor):                            
                 self.factor = factor                                 
               def __call__(self, weights):                           
                 return tf.reduce_sum(tf.abs(self.factor * weights))  
               def get_config(self):                                  
                 return {""factor"": self.factor}                       
          Note that you must implement the call() method for losses, layers (including activa‐
          tion functions), and models, or the __call__() method for regularizers, initializers,
          and constraints. For metrics, things are a bit different, as we will see now.
          Custom Metrics                                              
          Losses and metrics are conceptually not the same thing: losses (e.g., cross entropy)
          are used by Gradient Descent to train a model, so they must be differentiable (at least
          where they are evaluated), and their gradients should not be 0 everywhere. Plus, it’s
          OK if they are not easily interpretable by humans. In contrast, metrics (e.g., accuracy)
          are used to evaluate a model: they must be more easily interpretable, and they can be
          non-differentiable or have 0 gradients everywhere.          
          That said, in most cases, defining a custom metric function is exactly the same as
          defining a custom loss function. In fact, we could even use the Huber loss function we
          created earlier as a metric;6 it would work just fine (and persistence would also work
          the same way, in this case only saving the name of the function, ""huber_fn""):
                                                                      
                                                                      
                                                                      
                                                                      
          6 However, the Huber loss is seldom used as a metric (the MAE or MSE is preferred)."|accuracy; metrics
"                                                                      
                                                                      
                                                                      
                                                                      
            model.compile(loss=""mse"", optimizer=""nadam"", metrics=[create_huber(2.0)])
                                                                      
          For each batch during training, Keras will compute this metric and keep track of its
          mean since the beginning of the epoch. Most of the time, this is exactly what you
          want. But not always! Consider a binary classifier’s precision, for example. As we saw
          in Chapter 3, precision is the number of true positives divided by the number of posi‐
          tive predictions (including both true positives and false positives). Suppose the model
          made five positive predictions in the first batch, four of which were correct: that’s 80%
          precision. Then suppose the model made three positive predictions in the second
          batch, but they were all incorrect: that’s 0% precision for the second batch. If you just
          compute the mean of these two precisions, you get 40%. But wait a second—that’s not
          the model’s precision over these two batches! Indeed, there were a total of four true
          positives (4 + 0) out of eight positive predictions (5 + 3), so the overall precision is
          50%, not 40%. What we need is an object that can keep track of the number of true
          positives and the number of false positives and that can compute their ratio when
          requested. This is precisely what the keras.metrics.Precision class does:
            >>> precision = keras.metrics.Precision()                 
            >>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])
            <tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>
            >>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])
            <tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>
          In this example, we created a Precision object, then we used it like a function, pass‐
          ing it the labels and predictions for the first batch, then for the second batch (note
          that we could also have passed sample weights). We used the same number of true
          and false positives as in the example we just discussed. After the first batch, it returns
          a precision of 80%; then after the second batch, it returns 50% (which is the overall
          precision so far, not the second batch’s precision). This is called a streaming metric (or
          stateful metric), as it is gradually updated, batch after batch.
          At any point, we can call the result() method to get the current value of the metric.
          We can also look at its variables (tracking the number of true and false positives) by
          using the variables attribute, and we can reset these variables using the
          reset_states() method:                                      
            >>> precision.result()                                    
            <tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>
            >>> precision.variables                                   
            [<tf.Variable 'true_positives:0' [...] numpy=array([4.], dtype=float32)>,
             <tf.Variable 'false_positives:0' [...] numpy=array([4.], dtype=float32)>]
            >>> precision.reset_states() # both variables get reset to 0.0
          If you need to create such a streaming metric, create a subclass of the keras.met
          rics.Metric class. Here is a simple example that keeps track of the total Huber loss
                                                                      
                                                                      "|stateful metrics; streaming metrics
"                                                                      
                                                                      
                                                                      
                                                                      
                   Keras will take care of variable persistence seamlessly; no action is
                   required.                                          
                                                                      
                                                                      
                                                                      
          When you define a metric using a simple function, Keras automatically calls it for
          each batch, and it keeps track of the mean during each epoch, just like we did man‐
          ually. So the only benefit of our HuberMetric class is that the threshold will be saved.
          But of course, some metrics, like precision, cannot simply be averaged over batches:
          in those cases, there’s no other option than to implement a streaming metric.
                                                                      
          Now that we have built a streaming metric, building a custom layer will seem like a
          walk in the park!                                           
          Custom Layers                                               
                                                                      
          You may occasionally want to build an architecture that contains an exotic layer for
          which TensorFlow does not provide a default implementation. In this case, you will
          need to create a custom layer. Or you may simply want to build a very repetitive
          architecture, containing identical blocks of layers repeated many times, and it would
          be convenient to treat each block of layers as a single layer. For example, if the model
          is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to define a cus‐
          tom layer D containing layers A, B, C, so your model would then simply be D, D, D.
          Let’s see how to build custom layers.                       
          First, some layers have no weights, such as keras.layers.Flatten or keras.lay
          ers.ReLU. If you want to create a custom layer without any weights, the simplest
          option is to write a function and wrap it in a keras.layers.Lambda layer. For exam‐
          ple, the following layer will apply the exponential function to its inputs:
                                                                      
            exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))
          This custom layer can then be used like any other layer, using the Sequential API, the
          Functional API, or the Subclassing API. You can also use it as an activation function
          (or you could use activation=tf.exp, activation=keras.activations.exponen
          tial, or simply activation=""exponential""). The exponential layer is sometimes
          used in the output layer of a regression model when the values to predict have very
          different scales (e.g., 0.001, 10., 1,000.).                
          As you’ve probably guessed by now, to build a custom stateful layer (i.e., a layer with
          weights), you need to create a subclass of the keras.layers.Layer class. For exam‐
          ple, the following class implements a simplified version of the Dense layer:
                                                                      
                                                                      
                                                                      
                                                                      "|layers
"                                                                      
                                                                      
                                                                      
                                                                      
          This layer may now be used like any other layer, but of course only using the Func‐
          tional and Subclassing APIs, not the Sequential API (which only accepts layers with
          one input and one output).                                  
                                                                      
          If your layer needs to have a different behavior during training and during testing
          (e.g., if it uses Dropout or BatchNormalization layers), then you must add a train
          ing argument to the call() method and use this argument to decide what to do. For
          example, let’s create a layer that adds Gaussian noise during training (for regulariza‐
          tion) but does nothing during testing (Keras has a layer that does the same thing,
          keras.layers.GaussianNoise):                                
            class MyGaussianNoise(keras.layers.Layer):                
               def __init__(self, stddev, **kwargs):                  
                 super().__init__(**kwargs)                           
                 self.stddev = stddev                                 
               def call(self, X, training=None):                      
                 if training:                                         
                   noise = tf.random.normal(tf.shape(X), stddev=self.stddev)
                   return X + noise                                   
                 else:                                                
                   return X                                           
               def compute_output_shape(self, batch_input_shape):     
                 return batch_input_shape                             
          With that, you can now build any custom layer you need! Now let’s create custom
          models.                                                     
          Custom Models                                               
                                                                      
          We already looked at creating custom model classes in Chapter 10, when we dis‐
          cussed the Subclassing API.10 It’s straightforward: subclass the keras.Model class, cre‐
          ate layers and variables in the constructor, and implement the call() method to do
          whatever you want the model to do. Suppose you want to build the model repre‐
          sented in Figure 12-3.                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          10 The name “Subclassing API” usually refers only to the creation of custom models by subclassing, although
           many other things can be created by subclassing, as we saw in this chapter."|models
"                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 12-3. Custom model example: an arbitrary model with a custom ResidualBlock
          layer containing a skip connection                          
                                                                      
          The inputs go through a first dense layer, then through a residual block composed of
          two dense layers and an addition operation (as we will see in Chapter 14, a residual
          block adds its inputs to its outputs), then through this same residual block three more
          times, then through a second residual block, and the final result goes through a dense
          output layer. Note that this model does not make much sense; it’s just an example to
          illustrate the fact that you can easily build any kind of model you want, even one that
          contains loops and skip connections. To implement this model, it is best to first create
          a ResidualBlock layer, since we are going to create a couple of identical blocks (and
          we might want to reuse it in another model):                
            class ResidualBlock(keras.layers.Layer):                  
               def __init__(self, n_layers, n_neurons, **kwargs):     
                 super().__init__(**kwargs)                           
                 self.hidden = [keras.layers.Dense(n_neurons, activation=""elu"",
                                     kernel_initializer=""he_normal"")  
                          for _ in range(n_layers)]                   
               def call(self, inputs):                                
                 Z = inputs                                           
                 for layer in self.hidden:                            
                   Z = layer(Z)                                       
                 return inputs + Z                                    
          This layer is a bit special since it contains other layers. This is handled transparently
          by Keras: it automatically detects that the hidden attribute contains trackable objects
          (layers in this case), so their variables are automatically added to this layer’s list of
                                                                      "|residual blocks
"                                                                      
                                                                      
                                                                      
                                                                      
          at: first, how to define losses or metrics based on model internals, and second, how to
          build a custom training loop.                               
                                                                      
          Losses and Metrics Based on Model Internals                 
                                                                      
          The custom losses and metrics we defined earlier were all based on the labels and the
          predictions (and optionally sample weights). There will be times when you want to
          define losses based on other parts of your model, such as the weights or activations of
          its hidden layers. This may be useful for regularization purposes or to monitor some
          internal aspect of your model.                              
          To define a custom loss based on model internals, compute it based on any part of the
          model you want, then pass the result to the add_loss() method.For example, let’s
          build a custom regression MLP model composed of a stack of five hidden layers plus
          an output layer. This custom model will also have an auxiliary output on top of the
          upper hidden layer. The loss associated to this auxiliary output will be called the
          reconstruction loss (see Chapter 17): it is the mean squared difference between the
          reconstruction and the inputs. By adding this reconstruction loss to the main loss, we
          will encourage the model to preserve as much information as possible through the
          hidden layers—even information that is not directly useful for the regression task
          itself. In practice, this loss sometimes improves generalization (it is a regularization
          loss). Here is the code for this custom model with a custom reconstruction loss:
            class ReconstructingRegressor(keras.Model):               
               def __init__(self, output_dim, **kwargs):              
                 super().__init__(**kwargs)                           
                 self.hidden = [keras.layers.Dense(30, activation=""selu"",
                                     kernel_initializer=""lecun_normal"")
                          for _ in range(5)]                          
                 self.out = keras.layers.Dense(output_dim)            
               def build(self, batch_input_shape):                    
                 n_inputs = batch_input_shape[-1]                     
                 self.reconstruct = keras.layers.Dense(n_inputs)      
                 super().build(batch_input_shape)                     
               def call(self, inputs):                                
                 Z = inputs                                           
                 for layer in self.hidden:                            
                   Z = layer(Z)                                       
                 reconstruction = self.reconstruct(Z)                 
                 recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
                 self.add_loss(0.05 * recon_loss)                     
                 return self.out(Z)                                   
                                                                      
                                                                      "|losses and metrics; reconstruction loss
"                                                                      
                                                                      
                                                                      
                                                                      
          itself. Before we get there, we need to look at how to compute gradients automatically
          in TensorFlow.                                              
                                                                      
          Computing Gradients Using Autodiff                          
                                                                      
          To understand how to use autodiff (see Chapter 10 and Appendix D) to compute gra‐
          dients automatically, let’s consider a simple toy function: 
            def f(w1, w2):                                            
               return 3 * w1 ** 2 + 2 * w1 * w2                       
          If you know calculus, you can analytically find that the partial derivative of this func‐
          tion with regard to w1 is 6 * w1 + 2 * w2. You can also find that its partial derivative
          with regard to w2 is 2 * w1. For example, at the point (w1, w2) = (5, 3), these par‐
          tial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point
          is (36, 10). But if this were a neural network, the function would be much more com‐
          plex, typically with tens of thousands of parameters, and finding the partial deriva‐
          tives analytically by hand would be an almost impossible task. One solution could be
          to compute an approximation of each partial derivative by measuring how much the
          function’s output changes when you tweak the corresponding parameter:
            >>> w1, w2 = 5, 3                                         
            >>> eps = 1e-6                                            
            >>> (f(w1 + eps, w2) - f(w1, w2)) / eps                   
            36.000003007075065                                        
            >>> (f(w1, w2 + eps) - f(w1, w2)) / eps                   
            10.000000003174137                                        
          Looks about right! This works rather well and is easy to implement, but it is just an
          approximation, and importantly you need to call f() at least once per parameter (not
          twice, since we could compute f(w1, w2) just once). Needing to call f() at least once
          per parameter makes this approach intractable for large neural networks. So instead,
          we should use autodiff. TensorFlow makes this pretty simple:
            w1, w2 = tf.Variable(5.), tf.Variable(3.)                 
            with tf.GradientTape() as tape:                           
               z = f(w1, w2)                                          
            gradients = tape.gradient(z, [w1, w2])                    
          We first define two variables w1 and w2, then we create a tf.GradientTape context
          that will automatically record every operation that involves a variable, and finally we
          ask this tape to compute the gradients of the result z with regard to both variables
          [w1, w2]. Let’s take a look at the gradients that TensorFlow computed:
                                                                      
            >>> gradients                                             
            [<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,
             <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]
                                                                      "|computing gradients using Autodiff; automatic differentiation (autodiff)
"                                                                      
                                                                      
                                                                      
                                                                      
          This is because computing the gradients of this function using autodiff leads to some
          numerical difficulties: due to floating-point precision errors, autodiff ends up com‐
          puting infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐
          cally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which
          is numerically stable. Next, we can tell TensorFlow to use this stable function when
          computing the gradients of the my_softplus() function by decorating it with
          @tf.custom_gradient and making it return both its normal output and the function
          that computes the derivatives (note that it will receive as input the gradients that were
          backpropagated so far, down to the softplus function; and according to the chain rule,
          we should multiply them with this function’s gradients):    
            @tf.custom_gradient                                       
            def my_better_softplus(z):                                
               exp = tf.exp(z)                                        
               def my_softplus_gradients(grad):                       
                 return grad / (1 + 1 / exp)                          
               return tf.math.log(exp + 1), my_softplus_gradients     
          Now when we compute the gradients of the my_better_softplus() function, we get
          the proper result, even for large input values (however, the main output still explodes
          because of the exponential; one workaround is to use tf.where() to return the inputs
          when they are large).                                       
          Congratulations! You can now compute the gradients of any function (provided it is
          differentiable at the point where you compute it), even blocking backpropagation
          when needed, and write your own gradient functions! This is probably more flexibil‐
          ity than you will ever need, even if you build your own custom training loops, as we
          will see now.                                               
                                                                      
          Custom Training Loops                                       
                                                                      
          In some rare cases, the fit() method may not be flexible enough for what you need
          to do. For example, the Wide & Deep paper we discussed in Chapter 10 uses two dif‐
          ferent optimizers: one for the wide path and the other for the deep path. Since the
          fit() method only uses one optimizer (the one that we specify when compiling the
          model), implementing this paper requires writing your own custom loop.
          You may also like to write custom training loops simply to feel more confident that
          they do precisely what you intend them to do (perhaps you are unsure about some
          details of the fit() method). It can sometimes feel safer to make everything explicit.
          However, remember that writing a custom training loop will make your code longer,
          more error-prone, and harder to maintain.                   
                                                                      
                                                                      
                                                                      
                                                                      "|training loops
"                                                                      
                                                                      
                                                                      
                                                                      
           • At the end of each epoch, we display the status bar again to make it look com‐
            plete13 and to print a line feed, and we reset the states of the mean loss and the
            metrics.                                                  
                                                                      
          If you set the optimizer’s clipnorm or clipvalue hyperparameter, it will take care of
          this for you. If you want to apply any other transformation to the gradients, simply do
          so before calling the apply_gradients() method.             
          If you add weight constraints to your model (e.g., by setting kernel_constraint or
          bias_constraint when creating a layer), you should update the training loop to
                                                                      
          apply these constraints just after apply_gradients():       
            for variable in model.variables:                          
               if variable.constraint is not None:                    
                 variable.assign(variable.constraint(variable))       
          Most importantly, this training loop does not handle layers that behave differently
          during training and testing (e.g., BatchNormalization or Dropout). To handle these,
          you need to call the model with training=True and make sure it propagates this to
          every layer that needs it.                                  
          As you can see, there are quite a lot of things you need to get right, and it’s easy to
          make a mistake. But on the bright side, you get full control, so it’s your call.
                                                                      
          Now that you know how to customize any part of your models14 and training algo‐
          rithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it
          can speed up your custom code considerably, and it will also make it portable to any
          platform supported by TensorFlow (see Chapter 19).          
          TensorFlow Functions and Graphs                             
                                                                      
          In TensorFlow 1, graphs were unavoidable (as were the complexities that came with
          them) because they were a central part of TensorFlow’s API. In TensorFlow 2, they are
          still there, but not as central, and they’re much (much!) simpler to use. To show just
          how simple, let’s start with a trivial function that computes the cube of its input:
                                                                      
            def cube(x):                                              
               return x ** 3                                          
                                                                      
                                                                      
                                                                      
          13 The truth is we did not process every single instance in the training set, because we sampled instances ran‐
           domly: some were processed more than once, while others were not processed at all. Likewise, if the training
           set size is not a multiple of the batch size, we will miss a few instances. In practice that’s fine.
          14 With the exception of optimizers, as very few people ever customize these; see the “Custom Optimizers” sec‐
           tion in the notebook for an example.                       "|custom with TensorFlow; TensorFlow, functions and graphs
"                                                                      
                                                                      
                                                                      
                                                                      
          Moreover, when you write a custom loss function, a custom metric, a custom layer, or
          any other custom function and you use it in a Keras model (as we did throughout this
          chapter), Keras automatically converts your function into a TF Function—no need to
          use tf.function(). So most of the time, all this magic is 100% transparent.
                                                                      
                   You can tell Keras not to convert your Python functions to TF
                   Functions by setting dynamic=True when creating a custom layer
                   or a custom model. Alternatively, you can set run_eagerly=True
                   when calling the model’s compile() method.         
                                                                      
          By default, a TF Function generates a new graph for every unique set of input shapes
          and data types and caches it for subsequent calls. For example, if you call
          tf_cube(tf.constant(10)), a graph will be generated for int32 tensors of shape [].
          Then if you call tf_cube(tf.constant(20)), the same graph will be reused. But if
          you then call tf_cube(tf.constant([10, 20])), a new graph will be generated for
          int32 tensors of shape [2]. This is how TF Functions handle polymorphism (i.e., vary‐
          ing argument types and shapes). However, this is only true for tensor arguments: if
          you pass numerical Python values to a TF Function, a new graph will be generated for
          every distinct value: for example, calling tf_cube(10) and tf_cube(20) will generate
          two graphs.                                                 
                                                                      
                   If you call a TF Function many times with different numerical
                   Python values, then many graphs will be generated, slowing down
                   your program and using up a lot of RAM (you must delete the TF
                   Function to release it). Python values should be reserved for argu‐
                   ments that will have few unique values, such as hyperparameters
                   like the number of neurons per layer. This allows TensorFlow to
                   better optimize each variant of your model.        
                                                                      
          AutoGraph and Tracing                                       
          So how does TensorFlow generate graphs? It starts by analyzing the Python function’s
          source code to capture all the control flow statements, such as for loops, while loops,
          and if statements, as well as break, continue, and return statements. This first step
          is called AutoGraph. The reason TensorFlow has to analyze the source code is that
          Python does not provide any other way to capture control flow statements: it offers
          magic methods like __add__() and __mul__() to capture operators like + and *, but
          there are no __while__() or __if__() magic methods. After analyzing the function’s
          code, AutoGraph outputs an upgraded version of that function in which all the con‐
          trol flow statements are replaced by the appropriate TensorFlow operations, such as
          tf.while_loop() for loops and tf.cond() for if statements. For example, in
          Figure 12-4, AutoGraph analyzes the source code of the sum_squares() Python
                                                                      "|AutoGraph and tracing; AutoGraphs
"                                                                      
                                                                      
                                                                      
                                                                      
          function, and it generates the tf__sum_squares() function. In this function, the for
          loop is replaced by the definition of the loop_body() function (containing the body
          of the original for loop), followed by a call to the for_stmt() function. This call will
          build the appropriate tf.while_loop() operation in the computation graph.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 12-4. How TensorFlow generates graphs using AutoGraph and tracing
                                                                      
          Next, TensorFlow calls this “upgraded” function, but instead of passing the argument,
          it passes a symbolic tensor—a tensor without any actual value, only a name, a data
          type, and a shape. For example, if you call sum_squares(tf.constant(10)), then the
          tf__sum_squares() function will be called with a symbolic tensor of type int32 and
          shape []. The function will run in graph mode, meaning that each TensorFlow opera‐
          tion will add a node in the graph to represent itself and its output tensor(s) (as
          opposed to the regular mode, called eager execution, or eager mode). In graph mode,
          TF operations do not perform any computations. This should feel familiar if you
          know TensorFlow 1, as graph mode was the default mode. In Figure 12-4, you can see
          the tf__sum_squares() function being called with a symbolic tensor as its argument
          (in this case, an int32 tensor of shape []) and the final graph being generated during
          tracing. The nodes represent operations, and the arrows represent tensors (both the
          generated function and the graph are simplified).           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      "|symbolic tensors; graph mode; eager execution/eager mode
"                                                                      
                                                                      
                                                                      
                                                                      
                   To view the generated function’s source code, you can call tf.auto
                   graph.to_code(sum_squares.python_function). The code is not
                   meant to be pretty, but it can sometimes help for debugging.
                                                                      
                                                                      
          TF Function Rules                                           
                                                                      
          Most of the time, converting a Python function that performs TensorFlow operations
          into a TF Function is trivial: decorate it with @tf.function or let Keras take care of it
          for you. However, there are a few rules to respect:         
                                                                      
           • If you call any external library, including NumPy or even the standard library,
            this call will run only during tracing; it will not be part of the graph. Indeed, a
            TensorFlow graph can only include TensorFlow constructs (tensors, operations,
            variables, datasets, and so on). So, make sure you use tf.reduce_sum() instead
            of np.sum(), tf.sort() instead of the built-in sorted() function, and so on
            (unless you really want the code to run only during tracing). This has a few addi‐
            tional implications:                                      
             —If you define a TF Function f(x) that just returns np.random.rand(), a ran‐
              dom number will only be generated when the function is traced, so f(tf.con
              stant(2.)) and f(tf.constant(3.)) will return the same random number,
              but f(tf.constant([2., 3.])) will return a different one. If you replace
              np.random.rand() with tf.random.uniform([]), then a new random num‐
              ber will be generated upon every call, since the operation will be part of the
              graph.                                                  
             —If your non-TensorFlow code has side effects (such as logging something or
              updating a Python counter), then you should not expect those side effects to
              occur every time you call the TF Function, as they will only occur when the
              function is traced.                                     
             —You can wrap arbitrary Python code in a tf.py_function() operation, but
              doing so will hinder performance, as TensorFlow will not be able to do any
              graph optimization on this code. It will also reduce portability, as the graph
              will only run on platforms where Python is available (and where the right
              libraries are installed).                               
                                                                      
           • You can call other Python functions or TF Functions, but they should follow the
            same rules, as TensorFlow will capture their operations in the computation
            graph. Note that these other functions do not need to be decorated with
            @tf.function.                                             
           • If the function creates a TensorFlow variable (or any other stateful TensorFlow
            object, such as a dataset or a queue), it must do so upon the very first call, and
            only then, or else you will get an exception. It is usually preferable to create"|TF Function rules; rules
