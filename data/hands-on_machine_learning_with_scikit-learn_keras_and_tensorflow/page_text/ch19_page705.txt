                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 19-18. Data parallelism using the mirrored strategy  
                                                                      
          The tricky part when using this approach is to efficiently compute the mean of all the
          gradients from all the GPUs and distribute the result across all the GPUs. This can be
          done using an AllReduce algorithm, a class of algorithms where multiple nodes col‐
          laborate to efficiently perform a reduce operation (such as computing the mean, sum,
          and max), while ensuring that all nodes obtain the same final result. Fortunately,
          there are off-the-shelf implementations of such algorithms, as we will see.
          Data parallelism with centralized parameters                
                                                                      
          Another approach is to store the model parameters outside of the GPU devices per‐
          forming the computations (called workers), for example on the CPU (see
          Figure 19-19). In a distributed setup, you may place all the parameters on one or
          more CPU-only servers called parameter servers, whose only role is to host and
          update the parameters.                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      