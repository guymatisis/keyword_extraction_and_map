                                                                      
                                                                      
                                                                      
                                                                      
          The ELU activation function looks a lot like the ReLU function, with a few major
          differences:                                                
                                                                      
           • It takes on negative values when z < 0, which allows the unit to have an average
            output closer to 0 and helps alleviate the vanishing gradients problem. The
            hyperparameter α defines the value that the ELU function approaches when z is a
            large negative number. It is usually set to 1, but you can tweak it like any other
            hyperparameter.                                           
           • It has a nonzero gradient for z < 0, which avoids the dead neurons problem.
           • If α is equal to 1 then the function is smooth everywhere, including around z = 0,
            which helps speed up Gradient Descent since it does not bounce as much to the
            left and right of z = 0.                                  
                                                                      
          The main drawback of the ELU activation function is that it is slower to compute
          than the ReLU function and its variants (due to the use of the exponential function).
          Its faster convergence rate during training compensates for that slow computation,
          but still, at test time an ELU network will be slower than a ReLU network.
          Then, a 2017 paper7 by Günter Klambauer et al. introduced the Scaled ELU (SELU)
          activation function: as its name suggests, it is a scaled variant of the ELU activation
          function. The authors showed that if you build a neural network composed exclu‐
          sively of a stack of dense layers, and if all hidden layers use the SELU activation func‐
          tion, then the network will self-normalize: the output of each layer will tend to
          preserve a mean of 0 and standard deviation of 1 during training, which solves the
          vanishing/exploding gradients problem. As a result, the SELU activation function
          often significantly outperforms other activation functions for such neural nets (espe‐
          cially deep ones). There are, however, a few conditions for self-normalization to hap‐
          pen (see the paper for the mathematical justification):     
                                                                      
           • The input features must be standardized (mean 0 and standard deviation 1).
           • Every hidden layer’s weights must be initialized with LeCun normal initialization.
            In Keras, this means setting kernel_initializer="lecun_normal".
           • The network’s architecture must be sequential. Unfortunately, if you try to use
            SELU in nonsequential architectures, such as recurrent networks (see Chap‐
            ter 15) or networks with skip connections (i.e., connections that skip layers, such
            as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will
            not necessarily outperform other activation functions.    
                                                                      
                                                                      
                                                                      
                                                                      
          7 Günter Klambauer et al., “Self-Normalizing Neural Networks,” Proceedings of the 31st International Conference
           on Neural Information Processing Systems (2017): 972–981.  