                                                                      
                                                                      
                                                                      
                                                                      
          class, which can apply different transformers and concatenate their outputs. But you
          cannot specify different columns for each transformer; they all apply to the whole
          data. It is possible to work around this limitation using a custom transformer for col‐
          umn selection (see the Jupyter notebook for an example).    
                                                                      
          Select and Train a Model                                    
                                                                      
          At last! You framed the problem, you got the data and explored it, you sampled a
          training set and a test set, and you wrote transformation pipelines to clean up and
          prepare your data for Machine Learning algorithms automatically. You are now ready
          to select and train a Machine Learning model.               
                                                                      
          Training and Evaluating on the Training Set                 
          The good news is that thanks to all these previous steps, things are now going to be
          much simpler than you might think. Let’s first train a Linear Regression model, like
          we did in the previous chapter:                             
                                                                      
            from sklearn.linear_model import LinearRegression         
            lin_reg = LinearRegression()                              
            lin_reg.fit(housing_prepared, housing_labels)             
          Done! You now have a working Linear Regression model. Let’s try it out on a few
          instances from the training set:                            
                                                                      
            >>> some_data = housing.iloc[:5]                          
            >>> some_labels = housing_labels.iloc[:5]                 
            >>> some_data_prepared = full_pipeline.transform(some_data)
            >>> print("Predictions:", lin_reg.predict(some_data_prepared))
            Predictions: [ 210644.6045 317768.8069 210956.4333 59218.9888 189747.5584]
            >>> print("Labels:", list(some_labels))                   
            Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] 
          It works, although the predictions are not exactly accurate (e.g., the first prediction is
          off by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐
          ing set using Scikit-Learn’s mean_squared_error() function: 
            >>> from sklearn.metrics import mean_squared_error        
            >>> housing_predictions = lin_reg.predict(housing_prepared)
            >>> lin_mse = mean_squared_error(housing_labels, housing_predictions)
            >>> lin_rmse = np.sqrt(lin_mse)                           
            >>> lin_rmse                                              
            68628.19819848922                                         
          This is better than nothing, but clearly not a great score: most districts’ median_hous
          ing_values range between $120,000 and $265,000, so a typical prediction error of
          $68,628 is not very satisfying. This is an example of a model underfitting the training
          data. When this happens it can mean that the features do not provide enough