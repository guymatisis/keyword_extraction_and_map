                                                                      
                                                                      
                                                                      
                                                                      
          be a good idea to leave a gap between these sets to avoid the risk of a paragraph over‐
          lapping over two sets.                                      
                                                                      
          When dealing with time series, you would in general split across time,: for example,
          you might take the years 2000 to 2012 for the training set, the years 2013 to 2015 for
          the validation set, and the years 2016 to 2018 for the test set. However, in some cases
          you may be able to split along other dimensions, which will give you a longer time
          period to train on. For example, if you have data about the financial health of 10,000
          companies from 2000 to 2018, you might be able to split this data across the different
          companies. It’s very likely that many of these companies will be strongly correlated,
          though (e.g., whole economic sectors may go up or down jointly), and if you have
          correlated companies across the training set and the test set your test set will not be as
          useful, as its measure of the generalization error will be optimistically biased.
          So, it is often safer to split across time—but this implicitly assumes that the patterns
          the RNN can learn in the past (in the training set) will still exist in the future. In other
          words, we assume that the time series is stationary (at least in a wide sense).3 For
          many time series this assumption is reasonable (e.g., chemical reactions should be
          fine, since the laws of chemistry don’t change every day), but for many others it is not
          (e.g., financial markets are notoriously not stationary since patterns disappear as soon
          as traders spot them and start exploiting them). To make sure the time series is
          indeed sufficiently stationary, you can plot the model’s errors on the validation set
          across time: if the model performs much better on the first part of the validation set
          than on the last part, then the time series may not be stationary enough, and you
          might be better off training the model on a shorter time span.
          In short, splitting a time series into a training set, a validation set, and a test set is not
          a trivial task, and how it’s done will depend strongly on the task at hand.
          Now back to Shakespeare! Let’s take the first 90% of the text for the training set
          (keeping the rest for the validation set and the test set), and create a tf.data.Dataset
          that will return each character one by one from this set:   
                                                                      
            train_size = dataset_size * 90 // 100                     
            dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])
          Chopping the Sequential Dataset into Multiple Windows       
                                                                      
          The training set now consists of a single sequence of over a million characters, so we
          can’t just train the neural network directly on it: the RNN would be equivalent to a
                                                                      
                                                                      
          3 By definition, a stationary time series’s mean, variance, and autocorrelations (i.e., correlations between values
           in the time series separated by a given interval) do not change over time. This is quite restrictive; for example,
           it excludes time series with trends or cyclical patterns. RNNs are more tolerant in that they can learn trends
           and cyclical patterns.                                     