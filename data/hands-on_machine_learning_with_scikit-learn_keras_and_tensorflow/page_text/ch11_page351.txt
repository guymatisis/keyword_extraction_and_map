network would learn good feature detectors for faces, so reusing its lower layers
would allow you to train a good face classifier that uses little training data.
For <i>natural</i> <i>language</i> <i>processing</i> (NLP) applications, you can download a corpus of
millions of text documents and automatically generate labeled data from it. For exam‐
ple, you could randomly mask out some words and train a model to predict what the
missing words are (e.g., it should predict that the missing word in the sentence “What
___ you saying?” is probably “are” or “were”). If you can train a model to reach good
performance on this task, then it will already know quite a lot about language, and
you can certainly reuse it for your actual task and fine-tune it on your labeled data
(we will discuss more pretraining tasks in Chapter 15).
<i>Self-supervised</i> <i>learning</i> is when you automatically generate the
labels from the data itself, then you train a model on the resulting
“labeled” dataset using supervised learning techniques. Since this
approach requires no human labeling whatsoever, it is best classi‐
fied as a form of unsupervised learning.
<header><largefont><b>Faster</b></largefont> <largefont><b>Optimizers</b></largefont></header>
Training a very large deep neural network can be painfully slow. So far we have seen
four ways to speed up training (and reach a better solution): applying a good initiali‐
zation strategy for the connection weights, using a good activation function, using
Batch Normalization, and reusing parts of a pretrained network (possibly built on an
auxiliary task or using unsupervised learning). Another huge speed boost comes from
using a faster optimizer than the regular Gradient Descent optimizer. In this section
we will present the most popular algorithms: momentum optimization, Nesterov
Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam
optimization.
<header><largefont><b>Momentum</b></largefont> <largefont><b>Optimization</b></largefont></header>
Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start
out slowly, but it will quickly pick up momentum until it eventually reaches terminal
velocity (if there is some friction or air resistance). This is the very simple idea behind
<i>momentum</i> <i>optimization,</i> proposed by Boris Polyak in 1964.13 In contrast, regular
Gradient Descent will simply take small, regular steps down the slope, so the algo‐
rithm will take much more time to reach the bottom.
13 BorisT.Polyak,“SomeMethodsofSpeedingUptheConvergenceofIterationMethods,”USSRComputational
<i>MathematicsandMathematicalPhysics4,no.5(1964):1–17.</i>