that correspond to all the all the necessary statistics computed by Apache Beam (the
mean, standard deviation, and vocabulary).
With the Data API, TFRecords, the Keras preprocessing layers, and TF Transform,
you can build highly scalable input pipelines for training and benefit from fast and
portable data preprocessing in production.
But what if you just wanted to use a standard dataset? Well in that case, things are
much simpler: just use TFDS!
<header><largefont><b>The</b></largefont> <largefont><b>TensorFlow</b></largefont> <largefont><b>Datasets</b></largefont> <largefont><b>(TFDS)</b></largefont> <largefont><b>Project</b></largefont></header>
The TensorFlow Datasets project makes it very easy to download common datasets,
from small ones like MNIST or Fashion MNIST to huge datasets like ImageNet (you
will need quite a bit of disk space!). The list includes image datasets, text datasets
(including translation datasets), and audio and video datasets. You can visit <i>https://</i>
<i>homl.info/tfds</i> to view the full list, along with a description of each dataset.
tensorflow-
TFDS is not bundled with TensorFlow, so you need to install the
datasets library (e.g., using pip). Then call the tfds.load() function, and it will
download the data you want (unless it was already downloaded earlier) and return
the data as a dictionary of datasets (typically one for training and one for testing, but
this depends on the dataset you choose). For example, let’s download MNIST:
<b>import</b> <b>tensorflow_datasets</b> <b>as</b> <b>tfds</b>
dataset = tfds.load(name="mnist")
mnist_train, mnist_test = dataset["train"], dataset["test"]
You can then apply any transformation you want (typically shuffling, batching, and
prefetching), and you’re ready to train your model. Here is a simple example:
mnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)
<b>for</b> item <b>in</b> mnist_train:
images = item["image"]
labels = item["label"]
[...]
The load() function shuffles each data shard it downloads (only
for the training set). This may not be sufficient, so it’s best to shuf‐
fle the training data some more.
Note that each item in the dataset is a dictionary containing both the features and the
labels. But Keras expects each item to be a tuple containing two elements (again, the
features and the labels). You could transform the dataset using the map() method, like
this: