                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 6-8. Sensitivity to training set details             
                                                                      
          Random Forests can limit this instability by averaging predictions over many trees, as
          we will see in the next chapter.                            
                                                                      
          Exercises                                                   
                                                                      
           1. What is the approximate depth of a Decision Tree trained (without restrictions)
            on a training set with one million instances?             
           2. Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐
            ally lower/greater, or always lower/greater?              
                                                                      
           3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing
            max_depth?                                                
           4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling
            the input features?                                       
           5. If it takes one hour to train a Decision Tree on a training set containing 1 million
            instances, roughly how much time will it take to train another Decision Tree on a
            training set containing 10 million instances?             
           6. If your training set contains 100,000 instances, will setting presort=True speed
            up training?                                              
                                                                      
           7. Train and fine-tune a Decision Tree for the moons dataset by following these
            steps:                                                    
             a. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset.
            b. Use train_test_split() to split the dataset into a training set and a test set.
                                                                      
                                                                      