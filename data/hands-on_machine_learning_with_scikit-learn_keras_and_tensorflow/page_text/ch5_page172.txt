<i>Equation</i> <i>5-11.</i> <i>Making</i> <i>predictions</i> <i>with</i> <i>a</i> <i>kernelized</i> <i>SVM</i>
⊺
<i>m</i>
<i>n</i> ⊺ <i>n</i> <largefont>∑</largefont> <i>i</i> <i>i</i> <i>i</i> <i>n</i>
<i>h</i> <i>ϕ</i> <b>x</b> = <b>w</b> <i>ϕ</i> <b>x</b> + <i>b</i> = <i>α</i> <i>t</i> <i>ϕ</i> <b>x</b> <i>ϕ</i> <b>x</b> + <i>b</i>
<b>w,b</b>
<i>i</i> = 1
<i>m</i>
⊺
<i>i</i> <i>i</i> <i>i</i> <i>n</i>
<header>= <largefont>∑</largefont> <i>α</i> <i>t</i> <i>ϕ</i> <b>x</b> <i>ϕ</i> <b>x</b> + <i>b</i></header>
<i>i</i> = 1
<i>m</i>
<i>i</i> <i>i</i> <i>i</i> <i>n</i>
<header>= <largefont>∑</largefont> <i>α</i> <i>t</i> <i>K</i> <b>x</b> ,x + <i>b</i></header>
<i>i</i> = 1
<i>i</i>
<i>α</i> > 0
Note that since <i>α</i> (i) ≠ 0 only for support vectors, making predictions involves comput‐
<b>x(n)</b>
ing the dot product of the new input vector with only the support vectors, not all
the training instances. Of course, you need to use the same trick to compute the bias
term <i>b</i> (Equation 5-12).
<i>Equation</i> <i>5-12.</i> <i>Using</i> <i>the</i> <i>kernel</i> <i>trick</i> <i>to</i> <i>compute</i> <i>the</i> <i>bias</i> <i>term</i>
⊺
<i>m</i> <i>m</i> <i>m</i>
1 1
<i>i</i> ⊺ <i>i</i> <i>i</i> <i>j</i> <i>j</i> <i>j</i> <i>i</i>
<header><i>b</i> = <largefont>∑</largefont> <i>t</i> − <b>w</b> <i>ϕ</i> <b>x</b> = <largefont>∑</largefont> <i>t</i> − <largefont>∑</largefont> <i>α</i> <i>t</i> <i>ϕ</i> <b>x</b> <i>ϕ</i> <b>x</b></header>
<i>n</i> <i>n</i>
<i>i</i> = 1 <i>i</i> = 1 <i>j</i> = 1
<i>s</i> <i>s</i>
<i>i</i> <i>i</i>
<i>α</i> > 0 <i>α</i> > 0
<i>m</i> <i>m</i>
1
<i>i</i> <i>j</i> <i>j</i> <i>i</i> <i>j</i>
<header>= <largefont>∑</largefont> <i>t</i> − <largefont>∑</largefont> <i>α</i> <i>t</i> <i>K</i> <b>x</b> ,x</header>
<i>n</i>
<i>i</i> = 1 <i>j</i> = 1
<i>s</i>
<i>i</i> <i>j</i>
<i>α</i> > 0 <i>α</i> > 0
If you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side
effect of the kernel trick.
<header><largefont><b>Online</b></largefont> <largefont><b>SVMs</b></largefont></header>
Before concluding this chapter, let’s take a quick look at online SVM classifiers (recall
that online learning means learning incrementally, typically as new instances arrive).
For linear SVM classifiers, one method for implementing an online SVM classifier is
to use Gradient Descent (e.g., using SGDClassifier ) to minimize the cost function in
Equation 5-13, which is derived from the primal problem. Unfortunately, Gradient
Descent converges much more slowly than the methods based on QP.