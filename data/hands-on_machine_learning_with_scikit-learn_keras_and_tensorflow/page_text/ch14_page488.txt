                                                                      
                                                                      
                                                                      
                                                                      
          since each kernel contains a different set of weights for each input channel). Since an
          FCN contains only convolutional layers (and pooling layers, which have the same
          property), it can be trained and executed on images of any size!
                                                                      
          For example, suppose we’d already trained a CNN for flower classification and locali‐
          zation. It was trained on 224 × 224 images, and it outputs 10 numbers: outputs 0 to 4
          are sent through the softmax activation function, and this gives the class probabilities
          (one per class); output 5 is sent through the logistic activation function, and this gives
          the objectness score; outputs 6 to 9 do not use any activation function, and they rep‐
          resent the bounding box’s center coordinates, as well as its height and width. We can
          now convert its dense layers to convolutional layers. In fact, we don’t even need to
          retrain it; we can just copy the weights from the dense layers to the convolutional lay‐
          ers! Alternatively, we could have converted the CNN into an FCN before training.
          Now suppose the last convolutional layer before the output layer (also called the bot‐
          tleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image
          (see the left side of Figure 14-25). If we feed the FCN a 448 × 448 image (see the right
          side of Figure 14-25), the bottleneck layer will now output 14 × 14 feature maps.27
          Since the dense output layer was replaced by a convolutional layer using 10 filters of
          size 7 × 7, with "valid" padding and stride 1, the output will be composed of 10 fea‐
          tures maps, each of size 8 × 8 (since 14 – 7 + 1 = 8). In other words, the FCN will
          process the whole image only once, and it will output an 8 × 8 grid where each cell
          contains 10 numbers (5 class probabilities, 1 objectness score, and 4 bounding box
          coordinates). It’s exactly like taking the original CNN and sliding it across the image
          using 8 steps per row and 8 steps per column. To visualize this, imagine chopping the
          original image into a 14 × 14 grid, then sliding a 7 × 7 window across this grid; there
          will be 8 × 8 = 64 possible locations for the window, hence 8 × 8 predictions. How‐
          ever, the FCN approach is much more efficient, since the network only looks at the
          image once. In fact, You Only Look Once (YOLO) is the name of a very popular object
          detection architecture, which we’ll look at next.           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          27 This assumes we used only "same" padding in the network: indeed, "valid" padding would reduce the size of
           the feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round‐
           ing error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the
           feature maps may end up being smaller.                     