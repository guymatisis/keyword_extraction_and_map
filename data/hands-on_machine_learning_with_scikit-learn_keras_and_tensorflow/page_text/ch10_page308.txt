housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(
housing.data, housing.target)
X_train, X_valid, y_train, y_valid = train_test_split(
X_train_full, y_train_full)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_valid = scaler.transform(X_valid)
X_test = scaler.transform(X_test)
Using the Sequential API to build, train, evaluate, and use a regression MLP to make
predictions is quite similar to what we did for classification. The main differences are
the fact that the output layer has a single neuron (since we only want to predict a sin‐
gle value) and uses no activation function, and the loss function is the mean squared
error. Since the dataset is quite noisy, we just use a single hidden layer with fewer
neurons than before, to avoid overfitting:
model = keras.models.Sequential([
keras.layers.Dense(30, activation="relu", input_shape=X_train.shape[1:]),
keras.layers.Dense(1)
])
model.compile(loss="mean_squared_error", optimizer="sgd")
history = model.fit(X_train, y_train, epochs=20,
validation_data=(X_valid, y_valid))
mse_test = model.evaluate(X_test, y_test)
X_new = X_test[:3] <i>#</i> <i>pretend</i> <i>these</i> <i>are</i> <i>new</i> <i>instances</i>
y_pred = model.predict(X_new)
Sequen
As you can see, the Sequential API is quite easy to use. However, although
tial models are extremely common, it is sometimes useful to build neural networks
with more complex topologies, or with multiple inputs or outputs. For this purpose,
Keras offers the Functional API.
<header><largefont><b>Building</b></largefont> <largefont><b>Complex</b></largefont> <largefont><b>Models</b></largefont> <largefont><b>Using</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Functional</b></largefont> <largefont><b>API</b></largefont></header>
One example of a nonsequential neural network is a <i>Wide</i> <i>&</i> <i>Deep</i> neural network.
This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng
et al. 16 It connects all or part of the inputs directly to the output layer, as shown in
Figure 10-14. This architecture makes it possible for the neural network to learn both
path).17
deep patterns (using the deep path) and simple rules (through the short In
contrast, a regular MLP forces all the data to flow through the full stack of layers;
16 Heng-TzeChengetal.,“Wide&DeepLearningforRecommenderSystems,”ProceedingsoftheFirstWorkshop
<i>onDeepLearningforRecommenderSystems(2016):7–10.</i>
17 Theshortpathcanalsobeusedtoprovidemanuallyengineeredfeaturestotheneuralnetwork.