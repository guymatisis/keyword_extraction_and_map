                                                                      
                                                                      
                                                                      
                                                                      
          as in a Colab Runtime). The following code splits the first GPU into two virtual devi‐
          ces, with 2 GiB of RAM each (again, this must be done immediately after importing
          TensorFlow):                                                
                                                                      
            physical_gpus = tf.config.experimental.list_physical_devices("GPU")
            tf.config.experimental.set_virtual_device_configuration(  
               physical_gpus[0],                                      
               [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),
               tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])
          These two virtual devices will then be called /gpu:0 and /gpu:1, and you can place
          operations and variables on each of them as if they were really two independent
          GPUs. Now let’s see how TensorFlow decides which devices it should place variables
          and execute operations on.                                  
          Placing Operations and Variables on Devices                 
                                                                      
          The TensorFlow whitepaper13 presents a friendly dynamic placer algorithm that auto‐
          magically distributes operations across all available devices, taking into account
          things like the measured computation time in previous runs of the graph, estimations
          of the size of the input and output tensors for each operation, the amount of RAM
          available in each device, communication delay when transferring data into and out of
          devices, and hints and constraints from the user. In practice this algorithm turned out
          to be less efficient than a small set of placement rules specified by the user, so the Ten‐
          sorFlow team ended up dropping the dynamic placer.          
          That said, tf.keras and tf.data generally do a good job of placing operations and vari‐
          ables where they belong (e.g., heavy computations on the GPU, and data preprocess‐
          ing on the CPU). But you can also place operations and variables manually on each
          device, if you want more control:                           
           • As just mentioned, you generally want to place the data preprocessing operations
            on the CPU, and place the neural network operations on the GPUs.
                                                                      
           • GPUs usually have a fairly limited communication bandwidth, so it is important
            to avoid unnecessary data transfers in and out of the GPUs.
           • Adding more CPU RAM to a machine is simple and fairly cheap, so there’s usu‐
            ally plenty of it, whereas the GPU RAM is baked into the GPU: it is an expensive
            and thus limited resource, so if a variable is not needed in the next few training
            steps, it should probably be placed on the CPU (e.g., datasets generally belong on
            the CPU).                                                 
                                                                      
                                                                      
                                                                      
          13 Martín Abadi et al., “TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems”
           Google Research whitepaper (2015).                         