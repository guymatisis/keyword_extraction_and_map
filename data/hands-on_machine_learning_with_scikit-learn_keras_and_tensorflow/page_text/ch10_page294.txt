<header><largefont><b>Classification</b></largefont> <largefont><b>MLPs</b></largefont></header>
MLPs can also be used for classification tasks. For a binary classification problem,
you just need a single output neuron using the logistic activation function: the output
will be a number between 0 and 1, which you can interpret as the estimated probabil‐
ity of the positive class. The estimated probability of the negative class is equal to one
minus that number.
MLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For
example, you could have an email classification system that predicts whether each
incoming email is ham or spam, and simultaneously predicts whether it is an urgent
or nonurgent email. In this case, you would need two output neurons, both using the
logistic activation function: the first would output the probability that the email is
spam, and the second would output the probability that it is urgent. More generally,
you would dedicate one output neuron for each positive class. Note that the output
probabilities do not necessarily add up to 1. This lets the model output any combina‐
tion of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and per‐
haps even urgent spam (although that would probably be an error).
If each instance can belong only to a single class, out of three or more possible classes
(e.g., classes 0 through 9 for digit image classification), then you need to have one
output neuron per class, and you should use the softmax activation function for the
whole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4)
will ensure that all the estimated probabilities are between 0 and 1 and that they add
up to 1 (which is required if the classes are exclusive). This is called multiclass
classification.
<i>Figure</i> <i>10-9.</i> <i>A</i> <i>modern</i> <i>MLP</i> <i>(including</i> <i>ReLU</i> <i>and</i> <i>softmax)</i> <i>for</i> <i>classification</i>