tf.nn.sam
2015 by Sébastien Jean et al..11 In TensorFlow you can use the
pled_softmax_loss() function for this during training and use the normal soft‐
max function at inference time (sampled softmax cannot be used at inference
time because it requires knowing the target).
The TensorFlow Addons project includes many sequence-to-sequence tools to let you
easily build production-ready Encoder–Decoders. For example, the following code
creates a basic Encoder–Decoder model, similar to the one represented in
Figure 16-3:
<b>import</b> <b>tensorflow_addons</b> <b>as</b> <b>tfa</b>
encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)
decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)
sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)
embeddings = keras.layers.Embedding(vocab_size, embed_size)
encoder_embeddings = embeddings(encoder_inputs)
decoder_embeddings = embeddings(decoder_inputs)
encoder = keras.layers.LSTM(512, return_state=True)
encoder_outputs, state_h, state_c = encoder(encoder_embeddings)
encoder_state = [state_h, state_c]
sampler = tfa.seq2seq.sampler.TrainingSampler()
decoder_cell = keras.layers.LSTMCell(512)
output_layer = keras.layers.Dense(vocab_size)
decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,
output_layer=output_layer)
final_outputs, final_state, final_sequence_lengths = decoder(
decoder_embeddings, initial_state=encoder_state,
sequence_length=sequence_lengths)
Y_proba = tf.nn.softmax(final_outputs.rnn_output)
model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],
outputs=[Y_proba])
The code is mostly self-explanatory, but there are a few points to note. First, we set
return_state=True when creating the LSTM layer so that we can get its final hidden
state and pass it to the decoder. Since we are using an LSTM cell, it actually returns
TrainingSampler
two hidden states (short term and long term). The is one of several
samplers available in TensorFlow Addons: their role is to tell the decoder at each step
what it should pretend the previous output was. During inference, this should be the
11 SébastienJeanetal.,“OnUsingVeryLargeTargetVocabularyforNeuralMachineTranslation,”Proceedingsof
<i>the53rdAnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJointCon‐</i>
<i>ferenceonNaturalLanguageProcessingoftheAsianFederationofNaturalLanguageProcessing1(2015):1–10.</i>