<header><largefont><b>Grid</b></largefont> <largefont><b>Search</b></largefont></header>
One option would be to fiddle with the hyperparameters manually, until you find a
great combination of hyperparameter values. This would be very tedious work, and
you may not have time to explore many combinations.
Instead, you should get Scikit-Learn’s GridSearchCV to search for you. All you need
to do is tell it which hyperparameters you want it to experiment with and what values
to try out, and it will use cross-validation to evaluate all the possible combinations of
hyperparameter values. For example, the following code searches for the best combi‐
nation of hyperparameter values for the RandomForestRegressor :
<b>from</b> <b>sklearn.model_selection</b> <b>import</b> GridSearchCV
param_grid = [
{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
scoring='neg_mean_squared_error',
return_train_score=True)
grid_search.fit(housing_prepared, housing_labels)
When you have no idea what value a hyperparameter should have,
a simple approach is to try out consecutive powers of 10 (or a
smaller number if you want a more fine-grained search, as shown
n_estimators
in this example with the hyperparameter).
This param_grid tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of
n_estimators max_features dict
and hyperparameter values specified in the first
(don’t worry about what these hyperparameters mean for now; they will be explained
in Chapter 7), then try all 2 × 3 = 6 combinations of hyperparameter values in the
second dict , but this time with the bootstrap hyperparameter set to False instead of
True
(which is the default value for this hyperparameter).
The grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor
hyperparameter values, and it will train each model 5 times (since we are using five-
fold cross validation). In other words, all in all, there will be 18 × 5 = 90 rounds of
training! It may take quite a long time, but when it is done you can get the best com‐
bination of parameters like this:
<b>>>></b> grid_search.best_params_
{'max_features': 8, 'n_estimators': 30}