time-distributed, they have no way of knowing the position of each word
(either relative or absolute). Obviously, the relative and absolute word posi‐
tions are important, so we need to give this information to the Transformer
somehow, and positional embeddings are a good way to do this.
Let’s look a bit closer at both these novel components of the Transformer architecture,
starting with the positional embeddings.
<b>Positionalembeddings</b>
A positional embedding is a dense vector that encodes the position of a word within a
sentence: the <i>ith</i> positional embedding is simply added to the word embedding of the
<i>ith</i> word in the sentence. These positional embeddings can be learned by the model,
but in the paper the authors preferred to use fixed positional embeddings, defined
using the sine and cosine functions of different frequencies. The positional embed‐
ding matrix <b>P</b> is defined in Equation 16-2 and represented at the bottom of
Figure 16-9 (transposed), where <i>P</i> is the <i>ith</i> component of the embedding for the
<i>p,i</i>
word located at the <i>p</i> th position in the sentence.
<i>Equation</i> <i>16-2.</i> <i>Sine/cosine</i> <i>positional</i> <i>embeddings</i>
2i/d
<i>P</i> = sin <i>p/10000</i>
<i>p,2i</i>
2i/d
<i>P</i> = cos <i>p/10000</i>
<i>p,2i+1</i>
<i>Figure</i> <i>16-9.</i> <i>Sine/cosine</i> <i>positional</i> <i>embedding</i> <i>matrix</i> <i>(transposed,</i> <i>top)</i> <i>with</i> <i>a</i> <i>focus</i> <i>on</i>
<i>two</i> <i>values</i> <i>of</i> <i>i</i> <i>(bottom)</i>