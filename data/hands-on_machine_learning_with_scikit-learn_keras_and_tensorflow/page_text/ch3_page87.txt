To give you a feel for the complexity of the classification task, Figure 3-1 shows a few
more images from the MNIST dataset.
<i>Figure</i> <i>3-1.</i> <i>Digits</i> <i>from</i> <i>the</i> <i>MNIST</i> <i>dataset</i>
But wait! You should always create a test set and set it aside before inspecting the data
closely. The MNIST dataset is actually already split into a training set (the first 60,000
images) and a test set (the last 10,000 images):
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
The training set is already shuffled for us, which is good because this guarantees that
all cross-validation folds will be similar (you don’t want one fold to be missing some
digits). Moreover, some learning algorithms are sensitive to the order of the training
instances, and they perform poorly if they get many similar instances in a row. Shuf‐
happen.2
fling the dataset ensures that this won’t
2 Shufflingmaybeabadideainsomecontexts—forexample,ifyouareworkingontimeseriesdata(suchas
stockmarketpricesorweatherconditions).Wewillexplorethisinthenextchapters.