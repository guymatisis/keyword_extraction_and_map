                                                                      
                                                                      
                                                                      
                                                                      
          Computational Complexity                                    
                                                                      
          The LinearSVC class is based on the liblinear library, which implements an opti‐
          mized algorithm for linear SVMs.1 It does not support the kernel trick, but it scales
          almost linearly with the number of training instances and the number of features. Its
          training time complexity is roughly O(m × n).               
          The algorithm takes longer if you require very high precision. This is controlled by
          the tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification
          tasks, the default tolerance is fine.                       
                                                                      
          The SVC class is based on the libsvm library, which implements an algorithm that
          supports the kernel trick.2 The training time complexity is usually between O(m2 × n)
          and O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐
          ber of training instances gets large (e.g., hundreds of thousands of instances). This
          algorithm is perfect for complex small or medium-sized training sets. It scales well
          with the number of features, especially with sparse features (i.e., when each instance
          has few nonzero features). In this case, the algorithm scales roughly with the average
          number of nonzero features per instance. Table 5-1 compares Scikit-Learn’s SVM
          classification classes.                                     
          Table 5-1. Comparison of Scikit-Learn classes for SVM classification
                                                                      
          Class    Time complexity Out-of-core support Scaling required Kernel trick
          LinearSVC O(m × n) No       Yes     No                      
          SGDClassifier O(m × n) Yes  Yes     No                      
          SVC      O(m² × n) to O(m³ × n) No Yes Yes                  
                                                                      
          SVM Regression                                              
                                                                      
          As mentioned earlier, the SVM algorithm is versatile: not only does it support linear
          and nonlinear classification, but it also supports linear and nonlinear regression. To
          use SVMs for regression instead of classification, the trick is to reverse the objective:
          instead of trying to fit the largest possible street between two classes while limiting
          margin violations, SVM Regression tries to fit as many instances as possible on the
          street while limiting margin violations (i.e., instances off the street). The width of the
          street is controlled by a hyperparameter, ϵ. Figure 5-10 shows two linear SVM
                                                                      
                                                                      
          1 Chih-Jen Lin et al., “A Dual Coordinate Descent Method for Large-Scale Linear SVM,” Proceedings of the 25th
           International Conference on Machine Learning (2008): 408–415.
          2 John Platt, “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines”
           (Microsoft Research technical report, April 21, 1998), https://www.microsoft.com/en-us/research/wp-content/
           uploads/2016/02/tr-98-14.pdf.                              