                                                                      
                                                                      
                                                                      
                                                                      
            >>> bag_clf = BaggingClassifier(                          
            ...  DecisionTreeClassifier(), n_estimators=500,          
            ...  bootstrap=True, n_jobs=-1, oob_score=True)           
            ...                                                       
            >>> bag_clf.fit(X_train, y_train)                         
            >>> bag_clf.oob_score_                                    
            0.90133333333333332                                       
          According to this oob evaluation, this BaggingClassifier is likely to achieve about
          90.1% accuracy on the test set. Let’s verify this:          
            >>> from sklearn.metrics import accuracy_score            
            >>> y_pred = bag_clf.predict(X_test)                      
            >>> accuracy_score(y_test, y_pred)                        
            0.91200000000000003                                       
          We get 91.2% accuracy on the test set—close enough!         
          The oob decision function for each training instance is also available through the
          oob_decision_function_ variable. In this case (since the base estimator has a pre
          dict_proba() method), the decision function returns the class probabilities for each
          training instance. For example, the oob evaluation estimates that the first training
          instance has a 68.25% probability of belonging to the positive class (and 31.75% of
          belonging to the negative class):                           
            >>> bag_clf.oob_decision_function_                        
            array([[0.31746032, 0.68253968],                          
                [0.34117647, 0.65882353],                             
                [1.    , 0.   ],                                      
                ...                                                   
                [1.    , 0.   ],                                      
                [0.03108808, 0.96891192],                             
                [0.57291667, 0.42708333]])                            
          Random  Patches and Random Subspaces                        
          The BaggingClassifier class supports sampling the features as well. Sampling is
          controlled by two hyperparameters: max_features and bootstrap_features. They
          work the same way as max_samples and bootstrap, but for feature sampling instead
          of instance sampling. Thus, each predictor will be trained on a random subset of the
          input features.                                             
                                                                      
          This technique is particularly useful when you are dealing with high-dimensional
          inputs (such as images). Sampling both training instances and features is called the
          Random Patches method.7 Keeping all training instances (by setting bootstrap=False
                                                                      
                                                                      
          7 Gilles Louppe and Pierre Geurts, “Ensembles on Random Patches,” Lecture Notes in Computer Science 7523
           (2012): 346–361.                                           