<header><largefont><b>CHAPTER</b></largefont> <largefont><b>5</b></largefont></header>
<header><largefont><b>Support</b></largefont> <largefont><b>Vector</b></largefont> <largefont><b>Machines</b></largefont></header>
A <i>Support</i> <i>Vector</i> <i>Machine</i> (SVM) is a powerful and versatile Machine Learning
model, capable of performing linear or nonlinear classification, regression, and even
outlier detection. It is one of the most popular models in Machine Learning, and any‐
one interested in Machine Learning should have it in their toolbox. SVMs are partic‐
ularly well suited for classification of complex small- or medium-sized datasets.
This chapter will explain the core concepts of SVMs, how to use them, and how they
work.
<header><largefont><b>Linear</b></largefont> <largefont><b>SVM</b></largefont> <largefont><b>Classification</b></largefont></header>
The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1
shows part of the iris dataset that was introduced at the end of Chapter 4. The two
classes can clearly be separated easily with a straight line (they are <i>linearly</i> <i>separable).</i>
The left plot shows the decision boundaries of three possible linear classifiers. The
model whose decision boundary is represented by the dashed line is so bad that it
does not even separate the classes properly. The other two models work perfectly on
this training set, but their decision boundaries come so close to the instances that
these models will probably not perform as well on new instances. In contrast, the
solid line in the plot on the right represents the decision boundary of an SVM classi‐
fier; this line not only separates the two classes but also stays as far away from the
closest training instances as possible. You can think of an SVM classifier as fitting the
widest possible street (represented by the parallel dashed lines) between the classes.
This is called <i>large</i> <i>margin</i> <i>classification.</i>