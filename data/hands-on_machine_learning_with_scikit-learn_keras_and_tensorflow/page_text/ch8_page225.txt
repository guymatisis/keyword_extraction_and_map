<i>Figure</i> <i>8-9.</i> <i>MNIST</i> <i>compression</i> <i>that</i> <i>preserves</i> <i>95%</i> <i>of</i> <i>the</i> <i>variance</i>
The equation of the inverse transformation is shown in Equation 8-3.
<i>Equation</i> <i>8-3.</i> <i>PCA</i> <i>inverse</i> <i>transformation,</i> <i>back</i> <i>to</i> <i>the</i> <i>original</i> <i>number</i> <i>of</i>
<i>dimensions</i>
⊺
<b>X</b> = <b>X</b> <b>W</b>
recovered <i>d‐proj</i> <i>d</i>
<header><largefont><b>Randomized</b></largefont> <largefont><b>PCA</b></largefont></header>
If you set the svd_solver hyperparameter to "randomized" , Scikit-Learn uses a sto‐
chastic algorithm called <i>Randomized</i> <i>PCA</i> that quickly finds an approximation of the
first <i>d</i> principal components. Its computational complexity is <i>O(m</i> × <i>d</i> 2 ) + <i>O(d</i> 3 ),
<i>n2)</i> <i>O(n3)</i>
instead of <i>O(m</i> × + for the full SVD approach, so it is dramatically faster
than full SVD when <i>d</i> is much smaller than <i>n:</i>
rnd_pca = PCA(n_components=154, svd_solver="randomized")
X_reduced = rnd_pca.fit_transform(X_train)
By default, svd_solver is actually set to "auto" : Scikit-Learn automatically uses the
randomized PCA algorithm if <i>m</i> or <i>n</i> is greater than 500 and <i>d</i> is less than 80% of <i>m</i>
or <i>n,</i> or else it uses the full SVD approach. If you want to force Scikit-Learn to use full
svd_solver "full".
SVD, you can set the hyperparameter to
<header><largefont><b>Incremental</b></largefont> <largefont><b>PCA</b></largefont></header>
One problem with the preceding implementations of PCA is that they require the
whole training set to fit in memory in order for the algorithm to run. Fortunately,
<i>Incremental</i> <i>PCA</i> (IPCA) algorithms have been developed. They allow you to split the
training set into mini-batches and feed an IPCA algorithm one mini-batch at a time.