                                                                      
                                                                      
                                                                      
                                                                      
          Attention Mechanisms                                        
                                                                      
          Consider the path from the word “milk” to its translation “lait” in Figure 16-3: it is
          quite long! This means that a representation of this word (along with all the other
          words) needs to be carried over many steps before it is actually used. Can’t we make
          this path shorter?                                          
                                                                      
          This was the core idea in a groundbreaking 2014 paper13 by Dzmitry Bahdanau et al.
          They introduced a technique that allowed the decoder to focus on the appropriate
          words (as encoded by the encoder) at each time step. For example, at the time step
          where the decoder needs to output the word “lait,” it will focus its attention on the
          word “milk.” This means that the path from an input word to its translation is now
          much shorter, so the short-term memory limitations of RNNs have much less impact.
          Attention mechanisms revolutionized neural machine translation (and NLP in gen‐
          eral), allowing a significant improvement in the state of the art, especially for long
          sentences (over 30 words).14                                
          Figure 16-6 shows this model’s architecture (slightly simplified, as we will see). On the
          left, you have the encoder and the decoder. Instead of just sending the encoder’s final
          hidden state to the decoder (which is still done, although it is not shown in the fig‐
          ure), we now send all of its outputs to the decoder. At each time step, the decoder’s
          memory cell computes a weighted sum of all these encoder outputs: this determines
          which words it will focus on at this step. The weight α is the weight of the ith
                                            (t,i)                     
          encoder output at the tth decoder time step. For example, if the weight α is much
                                                     (3,2)            
          larger than the weights α and α , then the decoder will pay much more attention
                        (3,0) (3,1)                                   
          to word number 2 (“milk”) than to the other two words, at least at this time step. The
          rest of the decoder works just like earlier: at each time step the memory cell receives
          the inputs we just discussed, plus the hidden state from the previous time step, and
          finally (although it is not represented in the diagram) it receives the target word from
          the previous time step (or at inference time, the output from the previous time step).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          13 Dzmitry Bahdanau et al., “Neural Machine Translation by Jointly Learning to Align and Translate,” arXiv pre‐
           print arXiv:1409.0473 (2014).                              
          14 The most common metric used in NMT is the BiLingual Evaluation Understudy (BLEU) score, which com‐
           pares each translation produced by the model with several good translations produced by humans: it counts
           the number of n-grams (sequences of n words) that appear in any of the target translations and adjusts the
           score to take into account the frequency of the produced n-grams in the target translations.