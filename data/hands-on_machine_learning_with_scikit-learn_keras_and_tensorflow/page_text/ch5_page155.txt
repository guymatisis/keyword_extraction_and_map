<i>Figure</i> <i>5-3.</i> <i>Hard</i> <i>margin</i> <i>sensitivity</i> <i>to</i> <i>outliers</i>
To avoid these issues, use a more flexible model. The objective is to find a good bal‐
ance between keeping the street as large as possible and limiting the <i>margin</i> <i>violations</i>
(i.e., instances that end up in the middle of the street or even on the wrong side). This
is called <i>soft</i> <i>margin</i> <i>classification.</i>
When creating an SVM model using Scikit-Learn, we can specify a number of hyper‐
parameters. C is one of those hyperparameters. If we set it to a low value, then we end
up with the model on the left of Figure 5-4. With a high value, we get the model on
the right. Margin violations are bad. It’s usually better to have few of them. However,
in this case the model on the left has a lot of margin violations but will probably gen‐
eralize better.
<i>Figure</i> <i>5-4.</i> <i>Large</i> <i>margin</i> <i>(left)</i> <i>versus</i> <i>fewer</i> <i>margin</i> <i>violations</i> <i>(right)</i>
If your SVM model is overfitting, you can try regularizing it by
reducing C.
The following Scikit-Learn code loads the iris dataset, scales the features, and then
trains a linear SVM model (using the LinearSVC class with C=1 and the <i>hinge</i> <i>loss</i>
function, described shortly) to detect <i>Iris</i> <i>virginica</i> flowers:
<b>import</b> <b>numpy</b> <b>as</b> <b>np</b>
<b>from</b> <b>sklearn</b> <b>import</b> datasets
<b>from</b> <b>sklearn.pipeline</b> <b>import</b> Pipeline