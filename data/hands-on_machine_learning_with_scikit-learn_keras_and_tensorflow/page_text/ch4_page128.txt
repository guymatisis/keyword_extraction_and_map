                                                                      
                                                                      
                                                                      
                                                                      
          Let’s compare the algorithms we’ve discussed so far for Linear Regression6 (recall that
          m is the number of training instances and n is the number of features); see Table 4-1.
                                                                      
          Table 4-1. Comparison of algorithms for Linear Regression   
                                                                      
          Algorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn
          Normal Equation Fast No Slow 0  No     N/A                  
          SVD     Fast No      Slow 0     No     LinearRegression     
          Batch GD Slow No     Fast 2     Yes    SGDRegressor         
          Stochastic GD Fast Yes Fast ≥2  Yes    SGDRegressor         
          Mini-batch GD Fast Yes Fast ≥2  Yes    SGDRegressor         
                                                                      
                   There is almost no difference after training: all these algorithms
                   end up with very similar models and make predictions in exactly
                   the same way.                                      
                                                                      
                                                                      
          Polynomial Regression                                       
                                                                      
          What if your data is more complex than a straight line? Surprisingly, you can use a
          linear model to fit nonlinear data. A simple way to do this is to add powers of each
          feature as new features, then train a linear model on this extended set of features. This
          technique is called Polynomial Regression.                  
          Let’s look at an example. First, let’s generate some nonlinear data, based on a simple
          quadratic equation7 (plus some noise; see Figure 4-12):     
                                                                      
            m = 100                                                   
            X = 6 * np.random.rand(m, 1) - 3                          
            y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)            
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          6 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be
           used to train many other models, as we will see.           
          7 A quadratic equation is of the form y = ax2 + bx + c.     