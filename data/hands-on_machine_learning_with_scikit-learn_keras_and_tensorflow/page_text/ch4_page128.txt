Regression6
Let’s compare the algorithms we’ve discussed so far for Linear (recall that
<i>m</i> is the number of training instances and <i>n</i> is the number of features); see Table 4-1.
<i>Table</i> <i>4-1.</i> <i>Comparison</i> <i>of</i> <i>algorithms</i> <i>for</i> <i>Linear</i> <i>Regression</i>
<b>Algorithm</b> <b>Largem</b> <b>Out-of-coresupport</b> <b>Largen</b> <b>Hyperparams</b> <b>Scalingrequired</b> <b>Scikit-Learn</b>
NormalEquation Fast No Slow 0 No N/A
SVD Fast No Slow 0 No LinearRegression
BatchGD Slow No Fast 2 Yes SGDRegressor
StochasticGD Fast Yes Fast ≥2 Yes SGDRegressor
Mini-batchGD Fast Yes Fast ≥2 Yes SGDRegressor
There is almost no difference after training: all these algorithms
end up with very similar models and make predictions in exactly
the same way.
<header><largefont><b>Polynomial</b></largefont> <largefont><b>Regression</b></largefont></header>
What if your data is more complex than a straight line? Surprisingly, you can use a
linear model to fit nonlinear data. A simple way to do this is to add powers of each
feature as new features, then train a linear model on this extended set of features. This
technique is called <i>Polynomial</i> <i>Regression.</i>
Let’s look at an example. First, let’s generate some nonlinear data, based on a simple
<i>quadratic</i> <i>equation</i> 7 (plus some noise; see Figure 4-12):
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)
6 WhiletheNormalEquationcanonlyperformLinearRegression,theGradientDescentalgorithmscanbe
usedtotrainmanyothermodels,aswewillsee.
2
7 Aquadraticequationisoftheformy=ax +bx+c.