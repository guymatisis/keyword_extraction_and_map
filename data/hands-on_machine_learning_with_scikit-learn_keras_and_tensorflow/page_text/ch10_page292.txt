                                                                      
                                                                      
                                                                      
                                                                      
          The Rectified Linear Unit function: ReLU(z) = max(0, z)     
            The ReLU function is continuous but unfortunately not differentiable at z = 0
            (the slope changes abruptly, which can make Gradient Descent bounce around),
            and its derivative is 0 for z < 0. In practice, however, it works very well and has
            the advantage of being fast to compute, so it has become the default.12 Most
            importantly, the fact that it does not have a maximum output value helps reduce
            some issues during Gradient Descent (we will come back to this in Chapter 11).
                                                                      
          These popular activation functions and their derivatives are represented in
          Figure 10-8. But wait! Why do we need activation functions in the first place? Well, if
          you chain several linear transformations, all you get is a linear transformation. For
          example, if f(x) = 2x + 3 and g(x) = 5x – 1, then chaining these two linear functions
          gives you another linear function: f(g(x)) = 2(5x – 1) + 3 = 10x + 1. So if you don’t
          have some nonlinearity between layers, then even a deep stack of layers is equivalent
          to a single layer, and you can’t solve very complex problems with that. Conversely, a
          large enough DNN with nonlinear activations can theoretically approximate any con‐
          tinuous function.                                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 10-8. Activation functions and their derivatives     
                                                                      
          OK! You know where neural nets came from, what their architecture is, and how to
          compute their outputs. You’ve also learned about the backpropagation algorithm. But
          what exactly can you do with them?                          
                                                                      
          Regression MLPs                                             
          First, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,
          the price of a house, given many of its features), then you just need a single output
          neuron: its output is the predicted value. For multivariate regression (i.e., to predict
                                                                      
                                                                      
          12 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck
           to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is
           one of the cases where the biological analogy was misleading.