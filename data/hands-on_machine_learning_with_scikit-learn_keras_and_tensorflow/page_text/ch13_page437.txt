                                                                      
                                                                      
                                                                      
                                                                      
          Putting everything together, we can now create a Keras model that can process cate‐
          gorical features (along with regular numerical features) and learn an embedding for
          each category (as well as for each oov bucket):             
                                                                      
            regular_inputs = keras.layers.Input(shape=[8])            
            categories = keras.layers.Input(shape=[], dtype=tf.string)
            cat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)
            cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)
            encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])
            outputs = keras.layers.Dense(1)(encoded_inputs)           
            model = keras.models.Model(inputs=[regular_inputs, categories],
                            outputs=[outputs])                        
          This model takes two inputs: a regular input containing eight numerical features per
          instance, plus a categorical input (containing one categorical feature per instance). It
          uses a Lambda layer to look up each category’s index, then it looks up the embeddings
          for these indices. Next, it concatenates the embeddings and the regular inputs in
          order to give the encoded inputs, which are ready to be fed to a neural network. We
          could add any kind of neural network at this point, but we just add a dense output
          layer, and we create the Keras model.                       
          When the keras.layers.TextVectorization layer is available, you can call its
          adapt() method to make it extract the vocabulary from a data sample (it will take
          care of creating the lookup table for you). Then you can add it to your model, and it
          will perform the index lookup (replacing the Lambda layer in the previous code
          example).                                                   
                   One-hot encoding followed by a Dense layer (with no activation
                   function and no biases) is equivalent to an Embedding layer. How‐
                   ever, the Embedding layer uses way fewer computations (the perfor‐
                   mance difference becomes clear when the size of the embedding
                   matrix grows). The Dense layer’s weight matrix plays the role of the
                   embedding matrix. For example, using one-hot vectors of size 20
                   and a Dense layer with 10 units is equivalent to using an Embedding
                   layer with input_dim=20 and output_dim=10. As a result, it would
                   be wasteful to use more embedding dimensions than the number
                   of units in the layer that follows the Embedding layer.
          Now let’s look a bit more closely at the Keras preprocessing layers.
                                                                      
          Keras Preprocessing Layers                                  
                                                                      
          The TensorFlow team is working on providing a set of standard Keras preprocessing
          layers. They will probably be available by the time you read this; however, the API
          may change slightly by then, so please refer to the notebook for this chapter if any‐
          thing behaves unexpectedly. This new API will likely supersede the existing Feature