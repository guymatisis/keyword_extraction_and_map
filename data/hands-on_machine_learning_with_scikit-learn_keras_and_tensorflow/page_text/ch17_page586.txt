                                                                      
                                                                      
                                                                      
                                                                      
          Variational Autoencoders                                    
                                                                      
          Another important category of autoencoders was introduced in 2013 by Diederik
          Kingma and Max Welling and quickly became one of the most popular types of
          autoencoders: variational autoencoders.7                    
                                                                      
          They are quite different from all the autoencoders we have discussed so far, in these
          particular ways:                                            
           • They are probabilistic autoencoders, meaning that their outputs are partly deter‐
            mined by chance, even after training (as opposed to denoising autoencoders,
            which use randomness only during training).               
           • Most importantly, they are generative autoencoders, meaning that they can gener‐
            ate new instances that look like they were sampled from the training set.
                                                                      
          Both these properties make them rather similar to RBMs, but they are easier to train,
          and the sampling process is much faster (with RBMs you need to wait for the network
          to stabilize into a “thermal equilibrium” before you can sample a new instance).
          Indeed, as their name suggests, variational autoencoders perform variational Baye‐
          sian inference (introduced in Chapter 9), which is an efficient way to perform
          approximate Bayesian inference.                             
          Let’s take a look at how they work. Figure 17-12 (left) shows a variational autoen‐
          coder. You can recognize the basic structure of all autoencoders, with an encoder fol‐
          lowed by a decoder (in this example, they both have two hidden layers), but there is a
          twist: instead of directly producing a coding for a given input, the encoder produces a
          mean coding μ and a standard deviation σ. The actual coding is then sampled ran‐
          domly from a Gaussian distribution with mean μ and standard deviation σ. After that
          the decoder decodes the sampled coding normally. The right part of the diagram
          shows a training instance going through this autoencoder. First, the encoder pro‐
          duces μ and σ, then a coding is sampled randomly (notice that it is not exactly located
          at μ), and finally this coding is decoded; the final output resembles the training
          instance.                                                   
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          7 Diederik Kingma and Max Welling, “Auto-Encoding Variational Bayes,” arXiv preprint arXiv:1312.6114
           (2013).                                                    