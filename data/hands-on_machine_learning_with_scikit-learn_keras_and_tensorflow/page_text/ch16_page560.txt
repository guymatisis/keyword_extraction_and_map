                                                                      
                                                                      
                                                                      
                                                                      
           • V is a matrix containing one row per value. Its shape is [n , d ], where d
                                              keys values values      
            is the number of each value.                              
           • The shape of Q K⊺ is [n , n ]: it contains one similarity score for each
                            queries keys                              
            query/key pair. The output of the softmax function has the same shape, but all
            rows sum up to 1. The final output has a shape of [n , d ]: there is one row
                                           queries values             
            per query, where each row represents the query result (a weighted sum of the val‐
            ues).                                                     
           • The scaling factor scales down the similarity scores to avoid saturating the soft‐
            max function, which would lead to tiny gradients.         
           • It is possible to mask out some key/value pairs by adding a very large negative
            value to the corresponding similarity scores, just before computing the softmax.
            This is useful in the Masked Multi-Head Attention layer.  
          In the encoder, this equation is applied to every input sentence in the batch, with Q,
          K, and V all equal to the list of words in the input sentence (so each word in the sen‐
          tence will be compared to every word in the same sentence, including itself). Simi‐
          larly, in the decoder’s masked attention layer, the equation will be applied to every
          target sentence in the batch, with Q, K, and V all equal to the list of words in the tar‐
          get sentence, but this time using a mask to prevent any word from comparing itself to
          words located after it (at inference time the decoder will only have access to the words
          it already output, not to future words, so during training we must mask out future
          output tokens). In the upper attention layer of the decoder, the keys K and values V
          are simply the list of word encodings produced by the encoder, and the queries Q are
          the list of word encodings produced by the decoder.         
          The keras.layers.Attention layer implements Scaled Dot-Product Attention, effi‐
          ciently applying Equation 16-3 to multiple sentences in a batch. Its inputs are just like
          Q, K, and V, except with an extra batch dimension (the first dimension).
                   In TensorFlow, if A and B are tensors with more than two dimen‐
                   sions—say, of shape [2, 3, 4, 5] and [2, 3, 5, 6] respectively—then
                   tf.matmul(A, B) will treat these tensors as 2 × 3 arrays where each
                   cell contains a matrix, and it will multiply the corresponding matri‐
                   ces: the matrix at the ith row and jth column in A will be multiplied
                   by the matrix at the ith row and jth column in B. Since the product of
                   a 4 × 5 matrix with a 5 × 6 matrix is a 4 × 6 matrix, tf.matmul(A,
                   B) will return an array of shape [2, 3, 4, 6].     
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      