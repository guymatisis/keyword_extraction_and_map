<i>Figure</i> <i>14-5.</i> <i>Applying</i> <i>two</i> <i>different</i> <i>filters</i> <i>to</i> <i>get</i> <i>two</i> <i>feature</i> <i>maps</i>
<header><largefont><b>Stacking</b></largefont> <largefont><b>Multiple</b></largefont> <largefont><b>Feature</b></largefont> <largefont><b>Maps</b></largefont></header>
Up to now, for simplicity, I have represented the output of each convolutional layer as
a 2D layer, but in reality a convolutional layer has multiple filters (you decide how
many) and outputs one feature map per filter, so it is more accurately represented in
3D (see Figure 14-6). It has one neuron per pixel in each feature map, and all neurons
within a given feature map share the same parameters (i.e., the same weights and bias
term). Neurons in different feature maps use different parameters. A neuron’s recep‐
tive field is the same as described earlier, but it extends across all the previous layers’
feature maps. In short, a convolutional layer simultaneously applies multiple trainable
filters to its inputs, making it capable of detecting multiple features anywhere in its
inputs.
The fact that all neurons in a feature map share the same parame‐
ters dramatically reduces the number of parameters in the model.
Once the CNN has learned to recognize a pattern in one location, it
can recognize it in any other location. In contrast, once a regular
DNN has learned to recognize a pattern in one location, it can rec‐
ognize it only in that particular location.
Input images are also composed of multiple sublayers: one per <i>color</i> <i>channel.</i> There
are typically three: red, green, and blue (RGB). Grayscale images have just one