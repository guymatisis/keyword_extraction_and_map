• Shaded nodes indicate that the value is known. So, in this case, only the random
variables <b>x(i)</b> have known values: they are called <i>observed</i> <i>variables.</i> The unknown
random variables <i>z</i> (i) are called <i>latent</i> <i>variables.</i>
So, what can you do with such a model? Well, given the dataset <b>X,</b> you typically want
to start by estimating the weights <b>ϕ</b> and all the distribution parameters <b>μ(1)</b> to <b>μ(k)</b> and
<b>Σ</b> (1) to <b>Σ</b> (k) . Scikit-Learn’s GaussianMixture class makes this super easy:
<b>from</b> <b>sklearn.mixture</b> <b>import</b> GaussianMixture
gm = GaussianMixture(n_components=3, n_init=10)
gm.fit(X)
Let’s look at the parameters that the algorithm estimated:
<b>>>></b> gm.weights_
array([0.20965228, 0.4000662 , 0.39028152])
<b>>>></b> gm.means_
array([[ 3.39909717, 1.05933727],
[-1.40763984, 1.42710194],
[ 0.05135313, 0.07524095]])
<b>>>></b> gm.covariances_
array([[[ 1.14807234, -0.03270354],
[-0.03270354, 0.95496237]],
[[ 0.63478101, 0.72969804],
[ 0.72969804, 1.1609872 ]],
[[ 0.68809572, 0.79608475],
[ 0.79608475, 1.21234145]]])
Great, it worked fine! Indeed, the weights that were used to generate the data were
0.2, 0.4, and 0.4; and similarly, the means and covariance matrices were very close to
those found by the algorithm. But how? This class relies on the <i>Expectation-</i>
<i>Maximization</i> (EM) algorithm, which has many similarities with the K-Means algo‐
rithm: it also initializes the cluster parameters randomly, then it repeats two steps
until convergence, first assigning instances to clusters (this is called the <i>expectation</i>
<i>step)</i> and then updating the clusters (this is called the <i>maximization</i> <i>step).</i> Sounds
familiar, right? In the context of clustering, you can think of EM as a generalization of
K-Means that not only finds the cluster centers (μ (1) to <b>μ</b> (k) ), but also their size, shape,
and orientation (Σ(1) to <b>Σ(k)),</b> as well as their relative weights (ϕ(1) to <i>ϕ(k)).</i> Unlike K-
Means, though, EM uses soft cluster assignments, not hard assignments. For each
instance, during the expectation step, the algorithm estimates the probability that it
belongs to each cluster (based on the current cluster parameters). Then, during the
maximization step, each cluster is updated using <i>all</i> the instances in the dataset, with
each instance weighted by the estimated probability that it belongs to that cluster.
These probabilities are called the <i>responsibilities</i> of the clusters for the instances.