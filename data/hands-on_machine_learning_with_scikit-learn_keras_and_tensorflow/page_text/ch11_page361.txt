<i>Piecewise</i> <i>constant</i> <i>scheduling</i>
Use a constant learning rate for a number of epochs (e.g., <i>η</i> = 0.1 for 5 epochs),
0
then a smaller learning rate for another number of epochs (e.g., <i>η</i> = 0.001 for 50
1
epochs), and so on. Although this solution can work very well, it requires fid‐
dling around to figure out the right sequence of learning rates and how long to
use each of them.
<i>Performance</i> <i>scheduling</i>
Measure the validation error every <i>N</i> steps (just like for early stopping), and
reduce the learning rate by a factor of <i>λ</i> when the error stops dropping.
<i>1cycle</i> <i>scheduling</i>
Contrary to the other approaches, <i>1cycle</i> (introduced in a 2018 paper21 by Leslie
Smith) starts by increasing the initial learning rate <i>η</i> , growing linearly up to <i>η</i>
0 1
halfway through training. Then it decreases the learning rate linearly down to <i>η</i>
0
again during the second half of training, finishing the last few epochs by drop‐
ping the rate down by several orders of magnitude (still linearly). The maximum
learning rate <i>η</i> is chosen using the same approach we used to find the optimal
1
learning rate, and the initial learning rate <i>η</i> is chosen to be roughly 10 times
0
lower. When using a momentum, we start with a high momentum first (e.g.,
0.95), then drop it down to a lower momentum during the first half of training
(e.g., down to 0.85, linearly), and then bring it back up to the maximum value
(e.g., 0.95) during the second half of training, finishing the last few epochs with
that maximum value. Smith did many experiments showing that this approach
was often able to speed up training considerably and reach better performance.
For example, on the popular CIFAR10 image dataset, this approach reached
91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800
epochs through a standard approach (with the same neural network
architecture).
A 2013 paper22 by Andrew Senior et al. compared the performance of some of the
most popular learning schedules when using momentum optimization to train deep
neural networks for speech recognition. The authors concluded that, in this setting,
both performance scheduling and exponential scheduling performed well. They
favored exponential scheduling because it was easy to tune and it converged slightly
faster to the optimal solution (they also mentioned that it was easier to implement
21 LeslieN.Smith,“ADisciplinedApproachtoNeuralNetworkHyper-Parameters:Part1—LearningRate,Batch
Size,Momentum,andWeightDecay,”arXivpreprintarXiv:1803.09820(2018).
22 AndrewSenioretal.,“AnEmpiricalStudyofLearningRatesinDeepNeuralNetworksforSpeechRecogni‐
tion,”ProceedingsoftheIEEEInternationalConferenceonAcoustics,Speech,andSignalProcessing(2013):
6724–6728.