Now let’s look deeper into the heart of the Transformer model: the Multi-Head Atten‐
tion layer.
<b>Multi-HeadAttention</b>
To understand how a Multi-Head Attention layer works, we must first understand the
<i>Scaled</i> <i>Dot-Product</i> <i>Attention</i> layer, which it is based on. Let’s suppose the encoder
analyzed the input sentence “They played chess,” and it managed to understand that
the word “They” is the subject and the word “played” is the verb, so it encoded this
information in the representations of these words. Now suppose the decoder has
already translated the subject, and it thinks that it should translate the verb next. For
this, it needs to fetch the verb from the input sentence. This is analog to a dictionary
lookup: it’s as if the encoder created a dictionary {“subject”: “They”, “verb”: “played”,
…} and the decoder wanted to look up the value that corresponds to the key “verb.”
However, the model does not have discrete tokens to represent the keys (like “subject”
or “verb”); it has vectorized representations of these concepts (which it learned dur‐
ing training), so the key it will use for the lookup (called the <i>query)</i> will not perfectly
match any key in the dictionary. The solution is to compute a similarity measure
between the query and each key in the dictionary, and then use the softmax function
to convert these similarity scores to weights that add up to 1. If the key that represents
the verb is by far the most similar to the query, then that key’s weight will be close to
1. Then the model can compute a weighted sum of the corresponding values, so if the
weight of the “verb” key is close to 1, then the weighted sum will be very close to the
representation of the word “played.” In short, you can think of this whole process as a
differentiable dictionary lookup. The similarity measure used by the Transformer is
just the dot product, like in Luong attention. In fact, the equation is the same as for
Luong attention, except for a scaling factor. The equation is shown in Equation 16-3,
in a vectorized form.
<i>Equation</i> <i>16-3.</i> <i>Scaled</i> <i>Dot-Product</i> <i>Attention</i>
⊺

Attention  ,  ,  = softmax
<i>d</i>
<i>keys</i>
In this equation:
• <b>Q</b> is a matrix containing one row per query. Its shape is [n , <i>d</i> ], where
queries keys
<i>n</i> is the number of queries and <i>d</i> is the number of dimensions of each
queries keys
query and each key.
• <b>K</b> is a matrix containing one row per key. Its shape is [n , <i>d</i> ], where <i>n</i> is
keys keys keys
the number of keys and values.