ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and
the fully connected layer)17 containing 3 residual units that output 64 feature maps, 4
RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐
ment this architecture later in this chapter.
ResNets deeper than that, such as ResNet-152, use slightly different residual units.
Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three
convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4
times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer
with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature
maps (4 times 64) that restores the original depth. ResNet-152 contains 3 such RUs
that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024
maps, and finally 3 RUs with 2,048 maps.
Google’s Inception-v418 architecture merged the ideas of GoogLe‐
Net and ResNet and achieved a top-five error rate of close to 3% on
ImageNet classification.
<header><largefont><b>Xception</b></largefont></header>
Another variant of the GoogLeNet architecture is worth noting: Xception19 (which
stands for <i>Extreme</i> <i>Inception)</i> was proposed in 2016 by François Chollet (the author
of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350
million images and 17,000 classes). Just like Inception-v4, it merges the ideas of Goo‐
gLeNet and ResNet, but it replaces the inception modules with a special type of layer
called a <i>depthwise</i> <i>separable</i> <i>convolution</i> <i>layer</i> (or <i>separable</i> <i>convolution</i> <i>layer</i> for
short20). These layers had been used before in some CNN architectures, but they were
not as central as in the Xception architecture. While a regular convolutional layer
uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-
channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer
makes the strong assumption that spatial patterns and cross-channel patterns can be
modeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part
applies a single spatial filter for each input feature map, then the second part looks
17 Itisacommonpracticewhendescribinganeuralnetworktocountonlylayerswithparameters.
18 ChristianSzegedyetal.,“Inception–v4,Inception-ResNetandtheImpactofResidualConnectionsonLearn‐
ing,”arXivpreprintarXiv:1602.07261(2016).
19 FrançoisChollet,“Xception:DeepLearningwithDepthwiseSeparableConvolutions,”arXivpreprintarXiv:
1610.02357(2016).
20 Thisnamecansometimesbeambiguous,sincespatiallyseparableconvolutionsareoftencalled“separable
convolutions”aswell.