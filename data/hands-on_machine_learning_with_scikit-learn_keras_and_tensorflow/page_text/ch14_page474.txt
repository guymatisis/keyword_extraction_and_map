                                                                      
                                                                      
                                                                      
                                                                      
          ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and
          the fully connected layer)17 containing 3 residual units that output 64 feature maps, 4
          RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐
          ment this architecture later in this chapter.               
                                                                      
          ResNets deeper than that, such as ResNet-152, use slightly different residual units.
          Instead of two 3 × 3 convolutional layers with, say, 256 feature maps, they use three
          convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4
          times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer
          with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature
          maps (4 times 64) that restores the original depth. ResNet-152 contains 3 such RUs
          that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024
          maps, and finally 3 RUs with 2,048 maps.                    
                                                                      
                   Google’s Inception-v418 architecture merged the ideas of GoogLe‐
                   Net and ResNet and achieved a top-five error rate of close to 3% on
                   ImageNet classification.                           
                                                                      
          Xception                                                    
                                                                      
          Another variant of the GoogLeNet architecture is worth noting: Xception19 (which
          stands for Extreme Inception) was proposed in 2016 by François Chollet (the author
          of Keras), and it significantly outperformed Inception-v3 on a huge vision task (350
          million images and 17,000 classes). Just like Inception-v4, it merges the ideas of Goo‐
          gLeNet and ResNet, but it replaces the inception modules with a special type of layer
          called a depthwise separable convolution layer (or separable convolution layer for
          short20). These layers had been used before in some CNN architectures, but they were
          not as central as in the Xception architecture. While a regular convolutional layer
          uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-
          channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer
          makes the strong assumption that spatial patterns and cross-channel patterns can be
          modeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part
          applies a single spatial filter for each input feature map, then the second part looks
                                                                      
                                                                      
          17 It is a common practice when describing a neural network to count only layers with parameters.
          18 Christian Szegedy et al., “Inception–v4, Inception-ResNet and the Impact of Residual Connections on Learn‐
           ing,” arXiv preprint arXiv:1602.07261 (2016).              
          19 François Chollet, “Xception: Deep Learning with Depthwise Separable Convolutions,” arXiv preprint arXiv:
           1610.02357 (2016).                                         
          20 This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable
           convolutions” as well.                                     