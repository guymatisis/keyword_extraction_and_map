<header><largefont><b>Using</b></largefont> <largefont><b>Clustering</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Semi-Supervised</b></largefont> <largefont><b>Learning</b></largefont></header>
Another use case for clustering is in semi-supervised learning, when we have plenty
of unlabeled instances and very few labeled instances. Let’s train a Logistic Regression
model on a sample of 50 labeled instances from the digits dataset:
n_labeled = 50
log_reg = LogisticRegression()
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
What is the performance of this model on the test set?
<b>>>></b> log_reg.score(X_test, y_test)
0.8333333333333334
The accuracy is just 83.3%. It should come as no surprise that this is much lower than
earlier, when we trained the model on the full training set. Let’s see how we can do
better. First, let’s cluster the training set into 50 clusters. Then for each cluster, let’s
find the image closest to the centroid. We will call these images the <i>representative</i>
<i>images:</i>
k = 50
kmeans = KMeans(n_clusters=k)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis=0)
X_representative_digits = X_train[representative_digit_idx]
Figure 9-13 shows these 50 representative images.
<i>Figure</i> <i>9-13.</i> <i>Fifty</i> <i>representative</i> <i>digit</i> <i>images</i> <i>(one</i> <i>per</i> <i>cluster)</i>
Let’s look at each image and manually label it:
y_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])
Now we have a dataset with just 50 labeled instances, but instead of being random
instances, each of them is a representative image of its cluster. Let’s see if the perfor‐
mance is any better:
<b>>>></b> log_reg = LogisticRegression()
<b>>>></b> log_reg.fit(X_representative_digits, y_representative_digits)
<b>>>></b> log_reg.score(X_test, y_test)
0.9222222222222223