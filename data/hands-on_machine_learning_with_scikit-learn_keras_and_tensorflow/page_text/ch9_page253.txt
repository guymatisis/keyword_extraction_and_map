                                                                      
                                                                      
                                                                      
                                                                      
          Using Clustering for Semi-Supervised Learning               
                                                                      
          Another use case for clustering is in semi-supervised learning, when we have plenty
          of unlabeled instances and very few labeled instances. Let’s train a Logistic Regression
          model on a sample of 50 labeled instances from the digits dataset:
            n_labeled = 50                                            
            log_reg = LogisticRegression()                            
            log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])     
          What is the performance of this model on the test set?      
            >>> log_reg.score(X_test, y_test)                         
            0.8333333333333334                                        
                                                                      
          The accuracy is just 83.3%. It should come as no surprise that this is much lower than
          earlier, when we trained the model on the full training set. Let’s see how we can do
          better. First, let’s cluster the training set into 50 clusters. Then for each cluster, let’s
          find the image closest to the centroid. We will call these images the representative
          images:                                                     
            k = 50                                                    
            kmeans = KMeans(n_clusters=k)                             
            X_digits_dist = kmeans.fit_transform(X_train)             
            representative_digit_idx = np.argmin(X_digits_dist, axis=0)
            X_representative_digits = X_train[representative_digit_idx]
          Figure 9-13 shows these 50 representative images.           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 9-13. Fifty representative digit images (one per cluster)
                                                                      
          Let’s look at each image and manually label it:             
            y_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])
          Now we have a dataset with just 50 labeled instances, but instead of being random
          instances, each of them is a representative image of its cluster. Let’s see if the perfor‐
          mance is any better:                                        
                                                                      
            >>> log_reg = LogisticRegression()                        
            >>> log_reg.fit(X_representative_digits, y_representative_digits)
            >>> log_reg.score(X_test, y_test)                         
            0.9222222222222223                                        
                                                                      