<i>node</i> (i.e., it does not have any child nodes), so it does not ask any questions: simply
look at the predicted class for that node, and the Decision Tree predicts that your
flower is an <i>Iris</i> <i>setosa</i> ( class=setosa ).
Now suppose you find another flower, and this time the petal length is greater than
2.45 cm. You must move down to the root’s right child node (depth 1, right), which is
not a leaf node, so the node asks another question: is the petal width smaller than
1.75 cm? If it is, then your flower is most likely an <i>Iris</i> <i>versicolor</i> (depth 2, left). If not,
it is likely an <i>Iris</i> <i>virginica</i> (depth 2, right). It’s really that simple.
One of the many qualities of Decision Trees is that they require
very little data preparation. In fact, they don’t require feature scal‐
ing or centering at all.
A node’s samples attribute counts how many training instances it applies to. For
example, 100 training instances have a petal length greater than 2.45 cm (depth 1,
right), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left). A
node’s value attribute tells you how many training instances of each class this node
applies to: for example, the bottom-right node applies to 0 <i>Iris</i> <i>setosa,</i> 1 <i>Iris</i> <i>versicolor,</i>
gini
and 45 <i>Iris</i> <i>virginica.</i> Finally, a node’s attribute measures its <i>impurity:</i> a node is
“pure” ( gini=0 ) if all training instances it applies to belong to the same class. For
example, since the depth-1 left node applies only to <i>Iris</i> <i>setosa</i> training instances, it is
gini
pure and its score is 0. Equation 6-1 shows how the training algorithm com‐
putes the gini score <i>G</i> of the <i>ith</i> node. The depth-2 left node has a gini score equal to
<i>i</i>
1 – (0/54) 2 – (49/54) 2 – (5/54) 2 ≈ 0.168.
<i>Equation</i> <i>6-1.</i> <i>Gini</i> <i>impurity</i>
<i>n</i>
2
<i>G</i> = 1 − <largefont>∑</largefont> <i>p</i>
<i>i</i> <i>i,k</i>
<i>k</i> = 1
In this equation:
• <i>p</i> is the ratio of class <i>k</i> instances among the training instances in the <i>ith</i> node.
<i>i,k</i>
Scikit-Learn uses the CART algorithm, which produces only <i>binary</i>
<i>trees:</i> nonleaf nodes always have two children (i.e., questions only
have yes/no answers). However, other algorithms such as ID3 can
produce Decision Trees with nodes that have more than two
children.