                                                                      
                                                                      
                                                                      
                                                                      
          node (i.e., it does not have any child nodes), so it does not ask any questions: simply
          look at the predicted class for that node, and the Decision Tree predicts that your
          flower is an Iris setosa (class=setosa).                    
                                                                      
          Now suppose you find another flower, and this time the petal length is greater than
          2.45 cm. You must move down to the root’s right child node (depth 1, right), which is
          not a leaf node, so the node asks another question: is the petal width smaller than
          1.75 cm? If it is, then your flower is most likely an Iris versicolor (depth 2, left). If not,
          it is likely an Iris virginica (depth 2, right). It’s really that simple.
                                                                      
                   One of the many qualities of Decision Trees is that they require
                   very little data preparation. In fact, they don’t require feature scal‐
                   ing or centering at all.                           
                                                                      
                                                                      
          A node’s samples attribute counts how many training instances it applies to. For
          example, 100 training instances have a petal length greater than 2.45 cm (depth 1,
          right), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left). A
          node’s value attribute tells you how many training instances of each class this node
          applies to: for example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor,
          and 45 Iris virginica. Finally, a node’s gini attribute measures its impurity: a node is
          “pure” (gini=0) if all training instances it applies to belong to the same class. For
          example, since the depth-1 left node applies only to Iris setosa training instances, it is
          pure and its gini score is 0. Equation 6-1 shows how the training algorithm com‐
          putes the gini score G of the ith node. The depth-2 left node has a gini score equal to
                       i                                              
          1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168.                   
            Equation 6-1. Gini impurity                               
                  n                                                   
            G =1− ∑ p 2                                               
             i      i,k                                               
                 k=1                                                  
          In this equation:                                           
           • p is the ratio of class k instances among the training instances in the ith node.
             i,k                                                      
                                                                      
                   Scikit-Learn uses the CART algorithm, which produces only binary
                   trees: nonleaf nodes always have two children (i.e., questions only
                   have yes/no answers). However, other algorithms such as ID3 can
                   produce Decision Trees with nodes that have more than two
                   children.                                          
                                                                      