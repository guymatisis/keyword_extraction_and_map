equations simplify, so the latent loss can be computed quite simply using Equation
17-3:9
<i>Equation</i> <i>17-3.</i> <i>Variational</i> <i>autoencoder’s</i> <i>latent</i> <i>loss</i>
<i>K</i>
1
ℒ <largefont>∑</largefont> 2 2 2
= − 1 + log <i>σ</i> − <i>σ</i> − <i>μ</i>
<i>i</i> <i>i</i> <i>i</i>
2
<i>i</i> = 1
ℒ
In this equation, is the latent loss, <i>n</i> is the codings’ dimensionality, and <i>μ</i> and <i>σ</i> are
i i
the mean and standard deviation of the <i>ith</i> component of the codings. The vectors <b>μ</b>
and <b>σ</b> (which contain all the <i>μ</i> and <i>σ)</i> are output by the encoder, as shown in
i i
Figure 17-12 (left).
A common tweak to the variational autoencoder’s architecture is to make the encoder
output <b>γ</b> = log(σ2) rather than <b>σ.</b> The latent loss can then be computed as shown in
Equation 17-4. This approach is more numerically stable and speeds up training.
<i>Equation</i> <i>17-4.</i> <i>Variational</i> <i>autoencoder’s</i> <i>latent</i> <i>loss,</i> <i>rewritten</i> <i>using</i> <b>γ</b> <i>=</i> <i>log(σ</i> <i>2</i> <i>)</i>
<i>K</i>
1
ℒ <largefont>∑</largefont> 2
= − 1 + <i>γ</i> − exp <i>γ</i> − <i>μ</i>
<i>i</i> <i>i</i> <i>i</i>
2
<i>i</i> = 1
Let’s start building a variational autoencoder for Fashion MNIST (as shown in
Figure 17-12, but using the <b>γ</b> tweak). First, we will need a custom layer to sample the
codings, given <b>μ</b> and <b>γ:</b>
<b>class</b> <b>Sampling(keras.layers.Layer):</b>
<b>def</b> call(self, inputs):
mean, log_var = inputs
<b>return</b> K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean
This Sampling layer takes two inputs: mean (μ) and log_var (γ). It uses the function
K.random_normal()
to sample a random vector (of the same shape as <b>γ)</b> from the
Normal distribution, with mean 0 and standard deviation 1. Then it multiplies it by
exp(γ / 2) (which is equal to <b>σ,</b> as you can verify), and finally it adds <b>μ</b> and returns the
result. This samples a codings vector from the Normal distribution with mean <b>μ</b> and
standard deviation <b>σ.</b>
Next, we can create the encoder, using the Functional API because the model is not
entirely sequential:
9 Formoremathematicaldetails,checkouttheoriginalpaperonvariationalautoencoders,orCarlDoersch’s
greattutorial(2016).