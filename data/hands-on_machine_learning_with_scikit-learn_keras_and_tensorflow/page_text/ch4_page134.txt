                                                                      
                                                                      
                                                                      
                                                                      
           • There is a gap between the curves. This means that the model performs signifi‐
            cantly better on the training data than on the validation data, which is the hall‐
            mark of an overfitting model. If you used a much larger training set, however, the
            two curves would continue to get closer.                  
                                                                      
                                                                      
                   One way to improve an overfitting model is to feed it more training
                   data until the validation error reaches the training error.
                                                                      
                                                                      
                                                                      
                                                                      
                         The Bias/Variance Trade-off                  
           An important theoretical result of statistics and Machine Learning is the fact that a
           model’s generalization error can be expressed as the sum of three very different
           errors:                                                    
                                                                      
           Bias                                                       
              This part of the generalization error is due to wrong assumptions, such as assum‐
              ing that the data is linear when it is actually quadratic. A high-bias model is most
              likely to underfit the training data.8                  
           Variance                                                   
              This part is due to the model’s excessive sensitivity to small variations in the
              training data. A model with many degrees of freedom (such as a high-degree pol‐
              ynomial model) is likely to have high variance and thus overfit the training data.
           Irreducible error                                          
              This part is due to the noisiness of the data itself. The only way to reduce this
              part of the error is to clean up the data (e.g., fix the data sources, such as broken
              sensors, or detect and remove outliers).                
           Increasing a model’s complexity will typically increase its variance and reduce its bias.
           Conversely, reducing a model’s complexity increases its bias and reduces its variance.
           This is why it is called a trade-off.                      
                                                                      
                                                                      
          Regularized Linear Models                                   
                                                                      
          As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the
          model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be
                                                                      
                                                                      
                                                                      
          8 This notion of bias is not to be confused with the bias term of linear models.