                                                                      
                                                                      
                                                                      
                                                                      
          Dropout does tend to significantly slow down convergence, but it usually results in a
          much better model when tuned properly. So, it is generally well worth the extra time
          and effort.                                                 
                                                                      
                   If you want to regularize a self-normalizing network based on the
                   SELU activation function (as discussed earlier), you should use
                   alpha dropout: this is a variant of dropout that preserves the mean
                   and standard deviation of its inputs (it was introduced in the same
                   paper as SELU, as regular dropout would break self-normalization).
                                                                      
          Monte Carlo (MC) Dropout                                    
                                                                      
          In 2016, a paper25 by Yarin Gal and Zoubin Ghahramani added a few more good rea‐
          sons to use dropout:                                        
           • First, the paper established a profound connection between dropout networks
            (i.e., neural networks containing a Dropout layer before every weight layer) and
            approximate Bayesian inference,26 giving dropout a solid mathematical justifica‐
            tion.                                                     
                                                                      
           • Second, the authors introduced a powerful technique called MC Dropout, which
            can boost the performance of any trained dropout model without having to
            retrain it or even modify it at all, provides a much better measure of the model’s
            uncertainty, and is also amazingly simple to implement.   
          If this all sounds like a “one weird trick” advertisement, then take a look at the follow‐
          ing code. It is the full implementation of MC Dropout, boosting the dropout model
          we trained earlier without retraining it:                   
            y_probas = np.stack([model(X_test_scaled, training=True)  
                         for sample in range(100)])                   
            y_proba = y_probas.mean(axis=0)                           
          We just make 100 predictions over the test set, setting training=True to ensure that
          the Dropout layer is active, and stack the predictions. Since dropout is active, all the
          predictions will be different. Recall that predict() returns a matrix with one row per
          instance and one column per class. Because there are 10,000 instances in the test set
          and 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so
          y_probas is an array of shape [100, 10000, 10]. Once we average over the first
                                                                      
                                                                      
                                                                      
          25 Yarin Gal and Zoubin Ghahramani, “Dropout as a Bayesian Approximation: Representing Model Uncertainty
           in Deep Learning,” Proceedings of the 33rd International Conference on Machine Learning (2016): 1050–1059.
          26 Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian
           inference in a specific type of probabilistic model called a Deep Gaussian Process.