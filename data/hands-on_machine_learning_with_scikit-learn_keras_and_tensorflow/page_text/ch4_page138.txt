Figure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models with
Lasso models and uses smaller <i>α</i> values.
<i>Figure</i> <i>4-18.</i> <i>A</i> <i>linear</i> <i>model</i> <i>(left)</i> <i>and</i> <i>a</i> <i>polynomial</i> <i>model</i> <i>(right),</i> <i>both</i> <i>using</i> <i>various</i>
<i>levels</i> <i>of</i> <i>Lasso</i> <i>regularization</i>
An important characteristic of Lasso Regression is that it tends to eliminate the
weights of the least important features (i.e., set them to zero). For example, the
dashed line in the righthand plot in Figure 4-18 (with <i>α</i> = 10-7) looks quadratic,
almost linear: all the weights for the high-degree polynomial features are equal to
zero. In other words, Lasso Regression automatically performs feature selection and
outputs a <i>sparse</i> <i>model</i> (i.e., with few nonzero feature weights).
You can get a sense of why this is the case by looking at Figure 4-19: the axes repre‐
sent two model parameters, and the background contours represent different loss
functions. In the top-left plot, the contours represent the ℓ loss (|θ | + |θ |), which
1 1 2
drops linearly as you get closer to any axis. For example, if you initialize the model
parameters to <i>θ</i> = 2 and <i>θ</i> = 0.5, running Gradient Descent will decrement both
1 2
parameters equally (as represented by the dashed yellow line); therefore <i>θ</i> will reach
2
0 first (since it was closer to 0 to begin with). After that, Gradient Descent will roll
down the gutter until it reaches <i>θ</i> = 0 (with a bit of bouncing around, since the gradi‐
1
ents of ℓ never get close to 0: they are either –1 or 1 for each parameter). In the top-
1
right plot, the contours represent Lasso’s cost function (i.e., an MSE cost function plus
an ℓ loss). The small white circles show the path that Gradient Descent takes to opti‐
1
mize some model parameters that were initialized around <i>θ</i> = 0.25 and <i>θ</i> = –1:
1 2
notice once again how the path quickly reaches <i>θ</i> = 0, then rolls down the gutter and
2
ends up bouncing around the global optimum (represented by the red square). If we
increased <i>α,</i> the global optimum would move left along the dashed yellow line, while