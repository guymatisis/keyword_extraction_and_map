<i>Figure</i> <i>8-8.</i> <i>Explained</i> <i>variance</i> <i>as</i> <i>a</i> <i>function</i> <i>of</i> <i>the</i> <i>number</i> <i>of</i> <i>dimensions</i>
<header><largefont><b>PCA</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Compression</b></largefont></header>
After dimensionality reduction, the training set takes up much less space. As an
example, try applying PCA to the MNIST dataset while preserving 95% of its var‐
iance. You should find that each instance will have just over 150 features, instead of
the original 784 features. So, while most of the variance is preserved, the dataset is
now less than 20% of its original size! This is a reasonable compression ratio, and you
can see how this size reduction can speed up a classification algorithm (such as an
SVM classifier) tremendously.
It is also possible to decompress the reduced dataset back to 784 dimensions by
applying the inverse transformation of the PCA projection. This won’t give you back
the original data, since the projection lost a bit of information (within the 5% var‐
iance that was dropped), but it will likely be close to the original data. The mean
squared distance between the original data and the reconstructed data (compressed
and then decompressed) is called the <i>reconstruction</i> <i>error.</i>
The following code compresses the MNIST dataset down to 154 dimensions, then
inverse_transform()
uses the method to decompress it back to 784 dimensions:
pca = PCA(n_components = 154)
X_reduced = pca.fit_transform(X_train)
X_recovered = pca.inverse_transform(X_reduced)
Figure 8-9 shows a few digits from the original training set (on the left), and the cor‐
responding digits after compression and decompression. You can see that there is a
slight image quality loss, but the digits are still mostly intact.