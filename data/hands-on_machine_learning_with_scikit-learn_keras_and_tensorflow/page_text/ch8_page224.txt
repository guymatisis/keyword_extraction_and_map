                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 8-8. Explained variance as a function of the number of dimensions
                                                                      
          PCA for Compression                                         
                                                                      
          After dimensionality reduction, the training set takes up much less space. As an
          example, try applying PCA to the MNIST dataset while preserving 95% of its var‐
          iance. You should find that each instance will have just over 150 features, instead of
          the original 784 features. So, while most of the variance is preserved, the dataset is
          now less than 20% of its original size! This is a reasonable compression ratio, and you
          can see how this size reduction can speed up a classification algorithm (such as an
          SVM classifier) tremendously.                               
          It is also possible to decompress the reduced dataset back to 784 dimensions by
          applying the inverse transformation of the PCA projection. This won’t give you back
          the original data, since the projection lost a bit of information (within the 5% var‐
          iance that was dropped), but it will likely be close to the original data. The mean
          squared distance between the original data and the reconstructed data (compressed
          and then decompressed) is called the reconstruction error.  
          The following code compresses the MNIST dataset down to 154 dimensions, then
          uses the inverse_transform() method to decompress it back to 784 dimensions:
            pca = PCA(n_components = 154)                             
            X_reduced = pca.fit_transform(X_train)                    
            X_recovered = pca.inverse_transform(X_reduced)            
          Figure 8-9 shows a few digits from the original training set (on the left), and the cor‐
          responding digits after compression and decompression. You can see that there is a
          slight image quality loss, but the digits are still mostly intact.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      