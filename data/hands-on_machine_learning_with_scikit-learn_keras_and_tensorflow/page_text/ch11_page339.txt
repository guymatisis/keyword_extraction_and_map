paper,8
In a 2015 Sergey Ioffe and Christian Szegedy proposed a technique called
<i>Batch</i> <i>Normalization</i> (BN) that addresses these problems. The technique consists of
adding an operation in the model just before or after the activation function of each
hidden layer. This operation simply zero-centers and normalizes each input, then
scales and shifts the result using two new parameter vectors per layer: one for scaling,
the other for shifting. In other words, the operation lets the model learn the optimal
scale and mean of each of the layer’s inputs. In many cases, if you add a BN layer as
the very first layer of your neural network, you do not need to standardize your train‐
ing set (e.g., using a StandardScaler ); the BN layer will do it for you (well, approxi‐
mately, since it only looks at one batch at a time, and it can also rescale and shift each
input feature).
In order to zero-center and normalize the inputs, the algorithm needs to estimate
each input’s mean and standard deviation. It does so by evaluating the mean and stan‐
dard deviation of the input over the current mini-batch (hence the name “Batch Nor‐
malization”). The whole operation is summarized step by step in Equation 11-3.
<i>Equation</i> <i>11-3.</i> <i>Batch</i> <i>Normalization</i> <i>algorithm</i>
<i>m</i>
<i>B</i>
1
<largefont>∑</largefont> <i>i</i>
1. <b>μ</b> = <b>x</b>
<i>B</i>
<i>m</i>
<i>B</i> <i>i</i> = 1
<i>m</i>
<i>B</i>
1 2
2 <largefont>∑</largefont> <i>i</i>
2. <b>σ</b> = <b>x</b> − <b>μ</b>
<i>B</i> <i>B</i>
<i>m</i>
<i>B</i> <i>i</i> = 1
<i>i</i>
<b>x</b> − <b>μ</b>
<i>B</i>
<i>i</i>
3. <b>x</b> =
2
<b>σ</b> + <i>ε</i>
<i>B</i>
<i>i</i> ⊗ <i>i</i>
4. <b>z</b> = <b>γ</b> <b>x</b> + <b>β</b>
In this algorithm:
• <b>μ</b> is the vector of input means, evaluated over the whole mini-batch <i>B</i> (it con‐
<i>B</i>
tains one mean per input).
• <b>σ</b> is the vector of input standard deviations, also evaluated over the whole mini-
<i>B</i>
batch (it contains one standard deviation per input).
• <i>m</i> is the number of instances in the mini-batch.
<i>B</i>
• <b>x</b> (i) is the vector of zero-centered and normalized inputs for instance <i>i.</i>
8 SergeyIoffeandChristianSzegedy,“BatchNormalization:AcceleratingDeepNetworkTrainingbyReducing
InternalCovariateShift,”Proceedingsofthe32ndInternationalConferenceonMachineLearning(2015):448–
456.