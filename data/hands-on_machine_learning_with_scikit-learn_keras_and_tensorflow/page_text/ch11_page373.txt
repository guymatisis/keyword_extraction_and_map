                                                                      
                                                                      
                                                                      
                                                                      
          Exercises                                                   
                                                                      
                                                                      
           1. Is it OK to initialize all the weights to the same value as long as that value is
            selected randomly using He initialization?                
           2. Is it OK to initialize the bias terms to 0?             
           3. Name three advantages of the SELU activation function over ReLU.
           4. In which cases would you want to use each of the following activation functions:
            SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?
                                                                      
           5. What may happen if you set the momentum hyperparameter too close to 1 (e.g.,
            0.99999) when using an SGD optimizer?                     
           6. Name three ways you can produce a sparse model.         
           7. Does dropout slow down training? Does it slow down inference (i.e., making
            predictions on new instances)? What about MC Dropout?     
           8. Practice training a deep neural network on the CIFAR10 image dataset:
                                                                      
             a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but
              it’s the point of this exercise). Use He initialization and the ELU activation
              function.                                               
            b. Using Nadam optimization and early stopping, train the network on the
              CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_
              data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000
              for training, 10,000 for testing) with 10 classes, so you’ll need a softmax out‐
              put layer with 10 neurons. Remember to search for the right learning rate each
              time you change the model’s architecture or hyperparameters.
             c. Now try adding Batch Normalization and compare the learning curves: Is it
              converging faster than before? Does it produce a better model? How does it
              affect training speed?                                  
            d. Try replacing Batch Normalization with SELU, and make the necessary adjust‐
              ements to ensure the network self-normalizes (i.e., standardize the input fea‐
              tures, use LeCun normal initialization, make sure the DNN contains only a
              sequence of dense layers, etc.).                        
                                                                      
             e. Try regularizing the model with alpha dropout. Then, without retraining your
              model, see if you can achieve better accuracy using MC Dropout.
             f. Retrain your model using 1cycle scheduling and see if it improves training
              speed and model accuracy.                               
          Solutions to these exercises are available in Appendix A.   
                                                                      
                                                                      
                                                                      