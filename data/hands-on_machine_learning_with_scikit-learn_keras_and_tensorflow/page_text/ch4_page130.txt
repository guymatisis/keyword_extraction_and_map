<i>Figure</i> <i>4-13.</i> <i>Polynomial</i> <i>Regression</i> <i>model</i> <i>predictions</i>
2
Not bad: the model estimates <i>y</i> = 0.56x + 0.93x + 1.78 when in fact the original
1 1
2
function was <i>y</i> = 0.5x + 1.0x + 2.0 + Gaussian noise.
1 1
Note that when there are multiple features, Polynomial Regression is capable of find‐
ing relationships between features (which is something a plain Linear Regression
PolynomialFeatures
model cannot do). This is made possible by the fact that also
adds all combinations of features up to the given degree. For example, if there were
two features <i>a</i> and <i>b,</i> PolynomialFeatures with degree=3 would not only add the
2 3 2 3 2 2
features <i>a</i> , <i>a</i> , <i>b</i> , and <i>b</i> , but also the combinations <i>ab,</i> <i>a</i> <i>b,</i> and <i>ab</i> .
PolynomialFeatures(degree=d) transforms an array containing <i>n</i>
features into an array containing (n + <i>d)!</i> / <i>d!n!</i> features, where <i>n!</i> is
⋯
the <i>factorial</i> of <i>n,</i> equal to 1 × 2 × 3 × × <i>n.</i> Beware of the combi‐
natorial explosion of the number of features!
<header><largefont><b>Learning</b></largefont> <largefont><b>Curves</b></largefont></header>
If you perform high-degree Polynomial Regression, you will likely fit the training
data much better than with plain Linear Regression. For example, Figure 4-14 applies
a 300-degree polynomial model to the preceding training data, and compares the
result with a pure linear model and a quadratic model (second-degree polynomial).
Notice how the 300-degree polynomial model wiggles around to get as close as possi‐
ble to the training instances.