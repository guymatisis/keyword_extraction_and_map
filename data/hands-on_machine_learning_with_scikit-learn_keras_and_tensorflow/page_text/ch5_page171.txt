                                                                      
                                                                      
                                                                      
                                                                      
          based only on the original vectors a and b, without having to compute (or even to
          know about) the transformation ϕ. Equation 5-10 lists some of the most commonly
          used kernels.                                               
                                                                      
            Equation 5-10. Common kernels                             
                                                                      
                            ⊺                                         
                Linear: K a,b =a b                                    
                              ⊺  d                                    
              Polynomial: K a,b = γa b+r                              
            Gaussian RBF: K a,b = exp                                 
                               −γ∥a−b∥2                               
                                 ⊺                                    
               Sigmoid: K a,b = tanh γa b+r                           
                             Mercer’s Theorem                         
           According to Mercer’s theorem, if a function K(a, b) respects a few mathematical con‐
           ditions called Mercer’s conditions (e.g., K must be continuous and symmetric in its
           arguments so that K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a
           and b into another space (possibly with much higher dimensions) such that K(a, b) =
           ϕ(a)⊺ ϕ(b). You can use K as a kernel because you know ϕ exists, even if you don’t
           know what ϕ is. In the case of the Gaussian RBF kernel, it can be shown that ϕ maps
           each training instance to an infinite-dimensional space, so it’s a good thing you don’t
           need to actually perform the mapping!                      
           Note that some frequently used kernels (such as the sigmoid kernel) don’t respect all
           of Mercer’s conditions, yet they generally work well in practice.
                                                                      
          There is still one loose end we must tie up. Equation 5-7 shows how to go from the
          dual solution to the primal solution in the case of a linear SVM classifier. But if you
          apply the kernel trick, you end up with equations that include ϕ(x(i)). In fact, w must
          have the same number of dimensions as ϕ(x(i)), which may be huge or even infinite,
          so you can’t compute it. But how can you make predictions without knowing w? Well,
          the good news is that you can plug the formula for w from Equation 5-7 into the deci‐
          sion function for a new instance x(n), and you get an equation with only dot products
          between input vectors. This makes it possible to use the kernel trick (Equation 5-11).
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      