                                                                      
                                                                      
                                                                      
                                                                      
            some items (PriorityQueue), shuffle their items (RandomShuffleQueue), and
            batch items of different shapes by padding (PaddingFIFOQueue). These classes are
            all in the tf.queue package.                              
                                                                      
          With tensors, operations, variables, and various data structures at your disposal, you
          are now ready to customize your models and training algorithms!
                                                                      
          Customizing Models and Training Algorithms                  
                                                                      
          Let’s start by creating a custom loss function, which is a simple and common use case.
          Custom Loss Functions                                       
                                                                      
          Suppose you want to train a regression model, but your training set is a bit noisy. Of
          course, you start by trying to clean up your dataset by removing or fixing the outliers,
          but that turns out to be insufficient; the dataset is still noisy. Which loss function
          should you use? The mean squared error might penalize large errors too much and
          cause your model to be imprecise. The mean absolute error would not penalize outli‐
          ers as much, but training might take a while to converge, and the trained model
          might not be very precise. This is probably a good time to use the Huber loss (intro‐
          duced in Chapter 10) instead of the good old MSE. The Huber loss is not currently
          part of the official Keras API, but it is available in tf.keras (just use an instance of the
          keras.losses.Huber class). But let’s pretend it’s not there: implementing it is easy as
          pie! Just create a function that takes the labels and predictions as arguments, and use
          TensorFlow operations to compute every instance’s loss:     
            def huber_fn(y_true, y_pred):                             
               error = y_true - y_pred                                
               is_small_error = tf.abs(error) < 1                     
               squared_loss = tf.square(error) / 2                    
               linear_loss = tf.abs(error) - 0.5                      
               return tf.where(is_small_error, squared_loss, linear_loss)
                   For better performance, you should use a vectorized implementa‐
                   tion, as in this example. Moreover, if you want to benefit from Ten‐
                   sorFlow’s graph features, you should use only TensorFlow
                   operations.                                        
                                                                      
          It is also preferable to return a tensor containing one loss per instance, rather than
          returning the mean loss. This way, Keras can apply class weights or sample weights
          when requested (see Chapter 10).                            
                                                                      
                                                                      
                                                                      
                                                                      