                                                                      
                                                                      
                                                                      
                                                                      
            >>> from sklearn.metrics import confusion_matrix          
            >>> confusion_matrix(y_train_5, y_train_pred)             
            array([[53057, 1522],                                     
                [ 1325, 4096]])                                       
          Each row in a confusion matrix represents an actual class, while each column repre‐
          sents a predicted class. The first row of this matrix considers non-5 images (the nega‐
          tive class): 53,057 of them were correctly classified as non-5s (they are called true
          negatives), while the remaining 1,522 were wrongly classified as 5s (false positives).
          The second row considers the images of 5s (the positive class): 1,325 were wrongly
          classified as non-5s (false negatives), while the remaining 4,096 were correctly classi‐
          fied as 5s (true positives). A perfect classifier would have only true positives and true
          negatives, so its confusion matrix would have nonzero values only on its main diago‐
          nal (top left to bottom right):                             
            >>> y_train_perfect_predictions = y_train_5 # pretend we reached perfection
            >>> confusion_matrix(y_train_5, y_train_perfect_predictions)
            array([[54579, 0],                                        
                [  0, 5421]])                                         
          The confusion matrix gives you a lot of information, but sometimes you may prefer a
          more concise metric. An interesting one to look at is the accuracy of the positive pre‐
          dictions; this is called the precision of the classifier (Equation 3-1).
                                                                      
            Equation 3-1. Precision                                   
                    TP                                                
            precision=                                                
                   TP+FP                                              
          TP is the number of true positives, and FP is the number of false positives.
          A trivial way to have perfect precision is to make one single positive prediction and
          ensure it is correct (precision = 1/1 = 100%). But this would not be very useful, since
          the classifier would ignore all but one positive instance. So precision is typically used
          along with another metric named recall, also called sensitivity or the true positive rate
          (TPR): this is the ratio of positive instances that are correctly detected by the classifier
          (Equation 3-2).                                             
                                                                      
            Equation 3-2. Recall                                      
                                                                      
                  TP                                                  
            recall=                                                   
                 TP+FN                                                
          FN is, of course, the number of false negatives.            
          If you are confused about the confusion matrix, Figure 3-2 may help.
                                                                      
                                                                      