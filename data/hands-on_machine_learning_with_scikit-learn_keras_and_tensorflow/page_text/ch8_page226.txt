                                                                      
                                                                      
                                                                      
                                                                      
          This is useful for large training sets and for applying PCA online (i.e., on the fly, as
          new instances arrive).                                      
                                                                      
          The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s
          array_split() function) and feeds them to Scikit-Learn’s IncrementalPCA class5 to
          reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like
          before). Note that you must call the partial_fit() method with each mini-batch,
          rather than the fit() method with the whole training set:   
            from sklearn.decomposition import IncrementalPCA          
                                                                      
            n_batches = 100                                           
            inc_pca = IncrementalPCA(n_components=154)                
            for X_batch in np.array_split(X_train, n_batches):        
               inc_pca.partial_fit(X_batch)                           
            X_reduced = inc_pca.transform(X_train)                    
          Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a
          large array stored in a binary file on disk as if it were entirely in memory; the class
          loads only the data it needs in memory, when it needs it. Since the IncrementalPCA
          class uses only a small part of the array at any given time, the memory usage remains
          under control. This makes it possible to call the usual fit() method, as you can see
          in the following code:                                      
            X_mm = np.memmap(filename, dtype="float32", mode="readonly", shape=(m, n))
                                                                      
            batch_size = m // n_batches                               
            inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)
            inc_pca.fit(X_mm)                                         
          Kernel PCA                                                  
                                                                      
          In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly
          maps instances into a very high-dimensional space (called the feature space), enabling
          nonlinear classification and regression with Support Vector Machines. Recall that a
          linear decision boundary in the high-dimensional feature space corresponds to a
          complex nonlinear decision boundary in the original space.  
          It turns out that the same trick can be applied to PCA, making it possible to perform
          complex nonlinear projections for dimensionality reduction. This is called Kernel
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          5 Scikit-Learn uses the algorithm described in David A. Ross et al., “Incremental Learning for Robust Visual
           Tracking,” International Journal of Computer Vision 77, no. 1–3 (2008): 125–141.