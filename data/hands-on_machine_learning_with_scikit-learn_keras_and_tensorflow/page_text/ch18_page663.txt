ment. At regular intervals, but asynchronously (hence the name), each agent
pushes some weight updates to a master network, then it pulls the latest weights
from that network. Each agent thus contributes to improving the master network
and benefits from what the other agents have learned. Moreover, instead of esti‐
mating the Q-Values, the DQN estimates the advantage of each action (hence the
second A in the name), which stabilizes training.
<i>Advantage</i> <i>Actor-Critic</i> <i>(A2C)</i>
A variant of the A3C algorithm that removes the asynchronicity. All model
updates are synchronous, so gradient updates are performed over larger batches,
which allows the model to better utilize the power of the GPU.
<i>Soft</i> <i>Actor-Critic24</i> <i>(SAC)</i>
An Actor-Critic variant proposed in 2018 by Tuomas Haarnoja and other UC
Berkeley researchers. It learns not only rewards, but also to maximize the entropy
of its actions. In other words, it tries to be as unpredictable as possible while still
getting as many rewards as possible. This encourages the agent to explore the
environment, which speeds up training, and makes it less likely to repeatedly exe‐
cute the same action when the DQN produces imperfect estimates. This algo‐
rithm has demonstrated an amazing sample efficiency (contrary to all the
previous algorithms, which learn very slowly). SAC is available in TF-Agents.
<i>Proximal</i> <i>Policy</i> <i>Optimization</i> <i>(PPO)25</i>
An algorithm based on A2C that clips the loss function to avoid excessively large
weight updates (which often lead to training instabilities). PPO is a simplification
of the previous <i>Trust</i> <i>Region</i> <i>Policy</i> <i>Optimization26</i> (TRPO) algorithm, also by
John Schulman and other OpenAI researchers. OpenAI made the news in April
2019 with their AI called OpenAI Five, based on the PPO algorithm, which
defeated the world champions at the multiplayer game <i>Dota</i> <i>2.</i> PPO is also avail‐
able in TF-Agents.
24 TuomasHaarnojaetal.,“SoftActor-Critic:Off-PolicyMaximumEntropyDeepReinforcementLearningwith
aStochasticActor,”Proceedingsofthe35thInternationalConferenceonMachineLearning(2018):1856–1865.
25 JohnSchulmanetal.,“ProximalPolicyOptimizationAlgorithms,”arXivpreprintarXiv:1707.06347(2017).
26 JohnSchulmanetal.,“TrustRegionPolicyOptimization,”Proceedingsofthe32ndInternationalConferenceon
<i>MachineLearning(2015):1889–1897.</i>