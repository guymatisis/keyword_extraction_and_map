⊺ ∥ ∥2
We are minimizing ½ <b>w</b> <b>w,</b> which is equal to ½ <b>w</b> , rather than
minimizing ∥ <b>w</b> ∥ . Indeed, ½ ∥ <b>w</b> ∥2 has a nice, simple derivative (it
is just <b>w),</b> while ∥ <b>w</b> ∥ is not differentiable at <b>w</b> = 0. Optimization
algorithms work much better on differentiable functions.
To get the soft margin objective, we need to introduce a <i>slack</i> <i>variable</i> <i>ζ</i> (i) ≥ 0 for each
instance:4 <i>ζ(i)</i> <i>ith</i>
measures how much the instance is allowed to violate the margin. We
now have two conflicting objectives: make the slack variables as small as possible to
⊺
reduce the margin violations, and make ½ <b>w</b> <b>w</b> as small as possible to increase the
margin. This is where the C hyperparameter comes in: it allows us to define the trade‐
off between these two objectives. This gives us the constrained optimization problem
in Equation 5-4.
<i>Equation</i> <i>5-4.</i> <i>Soft</i> <i>margin</i> <i>linear</i> <i>SVM</i> <i>classifier</i> <i>objective</i>
<i>m</i>
1 ⊺
<largefont>∑</largefont> <i>i</i>
minimize <b>w</b> <b>w</b> + <i>C</i> <i>ζ</i>
<b>w,b,ζ</b> 2
<i>i</i> = 1
<i>i</i> ⊺ <i>i</i> <i>i</i> <i>i</i> ⋯
subject to <i>t</i> <b>w</b> <b>x</b> + <i>b</i> ≥ 1 − <i>ζ</i> and <i>ζ</i> ≥ 0 for <i>i</i> = 1,2, ,m
<header><largefont><b>Quadratic</b></largefont> <largefont><b>Programming</b></largefont></header>
The hard margin and soft margin problems are both convex quadratic optimization
problems with linear constraints. Such problems are known as <i>Quadratic</i> <i>Program‐</i>
<i>ming</i> (QP) problems. Many off-the-shelf solvers are available to solve QP problems
by using a variety of techniques that are outside the scope of this book. 5
4 Zeta(ζ)isthesixthletteroftheGreekalphabet.
5 TolearnmoreaboutQuadraticProgramming,youcanstartbyreadingStephenBoydandLievenVandenber‐
ghe’sbookConvexOptimization(CambridgeUniversityPress,2004)orwatchRichardBrown’sseriesofvideo
lectures.