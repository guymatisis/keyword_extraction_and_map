<i>Figure</i> <i>1-13.</i> <i>In</i> <i>online</i> <i>learning,</i> <i>a</i> <i>model</i> <i>is</i> <i>trained</i> <i>and</i> <i>launched</i> <i>into</i> <i>production,</i> <i>and</i>
<i>then</i> <i>it</i> <i>keeps</i> <i>learning</i> <i>as</i> <i>new</i> <i>data</i> <i>comes</i> <i>in</i>
Online learning is great for systems that receive data as a continuous flow (e.g., stock
prices) and need to adapt to change rapidly or autonomously. It is also a good option
if you have limited computing resources: once an online learning system has learned
about new data instances, it does not need them anymore, so you can discard them
(unless you want to be able to roll back to a previous state and “replay” the data). This
can save a huge amount of space.
Online learning algorithms can also be used to train systems on huge datasets that
cannot fit in one machine’s main memory (this is called <i>out-of-core</i> learning). The
algorithm loads part of the data, runs a training step on that data, and repeats the
process until it has run on all of the data (see Figure 1-14).
Out-of-core learning is usually done offline (i.e., not on the live
system), so <i>online</i> <i>learning</i> can be a confusing name. Think of it as
<i>incremental</i> <i>learning.</i>
One important parameter of online learning systems is how fast they should adapt to
changing data: this is called the <i>learning</i> <i>rate.</i> If you set a high learning rate, then your
system will rapidly adapt to new data, but it will also tend to quickly forget the old
data (you don’t want a spam filter to flag only the latest kinds of spam it was shown).
Conversely, if you set a low learning rate, the system will have more inertia; that is, it
will learn more slowly, but it will also be less sensitive to noise in the new data or to
sequences of nonrepresentative data points (outliers).