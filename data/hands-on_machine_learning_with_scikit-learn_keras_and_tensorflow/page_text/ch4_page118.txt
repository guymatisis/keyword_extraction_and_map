                                                                      
                                                                      
                                                                      
                                                                      
          Gradient Descent                                            
                                                                      
          Gradient Descent is a generic optimization algorithm capable of finding optimal solu‐
          tions to a wide range of problems. The general idea of Gradient Descent is to tweak
          parameters iteratively in order to minimize a cost function.
                                                                      
          Suppose you are lost in the mountains in a dense fog, and you can only feel the slope
          of the ground below your feet. A good strategy to get to the bottom of the valley
          quickly is to go downhill in the direction of the steepest slope. This is exactly what
          Gradient Descent does: it measures the local gradient of the error function with
          regard to the parameter vector θ, and it goes in the direction of descending gradient.
          Once the gradient is zero, you have reached a minimum!      
          Concretely, you start by filling θ with random values (this is called random initializa‐
          tion). Then you improve it gradually, taking one baby step at a time, each step
          attempting to decrease the cost function (e.g., the MSE), until the algorithm converges
          to a minimum (see Figure 4-3).                              
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 4-3. In this depiction of Gradient Descent, the model parameters are initialized
          randomly and get tweaked repeatedly to minimize the cost function; the learning step
          size is proportional to the slope of the cost function, so the steps gradually get smaller as
          the parameters approach the minimum                         
                                                                      
          An important parameter in Gradient Descent is the size of the steps, determined by
          the learning rate hyperparameter. If the learning rate is too small, then the algorithm
          will have to go through many iterations to converge, which will take a long time (see
          Figure 4-4).                                                
                                                                      
                                                                      
                                                                      
                                                                      