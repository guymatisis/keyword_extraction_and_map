<header><largefont><b>Gradient</b></largefont> <largefont><b>Descent</b></largefont></header>
<i>Gradient</i> <i>Descent</i> is a generic optimization algorithm capable of finding optimal solu‐
tions to a wide range of problems. The general idea of Gradient Descent is to tweak
parameters iteratively in order to minimize a cost function.
Suppose you are lost in the mountains in a dense fog, and you can only feel the slope
of the ground below your feet. A good strategy to get to the bottom of the valley
quickly is to go downhill in the direction of the steepest slope. This is exactly what
Gradient Descent does: it measures the local gradient of the error function with
regard to the parameter vector <b>θ,</b> and it goes in the direction of descending gradient.
Once the gradient is zero, you have reached a minimum!
Concretely, you start by filling <b>θ</b> with random values (this is called <i>random</i> <i>initializa‐</i>
<i>tion).</i> Then you improve it gradually, taking one baby step at a time, each step
attempting to decrease the cost function (e.g., the MSE), until the algorithm <i>converges</i>
to a minimum (see Figure 4-3).
<i>Figure</i> <i>4-3.</i> <i>In</i> <i>this</i> <i>depiction</i> <i>of</i> <i>Gradient</i> <i>Descent,</i> <i>the</i> <i>model</i> <i>parameters</i> <i>are</i> <i>initialized</i>
<i>randomly</i> <i>and</i> <i>get</i> <i>tweaked</i> <i>repeatedly</i> <i>to</i> <i>minimize</i> <i>the</i> <i>cost</i> <i>function;</i> <i>the</i> <i>learning</i> <i>step</i>
<i>size</i> <i>is</i> <i>proportional</i> <i>to</i> <i>the</i> <i>slope</i> <i>of</i> <i>the</i> <i>cost</i> <i>function,</i> <i>so</i> <i>the</i> <i>steps</i> <i>gradually</i> <i>get</i> <i>smaller</i> <i>as</i>
<i>the</i> <i>parameters</i> <i>approach</i> <i>the</i> <i>minimum</i>
An important parameter in Gradient Descent is the size of the steps, determined by
the <i>learning</i> <i>rate</i> hyperparameter. If the learning rate is too small, then the algorithm
will have to go through many iterations to converge, which will take a long time (see
Figure 4-4).