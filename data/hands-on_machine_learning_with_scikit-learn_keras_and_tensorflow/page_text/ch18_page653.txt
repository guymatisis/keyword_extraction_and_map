                                                                      
                                                                      
                                                                      
                                                                      
            from tf_agents.agents.dqn.dqn_agent import DqnAgent       
                                                                      
            train_step = tf.Variable(0)                               
            update_period = 4 # train the model every 4 steps         
            optimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0,
                                  epsilon=0.00001, centered=True)     
            epsilon_fn = keras.optimizers.schedules.PolynomialDecay(  
               initial_learning_rate=1.0, # initial ε                 
               decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames
               end_learning_rate=0.01) # final ε                      
            agent = DqnAgent(tf_env.time_step_spec(),                 
                      tf_env.action_spec(),                           
                      q_network=q_net,                                
                      optimizer=optimizer,                            
                      target_update_period=2000, # <=> 32,000 ALE frames
                      td_errors_loss_fn=keras.losses.Huber(reduction="none"),
                      gamma=0.99, # discount factor                   
                      train_step_counter=train_step,                  
                      epsilon_greedy=lambda: epsilon_fn(train_step))  
            agent.initialize()                                        
          Let’s walk through this code:                               
           • We first create a variable that will count the number of training steps.
           • Then we build the optimizer, using the same hyperparameters as in the 2015
            DQN paper.                                                
           • Next, we create a PolynomialDecay object that will compute the ε value for the ε-
            greedy collect policy, given the current training step (it is normally used to decay
            the learning rate, hence the names of the arguments, but it will work just fine to
            decay any other value). It will go from 1.0 down to 0.01 (the value used during in
            the 2015 DQN paper) in 1 million ALE frames, which corresponds to 250,000
            steps, since we use frame skipping with a period of 4. Moreover, we will train the
            agent every 4 steps (i.e., 16 ALE frames), so ε will actually decay over 62,500
            training steps.                                           
           • We then build the DQNAgent, passing it the time step and action specs, the QNet
            work to train, the optimizer, the number of training steps between target model
            updates, the loss function to use, the discount factor, the train_step variable,
            and a function that returns the ε value (it must take no argument, which is why
            we need a lambda to pass the train_step).                 
            Note that the loss function must return an error per instance, not the mean error,
            which is why we set reduction="none".                     
           • Lastly, we initialize the agent.                         
          Next, let’s build the replay buffer and the observer that will write to it.
                                                                      