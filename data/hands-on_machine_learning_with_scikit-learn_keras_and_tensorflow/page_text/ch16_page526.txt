                                                                      
                                                                      
                                                                      
                                                                      
          on random portions of text at each iteration, without any information on the rest of
          the text), then we will build a stateful RNN (which preserves the hidden state between
          training iterations and continues reading where it left off, allowing it to learn longer
          patterns). Next, we will build an RNN to perform sentiment analysis (e.g., reading
          movie reviews and extracting the rater’s feeling about the movie), this time treating
          sentences as sequences of words, rather than characters. Then we will show how
          RNNs can be used to build an Encoder–Decoder architecture capable of performing
          neural machine translation (NMT). For this, we will use the seq2seq API provided by
          the TensorFlow Addons project.                              
          In the second part of this chapter, we will look at attention mechanisms. As their name
          suggests, these are neural network components that learn to select the part of the
          inputs that the rest of the model should focus on at each time step. First we will see
          how to boost the performance of an RNN-based Encoder–Decoder architecture using
          attention, then we will drop RNNs altogether and look at a very successful attention-
          only architecture called the Transformer. Finally, we will take a look at some of the
          most important advances in NLP in 2018 and 2019, including incredibly powerful
          language models such as GPT-2 and BERT, both based on Transformers.
                                                                      
          Let’s start with a simple and fun model that can write like Shakespeare (well, sort of).
          Generating Shakespearean Text Using a Character RNN         
                                                                      
                                                                      
          In a famous 2015 blog post titled “The Unreasonable Effectiveness of Recurrent Neu‐
          ral Networks,” Andrej Karpathy showed how to train an RNN to predict the next
          character in a sentence. This Char-RNN can then be used to generate novel text, one
          character at a time. Here is a small sample of the text generated by a Char-RNN
          model after it was trained on all of Shakespeare’s work:    
            PANDARUS:                                                 
            Alas, I think he shall be come approached and the day     
            When little srain would be attain’d into being never fed, 
            And who is but a chain and subjects of his death,         
                                                                      
            I should not sleep.                                       
          Not exactly a masterpiece, but it is still impressive that the model was able to learn
          words, grammar, proper punctuation, and more, just by learning to predict the next
          character in a sentence. Let’s look at how to build a Char-RNN, step by step, starting
          with the creation of the dataset.                           
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      