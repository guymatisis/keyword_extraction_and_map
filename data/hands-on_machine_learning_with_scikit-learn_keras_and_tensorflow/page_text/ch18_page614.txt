                                                                      
                                                                      
                                                                      
                                                                      
          will learn to play an Atari game, you will need an Atari game simulator. If you want to
          program a walking robot, then the environment is the real world, and you can
          directly train your robot in that environment, but this has its limits: if the robot falls
          off a cliff, you can’t just click Undo. You can’t speed up time either; adding more com‐
          puting power won’t make the robot move any faster. And it’s generally too expensive
          to train 1,000 robots in parallel. In short, training is hard and slow in the real world,
          so you generally need a simulated environment at least for bootstrap training. For
          example, you may use a library like PyBullet or MuJoCo for 3D physics simulation.
                                                                      
          OpenAI Gym10 is a toolkit that provides a wide variety of simulated environments
          (Atari games, board games, 2D and 3D physical simulations, and so on), so you can
          train agents, compare them, or develop new RL algorithms.   
          Before installing the toolkit, if you created an isolated environment using virtualenv,
          you first need to activate it:                              
            $ cd $ML_PATH    # Your ML working directory (e.g., $HOME/ml)
            $ source my_env/bin/activate # on Linux or MacOS          
            $ .\my_env\Scripts\activate # on Windows                  
          Next, install OpenAI Gym (if you are not using a virtual environment, you will need
          to add the --user option, or have administrator rights):    
            $ python3 -m pip install -U gym                           
                                                                      
          Depending on your system, you may also need to install the Mesa OpenGL Utility
          (GLU) library (e.g., on Ubuntu 18.04 you need to run apt install libglu1-mesa).
          This library will be needed to render the first environment. Next, open up a Python
          shell or a Jupyter notebook and create an environment with make():
            >>> import gym                                            
            >>> env = gym.make("CartPole-v1")                         
            >>> obs = env.reset()                                     
            >>> obs                                                   
            array([-0.01258566, -0.00156614, 0.04207708, -0.00180545])
          Here, we’ve created a CartPole environment. This is a 2D simulation in which a cart
          can be accelerated left or right in order to balance a pole placed on top of it (see
          Figure 18-4). You can get the list of all available environments by running
          gym.envs.registry.all(). After the environment is created, you must initialize it
          using the reset() method. This returns the first observation. Observations depend
          on the type of environment. For the CartPole environment, each observation is a 1D
          NumPy array containing four floats: these floats represent the cart’s horizontal
                                                                      
                                                                      
                                                                      
          10 OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to pro‐
           mote and develop friendly AIs that will benefit humanity (rather than exterminate it).