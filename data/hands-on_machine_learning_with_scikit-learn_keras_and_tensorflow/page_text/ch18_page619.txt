                                                                      
                                                                      
                                                                      
                                                                      
          Here is the code to build this neural network policy using tf.keras:
                                                                      
            import tensorflow as tf                                   
            from tensorflow import keras                              
            n_inputs = 4 # == env.observation_space.shape[0]          
                                                                      
            model = keras.models.Sequential([                         
               keras.layers.Dense(5, activation="elu", input_shape=[n_inputs]),
               keras.layers.Dense(1, activation="sigmoid"),           
            ])                                                        
          After the imports, we use a simple Sequential model to define the policy network.
          The number of inputs is the size of the observation space (which in the case of Cart‐
          Pole is 4), and we have just five hidden units because it’s a simple problem. Finally, we
          want to output a single probability (the probability of going left), so we have a single
          output neuron using the sigmoid activation function. If there were more than two
          possible actions, there would be one output neuron per action, and we would use the
          softmax activation function instead.                        
          OK, we now have a neural network policy that will take observations and output
          action probabilities. But how do we train it?               
          Evaluating Actions: The Credit Assignment Problem           
                                                                      
          If we knew what the best action was at each step, we could train the neural network as
          usual, by minimizing the cross entropy between the estimated probability distribu‐
          tion and the target probability distribution. It would just be regular supervised learn‐
          ing. However, in Reinforcement Learning the only guidance the agent gets is through
          rewards, and rewards are typically sparse and delayed. For example, if the agent man‐
          ages to balance the pole for 100 steps, how can it know which of the 100 actions it
          took were good, and which of them were bad? All it knows is that the pole fell after
          the last action, but surely this last action is not entirely responsible. This is called the
          credit assignment problem: when the agent gets a reward, it is hard for it to know
          which actions should get credited (or blamed) for it. Think of a dog that gets rewar‐
          ded hours after it behaved well; will it understand what it is being rewarded for?
          To tackle this problem, a common strategy is to evaluate an action based on the sum
          of all the rewards that come after it, usually applying a discount factor γ (gamma) at
          each step. This sum of discounted rewards is called the action’s return. Consider the
          example in Figure 18-6). If an agent decides to go right three times in a row and gets
          +10 reward after the first step, 0 after the second step, and finally –50 after the third
          step, then assuming we use a discount factor γ = 0.8, the first action will have a return
          of 10 + γ × 0 + γ2 × (–50) = –22. If the discount factor is close to 0, then future
          rewards won’t count for much compared to immediate rewards. Conversely, if the
          discount factor is close to 1, then rewards far into the future will count almost as
                                                                      