mal learning rate will be a bit lower than the point at which the loss starts to
climb (typically about 10 times lower than the turning point). You can then reini‐
tialize your model and train it normally using this good learning rate. We will
look at more learning rate techniques in Chapter 11.
<i>Optimizer</i>
Choosing a better optimizer than plain old Mini-batch Gradient Descent (and
tuning its hyperparameters) is also quite important. We will see several advanced
optimizers in Chapter 11.
<i>Batch</i> <i>size</i>
The batch size can have a significant impact on your model’s performance and
training time. The main benefit of using large batch sizes is that hardware accel‐
erators like GPUs can process them efficiently (see Chapter 19), so the training
algorithm will see more instances per second. Therefore, many researchers and
practitioners recommend using the largest batch size that can fit in GPU RAM.
There’s a catch, though: in practice, large batch sizes often lead to training insta‐
bilities, especially at the beginning of training, and the resulting model may not
generalize as well as a model trained with a small batch size. In April 2018, Yann
LeCun even tweeted “Friends don’t let friends use mini-batches larger than 32,”
citing a 2018 paper24 by Dominic Masters and Carlo Luschi which concluded that
using small batches (from 2 to 32) was preferable because small batches led to
better models in less training time. Other papers point in the opposite direction,
al.25 al.26
however; in 2017, papers by Elad Hoffer et and Priya Goyal et showed
that it was possible to use very large batch sizes (up to 8,192) using various tech‐
niques such as warming up the learning rate (i.e., starting training with a small
learning rate, then ramping it up, as we will see in Chapter 11). This led to a very
short training time, without any generalization gap. So, one strategy is to try to
use a large batch size, using learning rate warmup, and if training is unstable or
the final performance is disappointing, then try using a small batch size instead.
<i>Activation</i> <i>function</i>
We discussed how to choose the activation function earlier in this chapter: in
general, the ReLU activation function will be a good default for all hidden layers.
For the output layer, it really depends on your task.
24 DominicMastersandCarloLuschi,“RevisitingSmallBatchTrainingforDeepNeuralNetworks,”arXivpre‐
printarXiv:1804.07612(2018).
25 EladHofferetal.,“TrainLonger,GeneralizeBetter:ClosingtheGeneralizationGapinLargeBatchTraining
ofNeuralNetworks,”Proceedingsofthe31stInternationalConferenceonNeuralInformationProcessingSystems
(2017):1729–1739.
26 PriyaGoyaletal.,“Accurate,LargeMinibatchSGD:TrainingImageNetin1Hour,”arXivpreprintarXiv:
1706.02677(2017).