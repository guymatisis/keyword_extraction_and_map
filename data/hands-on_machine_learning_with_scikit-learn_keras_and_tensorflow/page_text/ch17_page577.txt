                                                                      
                                                                      
                                                                      
                                                                      
          There is nothing special about the implementation: just train an autoencoder using
          all the training data (labeled plus unlabeled), then reuse its encoder layers to create a
          new neural network (see the exercises at the end of this chapter for an example).
                                                                      
          Next, let’s look at a few techniques for training stacked autoencoders.
          Tying Weights                                               
                                                                      
          When an autoencoder is neatly symmetrical, like the one we just built, a common
          technique is to tie the weights of the decoder layers to the weights of the encoder lay‐
          ers. This halves the number of weights in the model, speeding up training and limit‐
          ing the risk of overfitting. Specifically, if the autoencoder has a total of N layers (not
          counting the input layer), and W represents the connection weights of the Lth layer
                              L                                       
          (e.g., layer 1 is the first hidden layer, layer N/2 is the coding layer, and layer N is the
          output layer), then the decoder layer weights can be defined simply as: W = W ⊺
                                                     N–L+1 L          
          (with L = 1, 2, …, N/2).                                    
          To tie weights between layers using Keras, let’s define a custom layer:
            class DenseTranspose(keras.layers.Layer):                 
               def __init__(self, dense, activation=None, **kwargs):  
                 self.dense = dense                                   
                 self.activation = keras.activations.get(activation)  
                 super().__init__(**kwargs)                           
               def build(self, batch_input_shape):                    
                 self.biases = self.add_weight(name="bias", initializer="zeros",
                                   shape=[self.dense.input_shape[-1]])
                 super().build(batch_input_shape)                     
               def call(self, inputs):                                
                 z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)
                 return self.activation(z + self.biases)              
          This custom layer acts like a regular Dense layer, but it uses another Dense layer’s
          weights, transposed (setting transpose_b=True is equivalent to transposing the sec‐
          ond argument, but it’s more efficient as it performs the transposition on the fly within
          the matmul() operation). However, it uses its own bias vector. Next, we can build a
          new stacked autoencoder, much like the previous one, but with the decoder’s Dense
          layers tied to the encoder’s Dense layers:                  
            dense_1 = keras.layers.Dense(100, activation="selu")      
            dense_2 = keras.layers.Dense(30, activation="selu")       
            tied_encoder = keras.models.Sequential([                  
               keras.layers.Flatten(input_shape=[28, 28]),            
               dense_1,                                               
               dense_2                                                
            ])                                                        