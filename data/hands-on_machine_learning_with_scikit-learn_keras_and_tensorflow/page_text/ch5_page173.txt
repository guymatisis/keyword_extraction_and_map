<i>Equation</i> <i>5-13.</i> <i>Linear</i> <i>SVM</i> <i>classifier</i> <i>cost</i> <i>function</i>
<i>m</i>
1
⊺ <i>i</i> ⊺ <i>i</i>
<header><i>J</i> <b>w,b</b> = <b>w</b> <b>w</b> + <i>C</i> <largefont>∑</largefont> <i>max</i> 0,1 − <i>t</i> <b>w</b> <b>x</b> + <i>b</i></header>
2
<i>i</i> = 1
The first sum in the cost function will push the model to have a small weight vector
<b>w,</b> leading to a larger margin. The second sum computes the total of all margin viola‐
tions. An instance’s margin violation is equal to 0 if it is located off the street and on
the correct side, or else it is proportional to the distance to the correct side of the
street. Minimizing this term ensures that the model makes the margin violations as
small and as few as possible.
<header><largefont><b>Hinge</b></largefont> <largefont><b>Loss</b></largefont></header>
The function <i>max(0,</i> 1 – <i>t)</i> is called the <i>hinge</i> <i>loss</i> function (see the following image).
It is equal to 0 when <i>t</i> ≥ 1. Its derivative (slope) is equal to –1 if <i>t</i> < 1 and 0 if <i>t</i> > 1. It is
not differentiable at <i>t</i> = 1, but just like for Lasso Regression (see “Lasso Regression”
on page 137), you can still use Gradient Descent using any <i>subderivative</i> at <i>t</i> = 1 (i.e.,
any value between –1 and 0).
It is also possible to implement online kernelized SVMs, as described in the papers
Learning”8
“Incremental and Decremental Support Vector Machine and “Fast Kernel
Classifiers with Online and Active Learning”. 9 These kernelized SVMs are imple‐
8 GertCauwenberghsandTomasoPoggio,“IncrementalandDecrementalSupportVectorMachineLearning,”
<i>Proceedingsofthe13thInternationalConferenceonNeuralInformationProcessingSystems(2000):388–394.</i>
9 AntoineBordesetal.,“FastKernelClassifierswithOnlineandActiveLearning,”JournalofMachineLearning
<i>Research6(2005):1579–1619.</i>