                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-5. Neural network policy                          
                                                                      
          You may wonder why we are picking a random action based on the probabilities
          given by the neural network, rather than just picking the action with the highest
          score. This approach lets the agent find the right balance between exploring new
          actions and exploiting the actions that are known to work well. Here’s an analogy:
          suppose you go to a restaurant for the first time, and all the dishes look equally
          appealing, so you randomly pick one. If it turns out to be good, you can increase the
          probability that you’ll order it next time, but you shouldn’t increase that probability
          up to 100%, or else you will never try out the other dishes, some of which may be
          even better than the one you tried.                         
          Also note that in this particular environment, the past actions and observations can
          safely be ignored, since each observation contains the environment’s full state. If there
          were some hidden state, then you might need to consider past actions and observa‐
          tions as well. For example, if the environment only revealed the position of the cart
          but not its velocity, you would have to consider not only the current observation but
          also the previous observation in order to estimate the current velocity. Another exam‐
          ple is when the observations are noisy; in that case, you generally want to use the past
          few observations to estimate the most likely current state. The CartPole problem is
          thus as simple as can be; the observations are noise-free, and they contain the envi‐
          ronment’s full state.                                       
                                                                      