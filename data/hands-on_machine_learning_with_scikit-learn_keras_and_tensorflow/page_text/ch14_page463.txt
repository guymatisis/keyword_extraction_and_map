                                                                      
                                                                      
                                                                      
                                                                      
          LeNet-5                                                     
                                                                      
          The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As
          mentioned earlier, it was created by Yann LeCun in 1998 and has been widely used
          for handwritten digit recognition (MNIST). It is composed of the layers shown in
          Table 14-1.                                                 
                                                                      
          Table 14-1. LeNet-5 architecture                            
          Layer Type Maps Size Kernel size Stride Activation          
          Out Fully connected – 10 – – RBF                            
          F6  Fully connected – 84 – – tanh                           
          C5  Convolution 120 1 × 1 5 × 5 1 tanh                      
          S4  Avg pooling 16 5 × 5 2 × 2 2 tanh                       
          C3  Convolution 16 10 × 10 5 × 5 1 tanh                     
          S2  Avg pooling 6 14 × 14 2 × 2 2 tanh                      
                                                                      
          C1  Convolution 6 28 × 28 5 × 5 1 tanh                      
          In  Input  1  32 × 32 – –  –                                
                                                                      
          There are a few extra details to be noted:                  
           • MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and
            normalized before being fed to the network. The rest of the network does not use
            any padding, which is why the size keeps shrinking as the image progresses
            through the network.                                      
                                                                      
           • The average pooling layers are slightly more complex than usual: each neuron
            computes the mean of its inputs, then multiplies the result by a learnable coeffi‐
            cient (one per map) and adds a learnable bias term (again, one per map), then
            finally applies the activation function.                  
           • Most neurons in C3 maps are connected to neurons in only three or four S2
            maps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for
            details.                                                  
           • The output layer is a bit special: instead of computing the matrix multiplication
            of the inputs and the weight vector, each neuron outputs the square of the Eucli‐
            dian distance between its input vector and its weight vector. Each output meas‐
            ures how much the image belongs to a particular digit class. The cross-entropy
                                                                      
                                                                      
                                                                      
                                                                      
          10 Yann LeCun et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE 86,
           no. 11 (1998): 2278–2324.                                  