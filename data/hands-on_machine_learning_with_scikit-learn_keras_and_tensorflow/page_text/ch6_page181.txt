                                                                      
                                                                      
                                                                      
                                                                      
          impurity measure: a set’s entropy is zero when it contains instances of only one class.
          Equation 6-3 shows the definition of the entropy of the ith node. For example, the
          depth-2 left node in Figure 6-1 has an entropy equal to –(49/54) log (49/54) – (5/54)
                                                 2                    
          log (5/54) ≈ 0.445.                                         
            2                                                         
            Equation 6-3. Entropy                                     
                  n                                                   
            H = − ∑  p log p                                          
             i       i,k 2 i,k                                        
                 k=1                                                  
                 p ≠0                                                 
                 i,k                                                  
          So, should you use Gini impurity or entropy? The truth is, most of the time it does
          not make a big difference: they lead to similar trees. Gini impurity is slightly faster to
          compute, so it is a good default. However, when they differ, Gini impurity tends to
          isolate the most frequent class in its own branch of the tree, while entropy tends to
          produce slightly more balanced trees.5                      
          Regularization Hyperparameters                              
          Decision Trees make very few assumptions about the training data (as opposed to lin‐
          ear models, which assume that the data is linear, for example). If left unconstrained,
          the tree structure will adapt itself to the training data, fitting it very closely—indeed,
          most likely overfitting it. Such a model is often called a nonparametric model, not
          because it does not have any parameters (it often has a lot) but because the number of
          parameters is not determined prior to training, so the model structure is free to stick
          closely to the data. In contrast, a parametric model, such as a linear model, has a pre‐
          determined number of parameters, so its degree of freedom is limited, reducing the
          risk of overfitting (but increasing the risk of underfitting).
          To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom
          during training. As you know by now, this is called regularization. The regularization
          hyperparameters depend on the algorithm used, but generally you can at least restrict
          the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the
          max_depth hyperparameter (the default value is None, which means unlimited).
          Reducing max_depth will regularize the model and thus reduce the risk of overfitting.
          The DecisionTreeClassifier class has a few other parameters that similarly restrict
          the shape of the Decision Tree: min_samples_split (the minimum number of sam‐
          ples a node must have before it can be split), min_samples_leaf (the minimum num‐
          ber of samples a leaf node must have), min_weight_fraction_leaf (same as
                                                                      
                                                                      
                                                                      
          5 See Sebastian Raschka’s interesting analysis for more details.