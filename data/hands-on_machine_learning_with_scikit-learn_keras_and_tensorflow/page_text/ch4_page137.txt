                                                                      
                                                                      
                                                                      
                                                                      
          same. Equation 4-9 shows the closed-form solution, where A is the (n + 1) × (n + 1)
          identity matrix,11 except with a 0 in the top-left cell, corresponding to the bias term.
                                                                      
            Equation 4-9. Ridge Regression closed-form solution       
                                                                      
                ⊺    −1 ⊺                                             
            θ = X X+αA X  y                                           
          Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐
          tion (a variant of Equation 4-9 that uses a matrix factorization technique by André-
          Louis Cholesky):                                            
            >>> from sklearn.linear_model import Ridge                
            >>> ridge_reg = Ridge(alpha=1, solver="cholesky")         
            >>> ridge_reg.fit(X, y)                                   
            >>> ridge_reg.predict([[1.5]])                            
            array([[1.55071465]])                                     
          And using Stochastic Gradient Descent:12                    
            >>> sgd_reg = SGDRegressor(penalty="l2")                  
            >>> sgd_reg.fit(X, y.ravel())                             
            >>> sgd_reg.predict([[1.5]])                              
            array([1.47012588])                                       
          The penalty hyperparameter sets the type of regularization term to use. Specifying
          "l2" indicates that you want SGD to add a regularization term to the cost function
          equal to half the square of the ℓ norm of the weight vector: this is simply Ridge
                              2                                       
          Regression.                                                 
          Lasso Regression                                            
          Least Absolute Shrinkage and Selection Operator Regression (usually simply called
          Lasso Regression) is another regularized version of Linear Regression: just like Ridge
          Regression, it adds a regularization term to the cost function, but it uses the ℓ norm
                                                        1             
          of the weight vector instead of half the square of the ℓ norm (see Equation 4-10).
                                         2                            
            Equation 4-10. Lasso Regression cost function             
                       n                                              
            J θ =MSE θ +α∑ θ                                          
                       i=1 i                                          
                                                                      
                                                                      
          11 A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right).
          12 Alternatively you can use the Ridge class with the "sag" solver. Stochastic Average GD is a variant of Stochas‐
           tic GD. For more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient
           Algorithm” by Mark Schmidt et al. from the University of British Columbia.