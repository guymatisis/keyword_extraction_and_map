<header><largefont><b>Measuring</b></largefont> <largefont><b>Accuracy</b></largefont> <largefont><b>Using</b></largefont> <largefont><b>Cross-Validation</b></largefont></header>
A good way to evaluate a model is to use cross-validation, just as you did in Chap‐
ter 2.
<header><largefont><b>Implementing</b></largefont> <largefont><b>Cross-Validation</b></largefont></header>
Occasionally you will need more control over the cross-validation process than what
Scikit-Learn provides off the shelf. In these cases, you can implement cross-validation
yourself. The following code does roughly the same thing as Scikit-Learn’s
cross_val_score() function, and it prints the same result:
<b>from</b> <b>sklearn.model_selection</b> <b>import</b> StratifiedKFold
<b>from</b> <b>sklearn.base</b> <b>import</b> clone
skfolds = StratifiedKFold(n_splits=3, random_state=42)
<b>for</b> train_index, test_index <b>in</b> skfolds.split(X_train, y_train_5):
clone_clf = clone(sgd_clf)
X_train_folds = X_train[train_index]
y_train_folds = y_train_5[train_index]
X_test_fold = X_train[test_index]
y_test_fold = y_train_5[test_index]
clone_clf.fit(X_train_folds, y_train_folds)
y_pred = clone_clf.predict(X_test_fold)
n_correct = sum(y_pred == y_test_fold)
<b>print(n_correct</b> / len(y_pred)) <i>#</i> <i>prints</i> <i>0.9502,</i> <i>0.96565,</i> <i>and</i> <i>0.96495</i>
StratifiedKFold
The class performs stratified sampling (as explained in Chapter 2)
to produce folds that contain a representative ratio of each class. At each iteration the
code creates a clone of the classifier, trains that clone on the training folds, and makes
predictions on the test fold. Then it counts the number of correct predictions and
outputs the ratio of correct predictions.
Let’s use the cross_val_score() function to evaluate our SGDClassifier model,
using K-fold cross-validation with three folds. Remember that K-fold cross-validation
means splitting the training set into K folds (in this case, three), then making predic‐
tions and evaluating them on each fold using a model trained on the remaining folds
(see Chapter 2):
<b>>>></b> <b>from</b> <b>sklearn.model_selection</b> <b>import</b> cross_val_score
<b>>>></b> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
array([0.96355, 0.93795, 0.95615])
Wow! Above 93% accuracy (ratio of correct predictions) on all cross-validation folds?
This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very
dumb classifier that just classifies every single image in the “not-5” class: