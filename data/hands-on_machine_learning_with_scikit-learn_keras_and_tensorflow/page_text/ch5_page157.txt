<header><largefont><b>Nonlinear</b></largefont> <largefont><b>SVM</b></largefont> <largefont><b>Classification</b></largefont></header>
Although linear SVM classifiers are efficient and work surprisingly well in many
cases, many datasets are not even close to being linearly separable. One approach to
handling nonlinear datasets is to add more features, such as polynomial features (as
you did in Chapter 4); in some cases this can result in a linearly separable dataset.
Consider the left plot in Figure 5-5: it represents a simple dataset with just one fea‐
ture, <i>x</i> . This dataset is not linearly separable, as you can see. But if you add a second
1
feature <i>x</i> = (x )2, the resulting 2D dataset is perfectly linearly separable.
2 1
<i>Figure</i> <i>5-5.</i> <i>Adding</i> <i>features</i> <i>to</i> <i>make</i> <i>a</i> <i>dataset</i> <i>linearly</i> <i>separable</i>
Pipeline Polyno
To implement this idea using Scikit-Learn, create a containing a
mialFeatures transformer (discussed in “Polynomial Regression” on page 128), fol‐
StandardScaler LinearSVC
lowed by a and a . Let’s test this on the moons dataset: this
is a toy dataset for binary classification in which the data points are shaped as two
interleaving half circles (see Figure 5-6). You can generate this dataset using the
make_moons() function:
<b>from</b> <b>sklearn.datasets</b> <b>import</b> make_moons
<b>from</b> <b>sklearn.pipeline</b> <b>import</b> Pipeline
<b>from</b> <b>sklearn.preprocessing</b> <b>import</b> PolynomialFeatures
X, y = make_moons(n_samples=100, noise=0.15)
polynomial_svm_clf = Pipeline([
("poly_features", PolynomialFeatures(degree=3)),
("scaler", StandardScaler()),
("svm_clf", LinearSVC(C=10, loss="hinge"))
])
polynomial_svm_clf.fit(X, y)