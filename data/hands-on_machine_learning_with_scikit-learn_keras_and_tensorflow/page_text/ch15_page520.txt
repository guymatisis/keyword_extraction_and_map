                                                                      
                                                                      
                                                                      
                                                                      
          Equation 15-4 summarizes how to compute the cell’s state at each time step for a sin‐
          gle instance.                                               
                                                                      
            Equation 15-4. GRU computations                           
                                                                      
                   ⊺     ⊺                                            
            z =σ W  x +W  h   +b                                      
             t    xz t  hz t−1  z                                     
                   ⊺     ⊺                                            
            r =σ W  x +W  h   +b                                      
             t    xr t  hr t−1  r                                     
            g = tanh W ⊺ x +W ⊺ r ⊗h +b                               
             t       xg t  hg t   t−1  g                              
            h =z  ⊗h   + 1−z ⊗g                                       
             t   t  t−1     t   t                                     
          Keras provides a keras.layers.GRU layer (based on the keras.layers.GRUCell
          memory cell); using it is just a matter of replacing SimpleRNN or LSTM with GRU.
          LSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet
          while they can tackle much longer sequences than simple RNNs, they still have a
          fairly limited short-term memory, and they have a hard time learning long-term pat‐
          terns in sequences of 100 time steps or more, such as audio samples, long time series,
          or long sentences. One way to solve this is to shorten the input sequences, for exam‐
          ple using 1D convolutional layers.                          
          Using 1D convolutional layers to process sequences          
          In Chapter 14, we saw that a 2D convolutional layer works by sliding several fairly
          small kernels (or filters) across an image, producing multiple 2D feature maps (one
          per kernel). Similarly, a 1D convolutional layer slides several kernels across a
          sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a
          single very short sequential pattern (no longer than the kernel size). If you use 10 ker‐
          nels, then the layer’s output will be composed of 10 1-dimensional sequences (all of
          the same length), or equivalently you can view this output as a single 10-dimensional
          sequence. This means that you can build a neural network composed of a mix of
          recurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a
          1D convolutional layer with a stride of 1 and "same" padding, then the output
          sequence will have the same length as the input sequence. But if you use "valid"
          padding or a stride greater than 1, then the output sequence will be shorter than the
          input sequence, so make sure you adjust the targets accordingly. For example, the fol‐
          lowing model is the same as earlier, except it starts with a 1D convolutional layer that
          downsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is
          larger than the stride, so all inputs will be used to compute the layer’s output, and
          therefore the model can learn to preserve the useful information, dropping only the
          unimportant details. By shortening the sequences, the convolutional layer may help
          the GRU layers detect longer patterns. Note that we must also crop off the first three