Equation 15-4 summarizes how to compute the cell’s state at each time step for a sin‐
gle instance.
<i>Equation</i> <i>15-4.</i> <i>GRU</i> <i>computations</i>
⊺ ⊺
<b>z</b> = <i>σ</i> <b>W</b> <b>x</b> + <b>W</b> <b>h</b> + <b>b</b>
<i>t</i> <i>xz</i> <i>t</i> <i>hz</i> <i>t−1</i> <i>z</i>
⊺ ⊺
<b>r</b> = <i>σ</i> <b>W</b> <b>x</b> + <b>W</b> <b>h</b> + <b>b</b>
<i>t</i> <i>xr</i> <i>t</i> <i>hr</i> <i>t−1</i> <i>r</i>
⊺ ⊺
<b>g</b> = tanh <b>W</b> <b>x</b> + <b>W</b> <b>r</b> ⊗ <b>h</b> + <b>b</b>
<i>t</i> <i>xg</i> <i>t</i> <i>hg</i> <i>t</i> <i>t−1</i> <i>g</i>
⊗ ⊗
<b>h</b> = <b>z</b> <b>h</b> + 1 − <b>z</b> <b>g</b>
<i>t</i> <i>t</i> <i>t−1</i> <i>t</i> <i>t</i>
keras.layers.GRU keras.layers.GRUCell
Keras provides a layer (based on the
memory cell); using it is just a matter of replacing SimpleRNN or LSTM with GRU .
LSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet
while they can tackle much longer sequences than simple RNNs, they still have a
fairly limited short-term memory, and they have a hard time learning long-term pat‐
terns in sequences of 100 time steps or more, such as audio samples, long time series,
or long sentences. One way to solve this is to shorten the input sequences, for exam‐
ple using 1D convolutional layers.
<b>Using1Dconvolutionallayerstoprocesssequences</b>
In Chapter 14, we saw that a 2D convolutional layer works by sliding several fairly
small kernels (or filters) across an image, producing multiple 2D feature maps (one
per kernel). Similarly, a 1D convolutional layer slides several kernels across a
sequence, producing a 1D feature map per kernel. Each kernel will learn to detect a
single very short sequential pattern (no longer than the kernel size). If you use 10 ker‐
nels, then the layer’s output will be composed of 10 1-dimensional sequences (all of
the same length), or equivalently you can view this output as a single 10-dimensional
sequence. This means that you can build a neural network composed of a mix of
recurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a
1D convolutional layer with a stride of 1 and "same" padding, then the output
"valid"
sequence will have the same length as the input sequence. But if you use
padding or a stride greater than 1, then the output sequence will be shorter than the
input sequence, so make sure you adjust the targets accordingly. For example, the fol‐
lowing model is the same as earlier, except it starts with a 1D convolutional layer that
downsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is
larger than the stride, so all inputs will be used to compute the layer’s output, and
therefore the model can learn to preserve the useful information, dropping only the
unimportant details. By shortening the sequences, the convolutional layer may help
GRU
the layers detect longer patterns. Note that we must also crop off the first three