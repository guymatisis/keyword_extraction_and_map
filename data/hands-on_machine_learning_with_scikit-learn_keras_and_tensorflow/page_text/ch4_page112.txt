Next we will look at Polynomial Regression, a more complex model that can fit non‐
linear datasets. Since this model has more parameters than Linear Regression, it is
more prone to overfitting the training data, so we will look at how to detect whether
or not this is the case using learning curves, and then we will look at several regulari‐
zation techniques that can reduce the risk of overfitting the training set.
Finally, we will look at two more models that are commonly used for classification
tasks: Logistic Regression and Softmax Regression.
There will be quite a few math equations in this chapter, using basic
notions of linear algebra and calculus. To understand these equa‐
tions, you will need to know what vectors and matrices are; how to
transpose them, multiply them, and inverse them; and what partial
derivatives are. If you are unfamiliar with these concepts, please go
through the linear algebra and calculus introductory tutorials avail‐
able as Jupyter notebooks in the online supplemental material. For
those who are truly allergic to mathematics, you should still go
through this chapter and simply skip the equations; hopefully, the
text will be sufficient to help you understand most of the concepts.
<header><largefont><b>Linear</b></largefont> <largefont><b>Regression</b></largefont></header>
In Chapter 1 we looked at a simple regression model of life satisfaction: <i>life_satisfac‐</i>
<i>tion</i> = <i>θ</i> + <i>θ</i> × <i>GDP_per_capita.</i>
0 1
This model is just a linear function of the input feature GDP_per_capita . <i>θ</i> and <i>θ</i> are
0 1
the model’s parameters.
More generally, a linear model makes a prediction by simply computing a weighted
sum of the input features, plus a constant called the <i>bias</i> <i>term</i> (also called the <i>intercept</i>
<i>term),</i> as shown in Equation 4-1.
<i>Equation</i> <i>4-1.</i> <i>Linear</i> <i>Regression</i> <i>model</i> <i>prediction</i>
<i>y</i> = <i>θ</i> + <i>θ</i> <i>x</i> + <i>θ</i> <i>x</i> + ⋯ + <i>θ</i> <i>x</i>
0 1 1 2 2 <i>n</i> <i>n</i>
In this equation:
• <i>ŷ</i> is the predicted value.
• <i>n</i> is the number of features.
• <i>x</i> is the <i>ith</i> feature value.
<i>i</i>
• <i>θ</i> is the <i>jth</i> model parameter (including the bias term <i>θ</i> and the feature weights
<i>j</i> 0
<i>θ</i> , <i>θ</i> , ⋯ , <i>θ</i> ).
1 2 <i>n</i>