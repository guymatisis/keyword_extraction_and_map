and in general Adam performs better. So, this is just one more optimizer you can
try if you experience problems with Adam on some task.
<i>Nadam</i>
Nadam optimization is Adam optimization plus the Nesterov trick, so it will
often converge slightly faster than Adam. In his report introducing this techni‐
que,19 the researcher Timothy Dozat compares many different optimizers on vari‐
ous tasks and finds that Nadam generally outperforms Adam but is sometimes
outperformed by RMSProp.
Adaptive optimization methods (including RMSProp, Adam, and
Nadam optimization) are often great, converging fast to a good sol‐
ution. However, a 2017 paper 20 by Ashia C. Wilson et al. showed
that they can lead to solutions that generalize poorly on some data‐
sets. So when you are disappointed by your model’s performance,
try using plain Nesterov Accelerated Gradient instead: your dataset
may just be allergic to adaptive gradients. Also check out the latest
research, because it’s moving fast.
All the optimization techniques discussed so far only rely on the <i>first-order</i> <i>partial</i>
<i>derivatives</i> (Jacobians). The optimization literature also contains amazing algorithms
based on the <i>second-order</i> <i>partial</i> <i>derivatives</i> (the <i>Hessians,</i> which are the partial
derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply
to deep neural networks because there are <i>n2</i> Hessians per output (where <i>n</i> is the
number of parameters), as opposed to just <i>n</i> Jacobians per output. Since DNNs typi‐
cally have tens of thousands of parameters, the second-order optimization algorithms
often don’t even fit in memory, and even when they do, computing the Hessians is
just too slow.
19 TimothyDozat,“IncorporatingNesterovMomentumintoAdam”(2016).
20 AshiaC.Wilsonetal.,“TheMarginalValueofAdaptiveGradientMethodsinMachineLearning,”Advancesin
<i>NeuralInformationProcessingSystems30(2017):4148–4158.</i>