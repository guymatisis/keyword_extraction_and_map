<i>Figure</i> <i>19-1.</i> <i>TF</i> <i>Serving</i> <i>can</i> <i>serve</i> <i>multiple</i> <i>models</i> <i>and</i> <i>automatically</i> <i>deploy</i> <i>the</i> <i>latest</i>
<i>version</i> <i>of</i> <i>each</i> <i>model</i>
So let’s suppose you have trained an MNIST model using tf.keras, and you want to
deploy it to TF Serving. The first thing you have to do is export this model to Tensor‐
Flow’s <i>SavedModel</i> <i>format.</i>
<b>ExportingSavedModels</b>
tf.saved_model.save()
TensorFlow provides a simple function to export models to
the SavedModel format. All you need to do is give it the model, specifying its name
and version number, and the function will save the model’s computation graph and its
weights:
model = keras.models.Sequential([...])
model.compile([...])
history = model.fit([...])
model_version = "0001"
model_name = "my_mnist_model"
model_path = os.path.join(model_name, model_version)
tf.saved_model.save(model, model_path)
Alternatively, you can just use the model’s save() method ( model.save(model_
path) .h5
): as long as the file’s extension is not , the model will be saved using the
SavedModel format instead of the HDF5 format.
It’s usually a good idea to include all the preprocessing layers in the final model you
export so that it can ingest data in its natural form once it is deployed to production.
This avoids having to take care of preprocessing separately within the application that
uses the model. Bundling the preprocessing steps within the model also makes it sim‐
pler to update them later on and limits the risk of mismatch between a model and the
preprocessing steps it requires.