                                                                      
                                                                      
                                                                      
                                                                      
          from then on. To estimate this sum of future discounted rewards, we can simply exe‐
          cute the DQN on the next state s′ and for all possible actions a′. We get an approxi‐
          mate future Q-Value for each possible action. We then pick the highest (since we
          assume we will be playing optimally) and discount it, and this gives us an estimate of
          the sum of future discounted rewards. By summing the reward r and the future dis‐
          counted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a),
          as shown in Equation 18-7.                                  
                                                                      
            Equation 18-7. Target Q-Value                             
                                                                      
            Q   s,a =r+γ·max Q s′,a′                                  
             target         θ                                         
                        a′                                            
          With this target Q-Value, we can run a training step using any Gradient Descent algo‐
          rithm. Specifically, we generally try to minimize the squared error between the esti‐
          mated Q-Value Q(s, a) and the target Q-Value (or the Huber loss to reduce the
          algorithm’s sensitivity to large errors). And that’s all for the basic Deep Q-Learning
          algorithm! Let’s see how to implement it to solve the CartPole environment.
          Implementing Deep Q-Learning                                
                                                                      
                                                                      
          The first thing we need is a Deep Q-Network. In theory, you need a neural net that
          takes a state-action pair and outputs an approximate Q-Value, but in practice it’s
          much more efficient to use a neural net that takes a state and outputs one approxi‐
          mate Q-Value for each possible action. To solve the CartPole environment, we do not
          need a very complicated neural net; a couple of hidden layers will do:
            env = gym.make("CartPole-v0")                             
            input_shape = [4] # == env.observation_space.shape        
            n_outputs = 2 # == env.action_space.n                     
            model = keras.models.Sequential([                         
               keras.layers.Dense(32, activation="elu", input_shape=input_shape),
               keras.layers.Dense(32, activation="elu"),              
               keras.layers.Dense(n_outputs)                          
            ])                                                        
          To select an action using this DQN, we pick the action with the largest predicted Q-
          Value. To ensure that the agent explores the environment, we will use an ε-greedy
          policy (i.e., we will choose a random action with probability ε):
            def epsilon_greedy_policy(state, epsilon=0):              
               if np.random.rand() < epsilon:                         
                 return np.random.randint(2)                          
               else:                                                  
                 Q_values = model.predict(state[np.newaxis])          
                 return np.argmax(Q_values[0])                        