from then on. To estimate this sum of future discounted rewards, we can simply exe‐
<i>s′</i> <i>a′.</i>
cute the DQN on the next state and for all possible actions We get an approxi‐
mate future Q-Value for each possible action. We then pick the highest (since we
assume we will be playing optimally) and discount it, and this gives us an estimate of
the sum of future discounted rewards. By summing the reward <i>r</i> and the future dis‐
counted value estimate, we get a target Q-Value <i>y(s,</i> <i>a)</i> for the state-action pair (s, <i>a),</i>
as shown in Equation 18-7.
<i>Equation</i> <i>18-7.</i> <i>Target</i> <i>Q-Value</i>
<i>Q</i> <i>s,a</i> = <i>r</i> + <i>γ</i> · max <i>Q</i> <i>s</i> ′ ,a ′
target <b>θ</b>
′
<i>a</i>
With this target Q-Value, we can run a training step using any Gradient Descent algo‐
rithm. Specifically, we generally try to minimize the squared error between the esti‐
mated Q-Value <i>Q(s,</i> <i>a)</i> and the target Q-Value (or the Huber loss to reduce the
algorithm’s sensitivity to large errors). And that’s all for the basic Deep Q-Learning
algorithm! Let’s see how to implement it to solve the CartPole environment.
<header><largefont><b>Implementing</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Q-Learning</b></largefont></header>
The first thing we need is a Deep Q-Network. In theory, you need a neural net that
takes a state-action pair and outputs an approximate Q-Value, but in practice it’s
much more efficient to use a neural net that takes a state and outputs one approxi‐
mate Q-Value for each possible action. To solve the CartPole environment, we do not
need a very complicated neural net; a couple of hidden layers will do:
env = gym.make("CartPole-v0")
input_shape = [4] <i>#</i> <i>==</i> <i>env.observation_space.shape</i>
n_outputs = 2 <i>#</i> <i>==</i> <i>env.action_space.n</i>
model = keras.models.Sequential([
keras.layers.Dense(32, activation="elu", input_shape=input_shape),
keras.layers.Dense(32, activation="elu"),
keras.layers.Dense(n_outputs)
])
To select an action using this DQN, we pick the action with the largest predicted Q-
Value. To ensure that the agent explores the environment, we will use an <i>ε-greedy</i>
policy (i.e., we will choose a random action with probability <i>ε):</i>
<b>def</b> epsilon_greedy_policy(state, epsilon=0):
<b>if</b> np.random.rand() < epsilon:
<b>return</b> np.random.randint(2)
<b>else:</b>
Q_values = model.predict(state[np.newaxis])
<b>return</b> np.argmax(Q_values[0])