                                                                      
                                                                      
                                                                      
                                                                      
          Python code). Thanks to the TFPyEnvironment class, TF-Agents supports both pure
          Python environments and TensorFlow-based environments. More generally, TF-
          Agents supports and provides both pure Python and TensorFlow-based components
          (agents, replay buffers, metrics, and so on).               
                                                                      
          Now that we have a nice Breakout environment, with all the appropriate preprocess‐
          ing and TensorFlow support, we must create the DQN agent and the other compo‐
          nents we will need to train it. Let’s look at the architecture of the system we will build.
          Training Architecture                                       
                                                                      
          A TF-Agents training program is usually split into two parts that run in parallel, as
          you can see in Figure 18-13: on the left, a driver explores the environment using a
          collect policy to choose actions, and it collects trajectories (i.e., experiences), sending
          them to an observer, which saves them to a replay buffer; on the right, an agent pulls
          batches of trajectories from the replay buffer and trains some networks, which the col‐
          lect policy uses. In short, the left part explores the environment and collects trajecto‐
          ries, while the right part learns and updates the collect policy.
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
                                                                      
          Figure 18-13. A typical TF-Agents training architecture     
                                                                      
          This figure begs a few questions, which I’ll attempt to answer here:
                                                                      
           • Why are there multiple environments? Instead of exploring a single environ‐
            ment, you generally want the driver to explore multiple copies of the environ‐
            ment in parallel, taking advantage of the power of all your CPU cores, keeping
                                                                      