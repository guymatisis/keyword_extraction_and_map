We will cover many of the packages and functions of the Tensor‐
Flow API, but it’s impossible to cover them all, so you should really
take some time to browse through the API; you will find that it is
quite rich and well documented.
<i>Figure</i> <i>12-1.</i> <i>TensorFlow’s</i> <i>Python</i> <i>API</i>
At the lowest level, each TensorFlow operation (op for short) is implemented using
highly efficient C++ code.2 Many operations have multiple implementations called
<i>kernels:</i> each kernel is dedicated to a specific device type, such as CPUs, GPUs, or
even TPUs (tensor <i>processing</i> <i>units).</i> As you may know, GPUs can dramatically speed
up computations by splitting them into many smaller chunks and running them in
parallel across many GPU threads. TPUs are even faster: they are custom ASIC chips
built specifically for Deep Learning operations3 (we will discuss how to use Tensor‐
Flow with GPUs or TPUs in Chapter 19).
TensorFlow’s architecture is shown in Figure 12-2. Most of the time your code will
use the high-level APIs (especially tf.keras and tf.data); but when you need more flex‐
ibility, you will use the lower-level Python API, handling tensors directly. Note that
2 Ifyoueverneedto(butyouprobablywon’t),youcanwriteyourownoperationsusingtheC++API.
3 TolearnmoreaboutTPUsandhowtheywork,checkouthttps://homl.info/tpus.