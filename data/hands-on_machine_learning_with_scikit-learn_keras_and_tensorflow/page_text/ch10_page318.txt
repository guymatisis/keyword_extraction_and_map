                                                                      
                                                                      
                                                                      
                                                                      
          changing some hyperparameter value), you will end up with a directory structure
          similar to this one:                                        
                                                                      
            my_logs/                                                  
            ├── run_2019_06_07-15_15_22                               
            │  ├── train                                              
            │  │ ├── events.out.tfevents.1559891732.mycomputer.local.38511.694049.v2
            │  │ ├── events.out.tfevents.1559891732.mycomputer.local.profile-empty
            │  │ └── plugins/profile/2019-06-07_15-15-32              
            │  │   └── local.trace                                    
            │  └── validation                                         
            │    └── events.out.tfevents.1559891733.mycomputer.local.38511.696430.v2
            └── run_2019_06_07-15_15_49                               
               └── [...]                                              
          There’s one directory per run, each containing one subdirectory for training logs and
          one for validation logs. Both contain event files, but the training logs also include
          profiling traces: this allows TensorBoard to show you exactly how much time the
          model spent on each part of your model, across all your devices, which is great for
          locating performance bottlenecks.                           
          Next you need to start the TensorBoard server. One way to do this is by running a
          command in a terminal. If you installed TensorFlow within a virtualenv, you should
          activate it. Next, run the following command at the root of the project (or from any‐
          where else, as long as you point to the appropriate log directory):
            $ tensorboard --logdir=./my_logs --port=6006              
            TensorBoard 2.0.0 at http://mycomputer.local:6006/ (Press CTRL+C to quit)
          If your shell cannot find the tensorboard script, then you must update your PATH envi‐
          ronment variable so that it contains the directory in which the script was installed
          (alternatively, you can just replace tensorboard in the command line with python3
          -m tensorboard.main). Once the server is up, you can open a web browser and go to
          http://localhost:6006.                                      
          Alternatively, you can use TensorBoard directly within Jupyter, by running the fol‐
          lowing commands. The first line loads the TensorBoard extension, and the second
          line starts a TensorBoard server on port 6006 (unless it is already started) and con‐
          nects to it:                                                
            %load_ext tensorboard                                     
            %tensorboard --logdir=./my_logs --port=6006               
          Either way, you should see TensorBoard’s web interface. Click the SCALARS tab to
          view the learning curves (see Figure 10-17). At the bottom left, select the logs you
          want to visualize (e.g., the training logs from the first and second run), and click the
          epoch_loss scalar. Notice that the training loss went down nicely during both runs,
          but the second run went down much faster. Indeed, we used a learning rate of 0.05
          (optimizer=keras.optimizers.SGD(lr=0.05)) instead of 0.001. 