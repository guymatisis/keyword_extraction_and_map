<b>Needforregularization</b>
When crossing two categorical features both with large cardinality, we produce a
cross feature with multiplicative cardinality. Naturally, given more categories for an
individual feature, the number of categories in a feature cross can increase dramati‐
cally. If this gets to the point where individual buckets have too few items, it will hin‐
der the model’s ability to generalize. Think of the latitude and longitude example. If
we were to take very fine buckets for latitude and longitude, then a feature cross
would be so precise it would allow the model to memorize every point on the map.
However, if that memorization was based on just a handful of examples, the memori‐
zation would actually be an overfit.
To illustrate, take the example of predicting the taxi fare in New York given the
pickup and dropoff locations and the time of pickup:6
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL mlpatterns.taxi_l2reg
<b>TRANSFORM(</b>
fare_amount
, ML.FEATURE_CROSS(STRUCT(CAST(EXTRACT(DAYOFWEEK <b>FROM</b> pickup_datetime)
<b>AS</b> STRING) <b>AS</b> dayofweek,
<b>CAST(EXTRACT(HOUR</b> <b>FROM</b> pickup_datetime)
<b>AS</b> STRING) <b>AS</b> hourofday), 2) <b>AS</b> day_hr
, CONCAT(
ML.BUCKETIZE(pickuplon, GENERATE_ARRAY(-78, -70, <b>0.01)),</b>
ML.BUCKETIZE(pickuplat, GENERATE_ARRAY(37, 45, 0.01)),
ML.BUCKETIZE(dropofflon, GENERATE_ARRAY(-78, -70, 0.01)),
ML.BUCKETIZE(dropofflat, GENERATE_ARRAY(37, 45, 0.01))
) <b>AS</b> pickup_and_dropoff
)
<b>OPTIONS(input_label_cols=['fare_amount'],</b>
model_type='linear_reg', <b>l2_reg=0.1)</b>
<b>AS</b>
<b>SELECT</b> * <b>FROM</b> mlpatterns.taxi_data
There are two feature crosses here: one in time (of day of week and hour of day) and
the other in space (of the pickup and dropoff locations). The location, in particular, is
very high cardinality, and it is likely that some of the buckets will have very few
examples.
For this reason, it is advisable to pair feature crosses with L1 regularization, which
encourages sparsity of features, or L2 regularization, which limits overfitting. This
allows our model to ignore the extraneous noise generated by the many synthetic fea‐
tures and combat overfitting. Indeed, on this dataset, the regularization improves the
RMSE slightly, by 0.3%.
6 Fullcodeisin02_data_representation/feature_cross.ipynbinthecoderepositoryofthisbook.