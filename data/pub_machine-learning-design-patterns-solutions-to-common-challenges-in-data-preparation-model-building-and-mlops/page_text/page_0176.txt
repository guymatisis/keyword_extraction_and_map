<i>data</i> <i>parallelism</i> and <i>model</i> <i>parallelism.</i> In data parallelism, computation is split
across different machines and different workers train on different subsets of the
training data. In model parallelism, the model is split and different workers carry out
the computation for different parts of the model. In this section, we’ll focus on data
tf.distrib
parallelism and show implementations in TensorFlow using the
ute.Strategy library. We’ll discuss model parallelism in “Trade-Offs and Alterna‐
tives” on page 183.
To implement data parallelism, there must be a method in place for different workers
to compute gradients and share that information to make updates to the model
parameters. This ensures that all workers are consistent and each gradient step works
to train the model. Broadly speaking, data parallelism can be carried out either syn‐
chronously or asynchronously.
<b>Synchronoustraining</b>
In synchronous training, the workers train on different slices of input data in parallel
and the gradient values are aggregated at the end of each training step. This is per‐
formed via an <i>all-reduce</i> algorithm. This means that each worker, typically a GPU,
has a copy of the model on device and, for a single stochastic gradient descent (SGD)
step, a mini-batch of data is split among each of the separate workers. Each device
performs a forward pass with their portion of the mini-batch and computes gradients
for each parameter of the model. These locally computed gradients are then collected
from each device and aggregated (for example, averaged) to produce a single gradient
update for each parameter. A central server holds the most current copy of the model
parameters and performs the gradient step according to the gradients received from
the multiple workers. Once the model parameters are updated according to this
aggregated gradient step, the new model is sent back to the workers along with
another split of the next mini-batch, and the process repeats. Figure 4-15 shows a
typical all-reduce architecture for synchronous data distribution.
As with any parallelism strategy, this introduces additional overhead to manage tim‐
ing and communication between workers. Large models could cause I/O bottlenecks
as data is passed from the CPU to the GPU during training, and slow networks could
also cause delays.
In TensorFlow, tf.distribute.MirroredStrategy supports synchronous dis‐
tributed training across multiple GPUs on the same machine. Each model parameter
is mirrored across all workers and stored as a single conceptual variable called
MirroredVariable
. During the all-reduce step, all gradient tensors are made available
on each device. This helps to significantly reduce the overhead of synchronization.
There are also various other implementations for the all-reduce algorithm available,
many of which use NVIDIA NCCL.