(KPIs) at the onset of an ML project can help to ensure everyone is aligned on the
common goal. Ideally there is already some procedure in place that provides a conve‐
nient baseline against which to measure future progress. This could be a model
already in production, or even just a rules-based heuristic that is currently in use.
Machine learning is not the answer to all problems, and sometimes a rule-based heu‐
ristic is hard to beat. Development shouldn’t be done for development’s sake. A base‐
line model, no matter how simple, is helpful to guide design decisions down the road
and understand how each design choice moves the needle on that predetermined
evaluation metric. In Chapter 7, we discussed the role of a Heuristic Benchmark as
well as other topics related to Responsible AI that often come up when communicat‐
ing the impact and influence of machine learning with business stakeholders.
Of course, these conversations should also take place in the context of the data. A
business deep dive should go hand in hand with a deep dive of data exploration (Step
2 of Figure 8-2). As beneficial as a solution might be, if quality data is not available,
then there is no project. Or perhaps the data exists, but because of data privacy rea‐
sons, it cannot be used or must be scrubbed of relevant information needed for the
model. In any case, the viability of a project and the potential for success all rely on
the data. Thus, it is essential to have data stewards within the organization involved
in these conversations early.
The data guides the process and it’s important to understand the quality of the data
that is available. What are the distributions of the key features? How many missing
values are there? How will missing values be handled? Are there outliers? Are any
input values highly correlated? What features exist in the input data and which fea‐
tures should be engineered? Many machine learning models require a massive dataset
for training. Is there enough data? How can we augment the dataset? Is there bias in
the dataset? These are important questions, and they only touch the surface. One pos‐
sible decision at this stage is that more data, or data of a specific scenario, needs to be
collected before the project can proceed.
Data exploration is a key step in answering the question of whether data of sufficient
quality exists. Conversation alone is rarely a substitute for getting your hands dirty
and experimenting with the data. Visualization plays an important role during this
step. Density plots and histograms are helpful to understand the spread of different
input values. Box plots can help to identify outliers. Scatter plots are useful for dis‐
covering and describing bivariate relationships. Percentiles can help identify the
range for numeric data. Averages, medians, and standard deviations can help to
describe central tendency. These techniques and others can help determine which
features are likely to benefit the model as well as further understanding of which data
transformations will be needed to prepare the data for modeling.