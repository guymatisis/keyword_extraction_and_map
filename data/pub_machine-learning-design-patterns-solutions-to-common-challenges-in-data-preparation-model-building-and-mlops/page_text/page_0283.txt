and more people within an organization want to contribute to this code base, run‐
ning these steps from a single notebook will not scale.
<i>Figure</i> <i>6-6.</i> <i>The</i> <i>steps</i> <i>in</i> <i>a</i> <i>typical</i> <i>end-to-end</i> <i>ML</i> <i>workflow.</i> <i>This</i> <i>is</i> <i>not</i> <i>meant</i> <i>to</i> <i>be</i> <i>all</i>
<i>encompassing,</i> <i>but</i> <i>captures</i> <i>the</i> <i>most</i> <i>common</i> <i>steps</i> <i>in</i> <i>the</i> <i>ML</i> <i>development</i> <i>process.</i>
In traditional programming, <i>monolithic</i> <i>applications</i> are described as those where all
of the application’s logic is handled by a single program. To test a small feature in a
monolithic app, we must run the entire program. The same goes for deploying or
debugging monolithic applications. Deploying a small bug fix for one piece of the
program requires deploying the entire application, which can quickly become
unwieldy. When the entire codebase is inextricably linked, it becomes difficult for
individual developers to debug errors and work independently on different parts of
the application. In recent years, monolithic apps have been replaced in favor of a
<i>microservices</i> architecture where individual pieces of business logic are built and
deployed as isolated (micro) packages of code. With microservices, a large applica‐
tion is split into smaller, more manageable parts so that developers can build, debug,
and deploy pieces of an application independently.
This monolith-versus-microservice discussion provides a good analogy for scaling
ML workflows, enabling collaboration, and ensuring ML steps are reproducible and
reusable across different workflows. When someone is building an ML model on
their own, a “monolithic” approach may be faster to iterate on. It also often works
because one person is actively involved in developing and maintaining each piece:
data gathering and preprocessing, model development, training, and deployment.
However, when scaling this workflow, different people or groups in an organization
might be responsible for different steps. To scale the ML workflow, we need a way for
the team building out the model to run trials independently of the data preprocessing
step. We’ll also need to track the performance for each step of the pipeline and man‐
age the output files generated by each part of the process.
Additionally, when initial development for each step is complete, we’ll want to sched‐
ule operations like retraining, or create event-triggered pipeline runs that are invoked
in response to changes in your environment, like new training data being added to a
bucket. In such cases, it’ll be necessary for the solution to allow us to run the entire
workflow from end to end in one call while still being able to track output and trace
errors from individual steps.