While it’s tempting to assign significant meaning to the learned
weights in linear regression or decision tree models, we must be
extremely cautious when doing so. The conclusions we drew earlier
are still correct (i.e., inverse relationship between number of cylin‐
ders and fuel efficiency), but we cannot conclude from the magni‐
tude of coefficients, for example, that the categorical origin feature
or the number of cylinders are more important to our model than
horsepower or weight. First, each of these features is represented in
a different unit. One cylinder bears no equivalence to one pound—
the cars in this dataset have a maximum of 8 cylinders, but weigh
over 3,000 pounds. Additionally, origin is a categorical feature rep‐
resented with dummy values, so each origin value can only be 0 or
1. The coefficients also don’t tell us anything about the relationship
<i>between</i> features in our model. More cylinders are often correlated
with more horsepower, but we can’t conclude this from the learned
weights.4
When models are more complex, we use <i>post</i> <i>hoc</i> explainability methods to approxi‐
mate the relationships between a model’s features and its output. Typically, post hoc
methods perform this analysis without relying on model internals like learned
weights. This is an area of ongoing research, and there are a variety of proposed
explanation methods, along with tooling for adding these methods to your ML work‐
flow. The type of explanation methods we’ll discuss are known as <i>feature</i> <i>attributions.</i>
These methods aim to attribute a model’s output—whether it be an image, classifica‐
tion, or numerical value—to its features, by assigning attribution values to each fea‐
ture indicating how much that feature contributed to the output. There are two types
of feature attributions:
<i>Instance-level</i>
Feature attributions that explain a model’s output for an individual prediction.
For example, in a model predicting whether someone should be approved for a
line of credit, an instance-level feature attribution would provide insight into
why a specific person’s application was denied. In an image model, an instance-
level attribution might highlight the pixels in an image that caused it to predict it
contained a cat.
<i>Global</i>
Global feature attributions analyze the model’s behavior across an aggregate to
draw conclusions about how the model is behaving as a whole. Typically this is
done by averaging instance-level feature attributions from a test dataset. In a
model predicting whether a flight will be delayed, global attributions might tell
4 Thescikit-learndocumentationgoesintomoredetailonhowtocorrectlyinterpretthelearnedweightsin
linearmodels.