text|keyphrases
"values to be between [–1, 1]) and pushes the problematic values further out. Of these
three methods, zero-norming works best for mother_age because the raw age values
were somewhat of a bell curve. For other problems, min-max scaling, clipping, or
winsorizing might be better.
<i>Figure</i> <i>2-3.</i> <i>The</i> <i>histogram</i> <i>of</i> <i>mother_age</i> <i>in</i> <i>the</i> <i>baby</i> <i>weight</i> <i>prediction</i> <i>example</i> <i>is</i>
<i>shown</i> <i>in</i> <i>the</i> <i>top-left</i> <i>panel,</i> <i>and</i> <i>different</i> <i>scaling</i> <i>functions</i> <i>(see</i> <i>the</i> <i>x-axis</i> <i>label)</i> <i>are</i>
<i>shown</i> <i>in</i> <i>the</i> <i>remaining</i> <i>panels.</i>"|min-max scaling; winsorizing; z-score normalization
"The What-If Tool is model agnostic and can be used for any type
of model regardless of architecture or framework. It works with
models loaded within a notebook or in TensorBoard, models
served via TensorFlow Serving, and models deployed to Cloud AI
Platform Prediction. The What-If Tool team also created a tool for
text-based models called the Language Interpretability Tool (LIT).
Another important consideration for post-training evaluation is testing our model on
a balanced set of examples. If there are particular slices of our data that we anticipate
will be problematic for our model—like inputs that could be affected by data collec‐
tion or representation bias—we should ensure our test set includes enough of these
cases. After splitting our data, we’ll use the same type of analysis we employed in the
“Before training” part of this section on <i>each</i> split of our data: training, validation,
and test.
As seen from this analysis, there is no one-size-fits-all solution or evaluation metric
for model fairness. It is a continuous, iterative process that should be employed
throughout an ML workflow—from data collection to deployed model.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
There are many ways to approach model fairness in addition to the pre- and post-
training techniques discussed in the Solution section. Here, we’ll introduce a few
alternative tools and processes for achieving fair models. ML fairness is a rapidly
evolving area of research—the tools included in this section aren’t meant to provide
an exhaustive list, but rather a few techniques and tools currently available for
improving model fairness. We’ll also discuss the differences between the Fairness
Lens and Explainable Predictions design patterns, as they are related and often used
together.
<b>FairnessIndicators</b>
Fairness Indicators (FI) are a suite of open source tools designed to help in under‐
standing a dataset’s distribution before training, and evaluating model performance
using fairness metrics. The tools included in FI are TensorFlow Data Validation
(TFDV) and TensorFlow Model Analysis (TFMA). Fairness Indicators are most often
used as components in TFX pipelines (see “Design Pattern 25: Workflow Pipeline”
on page 282 in Chapter 6 for more details) or via TensorBoard. With TFX, there are
two pre-built components that utilize Fairness Indicator tools:
• ExampleValidator for data analysis, detecting drift, and training–serving skew
with TFDV.
• Evaluator uses the TFMA library to evaluate a model across subsets of a dataset.
An example of an interactive visualization generated from TFMA is shown in"|AI Platform Prediction; bias; Fairness Indicators; Fairness Lens design pattern; Language Interpretability Tool; TensorBoard; TensorFlow Data Validation; TensorFlow Serving; TFMA; TFX; What-If Tool
"environment outside this context. As an example, let’s say we train a model to iden‐
tify fraudulent credit card transactions and it finds, as a global-level feature attribu‐
tion, that a transaction’s amount is the feature most indicative of fraud. Following
this, it would be incorrect to conclude that amount is <i>always</i> the biggest indicator of
credit card fraud—this is only the case within the context of our training dataset,
model, and specified baseline value.
We can think of explanations as an important addition to accuracy, error, and other
metrics used to evaluate ML models. They provide useful insight into a model’s qual‐
ity and potential bias, but should not be the sole determinant of a high-quality model.
We recommend using explanations as one piece of model evaluation criteria in addi‐
tion to data and model evaluation, and many of the other patterns outlined in this
and previous chapters.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>30:</b></largefont> <largefont><b>Fairness</b></largefont> <largefont><b>Lens</b></largefont></header>
The Fairness Lens design pattern suggests the use of preprocessing and postprocess‐
ing techniques to ensure that model predictions are fair and equitable for different
groups of users and scenarios. Fairness in machine learning is a continuously evolv‐
ing area of research, and there is no single catch-all solution or definition to making a
model “fair.” Evaluating an entire end-to-end ML workflow—from data collection to
model deployment—through a fairness lens is essential to building successful, high-
quality models.
<header><largefont><b>Problem</b></largefont></header>
With the word “machine” in its name, it’s easy to assume that ML models can’t be
biased. After all, models are the result of patterns learned by a computer, right? The
problem with this thinking is that the datasets models learn from are created by
<i>humans,</i> not machines, and humans are full of bias. This inherent human bias is
inevitable, but is not necessarily always bad. Take for example a dataset used to train
a financial fraud detection model—this data will likely be heavily imbalanced with
very few fraudulent examples, since fraud is relatively rare in most cases. This is an
example of naturally occurring bias, as it is a reflection of the statistical properties of
the original dataset. Bias becomes <i>harmful</i> when it affects different groups of people
differently. This is known as <i>problematic</i> <i>bias,</i> and it’s what we’ll be focusing on
throughout this section. If this type of bias is not accounted for, it can find its way
into models, creating adverse effects as production models directly reflect the bias
present in the data.
Problematic bias is present even in situations where you may not expect it. As an
example, imagine we’re building a model to identify different types of clothing and
accessories. We’ve been tasked with collecting all of the shoe images for the training
dataset. When we think about shoes, we take note of the first thing that comes to"|bias; Explainable Predictions design pattern; Fairness Lens design pattern; model evaluation; problematic bias
"<i>Figure</i> <i>6-1.</i> <i>The</i> <i>model</i> <i>has</i> <i>three</i> <i>features</i> <i>computed</i> <i>from</i> <i>two</i> <i>inputs.</i>
But the SQL code above mixes up the inputs and features and doesn’t keep track of
the transformations that were carried out. This comes back to bite when we try to
predict with this model. Because the model was trained on three features, this is what
the prediction signature has to look like:
<b>SELECT</b> * <b>FROM</b> ML.PREDICT(MODEL ch09eu.bicycle_model,(
'Kings Cross' <b>AS</b> start_station_name
, '3' <b>as</b> dayofweek
, '18' <b>as</b> hourofday
))
Note that, at inference time, we have to know what features the model was trained
on, how they should be interpreted, and the details of the transformations that were
'3' '3'
applied. We have to know that we need to send in for dayofweek. That …is
that Tuesday or Wednesday? Depends on which library was used by the model, or
what we consider the start of a week!
<i>Training-serving</i> <i>skew,</i> caused by differences in any of these factors between the train‐
ing and serving environments, is one of the key reasons why productionization of ML
models is so hard.
<header><largefont><b>Solution</b></largefont></header>
The solution is to explicitly capture the transformations applied to convert the model
inputs into features. In BigQuery ML, this is done using the TRANSFORM clause. Using
TRANSFORM
ensures that these transformations are automatically applied during
ML.PREDICT
.
Given the support for TRANSFORM, the model above should be rewritten as:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL ch09eu.bicycle_model
<b>OPTIONS(input_label_cols=['duration'],</b>
model_type='linear_reg')
<b>TRANSFORM(</b>
<b>SELECT</b> * <b>EXCEPT(start_date)</b>
, <b>CAST(EXTRACT(dayofweek</b> <b>from</b> start_date) <b>AS</b> STRING)
<b>as</b> dayofweek <i>--</i> <i>feature1</i>
, <b>CAST(EXTRACT(hour</b> <b>from</b> start_date) <b>AS</b> STRING)"|BigQuery ML; training-serving skew; Transform design pattern
"Progress during this ingestion step is printed to the screen showing that we’ve inges‐
ted 28,247 rows into the taxi_rides feature set within Feast:
100%|██████████|28247/28247 [00:02<00:00, 2771.19rows/s]
Ingestion complete!
Ingestion statistics:
Success: 28247/28247 rows ingested
At this stage, calling client.list_feature_sets() will now list the feature set
taxi_rides [default/taxi_rides] default
we just created and return . Here, refers
to the project scope of the feature set within Feast. This can be changed when instan‐
tiating the feature set to keep certain feature sets within project access.
Datasets may change over time, causing feature sets to change as
well. In Feast, once a feature set is created, there are only a few
changes that can be made. For example, the following changes are
allowed:
• Adding new features.
• Removing existing features. (Note that features are tombs‐
toned and remain on record, so they are not removed com‐
pletely. This will affect new features being able to take the
names of previously deleted features.)
• Changing features’ schemas.
max_age
• Changing the feature set’s source or the of the feature
set examples.
The following changes are <i>not</i> allowed:
• Changes to the feature set name.
• Changes to entities.
• Changes to names of existing features.
<b>RetrievingdatafromFeast</b>
Once a feature set has been sourced with features, we can retrieve historical or online
features. Users and production systems retrieve feature data through a Feast serving
data access layer. Since Feast supports both offline and online store types, it’s com‐
mon to have Feast deployments for both, as shown in Figure 6-15. The same feature
data is contained within the two feature stores, ensuring consistency between training
and serving."|Feast; Feature Store design pattern
"<b>SELECT</b>
<b>DISTINCT(departure_airport)</b>
<b>FROM</b> `bigquery-samples.airline_ontime_data.flights`
Some airports had as few as one to three flights over the entire time period, and so we
expect that the training data vocabulary will be incomplete. 347 is large enough that
the feature will be quite sparse, and it is certainly the case that new airports will get
built. All three problems (incomplete vocabulary, high cardinality, cold start) will
exist if we one-hot encode the departure airport.
The airline dataset, like the natality dataset and nearly all the other datasets that we
use in this book for illustration, is a public dataset in BigQuery, so you can try the
query out. At the time we are writing this, 1 TB/month of querying is free, and there
is a sandbox available so that you can use BigQuery up to this limit without putting
down a credit card. We encourage you to bookmark our GitHub repository. For
example, see the notebook in GitHub for the full code.
<header><largefont><b>Solution</b></largefont></header>
The Hashed Feature design pattern represents a categorical input variable by doing
the following:
1. Converting the categorical input into a unique string. For the departure airport,
we can use the three-letter IATA code for the airport.
2. Invoking a deterministic (no random seeds or salt) and portable (so that the
same algorithm can be used in both training and serving) hashing algorithm on
the string.
3. Taking the remainder when the hash result is divided by the desired number of
buckets. Typically, the hashing algorithm returns an integer that can be negative
and the modulo of a negative integer is negative. So, the absolute value of the
result is taken.
In BigQuery SQL, these steps are achieved like this:
<b>ABS(MOD(FARM_FINGERPRINT(airport),</b> numbuckets))
The FARM_FINGERPRINT function uses FarmHash, a family of hashing algorithms that
is deterministic, well-distributed, and for which implementations are available in a
number of programming languages.
In TensorFlow, these steps are implemented by the feature_column function:
tf.feature_column.categorical_column_with_hash_bucket(
airport, num_buckets, dtype=tf.dtypes.string)
For example, Table 2-1 shows the FarmHash of some IATA airport codes when
hashed into 3, 10, and 1,000 buckets."|BigQuery; FarmHash; feature columns; Hashed Feature design pattern; TensorFlow; vocabulary
"second, or there could be a monthly file that is generated from an external system
reporting a summary of the last month’s transactions. Each of these need to be pro‐
cessed and ingested into the feature store. By the same token, there may be different
time horizons for retrieving data from the feature store. For example, a user-facing
online application may operate at very low latency using up-to-the-second features,
whereas when training the model, features are pulled offline as a larger batch but with
higher latency.
<i>Figure</i> <i>6-18.</i> <i>The</i> <i>Feature</i> <i>Store</i> <i>design</i> <i>pattern</i> <i>can</i> <i>handle</i> <i>both</i> <i>the</i> <i>requirements</i> <i>of</i> <i>data</i>
<i>being</i> <i>highly</i> <i>scalable</i> <i>for</i> <i>large</i> <i>batches</i> <i>during</i> <i>training</i> <i>and</i> <i>extremely</i> <i>low</i> <i>latency</i> <i>for</i>
<i>serving</i> <i>online</i> <i>applications.</i>
There is no single database that can handle both scaling to potentially terabytes of
data <i>and</i> extremely low latency on the order of milliseconds. The feature store ach‐
ieves this with separate online and offline feature stores and ensures that features are
handled in a consistent fashion in both scenarios.
Lastly, a feature store acts as a version-controlled repository for feature datasets,
allowing the same CI/CD practices of code and model development to be applied to
the feature engineering process. This means that new ML projects start with a process
of feature selection from a catalog instead of having to do feature engineering from
scratch, allowing organizations to achieve an economies-of-scale effect—as new fea‐
tures are created and added to the feature store, it becomes easier and faster to build
new models that reuse those features.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The Feast framework that we discussed is built on Google BigQuery, Redis, and
Apache Beam. However, there are feature stores that rely on other tools and tech
stacks. And, although a feature store is the recommended way to manage features at
tf.transform
scale, provides an alternative solution that addresses the issue of"|CI/CD; Feast; Feature Store design pattern
"<i>Figure</i> <i>3-17.</i> <i>A</i> <i>pipeline</i> <i>to</i> <i>train</i> <i>the</i> <i>cascade</i> <i>of</i> <i>models</i> <i>as</i> <i>a</i> <i>single</i> <i>job.</i>
Kubeflow Pipelines provides such a framework. Because it works with containers, the
underlying machine learning models and glue code can be written in nearly any pro‐
gramming or scripting language. Here, we will wrap the BigQuery SQL models above
into Python functions using the BigQuery client library. We could use TensorFlow or
scikit-learn or even R to implement individual components.
The pipeline code using Kubeflow Pipelines can be expressed quite simply as the fol‐
lowing (the full code can be found in the code repository of this book):
@dsl.pipeline(
name='Cascade pipeline on SF bikeshare',
description='Cascade pipeline on SF bikeshare'
)
<b>def</b> cascade_pipeline(
project_id = PROJECT_ID
):
ddlop = comp.func_to_container_op(run_bigquery_ddl,
packages_to_install=['google-cloud-bigquery'])
c1 = train_classification_model(ddlop, PROJECT_ID)
c1_model_name = c1.outputs['created_table']
c2a_input = create_training_data(ddlop,
PROJECT_ID, c1_model_name, 'Typical')
c2b_input = create_training_data(ddlop,
PROJECT_ID, c1_model_name, 'Long')"|Cascade design pattern; Kubeflow Pipelines
"<i>Figure</i> <i>5-12.</i> <i>A</i> <i>comparison</i> <i>of</i> <i>on-device</i> <i>phrase-based</i> <i>and</i> <i>(newer)</i> <i>neural-machine</i>
<i>translation</i> <i>models</i> <i>and</i> <i>online</i> <i>neural</i> <i>machine</i> <i>translation</i> <i>(source:</i> <i>The</i> <i>Keyword).</i>
Another example of a standalone single-phase model is Google Bolo, a speech-based
language learning app for children. The app works entirely offline and was developed
with the intention of helping populations where reliable internet access is not always
available.
<b>Offlinesupportforspecificusecases</b>
Another solution for making your application work for users with minimal internet
connectivity is to make only certain parts of your app available offline. This could
involve enabling a few common features offline or caching the results of an ML
model’s prediction for later use offline. With this alternative, we’re still employing
two prediction phases, but we’re limiting the use cases covered by our offline model.
In this approach, the app works sufficiently offline, but provides full functionality
when it regains connectivity.
Google Maps, for example, lets you download maps and directions in advance. To
avoid having directions take up too much space on a mobile device, only driving
directions might be made available offline (not walking or biking). Another example
could be a fitness application that tracks your steps and makes recommendations for
future activity. Let’s say the most common use of this app is checking how many
steps you have walked on the current day. To support this use case offline, we could
sync the fitness tracker’s data to a user’s device over Bluetooth to enable checking the
current day’s fitness status offline. To optimize our app’s performance, we might
decide to make fitness history and recommendations only available online.
We could further build upon this by storing the user’s queries while their device is
offline and sending them to a cloud model when they regain connectivity to provide
more detailed results. Additionally, we could even provide a basic recommendation
model available offline, with the intention of complementing this with improved"|Google Bolo; Two-Phase Predictions design pattern
"The first case (binary classification) is unique in that it is the only type of single-label
classification problem where we would consider using sigmoid as our activation
function. For nearly any other multiclass classification problem (for example, classi‐
fying text into one of five possible categories), we would use softmax. However, when
we only have two classes, softmax is redundant. Take for example a model that pre‐
dicts whether or not a specific transaction is fraudulent. Had we used a softmax out‐
put in this example, here’s what a fraudulent model prediction might look like:
[.02, .98]
In this example, the first index corresponds with “not fraudulent” and the second
index corresponds with “fraudulent.” This is redundant because we could also repre‐
sent this with a single scalar value, and thus use a sigmoid output. The same predic‐
tion could be represented as simply .98. Because each input can only be assigned a
.98
single class, we can infer from this output of that the model has predicted a 98%
chance of fraud and a 2% chance of nonfraud.
Therefore, for binary classification models, it is optimal to use an output shape of 1
with a sigmoid activation function. Models with a single output node are also more
efficient, since they will have fewer trainable parameters and will likely train faster.
Here is what the output layer of a binary classification model would look like:
keras.layers.Dense(1, activation='sigmoid')
For the second case where a training example could belong to <i>both</i> <i>possible</i> <i>classes</i>
and fits into the Multilabel design pattern, we’ll also want to use sigmoid, this time
with a two-element output:
keras.layers.Dense(2, activation='sigmoid')
<b>Whichlossfunctionshouldweuse?</b>
Now that we know when to use sigmoid as an activation function in our model, how
should we choose which loss function to use with it? For the binary classification case
where our model has a one-element output, use binary cross-entropy loss. In Keras,
we provide a loss function when we compile our model:
model.compile(loss='binary_crossentropy', optimizer='adam',
metrics=['accuracy'])
Interestingly, we also use binary cross-entropy loss for multilabel models with sig‐
moid output. This is because, as shown in Figure 3-9, a multilabel problem with three
classes is essentially three smaller binary classification problems."|binary classification; Keras; multilabel classification; Multilabel design pattern; sigmoid; sigmoid activation; softmax
"<b>Handlingnumericalfeatures</b>
We would never want to create a feature cross with a continuous input. Remember, if
one input takes <i>m</i> possible values and another input takes <i>n</i> possible values, then the
feature cross of the two would result in m*n elements. A numeric input is dense, tak‐
ing a continuum of values. It would be impossible to enumerate all possible values in
a feature cross of continuous input data.
Instead, if our data is continuous, then we can bucketize the data to make it categori‐
cal before applying a feature cross. For example, latitude and longitude are continu‐
ous inputs, and it makes intuitive sense to create a feature cross using these inputs
since location is determined by an ordered pair of latitude and longitude. However,
instead of creating a feature cross using the raw latitude and longitude, we would bin
these continuous values and cross the binned_latitude and the binned_longitude :
<b>import</b> <b>tensorflow.feature_column</b> <b>as</b> <b>fc</b>
<i>#</i> <i>Create</i> <i>a</i> <i>bucket</i> <i>feature</i> <i>column</i> <i>for</i> <i>latitude.</i>
latitude_as_numeric = fc.numeric_column(""latitude"")
lat_bucketized = fc.bucketized_column(latitude_as_numeric,
lat_boundaries)
<i>#</i> <i>Create</i> <i>a</i> <i>bucket</i> <i>feature</i> <i>column</i> <i>for</i> <i>longitude.</i>
longitude_as_numeric = fc.numeric_column(""longitude"")
lon_bucketized = fc.bucketized_column(longitude_as_numeric,
lon_boundaries)
<i>#</i> <i>Create</i> <i>a</i> <i>feature</i> <i>cross</i> <i>of</i> <i>latitude</i> <i>and</i> <i>longitude</i>
lat_x_lon = fc.crossed_column([lat_bucketized, lon_bucketized],
hash_bucket_size=nbuckets**4)
crossed_feature = fc.indicator_column(lat_x_lon)
<b>Handlinghighcardinality</b>
Because the cardinality of resulting categories from a feature cross increases multipli‐
catively with respect to the cardinality of the input features, feature crosses lead to
day_of_week hour_of_day
sparsity in our model inputs. Even with the and feature
cross, a feature cross would be a sparse vector of dimension 168 (see Figure 2-17).
It can be useful to pass a feature cross through an Embedding layer (see the “Design
Pattern 2: Embeddings” on page 39 in this chapter) to create a lower-dimensional
representation, as shown in Figure 2-18."|Feature Cross design pattern
"term for preprocessing is <i>feature</i> <i>engineering.</i> We’ll use these two terms interchangea‐
bly throughout the book.
There are various terms used to describe data as it goes through the feature engineer‐
ing process. <i>Input</i> describes a single column in your dataset before it has been pro‐
cessed, and <i>feature</i> describes a single column <i>after</i> it has been processed. For
example, a timestamp could be your input, and the feature would be day of the week.
To convert the data from timestamp to day of the week, you’ll need to do some data
preprocessing. This preprocessing step can also be referred to as <i>data</i> <i>transformation.</i>
An <i>instance</i> is an item you’d like to send to your model for prediction. An instance
could be a row in your test dataset (without the label column), an image you want to
classify, or a text document to send to a sentiment analysis model. Given a set of fea‐
tures about the instance, the model will calculate a predicted value. In order to do
that, the model is trained on <i>training</i> <i>examples,</i> which associate an instance with a
<i>label.</i> A <i>training</i> <i>example</i> refers to a single instance (row) of data from your dataset
that will be fed to your model. Building on the timestamp use case, a full training
example might include: “day of week,” “city,” and “type of car.” A <i>label</i> is the output
column in your dataset—the item your model is predicting. <i>Label</i> can refer both to
the target column in your dataset (also called a <i>ground</i> <i>truth</i> <i>label)</i> and the output
given by your model (also called a <i>prediction).</i> A sample label for the training exam‐
ple outlined above could be “trip duration”—in this case, a float value denoting
minutes.
Once you’ve assembled your dataset and determined the features for your model,
<i>data</i> <i>validation</i> is the process of computing statistics on your data, understanding
your schema, and evaluating the dataset to identify problems like drift and training-
serving skew. Evaluating various statistics on your data can help you ensure the data‐
set contains a balanced representation of each feature. In cases where it’s not possible
to collect more data, understanding data balance will help you design your model to
account for this. Understanding your schema involves defining the data type for each
feature and identifying training examples where certain values may be incorrect or
missing. Finally, data validation can identify inconsistencies that may affect the qual‐
ity of your training and test sets. For example, maybe the majority of your training
dataset contains <i>weekday</i> examples while your test set contains primarily <i>weekend</i>
examples.
<header><largefont><b>The</b></largefont> <largefont><b>Machine</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Process</b></largefont></header>
The first step in a typical machine learning workflow is <i>training—the</i> process of pass‐
ing training data to a model so that it can learn to identify patterns. After training,
the next step in the process is testing how your model performs on data outside of
your training set. This is known as model <i>evaluation.</i> You might run training and
evaluation multiple times, performing additional feature engineering and tweaking"|batch prediction; data transformation; data validation; evaluation; feature; ground truth label; input; instance; label; numerical data; online prediction; prediction; serving; training examples
"<i>Figure</i> <i>2-10.</i> <i>By</i> <i>learning</i> <i>a</i> <i>low-dimensional,</i> <i>dense</i> <i>embedding</i> <i>vector</i> <i>for</i> <i>each</i> <i>customer</i>
<i>and</i> <i>video,</i> <i>an</i> <i>embedding-based</i> <i>model</i> <i>is</i> <i>able</i> <i>to</i> <i>generalize</i> <i>well</i> <i>with</i> <i>less</i> <i>of</i> <i>a</i> <i>manual</i>
<i>feature</i> <i>engineering</i> <i>burden.</i>
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The main trade-off with using an embedding is the compromised representation of
the data. There is a loss of information involved in going from a high-cardinality rep‐
resentation to a lower-dimensional representation. In return, we gain information
about closeness and context of the items.
<b>Choosingtheembeddingdimension</b>
The exact dimensionality of the embedding space is something that we choose as a
practitioner. So, should we choose a large or small embedding dimension? Of course,
as with most things in machine learning, there is a trade-off. The lossiness of the rep‐
resentation is controlled by the size of the embedding layer. By choosing a very small
output dimension of an embedding layer, too much information is forced into a
small vector space and context can be lost. On the other hand, when the embedding
dimension is too large, the embedding loses the learned contextual importance of the
features. At the extreme, we’re back to the problem encountered with one-hot encod‐
ing. The optimal embedding dimension is often found through experimentation,
similar to choosing the number of neurons in a deep neural network layer.
If we’re in a hurry, one rule of thumb is to use the fourth root of the total number of
unique categorical elements while another is that the embedding dimension should
be approximately 1.6 times the square root of the number of unique elements in the
category, and no less than 600. For example, suppose we wanted to use an embedding
layer to encode a feature that has 625 unique values. Using the first rule of thumb, we
would choose an embedding dimension for plurality of 5, and using the second rule"|Embedding design pattern; one-hot encoding
"it is offline. When it regains connectivity, we could then send these clips to the cloud-
hosted model for prediction.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
While the Two-Phase Predictions pattern works for many cases, there are situations
where your end users may have very little internet connectivity and you therefore
cannot rely on being able to call a cloud-hosted model. In this section, we’ll discuss
two offline-only alternatives, a scenario where a client needs to make many predic‐
tion requests at a time, and suggestions on how to run continuous evaluation for off‐
line models.
<b>Standalonesingle-phasemodel</b>
Sometimes, the end users of your model may have little to no internet connectivity.
Even though these users’ devices won’t be able to reliably access a cloud model, it’s
still important to give them a way to access your application. For this case, rather
than relying on a two-phase prediction flow, you can make your first model robust
enough that it can be self-sufficient.
To do this, we can create a smaller version of our complex model, and give users the
option to download this simpler, smaller model for use when they are offline. These
offline models may not be quite as accurate as their larger online counterparts, but
this solution is infinitely better than having no offline support at all. To build more
complex models designed for offline inference, it’s best to use a tool that allows you
to quantize your model’s weights and other math operations both during and after
training. This is known as <i>quantization</i> <i>aware</i> <i>training.</i>
One example of an application that provides a simpler offline model is Google Trans‐
late. Google Translate is a robust, online translation service available in hundreds of
languages. However, there are many scenarios where you’d need to use a translation
service without internet access. To handle this, Google translate lets you download
offline translations in over 50 different languages. These offline models are small,
around 40 to 50 megabytes, and come close in accuracy to the more complex online
versions. Figure 5-12 shows a quality comparison of on-device and online translation
models."|Google Translate; quantization aware training; Two-Phase Predictions design pattern
"pipeline itself, like adding a new component. We can do both with the TFX CLI. We
can define the scaffolding for our pipeline in a single Python script, which has two
key parts:
• An instance of tfx.orchestration.pipeline where we define our pipeline and the
components it includes.
• An instance of kubeflow_dag_runner from the tfx library. We’ll use this to create
and run our pipeline. In addition to the Kubeflow runner, there’s also an API for
running TFX pipelines with Apache Beam, which we could use to run our pipe‐
line locally.
Our pipeline (see full code in GitHub) will have the five steps or components defined
above, and we can define our pipeline with the following:
pipeline.Pipeline(
pipeline_name='huricane_prediction',
pipeline_root='path/to/pipeline/code',
components=[
bigquery_gen, statistics_gen, schema_gen, train, model_pusher
]
)
BigQueryExampleGen
To use the component provided by TFX, we provide the query
that will fetch our data. We can define this component in one line of code, where
query is our BigQuery SQL query as a string:
bigquery_gen = BigQueryExampleGen(query=query)
Another benefit of using pipelines is that it provides tooling to keep track of the
input, output artifacts, and logs for each component. The output of the statis
tics_gen
component, for example, is a summary of our dataset, which we can see in
Figure 6-7. statistics_gen is a pre-built component available in TFX that uses TF
Data Validation to generate summary statistics on our dataset.
<i>Figure</i> <i>6-7.</i> <i>The</i> <i>output</i> <i>artifact</i> <i>from</i> <i>the</i> <i>statistics_gen</i> <i>component</i> <i>in</i> <i>a</i> <i>TFX</i> <i>pipeline.</i>"|BigQueryExampleGen component; TFX; Workflow Pipeline design pattern
"<i>Figure</i> <i>6-10.</i> <i>The</i> <i>relationship</i> <i>between</i> <i>TFX,</i> <i>Kubeflow</i> <i>Pipelines,</i> <i>Kubeflow,</i> <i>and</i> <i>under‐</i>
<i>lying</i> <i>infrastructure.</i> <i>TFX</i> <i>operates</i> <i>at</i> <i>the</i> <i>highest</i> <i>level</i> <i>on</i> <i>top</i> <i>of</i> <i>Kubeflow</i> <i>Pipelines,</i>
<i>with</i> <i>pre-built</i> <i>components</i> <i>offering</i> <i>specific</i> <i>approaches</i> <i>to</i> <i>common</i> <i>workflow</i> <i>steps.</i>
<i>Kubeflow</i> <i>Pipelines</i> <i>provides</i> <i>an</i> <i>API</i> <i>for</i> <i>defining</i> <i>and</i> <i>orchestrating</i> <i>an</i> <i>ML</i> <i>pipeline,</i> <i>pro‐</i>
<i>viding</i> <i>more</i> <i>flexibility</i> <i>in</i> <i>how</i> <i>each</i> <i>step</i> <i>is</i> <i>implemented.</i> <i>Both</i> <i>TFX</i> <i>and</i> <i>KFP</i> <i>run</i> <i>on</i>
<i>Kubeflow,</i> <i>a</i> <i>platform</i> <i>for</i> <i>running</i> <i>container-based</i> <i>ML</i> <i>workloads</i> <i>on</i> <i>Kubernetes.</i> <i>All</i> <i>of</i>
<i>the</i> <i>tools</i> <i>in</i> <i>this</i> <i>diagram</i> <i>are</i> <i>open</i> <i>source,</i> <i>so</i> <i>the</i> <i>underlying</i> <i>infrastructure</i> <i>where</i> <i>pipe‐</i>
<i>lines</i> <i>run</i> <i>is</i> <i>up</i> <i>to</i> <i>the</i> <i>user—some</i> <i>options</i> <i>include</i> <i>GKE,</i> <i>Anthos,</i> <i>Azure,</i> <i>AWS,</i> <i>or</i>
<i>on-premises.</i>
<b>Developmentversusproductionpipelines</b>
The way a pipeline is invoked often changes as we move from development to pro‐
duction. We’ll likely want to build and prototype our pipeline from a notebook,
where we can re-invoke our pipeline by running a notebook cell, debug errors, and
update code all from the same environment. Once we’re ready to productionize, we
can move our component code and pipeline definition to a single script. With our
pipeline defined in a script, we’ll be able to schedule runs and make it easier for oth‐
ers in our organization to invoke the pipeline in a reproducible way. One tool avail‐
able for productionizing pipelines is Kale, which takes Jupyter notebook code and
converts it into a script using the Kubeflow Pipelines API.
A production pipeline also allows for <i>orchestration</i> of an ML workflow. By orchestra‐
tion, we mean adding logic to our pipeline to determine which steps will be executed,
and what the outcome of those steps will be. For example, we might decide we only"|Kale; orchestration; Workflow Pipeline design pattern
"solid black or white image. In a text model, an uninformative baseline could be 0 val‐
ues for the model’s embedding matrices or stop words like “the,” “is,” or “and.” In a
model with numerical inputs, a common approach to choosing a baseline is to gener‐
ate a prediction using the median value for each feature in the model.
<header><largefont><b>Determining</b></largefont> <largefont><b>Baselines</b></largefont></header>
The way we think about a baseline will differ depending on whether our model is per‐
forming a regression or classification task. For a regression task, a model will have
<i>exactly</i> <i>one</i> numerical baseline prediction value. In our car mileage example, let’s
imagine we decide to use the median approach for calculating our baseline. The
median for the eight features in our dataset is the following array:
[151.0, 93.5, 2803.5, 15.5, 76.0, 1.0, 0.0, 0.0]
When we send this to our model, the predicted MPG is 22.9. Consequently, for every
prediction we make to this model, we’ll use 22.9 MPG as the baseline to compare
predictions.
Let’s now imagine that we follow the Reframing pattern to change this from a regres‐
sion to a classification problem. To do this, we’ll define “low,” “medium,” and “high”
buckets for fuel efficiency, and our model will therefore output a three-element soft‐
max array indicating the probability a given car corresponds with each class. Taking
the same median baseline input as above, our classification model now returns the
following as our baseline prediction:
[0.1, 0.7, 0.2]
With this, we now have a <i>different</i> baseline prediction value for each class. Let’s say
we generate a new prediction on an example from our test set, and our model outputs
the following array, predicting a 90% probability that this car has “low” fuel
efficiency:
[0.9, 0.06, 0.04]
The resulting feature attribution values should explain why the model predicted 0.9
compared to the baseline prediction value of 0.1 for the “low” class. We can also look
at feature attribution values for the other classes to understand, for example, why our
model predicted the same car had a 6% chance of belonging to our “medium” fuel
efficiency class.
Figure 7-2 shows instance-level feature attributions for a model that predicts the
duration of a bike trip. The uninformative baseline for this model is a trip duration of
13.6 minutes, which we get by generating a prediction using the median value for
each feature in our dataset. When a model’s prediction is <i>less</i> <i>than</i> the baseline pre‐
diction value, we should expect most attribution values to be negative, and vice versa.
In this example, we get a predicted duration of 10.71, which is less than the model’s"|baseline; Explainable Predictions design pattern; feature attributions; informative baseline; Reframing design pattern; uninformative baseline
"<i>Figure</i> <i>4-17.</i> <i>Comparison</i> <i>of</i> <i>throughput</i> <i>between</i> <i>different</i> <i>distribution</i> <i>setups.</i> <i>Here,</i>
<i>2W1PS</i> <i>indicates</i> <i>two</i> <i>workers</i> <i>and</i> <i>one</i> <i>parameter</i> <i>server.</i>
<i>Figure</i> <i>4-18.</i> <i>As</i> <i>the</i> <i>number</i> <i>of</i> <i>GPUs</i> <i>increases,</i> <i>the</i> <i>time</i> <i>to</i> <i>convergence</i> <i>during</i> <i>training</i>
<i>decreases.</i>"|Distribution Strategy design pattern
"<i>Figure</i> <i>7-11.</i> <i>A</i> <i>subset</i> <i>of</i> <i>the</i> <i>US</i> <i>mortgage</i> <i>dataset,</i> <i>binned</i> <i>by</i> <i>the</i> <i>agency_code</i> <i>column</i> <i>in</i>
<i>the</i> <i>dataset.</i>
We can repeat this analysis across other columns in our data and use our conclusions
to add examples and improve our data. There are many other options for creating
custom visualizations in the What-If Tool—see the full code on GitHub for more
ideas.
Another way to understand our data using the What-If Tool is through the Features
tab, shown in Figure 7-12. This shows how our data is balanced across each column
in our dataset. From this we can see where we need to add or remove data, or change
our prediction task. 9 For example, maybe we want to limit our model to making pre‐
dictions only on refinancing or home purchase loans since there may not be enough
loan_purpose
data available for other possible values in the column.
9 Tolearnmoreaboutchangingapredictiontask,see“DesignPattern5:Reframing”onpage80and“Design
Pattern9:NeutralClass”onpage117inChapter3."|Fairness Lens design pattern; What-If Tool
"Taking the Stack Overflow dataset example, there will likely be many questions tag‐
ged as both TensorFlow and Keras . But there will also be questions about Keras that
have nothing to do with TensorFlow. Similarly, we might see questions about plot‐
matplotlib pandas,
ting data that is tagged with both and and questions about data
preprocessing that are tagged both pandas and scikit-learn . In order for our model
to learn what is unique to each tag, we’ll want to ensure the training dataset consists
matplotlib
of varied combinations of each tag. If the majority of questions in our
dataset are also tagged pandas , the model won’t learn to classify matplotlib on its
own. To account for this, think about the different relationships between labels that
might be present in our model and count the number of training examples that
belong to each overlapping combination of labels.
When exploriing relationships between labels in our dataset, we may also encounter
hierarchical labels. ImageNet, the popular image classification dataset, contains thou‐
sands of labeled images and is often used as a starting point for transfer learning on
image models. All of the labels used in ImageNet are hierarchical, meaning all images
have at least one label, and many images have more specific labels that are part of a
hierarchy. Here’s an example of one label hierarchy in ImageNet:
animal → invertebrate → arthropod → arachnid → spider
Depending on the size and nature of the dataset, there are two common approaches
for handling hierarchical labels:
• Use a flat approach and put every label in the same output array regardless of
hierarchy, making sure you have enough examples of each “leaf node” label.
• Use the Cascade design pattern. Build one model to identify higher-level labels.
Based on the high-level classification, send the example to a separate model for a
more specific classification task. For example, we might have an initial model
that labels images as “Plant,” “Animal,” or “Person.” Depending on which labels
the first model applies, we’ll send the image to different model(s) to apply more
granular labels like “succulent” or “barbary lion.”
The flat approach is more straightforward than following the Cascade design pattern
since it only requires one model. However, this might cause the model to lose infor‐
mation about more detailed label classes since there will naturally be more training
examples with the higher-level labels in our dataset.
<b>Inputswithoverlappinglabels</b>
The Multilabel design pattern is also useful in cases where input data occasionally has
overlapping labels. Let’s take an image model that’s classifying clothing items for a
catalog as an example. If we have multiple people labeling each image in the training
dataset, one labeler may label an image of a skirt as “maxi skirt,” while another"|binary classification; Cascade design pattern; flat approach; ImageNet; labels; Multilabel design pattern
"learning continues to become more accessible, and one exciting development is the
availability of machine learning models that can be expressed in SQL. We’ll use <i>Big‐</i>
<i>Query</i> <i>ML</i> as an example of this, especially in situations where we want to combine
data preprocessing and model creation.
<i>Figure</i> <i>1-1.</i> <i>A</i> <i>breakdown</i> <i>of</i> <i>different</i> <i>types</i> <i>of</i> <i>machine</i> <i>learning,</i> <i>with</i> <i>a</i> <i>few</i> <i>examples</i> <i>of</i>
<i>each.</i> <i>Note</i> <i>that</i> <i>although</i> <i>it</i> <i>is</i> <i>not</i> <i>included</i> <i>in</i> <i>this</i> <i>diagram,</i> <i>neural</i> <i>networks</i> <i>like</i>
<i>autoencoders</i> <i>can</i> <i>also</i> <i>be</i> <i>used</i> <i>for</i> <i>unsupervised</i> <i>learning.</i>
Conversely, neural networks with only an input and output layer are another subset
of machine learning known as <i>linear</i> <i>models.</i> Linear models represent the patterns
they’ve learned from data using a linear function. <i>Decision</i> <i>trees</i> are machine learning
models that use your data to create a subset of paths with various branches. These
branches approximate the results of different outcomes from your data. Finally, <i>clus‐</i>
<i>tering</i> models look for similarities between different subsets of your data and use
these identified patterns to group data into clusters.
Machine learning problems (see Figure 1-1) can be broken into two types: supervised
and unsupervised learning. <i>Supervised</i> <i>learning</i> defines problems where you know the
ground truth label for your data in advance. For example, this could include labeling
an image as “cat” or labeling a baby as being 2.3 kg at birth. You feed this labeled data
to your model in hopes that it can learn enough to label new examples. With"|clustering; clustering models; decision trees; deep learning; dimensionality reduction; linear models; supervised learning; unsupervised learning
"model API that is capable of handling real-time data streams both for inference and
to collect data that is fed into the automated ML pipeline to refresh the model for
later training.
<i>Figure</i> <i>8-5.</i> <i>Pipelines</i> <i>phase</i> <i>of</i> <i>AI</i> <i>development.</i> <i>Figure</i> <i>adapted</i> <i>from</i> <i>Google</i> <i>Cloud</i> <i>doc‐</i>
<i>umentation.</i>
<b>Transformationalphase:Fullyautomatedprocesses</b>
Organizations in the transformational phase of AI Readiness are actively using AI to
stimulate innovation, support agility, and cultivate a culture where experimentation
and learning is ongoing. Strategic partnerships are used to innovate, co-create, and
augment technical resources within the company. Many of the design patterns
related to reproducibility and resilience in Chapters 5 and 6 arise in this phase of AI
Readiness.
In this phase, it is common to have product-specific AI teams embedded into the
broader product teams and supported by the advanced analytics team. In this way,
ML expertise is able to diffuse across various lines of business within the"|AI readiness
"Instead of trying to predict the amount of rainfall as a regression task, we can reframe
our objective as a classification problem. There are different ways this can be accom‐
plished. One approach is to model a discrete probability distribution, as shown in
Figure 3-1. Instead of predicting rainfall as a real-valued output, we model the output
as a multiclass classification giving the probability that the rainfall in the next 15
minutes is within a certain range of rainfall amounts.
<i>Figure</i> <i>3-1.</i> <i>Instead</i> <i>of</i> <i>predicting</i> <i>precipitation</i> <i>as</i> <i>a</i> <i>regression</i> <i>output,</i> <i>we</i> <i>can</i> <i>instead</i>
<i>model</i> <i>discrete</i> <i>probability</i> <i>distribution</i> <i>using</i> <i>a</i> <i>multiclass</i> <i>classification.</i>
Both the regression approach and this reframed-as-classification approach give a pre‐
diction of the rainfall for the next 15 minutes. However, the classification approach
allows the model to capture the probability distribution of rainfall of different quanti‐
ties instead of having to choose the mean of the distribution. Modeling a distribution
in this way is advantageous since precipitation does not exhibit the typical bell-
shaped curve of a normal distribution and instead follows a Tweedie distribution,
which allows for a preponderance of points at zero. Indeed, that’s the approach taken
in a Google Research paper that predicts precipitation rates in a given location using
a 512-way categorical distribution. Other reasons that modeling a distribution can be
advantageous is when the distribution is bimodal, or even when it is normal but with
a large variance. A recent paper that beats all benchmarks at predicting protein fold‐
ing structure also predicts the distance between amino acids as a 64-way classification
problem where the distances are bucketized into 64 bins.
Another reason to reframe a problem is when the objective is better in the other type
of model. For example, suppose we are trying to build a recommendation system for"|discrete probability distribution; recommendation systems; Reframing design pattern; Tweedie distribution
"<i>Figure</i> <i>5-13.</i> <i>The</i> <i>client</i> <i>supplies</i> <i>a</i> <i>unique</i> <i>key</i> <i>with</i> <i>each</i> <i>input</i> <i>instance.</i> <i>The</i> <i>serving</i> <i>sys‐</i>
<i>tem</i> <i>attaches</i> <i>those</i> <i>keys</i> <i>to</i> <i>the</i> <i>corresponding</i> <i>prediction.</i> <i>This</i> <i>allows</i> <i>the</i> <i>client</i> <i>to</i>
<i>retrieve</i> <i>the</i> <i>correct</i> <i>prediction</i> <i>for</i> <i>each</i> <i>input</i> <i>even</i> <i>if</i> <i>outputs</i> <i>are</i> <i>out</i> <i>of</i> <i>order.</i>
<b>HowtopassthroughkeysinKeras</b>
In order to get your Keras model to pass through keys, supply a serving signature
when exporting the model.
For example, this is the code to take a model that would otherwise take four inputs
(is_male, mother_age, plurality, gestation_weeks)
and and have it also take a key
that it will pass through to the output along with the original output of the model
(the babyweight ):
<i>#</i> <i>Serving</i> <i>function</i> <i>that</i> <i>passes</i> <i>through</i> <i>keys</i>
@tf.function(input_signature=[{
'is_male': tf.TensorSpec([None,], dtype=tf.string, name='is_male'),
'mother_age': tf.TensorSpec([None,], dtype=tf.float32,
name='mother_age'),
'plurality': tf.TensorSpec([None,], dtype=tf.string, name='plurality'),
'gestation_weeks': tf.TensorSpec([None,], dtype=tf.float32,
name='gestation_weeks'),
<b>'key':</b> <b>tf.TensorSpec([None,],</b> <b>dtype=tf.string,</b> <b>name='key')</b>
}])
<b>def</b> keyed_prediction(inputs):
feats = inputs.copy()
key = feats.pop('key') <i>#</i> <i>get</i> <i>the</i> <i>key</i> <i>from</i> <i>input</i>
output = model(feats) <i>#</i> <i>invoke</i> <i>model</i>"|Keyed Predictions design pattern; keys
"<b>Chapter</b> <b>Designpattern</b> <b>Problemsolved</b> <b>Solution</b>
ResponsibleAI Heuristic Explainingmodelperformanceusing CompareanMLmodelagainstasimple,
Benchmark complicatedevaluationmetricsdoes easy-to-understandheuristic.
notprovidetheintuitionthatbusiness
decisionmakersneed.
Explainable Sometimesitisnecessarytoknow Applymodelexplainabilitytechniquesto
Predictions whyamodelmakescertain understandhowandwhymodelsmake
predictionseitherfordebuggingorfor predictionsandimproveusertrustinML
regulatoryandcompliancestandards. systems.
FairnessLens Biascancausemachinelearning Usetoolstoidentifybiasindatasetsbefore
modelstonottreatallusersequally trainingandevaluatetrainedmodels
andcanhaveadverseeffectsonsome throughafairnesslenstoensuremodel
populations. predictionsareequitableacrossdifferent
groupsofusersanddifferentscenarios.
<header><largefont><b>Pattern</b></largefont> <largefont><b>Interactions</b></largefont></header>
Design patterns don’t exist in isolation. Many of them are closely related to one
another either directly or indirectly and often complement one another. The interac‐
tion diagram in Figure 8-1 summarizes the interdependencies and some relationships
between different design patterns. If you find yourself using a pattern, you might
benefit from thinking how you could incorporate other patterns that are related to it.
Here, we’ll highlight some of the ways in which these patterns are related and how
they can be used together when developing a full solution. For example, when work‐
ing with categorical features, the Hashed Feature design pattern may be combined
with the Embeddings design pattern. These two patterns work together to address
high-cardinality model inputs, such as working with text. In TensorFlow, this is
categorical_column_with_hash_bucket
demonstrated by wrapping a feature col‐
umn with an embedding feature column to convert the sparse, categorical text input
to a dense representation:
<b>import</b> <b>tensorflow.feature_column</b> <b>as</b> <b>fc</b>
keywords = fc.categorical_column_with_hash_bucket(""keywords"",
hash_bucket_size=10K)
keywords_embedded = fc.embedding_column(keywords, num_buckets=16)
We saw when discussing Embeddings that this technique is recommended when
using the Feature Cross design pattern. Hashed Features go hand in hand with the
Repeatable Splitting design pattern since the Farm Fingerprint hashing algorithm can
be used for data splitting. And, when using the Hashed Features or Embeddings
design pattern, it’s common to turn to concepts within Hyperparameter Tuning to
determine the optimal number of hash buckets or the right embedding dimension to
use."|Embedding design pattern; Feature Cross design pattern; Hashed Feature design pattern; Hyperparameter Tuning design pattern; Repeatable Splitting design pattern
"videos. A natural way to frame this problem is as a classification problem of predict‐
ing whether a user is likely to watch a certain video. This framing, however, can lead
to a recommendation system that prioritizes click bait. It might be better to reframe
this into a regression problem of predicting the fraction of the video that will be
watched.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Changing the context and reframing the task of a problem can help when building a
machine learning solution. Instead of learning a single real number, we relax our pre‐
diction target to be instead a discrete probability distribution. We lose a little preci‐
sion due to bucketing, but gain the expressiveness of a full probability density
function (PDF). The discretized predictions provided by the classification model are
more adept at learning a complex target than the more rigid regression model.
An added advantage of this classification framing is that we obtain posterior proba‐
bility distribution of our predicted values, which provides more nuanced informa‐
tion. For example, suppose the learned distribution is bimodal. By modeling a
classification as a discrete probability distribution, the model is able to capture the
bimodal structure of the predictions, as Figure 3-2 illustrates. Whereas, if only pre‐
dicting a single numeric value, this information would be lost. Depending on the use
case, this could make the task easier to learn and substantially more advantageous.
<i>Figure</i> <i>3-2.</i> <i>Reframing</i> <i>a</i> <i>classification</i> <i>task</i> <i>to</i> <i>model</i> <i>a</i> <i>probability</i> <i>distribution</i> <i>allows</i>
<i>the</i> <i>predictions</i> <i>to</i> <i>capture</i> <i>bimodal</i> <i>output.</i> <i>The</i> <i>prediction</i> <i>is</i> <i>not</i> <i>limited</i> <i>to</i> <i>a</i> <i>single</i>
<i>value</i> <i>as</i> <i>in</i> <i>a</i> <i>regression.</i>
<b>Capturinguncertainty</b>
Let’s look again at the natality dataset and the task of predicting baby weight. Since
baby weight is a positive real value, this is intuitively a regression problem. However,
weight_pounds
notice that for a given set of inputs, (the label) can take many differ‐
ent values. We see that the distribution of babies’ weights for a specific set of input
values (male babies born to 25-year-old mothers at 38 weeks) approximately follows
a normal distribution centered at about 7.5 pounds. The code to produce the graph in
Figure 3-3 can be found in the repository for this book."|discrete probability distribution; PDF; posterior probability distribution; Reframing design pattern
"of thumb, we’d choose 40. If we are doing hyperparameter tuning, it might be worth
searching within this range.
<b>Autoencoders</b>
Training embeddings in a supervised way can be hard because it requires a lot of
labeled data. For an image classification model like Inception to be able to produce
useful image embeddings, it is trained on ImageNet, which has 14 million labeled
images. Autoencoders provide one way to get around this need for a massive labeled
dataset.
The typical autoencoder architecture, shown in Figure 2-11, consists of a bottleneck
layer, which is essentially an embedding layer. The portion of the network before the
bottleneck (the “encoder”) maps the high-dimensional input into a lower-
dimensional embedding layer, while the latter network (the “decoder”) maps that
representation back to a higher dimension, typically the same dimension as the origi‐
nal. The model is typically trained on some variant of a reconstruction error, which
forces the model’s output to be as similar as possible to the input.
<i>Figure</i> <i>2-11.</i> <i>When</i> <i>training</i> <i>an</i> <i>autoencoder,</i> <i>the</i> <i>feature</i> <i>and</i> <i>the</i> <i>label</i> <i>are</i> <i>the</i> <i>same</i> <i>and</i>
<i>the</i> <i>loss</i> <i>is</i> <i>the</i> <i>reconstruction</i> <i>error.</i> <i>This</i> <i>allows</i> <i>the</i> <i>autoencoder</i> <i>to</i> <i>achieve</i> <i>nonlinear</i>
<i>dimension</i> <i>reduction.</i>
Because the input is the same as the output, no additional labels are needed. The
encoder learns an optimal nonlinear dimension reduction of the input. Similar to
how PCA achieves linear dimension reduction, the bottleneck layer of an autoen‐
coder is able to obtain nonlinear dimension reduction through the embedding.
This allows us to break a hard machine learning problem into two parts. First, we use
all the unlabeled data we have to go from high cardinality to lower cardinality by
using autoencoders as an <i>auxiliary</i> <i>learning</i> <i>task.</i> Then, we solve the actual image
classification problem for which we typically have much less labeled data using the
embedding produced by the auxiliary autoencoder task. This is likely to boost model
performance, because now the model only has to learn the weights for the lower-
dimension setting (i.e., it has to learn fewer weights)."|autoencoders; Embedding design pattern; ImageNet; PCA
"2010-02-03 08:45:00,19.0
Given that the flight above (at 08:45 on February 3) is 19 minutes late, is that unusual
or not? Commonly, to carry out ML inference on a flight, we only need the features
of that flight. In this case, however, the model requires information about all flights
to DFW airport between 06:45 and 08:45:
2010-02-03 06:45:00,?
2010-02-03 06:?:00,?
...
2010-02-03 08:45:00,19.0
It is not possible to carry out inference one flight at a time. We need to somehow pro‐
vide the model information about all the previous flights.
How do we carry out inference when the model requires not just one instance, but a
sequence of instances?
<header><largefont><b>Solution</b></largefont></header>
The solution is to carry out stateful stream processing—that is, stream processing
that keeps track of the model state through time:
• A sliding window is applied to flight arrival data. The sliding window will be over
2 hours, but the window can be closed more often, such as every 10 minutes. In
such a case, aggregate values will be calculated every 10 minutes over the previ‐
ous 2 hours.
• The internal model state (this could be the list of flights) is updated with flight
information every time a new flight arrives, thus building a 2-hour historical
record of flight data.
• Every time the window is closed (every 10 minutes in our example), a time-series
ML model is trained on the 2-hour list of flights. This model is then used to pre‐
dict future flight delays and the confidence bounds of such predictions.
• The time-series model parameters are externalized into a state variable. We could
use a time-series model such as autoregressive integrated moving average
(ARIMA) or long short-term memory (LSTMs), in which case, the model
parameters would be the ARIMA model coefficients or the LSTM model weights.
To keep the code understandable, we will use a zero-order regression model,2
and so our model parameters will be the average flight delay and the variance of
the flight delays over the two-hour window.
2 Inotherwords,wearecomputingtheaverage."|ARIMA; LSTM; stateful stream processing; Windowed Inference design pattern
"data = {
'time': [9,10,2],
'visibility': [0.2, 0.5, 0.1],
'inclement_weather': [[0,0,1], [0,0,1], [1,0,0]],
'location': [[0,1,0,0,0], [0,0,0,1,0], [1,0,0,0,0]]
}
We could then combine these tabular features into a single array for each example, so
that our model’s input shape would be 10. The input array for the first example
would look like the following:
[9, 0.2, 0, 0, 1, 0, 1, 0, 0, 0]
We could feed this input into a Dense fully connected layer, and the output of our
model would be a single value between 0 and 1 indicating whether or not the instance
contains a traffic violation. To combine this with our image data, we’ll use a similar
approach to what we discussed for text models. First, we’d define a convolution layer
to handle our image data, then a Dense layer to handle our tabular data, and finally
we’d concatenate both into a single output.
This approach is outlined in Figure 2-25.
<i>Figure</i> <i>2-25.</i> <i>Concatenating</i> <i>layers</i> <i>to</i> <i>handle</i> <i>image</i> <i>and</i> <i>tabular</i> <i>metadata</i> <i>features.</i>"|Dense layers; Multimodal Input design pattern
"<i>Figure</i> <i>5-8.</i> <i>Training</i> <i>a</i> <i>model</i> <i>on</i> <i>stale</i> <i>data</i> <i>and</i> <i>evaluating</i> <i>on</i> <i>current</i> <i>data</i> <i>mimics</i> <i>the</i>
<i>continued</i> <i>model</i> <i>evaluation</i> <i>process</i> <i>in</i> <i>an</i> <i>offline</i> <i>environment.</i>
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>19:</b></largefont> <largefont><b>Two-Phase</b></largefont> <largefont><b>Predictions</b></largefont></header>
The Two-Phase Predictions design pattern provides a way to address the problem of
keeping large, complex models performant when they have to be deployed on dis‐
tributed devices by splitting the use cases into two phases, with only the simpler
phase being carried out on the edge.
<header><largefont><b>Problem</b></largefont></header>
When deploying machine learning models, we cannot always rely on end users hav‐
ing reliable internet connections. In such situations, models are deployed at the <i>edge</i>
—meaning they are loaded on a user’s device and don’t require an internet connec‐
tion to generate predictions. Given device constraints, models deployed on the edge
typically need to be smaller than models deployed in the cloud, and consequently
require balancing trade-offs between model complexity and size, update frequency,
accuracy, and low latency.
There are various scenarios where we’d want our model deployed on an edge device.
One example is a fitness tracking device, where a model makes recommendations for
users based on their activity, tracked through accelerometer and gyroscope move‐
ment. It’s likely that a user could be exercising in a remote outdoor area without con‐
nectivity. In these cases, we’d still want our application to work. Another example is
an environmental application that uses temperature and other environmental data to
make predictions on future trends. In both of these examples, even if we have inter‐"|edge; Two-Phase Predictions design pattern
"<header><largefont><b>Output</b></largefont> <largefont><b>Layer</b></largefont> <largefont><b>Bias</b></largefont></header>
In conjunction with assigning class weights, it is also helpful to initialize the model’s
output layer with a bias to account for dataset imbalance. Why would we want to
manually set the initial bias for our output layer? When we have imbalanced datasets,
setting the output bias will help our model converge faster. This is because the bias of
the last (prediction) layer of a trained model will output, on average, the log of the
ratio of minority to majority examples in the dataset. By setting the bias, the model
already starts out at the “correct” value without having to discover it through gradient
descent.
By default, Keras uses a bias of zero. This corresponds with the bias we’d want to use
log(1/1) = 0
for a perfectly balanced dataset where . To calculate the correct bias
while taking our dataset balance into account, use:
bias = log(num_minority_examples / num_majority_examples)
<b>Upsampling</b>
Another common technique for handling imbalanced datasets is <i>upsampling.</i> With
upsampling, we overrepresent our minority class by both replicating minority class
examples and generating additional, synthetic examples. This is often done in combi‐
nation with downsampling the majority class. This approach—combining downsam‐
pling and upsampling—was proposed in 2002 and referred to as Synthetic Minority
Over-sampling Technique (SMOTE). SMOTE provides an algorithm that constructs
these synthetic examples by analyzing the feature space of minority class examples in
the dataset and then generates similar examples within this feature space using a
nearest neighbors approach. Depending on how many similar data points we choose
to consider at once (also referred to as the number of nearest neighbors), the SMOTE
approach randomly generates a new minority class example between these points.
Let’s look at the Pima Indian Diabetes Dataset to see how this works at a high level.
34% of this dataset contains examples of patients who had diabetes, so we’ll consider
this our minority class. Table 3-3 shows a subset of columns for two minority class
examples.
<i>Table</i> <i>3-3.</i> <i>A</i> <i>subset</i> <i>of</i> <i>features</i> <i>for</i> <i>two</i> <i>training</i> <i>examples</i> <i>from</i> <i>the</i> <i>minority</i> <i>class</i> <i>(has</i>
<i>diabetes)</i> <i>in</i> <i>the</i> <i>Pima</i> <i>Indian</i> <i>Diabetes</i> <i>Dataset</i>
<b>Glucose</b> <b>BloodPressure</b> <b>SkinThickness</b> <b>BMI</b>
148 72 35 33.6
183 64 0 23.3"|bias; Keras; output layer bias; Rebalancing design pattern; SMOTE; upsampling
"When this model is deployed and used for inference, the output JSON contains both
the logits and the probability:
{'predictions': [
{'positive_review_probability': [0.6674301028251648],
'positive_review_logits': [0.6965846419334412]},
{'positive_review_probability': <b>[0.8344818353652954],</b>
'positive_review_logits': [1.6177300214767456]},
{'positive_review_probability': [0.31987208127975464],
'positive_review_logits': [-0.754359781742096]}
]}
Note that add_prob is a function that we write. In this case, we did a bit of postpro‐
cessing of the output. However, we could have done pretty much any (stateless) thing
that we wanted inside that function.
<b>Multiplesignatures</b>
It is quite common for models to support multiple objectives or clients who have dif‐
ferent needs. While outputting a dictionary can allow different clients to pull out
whatever they want, this may not be ideal in some cases. For example, the function
tf.sigmoid()
we had to invoke to get a probability from the logits was simply . This
is pretty inexpensive, and there is no problem with computing it even for clients who
will discard it. On the other hand, if the function had been expensive, computing it
for clients who don’t need the value can add considerable overhead.
If a small number of clients require a very expensive operation, it is helpful to provide
multiple serving signatures and have the client inform the serving framework which
signature to invoke. This is done by specifying a name other than serving_default
when the model is exported. For example, we might write out two signatures using:
model.save(export_path, signatures={
'serving_default': func1,
'expensive_result': func2,
})
Then, the input JSON request includes the signature name to choose which serving
endpoint of the model is desired:
{
<b>""signature_name"":</b> ""expensive_result"",
{""instances"": …}
}
<b>Onlineprediction</b>
Because the exported serving function is ultimately just a file format, it can be used to
provide online prediction capabilities when the original machine learning training
framework does not natively support online predictions."|Stateless Serving Function design pattern
"Another, more sophisticated approach is to use a framework like TensorFlow Proba‐
bility to carry out regression. However, we have to explicitly model the distribution of
the output. For example, if the output is expected to be normally distributed around a
mean that’s dependent on the inputs, the model’s output layer would be:
tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1))
On the other hand, if we know the variance increases with the mean, we might be
able to model it using the lambda function. Reframing, on the other hand, doesn’t
require us to model the posterior distribution.
When training any machine learning model, the data is key. More
complex relationships typically require more training data exam‐
ples to find those elusive patterns. With that in mind, it is impor‐
tant to consider how data requirements compare for regression or
classification models. A common rule of thumb for classification
tasks is that we should have 10 times the number of model features
for each label category. For a regression model, the rule of thumb is
50 times the number of model features. Of course, these numbers
are just rough heuristics and not precise. However, the intuition is
that regression tasks typically require more training examples. Fur‐
thermore, this need for massive data only increases with the com‐
plexity of the task. Thus, there could be data limitations that should
be considered when considering the type of model used or, in the
case of classification, the number of label categories.
<b>Precisionofpredictions</b>
When thinking of reframing a regression model as a multiclass classification, the
width of the bins for the output label governs the precision of the classification
model. In the case of our baby weight example, if we needed more precise informa‐
tion from the discrete probability density function, we would need to increase the
number of bins of our categorical model. Figure 3-4 shows how the discrete probabil‐
ity distributions would look as either a 4-way or 10-way classification."|Reframing design pattern; TensorFlow Probability
"identifies it as “pleated skirt.” Both are correct. However, if we build a multiclass clas‐
sification model on this data, passing it multiple examples of the same image with
different labels, we’ll likely encounter situations where the model labels similar
images differently when making predictions. Ideally, we want a model that labels this
image as both “maxi skirt” and “pleated skirt” as seen in Figure 3-10, rather than
sometimes predicting only one of these labels.
<i>Figure</i> <i>3-10.</i> <i>Using</i> <i>input</i> <i>from</i> <i>multiple</i> <i>labelers</i> <i>to</i> <i>create</i> <i>overlapping</i> <i>labels</i> <i>in</i> <i>cases</i>
<i>where</i> <i>multiple</i> <i>descriptions</i> <i>of</i> <i>an</i> <i>item</i> <i>are</i> <i>correct.</i>
The Multilabel design pattern solves this by allowing us to associate both overlapping
labels with an image. In cases with overlapping labels where we have multiple labelers
evaluating each image in our training dataset, we can choose the maximum number
of labels we’d like labelers to assign to a given image, then take the most commonly
chosen tags to associate with an image during training. The threshold for “most com‐
monly chosen tags” will depend on our prediction task and the number of human
labelers we have. For example, if we have 5 labelers evaluating every image and 20
possible tags for each image, we might encourage labelers to give each image 3 tags.
From this list of 15 label “votes” per image, we could then choose the 2 to 3 with the
most votes from the labelers. When evaluating this model, we need to take note of the
average prediction confidence the model returns for each label and use this to itera‐
tively improve our dataset and label quality.
<b>Oneversusrest</b>
Another technique for handling Multilabel classification is to train multiple binary
classifiers instead of one multilabel model. This approach is called <i>one</i> <i>versus</i> <i>rest.</i> In
the case of the Stack Overflow example where we want to tag questions as Tensor‐
Flow, Python, and pandas, we’d train an individual classifier for each of these three
tags: Python or not, TensorFlow or not, and so forth. Then we’d choose a confidence
threshold and tag the original input question with tags from each binary classifier
above some threshold."|binary classification; binary classifier; confidence; labels; Multilabel design pattern; one versus rest approach
"evolutionary theory of natural selection. This theory, also known as “survival of the
fittest,” posits that the highest-performing (“fittest”) members of a population will
survive and pass their genes to future generations, while less-fit members will not.
Genetic algorithms have been applied to different types of optimization problems,
including hyperparameter tuning.
As it relates to hyperparameter search, a genetic approach works by first defining a
<i>fitness</i> <i>function.</i> This function measures the quality of a particular trial, and can typi‐
cally be defined by your model’s optimization metric (accuracy, error, and so on).
After defining your fitness function, you randomly select a few combinations of
hyperparameters from your search space and run a trial for each of those combina‐
tions. You then take the hyperparameters from the trials that performed best, and use
those values to define your new search space. This search space becomes your new
“population,” and you use it to generate new combinations of values to use in your
next set of trials. You continue this process, narrowing down the number of trials you
run until you’ve arrived at a result that satisfies your requirements.
Because they use the results of previous trials to improve, genetic algorithms are
“smarter” than manual, grid, and random search. However, when the hyperparame‐
ter search space is large, the complexity of genetic algorithms increases. Rather than
using a surrogate function as a proxy for model training like in Bayesian optimiza‐
tion, genetic algorithms require training your model for each possible combination of
hyperparameter values. Additionally, at the time of writing, genetic algorithms are
less common and there are fewer ML frameworks that support them out of the box
for hyperparameter tuning.
<header><largefont><b>Summary</b></largefont></header>
This chapter focused on design patterns that modify the typical SGD training loop of
machine learning. We started with looking at the <i>Useful</i> <i>Overfitting</i> pattern, which
covered situations where overfitting is beneficial. For example, when using data-
driven methods like machine learning to approximate solutions to complex dynami‐
cal systems or PDEs where the full input space can be covered, overfitting on the
training set is the goal. Overfitting is also useful as a technique when developing and
debugging ML model architectures. Next, we covered model <i>Checkpoints</i> and how to
use them when training ML models. In this design pattern, we save the full state of
the model periodically during training. These checkpoints can be used as the final
model, as in the case of early stopping, or used as the starting points in the case of
training failures or fine-tuning.
The <i>Transfer</i> <i>Learning</i> design pattern covered reusing parts of a previously trained
model. Transfer learning is a useful way to leverage the learned feature extraction lay‐
ers of the pre-trained model when your own dataset is limited. It can also be used to
fine-tune a pre-trained model that was trained on a large generic dataset to your"|fitness function; genetic algorithms; Hyperparameter Tuning design pattern
"<i>Figure</i> <i>8-1.</i> <i>Many</i> <i>of</i> <i>the</i> <i>patterns</i> <i>discussed</i> <i>in</i> <i>this</i> <i>book</i> <i>are</i> <i>related</i> <i>or</i> <i>can</i> <i>be</i> <i>used</i>
<i>together.</i> <i>This</i> <i>image</i> <i>is</i> <i>available</i> <i>in</i> <i>the</i> <i>GitHub</i> <i>repository</i> <i>for</i> <i>this</i> <i>book.</i>
In fact, the Hyperparameter Tuning design is a common part of the machine learning
workflow and is often used in conjunction with other patterns. For example, we
might use hyperparameter tuning to determine the number of older examples to use
if we’re implementing the Bridged Schema pattern. And, when using hyperparameter
tuning, it’s important to keep in mind how we’ve set up model Checkpoints using
virtual epochs and Distributed Training. Meanwhile, the Checkpoints design pattern
naturally connects to Transfer Learning since earlier model checkpoints are often
used during fine-tuning.
Embeddings show up throughout machine learning, so there are many ways in which
the Embeddings design pattern interacts with other patterns. Perhaps the most nota‐
ble is Transfer Learning since the output generated from the intermediate layers of a
pre-trained model are essentially learned feature embeddings. We also saw how
incorporating the Neutral Class design pattern in a classification model, either natu‐
rally or through the Reframing pattern, can improve those learned embeddings. Fur‐
ther downstream, if those embeddings are used as features for a model, it could be
advantageous to save them using the Feature Store pattern so they can be easily
accessed and versioned. Or, in the case of Transfer Learning, the pre-trained model
output could be viewed as the initial output of a Cascade pattern."|Bridged Schema design pattern; Cascade design pattern; Checkpoints design pattern; Embedding design pattern; epochs; Feature Store design pattern; Neutral Class design pattern; Reframing design pattern; Transfer Learning design pattern
"<b>SELECT</b>
*
<b>FROM</b>
ML.RECOMMEND(MODEL mlpatterns.recommendation_model)
Store it in a relational database such as MySQL, Datastore, or Cloud Spanner (there
are pre-built transfer services and Dataflow templates that can do this). When any
user visits, the recommendations for that user are pulled from the database and
served immediately and at very low latency.
In the background, the recommendations are refreshed periodically. For example, we
might retrain the recommendation model hourly based on the latest actions on the
website. We can then carry out inference for just those users who visited in the last
hour:
<b>SELECT</b>
*
<b>FROM</b>
ML.RECOMMEND(MODEL mlpatterns.recommendation_model,
(
<b>SELECT</b> <b>DISTINCT</b>
visitorId
<b>FROM</b>
mlpatterns.analytics_session_data
<b>WHERE</b>
visitTime > TIME_DIFF(CURRENT_TIME(), 1 HOUR)
))
We can then update the corresponding rows in the relational database used for
serving.
<b>Lambdaarchitecture</b>
A production ML system that supports both online serving and batch serving is
called a <i>Lambda</i> <i>architecture—such</i> a production ML system allows ML practitioners
to trade-off between latency (via the Stateless Serving Function pattern) and through‐
put (via the Batch Serving pattern).
AWS Lambda, in spite of its name, is not a Lambda architecture. It
is a serverless framework for scaling stateless functions, similar to
Google Cloud Functions or Azure Functions.
Typically, a Lambda architecture is supported by having separate systems for online
serving and batch serving. In Google Cloud, for example, the online serving infra‐
structure is provided by Cloud AI Platform Predictions and the batch serving infra‐
structure is provided by BigQuery and Cloud Dataflow (Cloud AI Platform
Predictions provides a convenient interface so that users don’t have to explicitly use"|AWS Lambda; batch serving; Batch Serving design pattern; BigQuery; Cloud AI Platform Predictions; Cloud Dataflow; Cloud Spanner; Datastore; Lambda architecture; MySQL; SavedModel
"Now that we’ve passed the tool our model, the resulting visualization shown in
Figure 7-13 plots our test datapoints according to our model’s prediction confidence
indicated on the y-axis.
<i>Figure</i> <i>7-13.</i> <i>The</i> <i>What-If</i> <i>Tool’s</i> <i>Datapoint</i> <i>editor</i> <i>for</i> <i>a</i> <i>binary</i> <i>classification</i> <i>model.</i> <i>The</i>
<i>y-axis</i> <i>is</i> <i>the</i> <i>model’s</i> <i>prediction</i> <i>output</i> <i>for</i> <i>each</i> <i>datapoint,</i> <i>ranging</i> <i>from</i> <i>0</i> <i>(denied)</i> <i>to</i> <i>1</i>
<i>(approved).</i>
The What-If Tool’s Performance & Fairness tab lets us evaluate our model’s fairness
across different data slices. By selecting one of our model’s features to “Slice by,” we
can compare our model’s results for different values of this feature. In Figure 7-14,
we’ve sliced by the agency_code_HUD feature—a boolean value indicating whether an
application was underwritten by HUD (0 for non-HUD loans, 1 for HUD loans).
<i>Figure</i> <i>7-14.</i> <i>The</i> <i>What-If</i> <i>Tool</i> <i>Performance</i> <i>&</i> <i>Fairness</i> <i>tab,</i> <i>showing</i> <i>our</i> <i>XGBoost</i>
<i>model</i> <i>performance</i> <i>across</i> <i>different</i> <i>feature</i> <i>values.</i>"|bias; Fairness Lens design pattern; What-If Tool
"model.compile(optimizer='adam',
loss=tf.keras.losses.BinaryCrossentropy(
from_logits=True),
metrics=['accuracy'])
When we use the model for prediction, the model naturally returns what it was
trained to predict and outputs the logits. What clients expect, however, is the proba‐
bility that the review is positive. To solve this, we need to return the sigmoid output
of the model.
We can do this by writing a custom serving function and exporting it instead. Here is
a custom serving function in Keras that adds a probability and returns a dictionary
that contains both the logits and the probabilities for each of the reviews provided as
input:
@tf.function(input_signature=[tf.TensorSpec([None],
dtype=tf.string)])
<b>def</b> add_prob(reviews):
logits = model(reviews, training=False) <i>#</i> <i>call</i> <i>model</i>
probs = tf.sigmoid(logits)
<b>return</b> {
'positive_review_logits' : logits,
'positive_review_probability' : probs
}
We can then export the above function as the serving default:
model.save(export_path,
signatures={'serving_default': add_prob})
add_prob
The method definition is saved in the export_path and will be invoked in
response to a client request.
The serving signature of the exported model reflects the new input name (note the
name of the input parameter to add_prob ) and the output dictionary keys and data
types:
The given SavedModel SignatureDef contains the following input(s):
inputs['reviews'] tensor_info:
dtype: DT_STRING
shape: (-1)
name: serving_default_reviews:0
The given SavedModel SignatureDef contains the following output(s):
outputs['positive_review_logits'] tensor_info:
dtype: DT_FLOAT
shape: (-1, 1)
name: StatefulPartitionedCall_2:0
outputs['positive_review_probability'] tensor_info:
dtype: DT_FLOAT
shape: (-1, 1)
name: StatefulPartitionedCall_2:1
Method name <b>is:</b> tensorflow/serving/predict"|Stateless Serving Function design pattern
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>3</b></largefont></header>
<header><largefont><b>Problem</b></largefont> <largefont><b>Representation</b></largefont> <largefont><b>Design</b></largefont> <largefont><b>Patterns</b></largefont></header>
Chapter 2 looked at design patterns that catalog the myriad ways in which inputs to
machine learning models can be represented. This chapter looks at different types of
machine learning problems and analyzes how the model architectures vary depend‐
ing on the problem.
The input and the output types are two key factors impacting the model architecture.
For instance, the output in supervised machine learning problems can vary depend‐
ing on whether the problem being solved is a classification or regression problem.
Special neural network layers exist for specific types of input data: convolutional lay‐
ers for images, speech, text, and other data with spatiotemporal correlation, recurrent
networks for sequential data, and so on. A huge literature has arisen around special
techniques such as max pooling, attention, and so forth on these types of layers. In
addition, special classes of solutions have been crafted for commonly occurring prob‐
lems like recommendations (such as matrix factorization) or time-series forecasting
(for example, ARIMA). Finally, a group of simpler models together with common
idioms can be used to solve more complex problems—for example, text generation
often involves a classification model whose outputs are postprocessed using a beam
search algorithm.
To limit our discussion and stay away from areas of active research, we will ignore
patterns and idioms associated with specialized machine learning domains. Instead,
we will focus on regression and classification and examine patterns with problem
representation in just these two types of ML models.
The <i>Reframing</i> design pattern takes a solution that is intuitively a regression problem
and poses it as a classification problem (and vice versa). The <i>Multilabel</i> design pat‐
tern handles the case that training examples can belong to more than one class. The
<i>Cascade</i> design pattern addresses situations where a machine learning problem can
be profitably broken into a series (or cascade) of ML problems. The <i>Ensemble</i> design"|ARIMA; beam search algorithm; Cascade design pattern; Ensemble design pattern; matrix factorization; Multilabel design pattern; Reframing design pattern
"<i>Figure</i> <i>3-5.</i> <i>The</i> <i>precision</i> <i>of</i> <i>the</i> <i>regression</i> <i>is</i> <i>indicated</i> <i>by</i> <i>the</i> <i>sharpness</i> <i>of</i> <i>the</i> <i>probabil‐</i>
<i>ity</i> <i>density</i> <i>function</i> <i>for</i> <i>a</i> <i>fixed</i> <i>set</i> <i>of</i> <i>input</i> <i>values.</i>
<b>Restrictingthepredictionrange</b>
Another reason to reframe the problem is when it is essential to restrict the range of
the prediction output. Let’s say, for example, that realistic output values for a regres‐
sion problem are in the range [3, 20]. If we train a regression model where the output
layer is a linear activation function, there is always the possibility that the model pre‐
dictions will fall outside this range. One way to limit the range of the output is to
reframe the problem.
Make the activation function of the last-but-one layer a sigmoid function (which is
typically associated with classification) so that it is in the range [0,1] and have the last
layer scale these values to the desired range:
MIN_Y = 3
MAX_Y = 20
input_size = 10
inputs = keras.layers.Input(shape=(input_size,))
h1 = keras.layers.Dense(20, 'relu')(inputs)
h2 = keras.layers.Dense(1, 'sigmoid')(h1) <i>#</i> <i>0-1</i> <i>range</i>
output = keras.layers.Lambda(
<b>lambda</b> y : (y*(MAX_Y-MIN_Y) + MIN_Y))(h2) <i>#</i> <i>scaled</i>
model = keras.Model(inputs, output)
We can verify (see the notebook on GitHub for full code) that this model now emits
numbers in the range [3, 20]. Note that because the output is a sigmoid, the model
will never actually hit the minimum and maximum of the range, and only get quite
close to it. When we trained the model above on some random data, we got values in
the range [3.03, 19.99]."|Reframing design pattern
"Within the discovery stage, it can be helpful to do a few modeling experiments to see
if there really is “signal in the noise.” At this point, it could be beneficial to perform a
machine learning feasibility study (Step 3). Just as it sounds, this is typically a short
technical sprint spanning only a few weeks whose goal is to assess the viability of the
data for solving the problem. This provides a chance to explore options for framing
the machine learning problem, experiment with algorithm selection, and learn which
feature engineering steps would be most beneficial. The feasibility study step in the
discovery stage is also a good point at which to create a Heuristic Benchmark (see
Chapter 7).
<b>Development</b>
After agreeing on key evaluation metrics and business KPIs, the development stage of
the machine learning life cycle begins. The details of developing an ML model are
covered in detail in many machine learning resources. Here, we highlight the key
components.
During the development stage, we begin by building data pipelines and engineering
features (Step 4 of Figure 8-2) to process the data inputs that will be fed to the model.
The data collected in real-world applications can have many issues such as missing
values, invalid examples, or duplicate data points. Data pipelines are needed to pre‐
process these data inputs so that they can be used by the model. Feature engineering
is the process of transforming raw input data into features that are more closely
aligned with the model’s learning objective and expressed in a format that can be fed
to the model for training. Feature engineering techniques can involve bucketizing
inputs, converting between data formats, tokenizing and stemming text, creating cat‐
egorical features or one-hot encoding, hashing inputs, creating feature crosses and
feature embeddings, and many others. Chapter 2 of this book discusses Data Repre‐
sentation design patterns and covers many data aspects that arise during this stage of
the ML life cycle. Chapter 5 and Chapter 6 describe patterns related to resilience and
reproducibility in ML systems, which help in building data pipelines.
This step may also involve engineering the labels for the problem and design deci‐
sions related to how the problem is represented. For example, for time-series prob‐
lems, this may involve creating feature windows and experimenting with lag times
and the size of label intervals. Or perhaps it’s helpful to reframe a regression problem
as a classification and change the representation of the labels entirely. Or maybe it is
necessary to employ rebalancing techniques, if the distribution of output classes is
overrepresented by a single class. Chapter 3 of this book is focused on problem repre‐
sentation and addresses these and other important design patterns that are related to
problem framing."|heuristic benchmark; KPI; machine learning feasibility study; ML life cycle
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>2</b></largefont></header>
<header><largefont><b>Data</b></largefont> <largefont><b>Representation</b></largefont> <largefont><b>Design</b></largefont> <largefont><b>Patterns</b></largefont></header>
At the heart of any machine learning model is a mathematical function that is defined
to operate on specific types of data only. At the same time, real-world machine learn‐
ing models need to operate on data that may not be directly pluggable into the math‐
ematical function. The mathematical core of a decision tree, for example, operates on
boolean variables. Note that we are talking here about the mathematical core of a
decision tree—decision tree machine learning software will typically also include
functions to learn an optimal tree from data and ways to read in and process different
types of numeric and categorical data. The mathematical function (see Figure 2-1)
that underpins a decision tree, however, operates on boolean variables and uses oper‐
ations such as AND (&& in Figure 2-1) and OR (+ in Figure 2-1).
<i>Figure</i> <i>2-1.</i> <i>The</i> <i>heart</i> <i>of</i> <i>a</i> <i>decision</i> <i>tree</i> <i>machine</i> <i>learning</i> <i>model</i> <i>to</i> <i>predict</i> <i>whether</i> <i>or</i>
<i>not</i> <i>a</i> <i>baby</i> <i>requires</i> <i>intensive</i> <i>care</i> <i>is</i> <i>a</i> <i>mathematical</i> <i>model</i> <i>that</i> <i>operates</i> <i>on</i> <i>boolean</i>
<i>variables.</i>"|boolean variables; decision trees
"Note that the 1s in this array correspond with the indices of <i>dataframe,</i> <i>graph,</i> and
<i>plot,</i> respectively. To summarize, Figure 2-21 shows how we transformed our input
from raw text to a BOW-encoded array based on our vocabulary.
Keras has some utility methods for encoding text as a bag of words, so we don’t need
to write the code for identifying the top words from our text corpus and encoding
raw text into multi-hot arrays from scratch.
<i>Figure</i> <i>2-21.</i> <i>Raw</i> <i>input</i> <i>text</i> <i>→</i> <i>identifying</i> <i>words</i> <i>present</i> <i>in</i> <i>this</i> <i>text</i> <i>from</i> <i>our</i> <i>vocabu‐</i>
<i>lary</i> <i>→</i> <i>transforming</i> <i>to</i> <i>a</i> <i>multi-hot</i> <i>BOW</i> <i>encoding.</i>
Given that there are two different approaches for representing text (Embedding and
BOW), which approach should we choose for a given task? As with many aspects of
machine learning, this depends on our dataset, the nature of our prediction task, and
the type of model we’re planning to use.
Embeddings add an extra layer to our model and provide extra information about
word meaning that is not available from the BOW encoding. However, embeddings
require training (unless we can use a pre-trained embedding for our problem). While
a deep learning model may achieve higher accuracy, we can also try using BOW
encoding in a linear regression or decision-tree model using frameworks like scikit-
learn or XGBoost. Using BOW encoding with a simpler model type can be useful for
fast prototyping or to verify that the prediction task we’ve chosen will work on our
dataset. Unlike embeddings, BOW doesn’t take into account the order or meaning of
words in a text document. If either of these are important to our prediction task,
embeddings may be the best approach.
There may also be benefits to building a deep model that combines <i>both</i> bag of words
<i>and</i> text embedding representations to extract more patterns from our data. To do
this, we can use the Multimodal Input approach, except that instead of concatenating
text and tabular features, we can concatenate the Embedding and BOW representa‐
tions (see code on GitHub). Here, the shape of our Input layer would be the vocabu‐
lary size of the BOW representation. Some benefits of representing text in multiple
ways include:"|BOW encoding; Multimodal Input design pattern; scikit-learn; Stack Overflow; vocabulary; XGBoost
"For performance reasons, the translation model will be set up to be stateless and
require the user to provide the context. For example, if the model is stateless, instan‐
ces of the model can be autoscaled in response to increased traffic, and can be
invoked in parallel to obtain faster translations. Thus, the translation of the famous
soliloquy from Shakespeare’s Hamlet into German might follow these steps, picking
off in the middle where the bolded word is the one to be translated:
<b>Input(9words,4oneitherside)</b> <b>Output</b>
Theundiscoveredcountry,fromwhosebournNotravellerreturns dessen
undiscoveredcountry,fromwhosebournNotravellerreturns,puzzles Bourn
country,fromwhosebournNotravellerreturns,puzzlesthe Kein
fromwhosebournNotravellerreturns,puzzlesthewill, Reisender
The client, therefore, will need a streaming pipeline. The pipeline could take the
input English text, tokenize it, send along nine tokens at a time, collect the outputs,
and concatenate them into German sentences and paragraphs.
Most sequence models, such as recurrent neural networks and LSTMs, require
streaming pipelines for high-performance inference.
<b>Statefulfeatures</b>
The Windowed Inference pattern can be useful if an input feature to the model
requires state, even if the model itself is stateless. For example, suppose we are train‐
ing a model to predict arrival delays, and one of the inputs to the model is the depar‐
ture delay. We might want to include, as an input to the model, the average departure
delay of flights from that airport in the past two hours.
During training, we can create the dataset using a SQL window function:
<b>WITH</b> <b>data</b> <b>AS</b> (
<b>SELECT</b>
SAFE.PARSE_DATETIME('%Y-%m-%d-%H%M',
CONCAT(CAST(date <b>AS</b> STRING), '-',
FORMAT('%04d', departure_schedule))
) <b>AS</b> scheduled_depart_time,
arrival_delay,
departure_delay,
departure_airport
<b>FROM</b> `bigquery-samples.airline_ontime_data.flights`
<b>WHERE</b> arrival_airport = 'DFW'
),
<b>SELECT</b>
* <b>EXCEPT(scheduled_depart_time),</b>
<b>EXTRACT(hour</b> <b>from</b> scheduled_depart_time) <b>AS</b> hour_of_day,
<b>AVG(departure_delay)</b> OVER (depart_time_window) <b>AS</b> avg_depart_delay
<b>FROM</b> <b>data</b>"|LSTM; Windowed Inference design pattern
"<i>BigQuery</i> is an enterprise data warehouse designed for analyzing large datasets
quickly with SQL. We’ll use BigQuery in our examples for data collection and feature
engineering. Data in BigQuery is organized by Datasets, and a Dataset can have mul‐
tiple Tables. Many of our examples will use data from <i>Google</i> <i>Cloud</i> <i>Public</i> <i>Datasets,</i> a
set of free, publicly available data hosted in BigQuery. Google Cloud Public Datasets
consists of hundreds of different datasets, including NOAA weather data since 1929,
Stack Overflow questions and answers, open source code from GitHub, natality data,
and more. To build some of the models in our examples, we’ll use <i>BigQuery</i> <i>Machine</i>
<i>Learning</i> (or BigQuery ML). BigQuery ML is a tool for building models from data
stored in BigQuery. With BigQuery ML, we can train, evaluate, and generate predic‐
tions on our models using SQL. It supports classification and regression models,
along with unsupervised clustering models. It’s also possible to import previously
trained TensorFlow models to BigQuery ML for prediction.
<i>Cloud</i> <i>AI</i> <i>Platform</i> includes a variety of products for training and serving custom
machine learning models on Google Cloud. In our examples, we’ll be using AI Plat‐
form Training and AI Platform Prediction. AI Platform Training provides infrastruc‐
ture for training machine learning models on Google Cloud. With AI Platform
Prediction, you can deploy your trained models and generate predictions on them
using an API. Both services support TensorFlow, scikit-Learn, and XGBoost models,
along with custom containers for models built with other frameworks. We’ll also ref‐
erence <i>Explainable</i> <i>AI,</i> a tool for interpreting the results of your model’s predictions,
available for models deployed to AI Platform.
<header><largefont><b>Roles</b></largefont></header>
Within an organization, there are many different job roles relating to data and
machine learning. Below we’ll define a few common ones referenced frequently
throughout the book. This book is targeted primarily at data scientists, data engi‐
neers, and ML engineers, so let’s start with those.
A <i>data</i> <i>scientist</i> is someone focused on collecting, interpreting, and processing data‐
sets. They run statistical and exploratory analysis on data. As it relates to machine
learning, a data scientist may work on data collection, feature engineering, model
building, and more. Data scientists often work in Python or R in a notebook
environment, and are usually the first to build out an organization’s machine learn‐
ing models.
A <i>data</i> <i>engineer</i> is focused on the infrastructure and workflows powering an organi‐
zation’s data. They might help manage how a company ingests data, data pipelines,
and how data is stored and transferred. Data engineers implement infrastructure and
pipelines around data.
<i>Machine</i> <i>learning</i> <i>engineers</i> do similar tasks to data engineers, but for ML models.
They take models developed by data scientists, and manage the infrastructure and"|AI Platform Prediction; AI Platform Training; BigQuery; BigQuery ML; Cloud AI Platform; data engineers; data scientists; Explainable AI; Google Cloud Public Datasets; ML engineers
"pixels, it doesn’t rain. In such a case, it can be helpful to create a stacked classification
model followed by a regression model:
1. First, predict whether or not it is going to rain.
2. For pixels where the model predicts rain is not likely, predict a rainfall amount of
zero.
3. Train a regression model to predict the rainfall amount on pixels where the
model predicts that rain is likely.
It is critical to realize that the classification model is not perfect, and so the regression
model has to be trained on the pixels that the classification model predicts as likely to
be raining (and not just on pixels that correspond to rain in the labeled dataset). For
complementary solutions to this problem, also see the discussions on “Design Pattern
10: Rebalancing ” on page 122 and “Design Pattern 5: Reframing ” on page 80.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>9:</b></largefont> <largefont><b>Neutral</b></largefont> <largefont><b>Class</b></largefont></header>
In many classification situations, creating a neutral class can be helpful. For example,
instead of training a binary classifier that outputs the probability of an event, train a
three-class classifier that outputs disjoint probabilities for Yes, No, and Maybe. Dis‐
joint here means that the classes do not overlap. A training pattern can belong to only
one class, and so there is no overlap between Yes and Maybe, for example. The
Maybe in this case is the neutral class.
<header><largefont><b>Problem</b></largefont></header>
Imagine that we are trying to create a model that provides guidance on pain relievers.
There are two choices, ibuprofen and acetaminophen, 2 and it turns out in our histori‐
cal dataset that acetaminophen tends to be prescribed preferentially to patients at risk
of stomach problems, and ibuprofen tends to be prescribed preferentially to patients
at risk of liver damage. Beyond that, things tend to be quite random; some physicians
default to acetaminophen and others to ibuprofen.
Training a binary classifier on such a dataset will lead to poor accuracy because the
model will need to get the essentially arbitrary cases correct.
2 Thisisjustanexamplebeingusedforillustrativepurposes;pleasedon’ttakethisasmedicaladvice!"|binary classifier; Cascade design pattern; Neutral Class design pattern
"SELECT
IF(apgar_1min = 10, 'Healthy',
IF(apgar_1min >= 8, 'Neutral', 'NeedsAttention')) AS health,
plurality,
mother_age,
gestation_weeks,
ever_born
FROM `bigquery-public-data.samples.natality`
WHERE apgar_1min <= 10
This model achieves an accuracy of 0.79 on a held-out evaluation dataset, much
higher than the 0.56 that was achieved with two classes.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The Neutral Class design pattern is one to keep in mind at the beginning of a
machine learning problem. Collect the right data, and we can avoid a lot of sticky
problems down the line. Here are a few situations where having a neutral class can be
helpful.
<b>Whenhumanexpertsdisagree</b>
The neutral class is helpful in dealing with disagreements among human experts.
Suppose we have human labelers to whom we show patient history and ask them
what medication they would prescribe. We might have a clear signal for acetamino‐
phen in some cases, a clear signal for ibuprofen in other cases, and a huge swath of
cases for which human labelers disagree. The neutral class provides a way to deal with
such cases.
In the case of human labeling (unlike with the historical dataset of actual doctor
actions where a patient was seen by only one doctor), every pattern is labeled by mul‐
tiple experts. Therefore, we know a priori which cases humans disagree about. It
might seem far simpler to simply discard such cases, and simply train a binary classi‐
fier. After all, it doesn’t matter what the model does on the neutral cases. This has two
problems:
1. False confidence tends to affect the acceptance of the model by human experts. A
model that outputs a neutral determination is often more acceptable to experts
than a model that is wrongly confident in cases where the human expert would
have chosen the alternative.
2. If we are training a cascade of models, then downstream models will be
extremely sensitive to the neutral classes. If we continue to improve this model,
downstream models could change dramatically from version to version."|confidence; Neutral Class design pattern
"In practice, we’d have a larger dataset and would want to calculate global-level attri‐
butions on more examples. We could then use this analysis to summarize the behav‐
ior on our model to other stakeholders within and outside our organization.
<i>Figure</i> <i>7-4.</i> <i>An</i> <i>example</i> <i>of</i> <i>global-level</i> <i>feature</i> <i>attributions</i> <i>for</i> <i>the</i> <i>fuel</i> <i>efficiency</i> <i>model,</i>
<i>calculated</i> <i>on</i> <i>the</i> <i>first</i> <i>10</i> <i>examples</i> <i>from</i> <i>the</i> <i>test</i> <i>dataset.</i>
<b>Explanationsfromdeployedmodels</b>
SHAP provides an intuitive API for getting attributions in Python, typically used in a
script or notebook environment. This works well during model development, but
there are scenarios where you’d want to get explanations on a deployed model in
addition to the model’s prediction output. In this case, cloud-based explainability
tools are the best option. Here, we’ll demonstrate how to get feature attributions on a
deployed model using Google Cloud’s Explainable AI. At the time of this writing,
Explainable AI works with custom TensorFlow models and tabular data models built
with AutoML.
We’ll deploy an image model to AI Platform to show explanations, but we could also
use Explainable AI with TensorFlow models trained on tabular or text data. To start,
we’ll deploy a TensorFlow Hub model trained on the ImageNet dataset. So that we
can focus on the task of getting explanations, we won’t do any transfer learning on
the model and will use ImageNet’s original 1,000 label classes:
model = tf.keras.Sequential([
hub.KerasLayer("".../mobilenet_v2/classification/2"",
input_shape=(224,224,3)),
tf.keras.layers.Softmax()
])"|baseline; explainability; Explainable AI; Explainable Predictions design pattern; feature attributions; SHAP; TensorFlow; TensorFlow hub
"us that overall, extreme weather is the most significant feature when predicting
delays.
The two feature attribution methods we’ll explore 5 are outlined in Table 7-2 and pro‐
vide different approaches that can be used for both instance-level and global
explanations.
<i>Table</i> <i>7-2.</i> <i>Descriptions</i> <i>of</i> <i>different</i> <i>explanation</i> <i>methods</i> <i>and</i> <i>links</i> <i>to</i> <i>their</i> <i>research</i> <i>papers</i>
<b>Name</b> <b>Description</b> <b>Paper</b>
a
SampledShapley BasedontheconceptofShapleyValue, thisapproach https://oreil.ly/ubEjW
determinesafeature’smarginalcontributionbycalculatinghow
muchaddingandremovingthatfeatureaffectsaprediction,
analyzedovermultiplecombinationsoffeaturevalues.
IntegratedGradients(IG) Usingapredefinedmodelbaseline,IGcalculatesthederivatives https://oreil.ly/sy8f8
(gradients)alongthepathfromthisbaselinetoaspecificinput.
a TheShapleyValuewasintroducedinapaperbyLloydShapleyin1951,andisbasedonconceptsfromgametheory.
While we could implement these approaches from scratch, there is tooling designed
to simplify the process of getting feature attributions. The available open source and
cloud-based explainability tools let us focus on debugging, improving, and summa‐
rizing our models.
<b>Modelbaseline</b>
In order to use these tools, we first need to understand the concept of a <i>baseline</i> as it
applies to explaining models with feature attributions. The goal of any explainability
method is to answer the question, “Why did the model predict X?” Feature attribu‐
tions attempt to do this by providing numerical values for each feature indicating
how much that feature contributed to the final output. Take for example a model
predicting whether a patient has heart disease given some demographic and health
data. For a single example in our test dataset, let’s imagine that the attribution value
for a patient’s cholesterol feature is 0.4, and the attribution for their blood pressure is
−0.2. Without context, these attribution values don’t mean much, and our first ques‐
tion will likely be, “0.4 and −0.2 relative to what?” That “what” is the model’s <i>baseline.</i>
Whenever we get feature attribution values, they are all relative to a predefined base‐
line prediction value for our model. Baseline predictions can either be <i>informative</i> or
<i>uninformative.</i> Uninformative baselines typically compare against some average case
across a training dataset. In an image model, an uninformative baseline could be a
5 We’refocusingonthesetwoexplainabilitymethodssincetheyarewidelyusedandcoveravarietyofmodel
types,buttherearemanyothermethodsandframeworksnotincludedinthisanalysis,suchasLIMEand
ELI5."|baseline; Explainable Predictions design pattern; feature attributions; informative baseline; Shapley Value; uninformative baseline
"<b>Bridgedschema</b>
Consider the case where the older data has two categories (cash and card). In the new
schema, the card category is now much more granular (gift card, debit card, credit
card). What we do know is that a transaction coded as “card” in the old data would
have been one of these types but the actual type was not recorded. It’s possible to
bridge the schema probabilistically or statically. The static method is what we recom‐
mend, but it is easier to understand if we walk through the probabilistic method first.
<b>Probabilisticmethod.</b> Imagine that we estimate from the newer training data that of
the card transactions, 10% are gift cards, 30% are debit cards, and 60% are credit
cards. Each time an older training example is loaded into the trainer program, we
could choose the card type by generating a uniformly distributed random number in
the range [0, 100) and choosing a gift card when the random number is less than 10, a
debit card if it is in [10, 40), and a credit card otherwise. Provided we train for
enough epochs, any training example would be presented as all three categories, but
proportional to the actual frequency of occurrence. The newer training examples, of
course, would always have the actually recorded category.
The justification for the probabilistic approach is that we treat each older example as
having happened hundreds of times. As the trainer goes through the data, in each
epoch, we simulate one of those instances. In the simulation, we expect that 10% of
the time that a card was used, the transaction would have occurred with a gift card.
That’s why we pick “gift card” for the value of the categorical input 10% of the time.
This is, of course, simplistic—just because gift cards are used 10% of the time overall,
it is not the case that gift cards will be used 10% of the time for any specific transac‐
tion. As an extreme example, maybe taxi companies disallow use of gift cards on air‐
port trips, and so a gift card is not even a legal value for some historical examples.
However, in the absence of any extra information, we will assume that the frequency
distribution is the same for all the historical examples.
<b>Staticmethod.</b> Categorical variables are usually one-hot encoded. If we follow the
probabilistic approach above and train long enough, the average one-hot encoded
value presented to the training program of a “card” in the older data will be [0, 0.1,
0.3, 0.6]. The first 0 corresponds to the cash category. The second number is 0.1
because 10% of the time, on card transactions, this number will be 1 and it will be
zero in all other cases. Similarly, we have 0.3 for debit cards and 0.6 for credit cards.
To bridge the older data into the newer schema, we can transform the older categori‐
cal data into this representation where we insert the a priori probability of the new
classes as estimated from the training data. The newer data, on the other hand, will
have [0, 0, 1, 0] for a transaction that is known to have been paid by a debit card.
We recommend the static method over the probabilistic method because it is effec‐
tively what happens if the probabilistic method runs for long enough. It is also much"|Bridged Schema design pattern; one-hot encoding
"more. This group of users is less likely to run into backward compatibility issues, but
may want the option to choose when to start using a new feature in our app. Also, if
we can break users into distinct groups (i.e., based on their app usage), we can serve
each group different model versions based on their preferences.
<b>Modelversioningwithamanagedservice</b>
To demonstrate versioning, we’ll build a model that predicts flight delays and deploy
this model to Cloud AI Platform Prediction. Because we looked at TensorFlow’s
SavedModel in previous chapters, we’ll use an XGBoost model here.
Once we’ve trained our model, we can export it to get it ready for serving:
model.save_model('model.bst')
To deploy this model to AI Platform, we need to create a model version that will
model.bst
point to this in a Cloud Storage Bucket.
In AI Platform, a model resource can have many versions associated with it. To cre‐
ate a new version using the gcloud CLI, we’ll run the following in a Terminal:
gcloud ai-platform versions create 'v1' <b>\</b>
--model 'flight_delay_prediction' <b>\</b>
--origin gs://your-gcs-bucket <b>\</b>
--runtime-version=1.15 <b>\</b>
--framework 'XGBOOST' <b>\</b>
--python-version=3.7
With this model deployed, it’s now accessible via the endpoint <i>/models/</i>
<i>flight_delay_predictions/versions/v1</i> in an HTTPS URL tied to our project. Since this
is the only version we’ve deployed so far, it’s considered the <i>default.</i> This means
that if we don’t specify a version in our API request, the prediction service will use v1.
Now we can make predictions to our deployed model by sending it examples in the
format our model expects—in this case, a 110-element array of dummy-coded airport
codes (for the full code, see the notebook on GitHub). The model returns sigmoid
output, a float value between 0 and 1 indicating the likelihood a given flight was
delayed more than 30 minutes.
To make a prediction request to our deployed model, we’ll use the following gcloud
command, where <i>input.json</i> is a file with our newline delimited examples to send for
prediction:
gcloud ai-platform predict --model 'flight_delay_prediction'
--version 'v1'
--json-request 'input.json'
If we send five examples for prediction, we’ll get a five-element array back corre‐
sponding with the sigmoid output for each test example, like the following:
[0.019, 0.998, 0.213, 0.002, 0.004]"|default; Model Versioning design pattern
"c3a_model = train_distance_model(ddlop,
PROJECT_ID, c2a_input.outputs['created_table'], 'Typical')
c3b_model = train_distance_model(ddlop,
PROJECT_ID, c2b_input.outputs['created_table'], 'Long')
...
The entire pipeline can be submitted for running, and different runs of the experi‐
ment tracked using the Pipelines framework.
If we are using TFX as our pipeline framework (we can run TFX on
Kubeflow Pipelines), then it is not necessary to deploy the
upstream models in order to use their output predictions in down‐
stream models. Instead, we can use the TensorFlow Transform
method tft.apply_saved_model as part of our preprocessing
operations. The Transform design pattern is discussed in
Chapter 6.
Use of a pipeline-experiment framework is strongly suggested whenever we will have
chained ML models. Such a framework will ensure that downstream models are
retrained whenever upstream models are revised and that we have a history of all the
previous training runs.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Don’t go overboard with the Cascade design pattern—unlike many of the design pat‐
terns we cover in this book, Cascade is not necessarily a best practice. It adds quite a
bit of complexity to your machine learning workflows and may actually result in
poorer performance. Note that a pipeline-experiment framework is definitely best
practice, but as much as possible, try to limit a pipeline to a single machine learning
problem (ingest, preprocessing, data validation, transformation, training, evaluation,
and deployment). Avoid having, as in the Cascade pattern, multiple machine learning
models in the same pipeline."|Cascade design pattern; TensorFlow Transform method
"and more people within an organization want to contribute to this code base, run‐
ning these steps from a single notebook will not scale.
<i>Figure</i> <i>6-6.</i> <i>The</i> <i>steps</i> <i>in</i> <i>a</i> <i>typical</i> <i>end-to-end</i> <i>ML</i> <i>workflow.</i> <i>This</i> <i>is</i> <i>not</i> <i>meant</i> <i>to</i> <i>be</i> <i>all</i>
<i>encompassing,</i> <i>but</i> <i>captures</i> <i>the</i> <i>most</i> <i>common</i> <i>steps</i> <i>in</i> <i>the</i> <i>ML</i> <i>development</i> <i>process.</i>
In traditional programming, <i>monolithic</i> <i>applications</i> are described as those where all
of the application’s logic is handled by a single program. To test a small feature in a
monolithic app, we must run the entire program. The same goes for deploying or
debugging monolithic applications. Deploying a small bug fix for one piece of the
program requires deploying the entire application, which can quickly become
unwieldy. When the entire codebase is inextricably linked, it becomes difficult for
individual developers to debug errors and work independently on different parts of
the application. In recent years, monolithic apps have been replaced in favor of a
<i>microservices</i> architecture where individual pieces of business logic are built and
deployed as isolated (micro) packages of code. With microservices, a large applica‐
tion is split into smaller, more manageable parts so that developers can build, debug,
and deploy pieces of an application independently.
This monolith-versus-microservice discussion provides a good analogy for scaling
ML workflows, enabling collaboration, and ensuring ML steps are reproducible and
reusable across different workflows. When someone is building an ML model on
their own, a “monolithic” approach may be faster to iterate on. It also often works
because one person is actively involved in developing and maintaining each piece:
data gathering and preprocessing, model development, training, and deployment.
However, when scaling this workflow, different people or groups in an organization
might be responsible for different steps. To scale the ML workflow, we need a way for
the team building out the model to run trials independently of the data preprocessing
step. We’ll also need to track the performance for each step of the pipeline and man‐
age the output files generated by each part of the process.
Additionally, when initial development for each step is complete, we’ll want to sched‐
ule operations like retraining, or create event-triggered pipeline runs that are invoked
in response to changes in your environment, like new training data being added to a
bucket. In such cases, it’ll be necessary for the solution to allow us to run the entire
workflow from end to end in one call while still being able to track output and trace
errors from individual steps."|microservices architecture; monolithic applications; Workflow Pipeline design pattern
"<i>Figure</i> <i>2-19.</i> <i>Model</i> <i>combining</i> <i>image</i> <i>and</i> <i>numerical</i> <i>features</i> <i>to</i> <i>predict</i> <i>whether</i> <i>footage</i>
<i>of</i> <i>an</i> <i>intersection</i> <i>depicts</i> <i>a</i> <i>traffic</i> <i>violation.</i>
<i>Figure</i> <i>2-20.</i> <i>Model</i> <i>combining</i> <i>free-form</i> <i>text</i> <i>input</i> <i>with</i> <i>tabular</i> <i>data</i> <i>to</i> <i>predict</i> <i>the</i> <i>rat‐</i>
<i>ing</i> <i>of</i> <i>a</i> <i>restaurant</i> <i>review.</i>
<header><largefont><b>Solution</b></largefont></header>
To start, let’s take the example above with text from a restaurant review combined
with tabular metadata about the meal referenced by the review. We’ll first combine
the numerical and categorical features. There are three possible options for"|Embedding design pattern; Multimodal Input design pattern
"batch_size=BATCH_SIZE,
callbacks=[cp_callback])
<i>Figure</i> <i>4-12.</i> <i>Defining</i> <i>a</i> <i>virtual</i> <i>epoch</i> <i>in</i> <i>terms</i> <i>of</i> <i>the</i> <i>desired</i> <i>number</i> <i>of</i> <i>steps</i> <i>between</i>
<i>checkpoints.</i>
When you get more data, first train it with the old settings, then increase the number
of examples to reflect the new data, and finally change the STOP_POINT to reflect the
number of times you have to traverse the data to attain convergence.
This is now safe even with hyperparameter tuning (discussed later in this chapter)
and retains all the advantages of keeping the number of steps constant.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>13:</b></largefont> <largefont><b>Transfer</b></largefont> <largefont><b>Learning</b></largefont></header>
In Transfer Learning, we take part of a previously trained model, freeze the weights,
and incorporate these nontrainable layers into a new model that solves a similar
problem, but on a smaller dataset.
<header><largefont><b>Problem</b></largefont></header>
Training custom ML models on unstructured data requires extremely large datasets,
which are not always readily available. Consider the case of a model identifying
whether an x-ray of an arm contains a broken bone. To achieve high accuracy, you’ll
need hundreds of thousands of images, if not more. Before your model learns what a
broken bone looks like, it needs to first learn to make sense of the pixels, edges, and
shapes that are part of the images in your dataset. The same is true for models trained
on text data. Let’s say we’re building a model that takes descriptions of patient symp‐
toms and predicts the possible conditions associated with those symptoms. In addi‐
tion to learning which words differentiate a cold from pneumonia, the model also
needs to learn basic language semantics and how the sequence of words creates
meaning. For example, the model would need to not only learn to detect the presence
of the word <i>fever,</i> but that the sequence <i>no</i> <i>fever</i> carries a very different meaning than
<i>high</i> <i>fever.</i>"|Checkpoints design pattern; Transfer Learning design pattern
"It is extremely important to compare the performance of the newer
model trained on bridged examples against the older, unchanged
model on the evaluation dataset. It might be the case that the new
information does not yet have adequate value.
Because we will be using the evaluation dataset to test whether or
not the bridged model has value, it is critical that the evaluation
dataset not be used during training or hyperparameter tuning. So,
techniques like early stopping or checkpoint selection must be
avoided. Instead, use regularization to control overfitting. The
training loss will have to serve as the hyperparameter tuning met‐
ric. See the discussion of the Checkpoints design pattern in Chap‐
ter 4 for more details on how to conserve data by using only two
splits.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Let’s look at a commonly proposed approach that doesn’t work, a complex alternative
to bridging, and an extension of the solution to a similar problem.
<b>Unionschema</b>
It can be tempting to simply create a union of the older and newer schemas. For
example, we could define the schema for the payment type as having five possible val‐
ues: cash, card, gift card, debit card, and credit card. This will make both the histori‐
cal data and the newer data valid and is the approach that we would take in data
warehousing to deal with changes like this. This way, the old data and the new data
are valid as is and without any changes.
The backward-compatible, union-of-schemas approach doesn’t work for machine
learning though.
At prediction time, we will never get the value “card” for the payment type because
the input providers have all been upgraded. Effectively, all those training instances
will have been for nought. For reproducibility (this is the reason that this pattern is
classified as a reproducibility pattern), we need to bridge the older schema into the
newer schema and can’t do a union of the two schemas.
<b>Cascademethod</b>
Imputation in statistics is a set of techniques that can be used to replace missing data
by some valid value. A common imputation technique is to replace a NULL value by
the mean value of that column in the training data. Why do we choose the mean?
Because, in the absence of any more information and assuming that the values are
normally distributed, the most likely value is the mean."|Bridged Schema design pattern; cascade; imputation; regularization
"Regardless of the data modality we’re working with, it’s useful to experiment with dif‐
ferent model architectures to see which performs best on our imbalanced data.
<b>Importanceofexplainability</b>
When building models for flagging rare occurrences in data such as anomalies, it’s
especially important to understand how our model is making predictions. This can
both verify that the model is picking up on the correct signals to make its predictions
and help explain the model’s behavior to end users. There are a few tools available to
help us interpret models and explain predictions, including the open source frame‐
work SHAP, the What-If Tool, and Explainable AI on Google Cloud.
Model explanations can take many forms, one of which is called <i>attribution</i> <i>values.</i>
Attribution values tell us how much each feature in our model influenced the model’s
prediction. Positive attribution values mean a particular feature pushed our
model’s prediction up, and negative attribution values mean the feature pushed our
model’s prediction down. The higher the absolute value of an attribution, the bigger
impact it had on our model’s prediction. In image and text models, attributions can
show you the pixels or words that signaled your model’s prediction most. For tabular
models, attributions provide numerical values for each feature, indicating its overall
effect on the model’s prediction.
After training a TensorFlow model on the synthetic fraud detection dataset from
Kaggle and deploying it to Explainable AI on Google Cloud, let’s take a look at some
examples of instance-level attributions. In Figure 3-21, we see two example transac‐
tions that our model correctly identified as fraud, along with their feature
attributions.
In the first example where the model predicted a 99% chance of fraud, the old bal‐
ance at the origin account before the transaction was made was the biggest indicator
of fraud. In the second example, our model was 89% confident in its prediction of
fraud with the amount of the transaction identified as the biggest signal of fraud.
However, the balance at the origin account made our model <i>less</i> <i>confident</i> in its pre‐
diction of fraud and explains <i>why</i> the prediction confidence is slightly <i>lower</i> by 10
percentage points.
Explanations are important for any type of machine learning model, but we can see
how they are especially useful for models following the Rebalancing design pattern.
When dealing with imbalanced data, it’s important to look beyond our model’s accu‐
racy and error metrics to verify that it’s picking up on meaningful signals in our data."|anomaly detection; attribution values; Explainable AI; Rebalancing design pattern; SHAP; What-If Tool
"once each quarter, it doesn’t make sense to train more frequently than that. However,
if the volume and occurrence of new data is high, then it would be beneficial to
retrain more frequently. The most extreme version of this is online machine learning.
Some machine learning applications, such as ad placement or newsfeed recommen‐
dation, require online, real-time decision, and can continuously improve perfor‐
mance by retraining and updating parameter weights with each new training
example.
In general, the optimal time frame is something you as a practitioner will determine
through experience and experimentation. If you are trying to model a rapidly moving
task, such as adversary or competitive behavior, then it makes sense to set a more fre‐
quent retraining schedule. If the problem is fairly static, like predicting a baby’s birth
weight, then less frequent retrainings should suffice.
In either case, it is helpful to have an automated pipeline set up that can execute the
full retraining process with a single API call. Tools like Cloud Composer/Apache Air‐
flow and AI Platform Pipelines are useful to create, schedule, and monitor ML work‐
flows from preprocessing raw data and training to hyperparameter tuning and
deployment. We discuss this further in the next chapter in “Design Pattern 25: Work‐
flow Pipeline”.
<b>DatavalidationwithTFX</b>
Data distributions can change over time, as shown in Figure 5-7. For example, con‐
sider the natality birth weight dataset. As medicine and societal standards change
over time, the relationship between model features, such as the mother’s age or the
number of gestation weeks, change with respect to the model label, the weight of the
baby. This data drift negatively impacts the model’s ability to generalize to new data.
In short, your model has gone <i>stale,</i> and it needs to be retrained on fresh data.
<i>Figure</i> <i>5-7.</i> <i>Data</i> <i>distributions</i> <i>can</i> <i>change</i> <i>over</i> <i>time.</i> <i>Data</i> <i>drift</i> <i>refers</i> <i>to</i> <i>any</i> <i>change</i>
<i>that</i> <i>has</i> <i>occurred</i> <i>to</i> <i>the</i> <i>data</i> <i>being</i> <i>fed</i> <i>to</i> <i>your</i> <i>model</i> <i>for</i> <i>prediction</i> <i>as</i> <i>compared</i> <i>to</i> <i>the</i>
<i>data</i> <i>used</i> <i>for</i> <i>training.</i>"|AI Platform Pipelines; Cloud Composer/Apache Airflow; Continued Model Evaluation design pattern; online machine learning
"Nevertheless, in practice, each specific chaotic phenomenon has a specific resolution
threshold beyond which it is possible for models to forecast it over short time peri‐
ods. Therefore, provided the lookup table is fine-grained enough and the limits of
resolvability are understood, useful approximations can result.
<b>MonteCarlomethods</b>
In reality, tabulating all possible inputs might not be possible, and you might take a
Monte Carlo approach of sampling the input space to create the set of inputs, espe‐
cially where not all possible combinations of inputs are physically possible.
In such cases, overfitting is technically possible (see Figure 4-5, where the unfilled cir‐
cles are approximated by wrong estimates shown by crossed circles).
<i>Figure</i> <i>4-5.</i> <i>If</i> <i>the</i> <i>input</i> <i>space</i> <i>is</i> <i>sampled,</i> <i>not</i> <i>tabulated,</i> <i>then</i> <i>you</i> <i>need</i> <i>to</i> <i>take</i> <i>care</i> <i>to</i>
<i>limit</i> <i>model</i> <i>complexity.</i>
However, even here, you can see that the ML model will be interpolating between
known answers. The calculation is always deterministic, and it is only the input
points that are subject to random selection. Therefore, these known answers do not
contain noise, and because there are no unobserved variables, errors at unsampled
points will be strictly bounded by the model complexity. Here, the overfitting danger
comes from model complexity and not from fitting to noise. Overfitting is not as
much of a concern when the size of the dataset is larger than the number of free
parameters. Therefore, using a combination of low-complexity models and mild reg‐
ularization provides a practical way to avoid unacceptable overfitting in the case of
Monte Carlo selection of the input space.
<b>Data-drivendiscretizations</b>
Although deriving a closed-form solution is possible for some PDEs, determining
solutions using numerical methods is more common. Numerical methods of PDEs
are already a deep field of research, and there are many books, courses, and journals
devoted to the subject. One common approach is to use finite difference methods,
similar to Runge-Kutta methods, for solving ordinary differential equations. This is
typically done by discretizing the differential operator of the PDE and finding a"|Monte Carlo approach; PDE; regularization; Runge-Kutta methods; Useful Overfitting design pattern
"Suppose we have a decision tree to predict whether a baby will require intensive care
(IC) or can be normally discharged (ND), and suppose that the decision tree takes as
inputs two variables, <i>x1</i> and <i>x2.</i> The trained model might look something like
Figure 2-1.
It is pretty clear that <i>x1</i> and <i>x2</i> need to be boolean variables in order for <i>f(x1,</i> <i>x2)</i> to
work. Suppose that two of the pieces of information we’d like the model to consider
when classifying a baby as requiring intensive care or not is the hospital that the baby
is born in and the baby’s weight. Can we use the hospital that a baby is born in as an
input to the decision tree? No, because the hospital takes neither the value True nor
the value False and cannot be fed into the && (AND) operator. It’s mathematically
not compatible. Of course, we can “make” the hospital value boolean by performing
an operation such as:
x1 = (hospital IN France)
so that <i>x1</i> is True when the hospital is in France, and False if not. Similarly, a baby’s
weight cannot be fed directly into the model, but by performing an operation such as:
x1 = (babyweight < 3 kg)
we can use the hospital or the baby weight as an input to the model. This is an exam‐
ple of how input data (hospital, a complex object or baby weight, a floating point
number) can be represented in the form (boolean) expected by the model. This is
what we mean by <i>data</i> <i>representation.</i>
In this book, we will use the term <i>input</i> to represent the real-world data fed to the
model (for example, the baby weight) and the term <i>feature</i> to represent the trans‐
formed data that the model actually operates on (for example, whether the baby
weight is less than 3 kilograms). The process of creating features to represent the
input data is called <i>feature</i> <i>engineering,</i> and so we can think of feature engineering as
a way of selecting the data representation.
Of course, rather than hardcoding parameters such as the threshold value of 3 kilo‐
grams, we’d prefer the machine learning model to learn how to create each node by
selecting the input variable and the threshold. Decision trees are an example of
machine learning models that are capable of learning the data representation.1 Many
of the patterns that we look at in this chapter will involve similarly <i>learnable</i>
<i>data</i> <i>representations.</i>
The <i>Embeddings</i> design pattern is the canonical example of a data representation that
deep neural networks are capable of learning on their own. In an embedding, the
learned representation is dense and lower-dimensional than the input, which could
baby weight
1 Here,thelearneddatarepresentationconsistsof astheinputvariable,thelessthanoperator,
andthethresholdof3kg."|data representation; decision trees; Embedding design pattern; feature engineering; feature; input
"We see the first hint of learning in Figure 4-8(b), and see that the model has learned
the high-level view of the data by Figure 4-8(c). From then on, the model is adjusting
the boundaries to get more and more of the blue points into the center region while
keeping the orange points out. This helps, but only up to point. By the time we get to
Figure 4-8(e), the adjustment of weights is starting to reflect random perturbations in
the training data, and these are counterproductive on the validation dataset.
We can therefore break the training into three phases. In the first phase, between
stages (a) and (c), the model is learning high-level organization of the data. In the
second phase, between stages and (c) and (e), the model is learning the details. By the
time we get to the third phase, stage (f), the model is overfitting. A partially trained
model from the end of phase 1 or from phase 2 has some advantages precisely
because it has learned the high-level organization but is not caught up in the details.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Besides providing resilience, saving intermediate checkpoints also allows us to imple‐
ment early stopping and fine-tuning capabilities.
<b>Earlystopping</b>
In general, the longer you train, the lower the loss on the training dataset. However,
at some point, the error on the validation dataset might stop decreasing. If you are
starting to overfit to the training dataset, the validation error might even start to
increase, as shown in Figure 4-9.
<i>Figure</i> <i>4-9.</i> <i>Typically,</i> <i>the</i> <i>training</i> <i>loss</i> <i>continues</i> <i>to</i> <i>drop</i> <i>the</i> <i>longer</i> <i>you</i> <i>train,</i> <i>but</i> <i>once</i>
<i>overfitting</i> <i>starts,</i> <i>the</i> <i>validation</i> <i>error</i> <i>on</i> <i>a</i> <i>withheld</i> <i>dataset</i> <i>starts</i> <i>to</i> <i>go</i> <i>up.</i>"|checkpoints; Checkpoints design pattern
"bucket boundaries to fit the desired output distribution. A principled approach to
choosing these buckets is to do <i>histogram</i> <i>equalization,</i> where the bins of the histo‐
gram are chosen based on quantiles of the raw distribution, (see the third panel of
Figure 2-4). In the ideal situation, histogram equalization results in a uniform distri‐
bution (although not in this case, because of repeated values in the quantiles).
To carry out histogram equalization in BigQuery, we can do:
ML.BUCKETIZE(num_views, bins) <b>AS</b> bin
where the bins are obtained from:
APPROX_QUANTILES(num_views, 100) <b>AS</b> bins
See the notebook in the code repository of this book for full details.
Another method to handle skewed distributions is to use a parametric transforma‐
tion technique like the <i>Box-Cox</i> <i>transform.</i> Box-Cox chooses its single parameter,
lambda, to control the “heteroscedasticity” so that the variance no longer depends on
the magnitude. Here, the variance among rarely viewed Wikipedia pages will be
much smaller than the variance among frequently viewed pages, and Box-Cox tries to
equalize the variance across all ranges of the number of views. This can be done using
Python’s SciPy package:
traindf['boxcox'], est_lambda = (
scipy.stats.boxcox(traindf['num_views']))
The parameter estimated over the training dataset (est_lambda) is then used to
transform other values:
evaldf['boxcox'] = scipy.stats.boxcox(evaldf['num_views'], est_lambda)
<b>Arrayofnumbers</b>
Sometimes, the input data is an array of numbers. If the array is of fixed length, data
representation can be rather simple: flatten the array and treat each position as a sep‐
arate feature. But often, the array will be of variable length. For example, one of the
inputs to the model to predict the sales of a nonfiction book might be the sales of all
previous books on the topic. An example input might be:
[2100, 15200, 230000, 1200, 300, 532100]
Obviously, the length of this array will vary in each row because there are different
numbers of books published on different topics."|arrays; Box-Cox transform; nonlinear transformations
"The given SavedModel SignatureDef contains the following input(s):
inputs['full_text_input'] tensor_info:
dtype: DT_STRING
shape: (-1)
name: serving_default_full_text_input:0
The given SavedModel SignatureDef contains the following output(s):
outputs['positive_review_logits'] tensor_info:
dtype: DT_FLOAT
shape: (-1, 1)
name: StatefulPartitionedCall_2:0
Method name is: tensorflow/serving/predict
The signature specifies that the prediction method takes a one-element array as input
full_text_input
(called ) that is a string, and outputs one floating point number
whose name is positive_review_logits. These names come from the names that we
assigned to the Keras layers:
hub_layer = hub.KerasLayer(..., name='full_text')
...
model.add(tf.keras.layers.Dense(1, name='positive_review_logits'))
Here is how we can obtain the serving function and use it for inference:
serving_fn = tf.keras.models.load_model(export_path). \
signatures['serving_default']
outputs = serving_fn(full_text_input=
tf.constant([review1, review2, review3]))
logit = outputs['positive_review_logits']
Note how we are using the input and output names from the serving function in the
code.
<b>Createwebendpoint</b>
The code above can be put into a web application or serverless framework such as
Google App Engine, Heroku, AWS Lambda, Azure Functions, Google Cloud Func‐
tions, Cloud Run, and so on. What all these frameworks have in common is that they
allow the developer to specify a function that needs to be executed. The frameworks
take care of autoscaling the infrastructure so as to handle large numbers of prediction
requests per second at low latency.
For example, we can invoke the serving function from within Cloud Functions as
follows:
serving_fn = None
<b>def</b> handler(request):
<b>global</b> serving_fn
<b>if</b> serving_fn <b>is</b> None:
serving_fn = (tf.keras.models.load_model(export_path)
.signatures['serving_default'])
request_json = request.get_json(silent=True)"|AWS Lambda; Azure Functions; Cloud Run; Google App Engine; Google Cloud Functions; Heroku; Stateless Serving Function design pattern
"baseline, and explains why many of the attribution values are negative. We can deter‐
mine the most important features by taking the absolute value of the feature attribu‐
tions. In this example, the trip’s distance was the most important feature, causing our
model’s prediction to decrease 2.4 minutes from the baseline. Additionally, as a san‐
ity check, we should ensure that the feature attribution values roughly add up to the
difference between the current prediction and the baseline prediction.
<i>Figure</i> <i>7-2.</i> <i>The</i> <i>feature</i> <i>attribution</i> <i>values</i> <i>for</i> <i>a</i> <i>single</i> <i>example</i> <i>in</i> <i>a</i> <i>model</i> <i>predicting</i>
<i>bike</i> <i>trip</i> <i>duration.</i> <i>The</i> <i>model’s</i> <i>baseline,</i> <i>calculated</i> <i>using</i> <i>the</i> <i>median</i> <i>of</i> <i>each</i> <i>feature</i>
<i>value,</i> <i>is</i> <i>13.6</i> <i>minutes,</i> <i>and</i> <i>the</i> <i>attribution</i> <i>values</i> <i>show</i> <i>how</i> <i>much</i> <i>each</i> <i>feature</i> <i>influ‐</i>
<i>enced</i> <i>the</i> <i>prediction.</i>
Informative baselines, on the other hand, compare a model’s prediction with a spe‐
cific alternative scenario. In a model identifying fraudulent transactions, an informa‐
tive baseline might answer the question, “Why was this transaction flagged as fraud
instead of nonfraudulent?” Instead of using the median feature values across the
entire training dataset to calculate the baseline, we would take the median of only the
nonfraudulent values. In an image model, maybe the training images contain a signif‐
icant portion of solid black and white pixels, and using these as a baseline would
result in inaccurate predictions. In this case, we’d need to come up with a different
<i>informative</i> baseline image."|baseline; Explainable Predictions design pattern; feature attributions; informative baseline; uninformative baseline
"functionality and many large software companies have developed their own end-to-
end ML platforms, like Uber’s Michelangelo or Google’s TFX, which are also open
source.
Successful operationalization incorporates components of continuous integration
and continuous delivery (CI/CD) that are the familiar best practices of software
development. These CI/CD practices are focused on reliability, reproducibility,
speed, security, and version control within code development. ML/AI workflows ben‐
efit from the same considerations, though there are some notable differences. For
example, in addition to the code that is used to develop the model, it is important to
apply these CI/CD principles to the data, including data cleaning, versioning, and
orchestration of data pipelines.
The final step to be considered in the deployment stage is to monitor and maintain
the model. Once the model has been operationalized and is in production, it’s neces‐
sary to monitor the model’s performance. Over time, data distributions change, caus‐
ing the model to become stale. This model staleness (see Figure 8-3) can occur for
many reasons, from changes in customer behavior to shifts in the environment. For
this reason, it is important to have in place mechanisms to efficiently monitor the
machine learning model and all the various components that contribute to its perfor‐
mance, from data collection to the quality of the predictions during serving. The dis‐
cussion of “Design Pattern 18: Continued Model Evaluation” on page 220 in
Chapter 5 covers this common problem and its solution in detail.
<i>Figure</i> <i>8-3.</i> <i>Model</i> <i>staleness</i> <i>can</i> <i>occur</i> <i>for</i> <i>many</i> <i>reasons.</i> <i>Retraining</i> <i>models</i> <i>periodically</i>
<i>can</i> <i>help</i> <i>to</i> <i>improve</i> <i>their</i> <i>performance</i> <i>over</i> <i>time.</i>
For example, it is important to monitor the distribution of feature values to compare
against the distributions that were used during the development steps. It is also
important to monitor the distribution of label values to ensure that some data drift
hasn’t caused an imbalance or shift in label distribution. Oftentimes, a machine
learning model relies on data collected from an outside source. Perhaps our model
relies on a third-party traffic API to predict wait times for car pickups or uses data
from a weather API as input to a model that predicts flight delays. These APIs are not
managed by our team. If that API fails or its output format changes in a significant
way, it will have consequences for our production model. In this case, it is important"|CI/CD; ML life cycle
"'euclid_dist']
label = 'fare_amt'
features = model_features + [label]
<i>#</i> <i>Retrieve</i> <i>training</i> <i>dataset</i> <i>from</i> <i>Feast</i>
dataset = _feast_batch_client.get_batch_features(
feature_refs=[FS_NAME + "":"" + feature <b>for</b> feature <b>in</b> features],
entity_rows=entity_df).to_dataframe()
The dataframe dataset now contains all features and the label for our model, pulled
directly from the feature store.
<b>Onlineserving.</b>
For online serving, Feast only stores the latest entity values, as
opposed to historical serving where all historical values are stored. Online serving
with Feast is built to be very low latency, and Feast provides a gRPC API backed by
Redis. To retrieve online features, for example, when making online predictions with
.get_online_features(...)
the trained model, we use specifying the features we
want to capture and the entity:
<i>#</i> <i>retrieve</i> <i>online</i> <i>features</i> <i>for</i> <i>a</i> <i>single</i> <i>taxi_id</i>
online_features = _feast_online_client.get_online_features(
feature_refs=[""taxi_rides:pickup_lat"",
""taxi_rides:pickup_lon"",
""taxi_rides:dropoff_lat"",
""taxi_rides:dropoff_lon"",
""taxi_rides:num_pass"",
""taxi_rides:euclid_dist""],
entity_rows=[
GetOnlineFeaturesRequest.EntityRow(
fields={
""taxi_id"": Value(
int64_val=5)
}
)
]
)
online_features
This saves as a list of maps where each item in the list contains the
latest feature values for the provided entity, here, taxi_id = 5 :
field_values {
fields {
key: ""taxi_id""
value {
int64_val: 5
}
}
fields {
key: ""taxi_rides:dropoff_lat""
value {"|Feast; Feature Store design pattern
"network with just one hidden layer that approximates that function as closely as we
want.1
Deep learning approaches to solving differential equations or complex dynamical
systems aim to represent a function defined implicitly by a differential equation, or
system of equations, using a neural network.
Overfitting is useful when the following two conditions are met:
• There is no noise, so the labels are accurate for all instances.
• You have the complete dataset at your disposal (you have all the examples there
are). In this case, overfitting becomes interpolating the dataset.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
We introduced overfitting as being useful when the set of inputs can be exhaustively
listed and the accurate label for each set of inputs can be calculated. If the full input
space can be tabulated, overfitting is not a concern because there is no unseen data.
However, the Useful Overfitting design pattern is useful beyond this narrow use case.
In many real-world situations, even if one or more of these conditions has to be
relaxed, the concept that overfitting can be useful remains valid.
<b>Interpolationandchaostheory</b>
The machine learning model essentially functions as an approximation to a lookup
table of inputs to outputs. If the lookup table is small, just use it as a lookup table!
There is no need to approximate it by a machine learning model. An ML approxima‐
tion is useful in situations where the lookup table will be too large to effectively use. It
is when the lookup table is too unwieldy that it becomes better to treat it as the train‐
ing dataset for a machine learning model that approximates the lookup table.
Note that we assumed that the observations would have a finite number of possibili‐
ties. For example, we posited that temperature would be measured in 0.01°C incre‐
ments and lie between 60°C and 80°C. This will be the case if the observations are
made by digital instruments. If this is not the case, the ML model is needed to inter‐
polate between entries in the lookup table.
Machine learning models interpolate by weighting unseen values by the distance of
these unseen values from training examples. Such interpolation works only if the
underlying system is not chaotic. In chaotic systems, even if the system is determinis‐
tic, small differences in initial conditions can lead to dramatically different outcomes.
1 Itmay,ofcourse,notbethecasethatwecanlearnthenetworkusinggradientdescentjustbecausethere
existssuchaneuralnetwork(thisiswhychangingthemodelarchitecturebyaddinglayershelps—itmakes
thelosslandscapemoreamenabletoSGD)."|chaos theory; ML approximation; Uniform Approximation Theorem; Useful Overfitting design pattern
"<i>Figure</i> <i>4-13.</i> <i>Transfer</i> <i>learning</i> <i>involves</i> <i>training</i> <i>a</i> <i>model</i> <i>on</i> <i>a</i> <i>large</i> <i>dataset.</i> <i>The</i> <i>“top”</i>
<i>of</i> <i>the</i> <i>model</i> <i>(typically,</i> <i>just</i> <i>the</i> <i>output</i> <i>layer)</i> <i>is</i> <i>removed</i> <i>and</i> <i>the</i> <i>remaining</i> <i>layers</i> <i>have</i>
<i>their</i> <i>weights</i> <i>frozen.</i> <i>The</i> <i>last</i> <i>layer</i> <i>of</i> <i>the</i> <i>remaining</i> <i>model</i> <i>is</i> <i>called</i> <i>the</i> <i>bottleneck</i> <i>layer.</i>
<b>Bottlenecklayer</b>
In relation to an entire model, the bottleneck layer represents the input (typically an
image or text document) in the lowest-dimensionality space. More specifically, when
we feed data into our model, the first layers see this data nearly in its original form.
To see how this works, let’s continue with a medical imaging example, but this time
we’ll build a model with a colorectal histology dataset to classify the histology images
into one of eight categories.
To explore the model we are going to use for transfer learning, let’s load the VGG
model architecture pre-trained on the ImageNet dataset:
vgg_model_withtop = tf.keras.applications.VGG19(
include_top=True,
weights='imagenet',
)"|bottleneck layer; Transfer Learning design pattern; VGG
"train_data_gen = image_generator.flow_from_directory(
directory=data_dir,
batch_size=32,
shuffle=True,
target_size=(128,128),
classes = ['not_instrument','instrument'],
class_mode='binary')
With our training and validation datasets ready, we can train the model as we nor‐
mally would. The typical approach for exporting trained models for serving is to use
TensorFlow’s model.save() method. However, remember that this model will be
served on-device, and as a result we want to keep it as small as possible. To build a
model that fits these requirements, we’ll use TensorFlow Lite, a library optimized for
building and serving models directly on mobile and embedded devices that may not
have reliable internet connectivity. TF Lite has some built-in utilities for quantizing
models both during and after training.
To prepare the trained model for edge serving, we use TF Lite to export it in an opti‐
mized format:
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
open('converted_model.tflite', 'wb').write(tflite_model)
This is the fastest way to quantize a model <i>after</i> training. Using the TF Lite optimiza‐
tion defaults, it will reduce our model’s weights to their 8-bit representation. It will
also quantize inputs at inference time when we make predictions on our model. By
running the code above, the resulting exported TF Lite model is one-fourth the size it
would have been if we had exported it without quantization.
To further optimize your model for offline inference, you can also
quantize your model’s weights <i>during</i> training or quantize all of
your model’s math operations in addition to weights. At the time
of writing, quantization-optimized training for TensorFlow 2 mod‐
els is on the roadmap.
To generate a prediction on a TF Lite model, you use the TF Lite interpreter, which is
optimized for low latency. You’ll likely want to load your model on an Android or
iOS device and generate predictions directly from your application code. There are
APIs for both platforms, but we’ll show the Python code for generating predictions
here so that you can run it from the same notebook where you created your model.
First, we create an instance of TF Lite’s interpreter and get details on the input and
output format it’s expecting:
interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
interpreter.allocate_tensors()"|low latency; quantization; Tensorflow Lite; TF Lite Interpreter; Two-Phase Predictions design pattern
"organization. The established common patterns and best practices as well as standard
tools and libraries for accelerating ML projects are shared easily among different
groups within the organization.
Datasets are stored in a platform that is accessible to all teams, making it easy to dis‐
cover, share, and reuse datasets and ML assets. There are standardized ML feature
stores, and collaborations across the entire organization are encouraged. Fully auto‐
mated organizations operate an integrated ML experimentation and production plat‐
form where models are built and deployed and ML practices are accessible to
everyone in the organization. That platform is supported by scalable and serverless
computation for batch and online data ingestion and processing. Specialized ML
accelerators such as GPUs and TPUs are available on demand and there are orches‐
trated experiments for end-to-end data and ML pipelines.
The development and production environments are similar to the pipeline stage (see
Figure 8-6) but have incorporated CI/CD practices into each of the various stages of
their ML workflow as well. These CI/CD best practices focus on reliability, reprodu‐
cibility, and version control for the code to produce the ML models as well as the data
and the data pipelines and their orchestration. This allows for building, testing, and
packaging of various pipeline components. Model versioning is maintained by an ML
Model Registry that also stores necessary ML metadata and artifacts."|AI readiness; CI/CD; GPU; TPU
"<i>Figure</i> <i>5-2.</i> <i>Third</i> <i>and</i> <i>subsequent</i> <i>steps</i> <i>of</i> <i>the</i> <i>query</i> <i>to</i> <i>find</i> <i>the</i> <i>five</i> <i>most</i> <i>“positive”</i>
<i>complaints.</i>
The third step sorts the dataset in descending order and takes five. This is done on
each worker, so each of the 10 workers finds the 5 most positive complaints in “their”
shard. The remaining steps retrieve and format the remaining bits of data and write
them to the output.
The final step (not shown) takes the 50 complaints, sorts them, and selects the 5 that
form the actual result. The ability to separate work in this way across many workers is
what enables BigQuery to carry out the entire operation on 1.5 million complaint
documents in 35 seconds.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The Batch Serving design pattern depends on the ability to split a task across multiple
workers. So, it is not restricted to data warehouses or even to SQL. Any MapReduce
framework will work. However, SQL data warehouses tend to be the easiest and are
often the default choice, especially when the data is structured in nature.
Even though batch serving is used when latency is not a concern, it is possible to
incorporate precomputed results and periodic refreshing to use this in scenarios
where the space of possible prediction inputs is limited.
<b>Batchandstreampipelines</b>
Frameworks like Apache Spark or Apache Beam are useful when the input needs pre‐
processing before it can be supplied to the model, if the machine learning model out‐
puts require postprocessing, or if either the preprocessing or postprocessing are hard
to express in SQL. If the inputs to the model are images, audio, or video, then SQL is
not an option and it is necessary to use a data processing framework that can handle
unstructured data. These frameworks can also take advantage of accelerated hard‐
ware like TPUs and GPUs to carry out preprocessing of the images."|Apache Beam; Apache Spark; Batch Serving design pattern; BigQuery
"The next step (Step 5 in Figure 8-2) of the development stage is focused on building
the ML model. During this development step, it is crucial to adhere to best practices
of capturing ML workflows in a pipeline: see “Design Pattern 25: Workflow Pipeline”
on page 282 in Chapter 6. This includes creating repeatable splits for training/valida‐
tion/test sets before any model development has begun to ensure there is no data
leakage. Different model algorithms or combinations of algorithms may be trained to
assess their performance on the validation set and to examine the quality of their pre‐
dictions. Parameter and hyperparameters are tuned, regularization techniques are
employed, and edge cases are explored. The typical ML model training loop is
described in detail at the beginning of Chapter 4 where we also address useful design
patterns for changing the training loop to attain specific objectives.
Many steps of the ML life cycle are iterative, and this is particularly true during
model development. Many times, after some experimentation, it may be necessary to
revisit the data, business objectives, and KPIs. New data insights are gleaned during
the model development stage and these insights can shed additional light on what is
possible (and what is not possible). It is not uncommon to spend a long time in the
model development phase, particularly when developing a custom model. Chapter 6
addresses many other reproducibility design patterns that address challenges that
arise during this iterative phase of model development.
Throughout development of the model, each new adjustment or approach is meas‐
ured against the evaluation metrics that were set in the discovery stage. Hence, suc‐
cessful execution of the discovery stage is crucial, and it is necessary to have
alignment on the decisions made during that stage. Ultimately, model development
culminates in a final evaluation step (Step 6 of Figure 8-2). At this point, model
development ceases and the model performance is assessed against those predeter‐
mined evaluation metrics.
One of the key outcomes of the development stage is to interpret and present results
(Step 7 of Figure 8-2) to the stakeholders and regulatory groups within the business.
This high-level evaluation is crucial and necessary to communicate the value of the
development stage to management. This step is focused on creating numbers and vis‐
uals for initial reports that will be brought to stakeholders within the organization.
Chapter 7 discusses some of the common design patterns that ensure AI is being used
responsibly and can help with stakeholder management. Typically, this is a key deci‐
sion point in determining if further resources will be devoted to the final stage of the
life cycle, machine learning productionization and deployment.
<b>Deployment</b>
Assuming successful completion of the model development and evidence of promis‐
ing results, the next stage is focused on productionization of the model, with the first
step (Step 8 in Figure 8-2) being to plan for deployment."|KPI; ML life cycle; Repeatable Splitting design pattern; responsible AI
"However, the feature store provides an added advantage of feature reusability that
tf.transform does not have. Although tf.transform pipelines ensure reproducibil‐
ity, the features are created and developed only for that model and are not easily
shared or reused by other models and pipelines.
tf.transform
On the other hand, takes special care to ensure that feature creation
during serving is carried out on accelerated hardware, since it is part of the serving
graph. Feature stores typically do not provide this capability today.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>27:</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>Versioning</b></largefont></header>
In the Model Versioning design pattern, backward compatibility is achieved by
deploying a changed model as a microservice with a different REST endpoint. This is
a necessary prerequisite for many of the other patterns discussed in this chapter.
<header><largefont><b>Problem</b></largefont></header>
As we’ve seen with <i>data</i> <i>drift</i> (introduced in Chapter 1), models can become stale
over time and need to be updated regularly to make sure they reflect an organiza‐
tion’s changing goals, and the environment associated with their training data.
Deploying model updates to production will inevitably affect the way models behave
on new data, which presents a challenge—we need an approach for keeping produc‐
tion models up to date while still ensuring backward compatibility for existing model
users.
Updates to an existing model might include changing a model’s architecture in order
to improve accuracy, or retraining a model on more recent data to address drift.
While these types of changes likely won’t require a different model output format,
they will affect the prediction results users get from a model. As an example, let’s
imagine we’re building a model that predicts the genre of a book from its description
and uses the predicted genres to make recommendations to users. We trained our
initial model on a dataset of older classic books, but now have access to new data on
thousands of more recent books to use for training. Training on this updated dataset
improves our overall model accuracy, but slightly reduces accuracy on older “classic”
books. To handle this, we’ll need a solution that lets users choose an older version of
our model if they prefer.
Alternatively, our model’s end users might start to require more information on <i>how</i>
the model is arriving at a specific prediction. In a medical use case, a doctor might
need to see the regions in an x-ray that caused a model to predict the presence of dis‐
ease rather than rely solely on the predicted label. In this case, the response from a
deployed model would need to be updated to include these highlighted regions. This
process is known as <i>explainability</i> and is discussed further in Chapter 7."|data drift; explainability; Feature Store design pattern; Model Versioning design pattern
"task and model architecture are the best fit for edge devices. By “simpler,” we mean
trade-offs like favoring binary classification over multiclass or choosing a less com‐
plex model architecture (like a decision tree or linear regression model) when
possible.
When you need to deploy models to the edge while still adhering to certain model
size and complexity constraints, it’s worth looking at edge hardware designed specifi‐
cally with ML inference in mind. For example, the Coral Edge TPU board provides a
custom ASIC optimized for high-performance, offline ML inference on TensorFlow
Lite models. Similarly, NVIDIA offers the Jetson Nano for edge-optimized, low-
power ML inference. The hardware support for ML inference is rapidly evolving as
embedded, on-device ML becomes more common.
<b>Phase2:Buildingthecloudmodel</b>
Since our cloud-hosted model doesn’t need to be optimized for inference without a
network connection, we can follow a more traditional approach for training, export‐
ing, and deploying this model. Depending on your Two-Phase Prediction use case,
this second model could take many different forms. In the Google Home example,
phase 2 might include multiple models: one that converts a speaker’s audio input to
text, and a second one that performs NLP to understand the text and route the user’s
query. If the user asks for something more complex, there could even be a third
model to provide a recommendation based on user preferences or past activity.
In our instrument example, the second phase of our solution will be a multiclass
model that classifies sounds into one of 18 possible instrument categories. Since this
model doesn’t need to be deployed on-device, we can use a larger model architecture
like VGG as a starting point and then follow the Transfer Learning design pattern
outlined in Chapter 4.
We’ll load VGG trained on the ImageNet dataset, specify the size of our spectrogram
images in the input_shape parameter, and freeze the model’s weights before adding
our own softmax classification output layer:
vgg_model = tf.keras.applications.VGG19(
include_top=False,
weights='imagenet',
input_shape=((128,128,3))
)
vgg_model.trainable = False
Our output will be an 18-element array of softmax probabilities:
prediction_layer = tf.keras.layers.Dense(18, activation='softmax')
We’ll limit our dataset to only the audio clips of instruments, then transform the
instrument labels to 18-element one-hot vectors. We can use the same"|ASIC; Coral Edge TPU; Jetson Nano; Two-Phase Predictions design pattern; VGG
"do image classification. We’ll then remove the last layer from that model, freeze the
weights of that model, and continue training using our 400 x-ray images. We’d ide‐
ally find a model trained on a dataset with similar images to our x-rays, like images
taken in a lab or another controlled condition. However, we can still utilize transfer
learning if the datasets are different, so long as the prediction task is the same. In this
case we’re doing image classification.
You can use transfer learning for many prediction tasks in addition to image classifi‐
cation, so long as there is an existing pre-trained model that matches the task you’d
like to perform on your dataset. For example, transfer learning is also frequently
applied in image object detection, image style transfer, image generation, text classifi‐
cation, machine translation, and more.
Transfer learning works because it lets us stand on the shoulders of
giants, utilizing models that have already been trained on
extremely large, labeled datasets. We’re able to use transfer learn‐
ing thanks to years of research and work others have put into creat‐
ing these datasets for us, which has advanced the state-of-the-art in
transfer learning. One example of such a dataset is the ImageNet
project, started in 2006 by Fei-Fei Li and published in 2009. Image‐
Net3
has been essential to the development of transfer learning and
paved the way for other large datasets like COCO and Open
Images.
The idea behind transfer learning is that you can utilize the weights and layers from a
model trained in the same domain as your prediction task. In most deep learning
models, the final layer contains the classification label or output specific to your pre‐
diction task. With transfer learning, we remove this layer, freeze the model’s trained
weights, and replace the final layer with the output for our specialized prediction task
before continuing to train. We can see how this works in Figure 4-13.
Typically, the penultimate layer of the model (the layer before the model’s output
layer) is chosen as the <i>bottleneck</i> <i>layer.</i> Next, we’ll explain the bottleneck layer, along
with different ways to implement transfer learning in TensorFlow.
3 JiaDengetal.,“ImageNet:ALarge-ScaleHierarchicalImageDatabase,”IEEEComputerSocietyConference
onComputerVisionandPatternRecognition(CVPR)(2009):248–255."|bottleneck layer; ImageNet; Transfer Learning design pattern
"<header><largefont><b>Simple</b></largefont> <largefont><b>Data</b></largefont> <largefont><b>Representations</b></largefont></header>
Before we delve into learnable data representations, feature crosses, and more, let’s
look at simpler data representations. We can think of these simple data representa‐
tions as common <i>idioms</i> in machine learning—not quite patterns, but commonly
employed solutions nevertheless.
<header><largefont><b>Numerical</b></largefont> <largefont><b>Inputs</b></largefont></header>
Most modern, large-scale machine learning models (random forests, support vector
machines, neural networks) operate on numerical values, and so if our input is
numeric, we can pass it through to the model unchanged.
<b>Whyscalingisdesirable</b>
Often, because the ML framework uses an optimizer that is tuned to work well with
numbers in the [–1, 1] range, scaling the numeric values to lie in that range can be
beneficial.
<header><largefont><b>Why</b></largefont> <largefont><b>Scale</b></largefont> <largefont><b>Numeric</b></largefont> <largefont><b>Values</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Lie</b></largefont> <largefont><b>in</b></largefont> <largefont><b>[–1,</b></largefont> <largefont><b>1]?</b></largefont></header>
Gradient descent optimizers require more steps to converge as the curvature of the
loss function increases. This is because the derivatives of features with larger relative
magnitudes will tend to be larger as well, and so lead to abnormal weight updates.
The abnormally large weight updates will require more steps to converge and thereby
increase the computation load.
“Centering” the data to lie in the [–1, 1] range makes the error function more spheri‐
cal. Therefore, models trained with transformed data tend to converge faster and are
therefore faster/cheaper to train. In addition, the [–1, 1] range offers the highest float‐
ing point precision.
A quick test with one of scikit-learn’s built-in datasets can prove the point (this is
an excerpt from this book’s code repository):
<b>from</b> <b>sklearn</b> <b>import</b> datasets, linear_model
diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)
raw = diabetes_X[:, None, 2]
max_raw = max(raw)
min_raw = min(raw)
scaled = (2*raw - max_raw - min_raw)/(max_raw - min_raw)
<b>def</b> train_raw():
linear_model.LinearRegression().fit(raw, diabetes_y)
<b>def</b> train_scaled():"|idioms; scaling; scikit-learn
"<i>Figure</i> <i>4-7.</i> <i>Starting</i> <i>point</i> <i>of</i> <i>the</i> <i>spiral</i> <i>classification</i> <i>problem.</i> <i>You</i> <i>can</i> <i>get</i> <i>to</i> <i>this</i> <i>setup</i>
<i>by</i> <i>opening</i> <i>up</i> <i>this</i> <i>link</i> <i>in</i> <i>a</i> <i>web</i> <i>browser.</i>
In the playground, we are trying to build a classifier to distinguish between blue dots
and orange dots (if you are reading this in the print book, please do follow along by
navigating to the link in a web browser). The two input features are x and x , which
1 2
are the coordinates of the points. Based on these features, the model needs to output
the probability that the point is blue. The model starts with random weights and the
background of the dots shows the model prediction for each coordinate point. As you
can see, because the weights are random, the probability tends to hover near the cen‐
ter value for all the pixels.
Starting the training by clicking on the arrow at the top left of the image, we see the
model slowly start to learn with successive epochs, as shown in Figure 4-8.
<i>Figure</i> <i>4-8.</i> <i>What</i> <i>the</i> <i>model</i> <i>learns</i> <i>as</i> <i>training</i> <i>progresses.</i> <i>The</i> <i>graphs</i> <i>at</i> <i>the</i> <i>top</i> <i>are</i> <i>the</i>
<i>training</i> <i>loss</i> <i>and</i> <i>validation</i> <i>error,</i> <i>while</i> <i>the</i> <i>images</i> <i>show</i> <i>how</i> <i>the</i> <i>model</i> <i>at</i> <i>that</i> <i>stage</i>
<i>would</i> <i>predict</i> <i>the</i> <i>color</i> <i>of</i> <i>a</i> <i>point</i> <i>at</i> <i>each</i> <i>coordinate</i> <i>in</i> <i>the</i> <i>grid.</i>"|binary classifier; checkpoints; Checkpoints design pattern
"<i>Figure</i> <i>3-3.</i> <i>Given</i> <i>a</i> <i>specific</i> <i>set</i> <i>of</i> <i>inputs</i> <i>(for</i> <i>example,</i> <i>male</i> <i>babies</i> <i>born</i> <i>to</i> <i>25-year-old</i>
<i>mothers</i> <i>at</i> <i>38</i> <i>weeks)</i> <i>the</i> <i>weight_pounds</i> <i>variable</i> <i>takes</i> <i>a</i> <i>range</i> <i>of</i> <i>values,</i> <i>approxi‐</i>
<i>mately</i> <i>following</i> <i>a</i> <i>normal</i> <i>distribution</i> <i>centered</i> <i>at</i> <i>7.5</i> <i>lbs.</i>
However, notice the width of the distribution—even though the distribution peaks at
7.5 pounds, there is a nontrivial likelihood (actually 33%) that a given baby is less
than 6.5 pounds or more than 8.5 pounds! The width of this distribution indicates
the irreducible error inherent to the problem of predicting baby weight. Indeed, the
best root mean square error we can obtain on this problem, if we frame it as a regres‐
sion problem, is the standard deviation of the distribution seen in Figure 3-3.
If we frame this as a regression problem, we’d have to state the prediction result as
7.5 +/- 1.0 (or whatever the standard deviation is). Yet, the width of the distribution
will vary for different combinations of inputs, and so learning the width is another
machine learning problem in and of itself. For example, at the 36th week, for mothers
of the same age, the standard deviation is 1.16 pounds. <i>Quantiles</i> <i>regression,</i> covered
later in the pattern discussion, tries to do exactly this but in a nonparametric way.
Had the distribution been multimodal (with multiple peaks), the
case for reframing the problem as a classification would be even
stronger. However, it is helpful to realize that because of the law of
large numbers, as long as we capture all of the relevant inputs,
many of the distributions we will encounter on large datasets will
be bell-shaped, although other distributions are possible. The wider
the bell curve, and the more this width varies at different values of
inputs, the more important it is to capture uncertainty and the
stronger the case for reframing the regression problem as a classifi‐
cation one.
By reframing the problem, we train the model as a multiclass classification that learns
a discrete probability distribution for the given training examples. These discretized
predictions are more flexible in terms of capturing uncertainty and better able to"|discrete probability distribution; PDF; quantile regression; Reframing design pattern
"image_generator
approach above to feed our images to our model for training.
Instead of exporting this as a TF Lite model, we can use model.save() to export our
model for serving.
To demonstrate deploying the phase 2 model to the cloud, we’ll use Cloud AI Plat‐
form Prediction. We’ll need to upload our saved model assets to a Cloud Storage
bucket, then deploy the model by specifying the framework and pointing AI Platform
Prediction to our storage bucket.
You can use any cloud-based custom model deployment tool for
the second phase of the Two-Phase Predictions design pattern. In
addition to Google Cloud’s AI Platform Prediction, AWS Sage‐
Maker and Azure Machine Learning both offer services for deploy‐
ing custom models.
When we export our model as a TensorFlow SavedModel, we can pass a Cloud Stor‐
age bucket URL directly to the save model method:
model.save('gs://your_storage_bucket/path')
This will export our model in the TF SavedModel format and upload it to our Cloud
Storage bucket.
In AI Platform, a model resource contains different versions of your model. Each
model can have hundreds of versions. We’ll first create the model resource using
gcloud, the Google Cloud CLI:
gcloud ai-platform models create instrument_classification
There are a few ways to deploy your model. We’ll use gcloud and point AI Platform
at the storage subdirectory that contains our saved model assets:
gcloud ai-platform versions create v1 <b>\</b>
--model instrument_classification <b>\</b>
--origin 'gs://your_storage_bucket/path/model_timestamp' <b>\</b>
--runtime-version=2.1 <b>\</b>
--framework='tensorflow' <b>\</b>
--python-version=3.7
We can now make prediction requests to our model via the AI Platform Prediction
API, which supports online and batch prediction. Online prediction lets us get pre‐
dictions in near real time on a few examples at once. If we have hundreds or thou‐
sands of examples we want to send for prediction, we can create a batch prediction
job that will run asynchronously in the background and output the prediction results
to a file when complete.
To handle cases where the device calling our model may not always be connected to
the internet, we could store audio clips for instrument prediction on the device while"|batch prediction; Two-Phase Predictions design pattern
"be sparse. The learning algorithm needs to extract the most salient information from
the input and represent it in a more concise way in the feature. The process of learn‐
ing features to represent the input data is called <i>feature</i> <i>extraction,</i> and we can think
of learnable data representations (like embeddings) as automatically engineered
features.
The data representation doesn’t even need to be of a single input variable—an obli‐
que decision tree, for example, creates a boolean feature by thresholding a linear
combination of two or more input variables. A decision tree where each node can
represent only one input variable reduces to a stepwise linear function, whereas an
oblique decision tree where each node can represent a linear combination of input
variables reduces to a piecewise linear function (see Figure 2-2). Considering how
many steps will have to be learned to adequately represent the line, the piecewise lin‐
ear model is simpler and faster to learn. An extension of this idea is the <i>Feature</i> <i>Cross</i>
design pattern, which simplifies the learning of AND relationships between multival‐
ued categorical variables.
<i>Figure</i> <i>2-2.</i> <i>A</i> <i>decision</i> <i>tree</i> <i>classifier</i> <i>where</i> <i>each</i> <i>node</i> <i>can</i> <i>threshold</i> <i>only</i> <i>one</i> <i>input</i>
<i>value</i> <i>(x1</i> <i>or</i> <i>x2)</i> <i>will</i> <i>result</i> <i>in</i> <i>a</i> <i>stepwise</i> <i>linear</i> <i>boundary</i> <i>function,</i> <i>whereas</i> <i>an</i> <i>oblique</i>
<i>tree</i> <i>classifier</i> <i>where</i> <i>a</i> <i>node</i> <i>can</i> <i>threshold</i> <i>a</i> <i>linear</i> <i>combination</i> <i>of</i> <i>input</i> <i>variables</i> <i>will</i>
<i>result</i> <i>in</i> <i>a</i> <i>piecewise</i> <i>linear</i> <i>boundary</i> <i>function.</i> <i>The</i> <i>piecewise</i> <i>linear</i> <i>function</i> <i>requires</i>
<i>fewer</i> <i>nodes</i> <i>and</i> <i>can</i> <i>achieve</i> <i>greater</i> <i>accuracy.</i>
The data representation doesn’t need to be learned or fixed—a hybrid is also possible.
The <i>Hashed</i> <i>Feature</i> design pattern is deterministic, but doesn’t require a model to
know all the potential values that a particular input can take.
The data representations we have looked at so far are all one-to-one. Although we
could represent input data of different types separately or represent each piece of data
as just one feature, it can be more advantageous to use <i>Multimodal</i> <i>Input.</i> That is the
fourth design pattern we will explore in this chapter."|data representation; decision trees; Embedding design pattern; Feature Cross design pattern; feature extraction; Hashed Feature design pattern
"<b>Creatingcustomcomponents</b>
Instead of using pre-built or customizable TFX components to construct our pipe‐
line, we can define our own containers to use as components, or convert a Python
function to a component.
To use the container-based components provided by TFX, we use the create_con
tainer_component method, passing it the inputs and outputs for our component and
a base Docker image along with any entrypoint commands for the container. For
example, the following container-based component invokes the command-line tool
bq to download a BigQuery dataset:
component = create_container_component(
name='DownloadBQData',
parameters={
'dataset_name': string,
'storage_location': string
},
image='google/cloud-sdk:278.0.0',
,
command=[
'bq', 'extract', '--compression=csv', '--field_delimiter=,',
InputValuePlaceholder('dataset_name'),
InputValuePlaceholder('storage_location'),
]
)
It’s best to use a base image that already has most of the dependencies we need. We’re
using the Google Cloud SDK image, which provides us the bq command-line tool.
It is also possible to convert a custom Python function into a TFX component using
the @component decorator. To demonstrate it, let’s say we have a step for preparing
resources used throughout our pipeline where we create a Cloud Storage bucket. We
can define this custom step using the following code:
<b>from</b> <b>google.cloud</b> <b>import</b> storage
client = storage.Client(project=""your-cloud-project"")
@component
<b>def</b> CreateBucketComponent(
bucket_name: Parameter[string] = 'your-bucket-name',
) -> OutputDict(bucket_info=string):
client.create_bucket('gs://' + bucket_name)
bucket_info = storage_client.get_bucket('gs://' + bucket_name)
<b>return</b> {
'bucket_info': bucket_info
}
We can then add this component to our pipeline definition:"|TFX; Workflow Pipeline design pattern
"trees versus the random forest. A decision tree ultimately learns boundary values for
each feature that guide a single instance to the model’s final prediction. As such, it is
easy to explain why a decision tree makes the predictions it did. The random forest,
being an ensemble of many decision trees, loses this level of local interpretability.
<b>Choosingtherighttoolfortheproblem</b>
It’s also important to keep in mind the bias–variance trade-off. Some ensemble tech‐
niques are better at addressing bias or variance than others (Table 3-2). In particular,
boosting is adapted for addressing high bias, while bagging is useful for correcting
high variance. That being said, as we saw in the section on “Bagging” on page 100,
combining two models with highly correlated errors will do nothing to help lower the
variance. In short, using the wrong ensemble method for our problem won’t neces‐
sarily improve performance; it will just add unnecessary overhead.
<i>Table</i> <i>3-2.</i> <i>A</i> <i>summary</i> <i>of</i> <i>the</i> <i>trade-off</i> <i>between</i> <i>bias</i> <i>and</i> <i>variance</i>
<b>Problem</b> <b>Ensemblesolution</b>
Highbias(underfitting) Boosting
Highvariance(overfitting) Bagging
<b>Otherensemblemethods</b>
We’ve discussed some of the more common ensemble techniques in machine learn‐
ing. The list discussed earlier is by no means exhaustive and there are different
algorithms that fit with these broad categories. There are also other ensemble techni‐
ques, including many that incorporate a Bayesian approach or that combine neural
architecture search and reinforcement learning, like Google’s AdaNet or AutoML
techniques. In short, the Ensemble design pattern encompasses techniques that com‐
bine multiple machine learning models to improve overall model performance and
can be particularly useful when addressing common training issues like high bias or
high variance.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>8:</b></largefont> <largefont><b>Cascade</b></largefont></header>
The Cascade design pattern addresses situations where a machine learning problem
can be profitably broken into a series of ML problems. Such a cascade often requires
careful design of the ML experiment.
<header><largefont><b>Problem</b></largefont></header>
What happens if we need to predict a value during both usual and unusual activity?
The model will learn to ignore the unusual activity because it is rare. If the unusual
activity is also associated with abnormal values, then trainability suffers."|AdaNet; AutoML technique; bagging; bias-variance tradeoff; boosting; Cascade design pattern; Ensemble design pattern
"If we want to trigger our pipeline based on changes to source code, a managed CI/CD
service like Cloud Build can help. When Cloud Build executes our code, it is run as a
series of containerized steps. This approach fits well within the context of pipelines.
We can connect Cloud Build to GitHub Actions or GitLab Triggers on the repository
where our pipeline code is located. When the code is committed, Cloud Build will
then build the containers associated with our pipeline based on the new code and cre‐
ate a run.
<b>ApacheAirflowandKubeflowPipelines</b>
In addition to TFX, Apache Airflow and Kubeflow Pipelines are both alternatives for
implementing the Workflow Pipeline pattern. Like TFX, both Airflow and KFP treat
pipelines as a DAG where the workflow for each step is defined in a Python script.
They then take this script and provide APIs to handle scheduling and orchestrating
the graph on the specified infrastructure. Both Airflow and KFP are open source and
can therefore run on-premises or in the cloud.
It’s common to use Airflow for data engineering, so it’s worth considering for an
organization’s data ETL tasks. However, while Airflow provides robust tooling for
running jobs, it was built as a general-purpose solution and wasn’t designed with ML
workloads in mind. KFP, on the other hand, was designed specifically for ML and
operates at a lower level than TFX, providing more flexibility in how pipeline steps
are defined. While TFX implements its own approach to orchestration, KFP lets us
choose how to orchestrate our pipelines through its API. The relationship between
TFX, KFP, and Kubeflow is summarized in Figure 6-10."|Apache Airflow; CI/CD; Cloud Build; DAG; GitHub Actions; GitLab Triggers; Kubeflow Pipelines; TFX; Workflow Pipeline design pattern
"In such cases, it is helpful to consider what the end goal is. The end goal of the ML
model is not to predict whether a stock will go up or down. We will be unable to buy
every stock that we predict will go up, and unable to sell stocks that we don’t hold.
The better strategy might be to buy call options3 for the 10 stocks that are most likely
to go up more than 5% over the next 6 months, and buy put options for stocks that
are most likely to go down more than 5% over the next 6 months.
The solution, then, is to create a training dataset consisting of three classes:
• Stocks that went up more than 5%—call.
• Stocks that went down more than 5%—put.
• The remaining stocks are in the neutral category.
Rather than train a regression model on how much stocks will go up, we can now
train a classification model with these three classes and pick the most confident pre‐
dictions from our model.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>10:</b></largefont> <largefont><b>Rebalancing</b></largefont></header>
The Rebalancing design pattern provides various approaches for handling datasets
that are inherently imbalanced. By this we mean datasets where one label makes up
the majority of the dataset, leaving far fewer examples of other labels.
This design pattern does <i>not</i> address scenarios where a dataset lacks representation
for a specific population or real-world environment. Cases like this can often only be
solved by additional data collection. The Rebalancing design pattern primarily
addresses how to build models with datasets where few examples exist for a specific
class or classes.
<header><largefont><b>Problem</b></largefont></header>
Machine learning models learn best when they are given a similar number of exam‐
ples for each label class in a dataset. Many real-world problems, however, are not so
neatly balanced. Take for example a fraud detection use case, where you are building
a model to identify fraudulent credit card transactions. Fraudulent transactions are
much rarer than regular transactions, and as such, there is less data on fraud cases
available to train a model. The same is true for other problems like detecting whether
someone will default on a loan, identifying defective products, predicting the
presence of a disease given medical images, filtering spam emails, flagging error logs
in a software application, and more.
3 Seehttps://oreil.ly/kDndFforaprimeroncallandputoptions."|fraud detection; Neutral Class design pattern; Rebalancing design pattern
"<i>Table</i> <i>2-1.</i> <i>The</i> <i>FarmHash</i> <i>of</i> <i>some</i> <i>IATA</i> <i>airport</i> <i>codes</i> <i>when</i> <i>hashed</i> <i>into</i> <i>different</i>
<i>numbers</i> <i>of</i> <i>buckets</i>
<b>Row</b> <b>departure_airport</b> <b>hash3</b> <b>hash10</b> <b>hash1000</b>
1 DTW 1 3 543
2 LBB 2 9 709
3 SNA 2 7 587
4 MSO 2 7 737
5 ANC 0 8 508
6 PIT 1 7 267
7 PWM 1 9 309
8 BNA 1 4 744
9 SAF 1 2 892
10 IPL 2 1 591
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Assume that we have chosen to hash the airport code using 10 buckets (hash10 in
Table 2-1). How does this address the problems we identified?
<b>Out-of-vocabularyinput</b>
Even if an airport with a handful of flights is not part of the training dataset, its
hashed feature value will be in the range [0–9]. Therefore, there is no resilience prob‐
lem during serving—the unknown airport will get the predictions corresponding
with other airports in the hash bucket. The model will not error out.
If we have 347 airports, an average of 35 airports will get the same hash bucket code if
we hash it into 10 buckets. An airport that is missing from the training dataset will
“borrow” its characteristics from the other similar ~35 airports in the hash bucket. Of
course, the prediction for a missing airport won’t be accurate (it is unreasonable to
expect accurate predictions for unknown inputs), but it will be in the right range.
Choose the number of hash buckets by balancing the need to handle out-of-
vocabulary inputs reasonably and the need to have the model accurately reflect the
categorical input. With 10 hash buckets, ~35 airports get commingled. A good rule of
thumb is to choose the number of hash buckets such that each bucket gets about five
entries. In this case, that would mean that 70 hash buckets is a good compromise.
<b>Highcardinality</b>
It’s easy to see that the high cardinality problem is addressed as long as we choose a
small enough number of hash buckets. Even if we have millions of airports or hospi‐
tals or physicians, we can hash them into a few hundred buckets, thus keeping the
system’s memory and model size requirements practical."|heuristic to choose numbers; Hashed Feature design pattern; high cardinality; vocabulary
"ABS(FARM_FINGERPRINT(airport))
we would run into a rare and likely unreproducible overflow error if the
FARM_FINGERPRINT operation happened to return –9,223,372,036,854,775,808
INT64!
since its absolute value can not be represented using an
<b>Emptyhashbuckets</b>
Although unlikely, there is a remote possibility that even if we choose 10 hash buck‐
ets to represent 347 airports, one of the hash buckets will be empty. Therefore, when
using hashed feature columns, it may be beneficial to also use L2 regularization so
that the weights associated with an empty bucket will be driven to near-zero. This
way, if an out-of-vocabulary airport does fall into an empty bucket, it will not cause
the model to become numerically unstable.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>2:</b></largefont> <largefont><b>Embeddings</b></largefont></header>
Embeddings are a learnable data representation that map high-cardinality data into a
lower-dimensional space in such a way that the information relevant to the learning
problem is preserved. Embeddings are at the heart of modern-day machine learning
and have various incarnations throughout the field.
<header><largefont><b>Problem</b></largefont></header>
Machine learning models systematically look for patterns in data that capture how
the properties of the model’s input features relate to the output label. As a result, the
data representation of the input features directly affects the quality of the final model.
While handling structured, numeric input is fairly straightforward, the data needed
to train a machine learning model can come in myriad varieties, such as categorical
features, text, images, audio, time series, and many more. For these data representa‐
tions, we need a meaningful numeric value to supply our machine learning model so
these features can fit within the typical training paradigm. Embeddings provide a way
to handle some of these disparate data types in a way that preserves similarity
between items and thus improves our model’s ability to learn those essential patterns.
One-hot encoding is a common way to represent categorical input variables. For
dataset.3
example, consider the plurality input in the natality This is a categorical
['Single(1)', 'Multiple(2+)', 'Twins(2)',
input that has six possible values:
'Triplets(3)' , 'Quadruplets(4)' , 'Quintuplets(5)'] . We can handle this
categorical input using a one-hot encoding that maps each potential input string
6
value to a unit vector in R , as shown in Table 2-3.
3 ThisdatasetisavailableinBigQuery:bigquery-public-data.samples.natality."|Embedding design pattern; feature columns; hash buckets; Hashed Feature design pattern; one-hot encoding
"<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>26:</b></largefont> <largefont><b>Feature</b></largefont> <largefont><b>Store</b></largefont></header>
The <i>Feature</i> <i>Store</i> design pattern simplifies the management and reuse of features
across projects by decoupling the feature creation process from the development of
models using those features.
<header><largefont><b>Problem</b></largefont></header>
Good feature engineering is crucial for the success of many machine learning solu‐
tions. However, it is also one of the most time-consuming parts of model develop‐
ment. Some features require significant domain knowledge to calculate correctly, and
changes in the business strategy can affect how a feature should be computed. To
ensure such features are computed in a consistent way, it’s better for these features to
be under the control of domain experts rather than ML engineers. Some input fields
might allow for different choices of data representations (see Chapter 2) to make
them more amenable for machine learning. An ML engineer or data scientist will
typically experiment with multiple different transformations to determine which are
helpful and which aren’t, before deciding which features will be used in the final
model. Many times, the data used for the ML model isn’t drawn from a single source.
Some data may come from a data warehouse, some data may sit in a storage bucket as
unstructured data, and other data may be collected in real time through streaming.
The structure of the data may also vary between each of these sources, requiring each
input to have its own feature engineering steps before it can be fed into a model. This
development is often done on a VM or personal machine, causing the feature cre‐
ation to be tied to the software environment where the model is built, and the more
complex the model gets, the more complicated these data pipelines become.
An ad hoc approach where features are created as needed by ML projects may work
for one-off model development and training, but as organizations scale, this method
of feature engineering becomes impractical and significant problems arise:
• Ad hoc features aren’t easily reused. Features are re-created over and over again,
either by individual users or within teams, or never leave the pipelines (or note‐
books) in which they are created. This is particularly problematic for higher-level
features that are complex to calculate. This could be because they are derived
through expensive processes, such as pre-trained user or catalog item embed‐
dings. Other times, it could be because the features are captured from upstream
processes such as business priorities, availability of contracting, or market seg‐
mentations. Another source of complexity is when higher-level features, such as
the number of orders by a customer in the past month, involve aggregations over
time. Effort and time are wasted creating the same features from scratch for each
new project."|data scientists; feature engineering; Feature Store design pattern; ML engineers
"that while the model is executing training step <i>N,</i> the input pipeline is reading and
preparing data for training step <i>N</i> <i>+</i> <i>1,</i> as shown in Figure 4-22.
<i>Figure</i> <i>4-21.</i> <i>With</i> <i>distributed</i> <i>training</i> <i>on</i> <i>multiple</i> <i>GPU/TPUs</i> <i>available,</i> <i>it</i> <i>is</i> <i>necessary</i>
<i>to</i> <i>have</i> <i>efficient</i> <i>input</i> <i>pipelines.</i>
<i>Figure</i> <i>4-22.</i> <i>Prefetching</i> <i>overlaps</i> <i>preprocessing</i> <i>and</i> <i>model</i> <i>execution,</i> <i>so</i> <i>that</i> <i>while</i> <i>the</i>
<i>model</i> <i>is</i> <i>executing</i> <i>one</i> <i>training</i> <i>step,</i> <i>the</i> <i>input</i> <i>pipeline</i> <i>is</i> <i>reading</i> <i>and</i> <i>preparing</i> <i>data</i>
<i>for</i> <i>the</i> <i>next.</i>
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>15:</b></largefont> <largefont><b>Hyperparameter</b></largefont> <largefont><b>Tuning</b></largefont></header>
In Hyperparameter Tuning, the training loop is itself inserted into an optimization
method to find the optimal set of model hyperparameters.
<header><largefont><b>Problem</b></largefont></header>
In machine learning, model training involves finding the optimal set of breakpoints
(in the case of decision trees), weights (in the case of neural networks), or support
vectors (in the case of support vector machines). We term these <i>model</i> parameters.
However, in order to carry out model training and find the optimal model parame‐
ters, we often have to hardcode a variety of things. For example, we might decide that
the maximum depth of a tree will be 5 (in the case of decision trees), or that the acti‐
vation function will be ReLU (for neural networks) or choose the set of kernels that
we will employ (in SVMs). These parameters are called <i>hyperparameters.</i>
Model parameters refer to the weights and biases learned by your model. You do not
have direct control over model parameters, since they are largely a function of your
training data, model architecture, and many other factors. In other words, you can‐
not manually set model parameters. Your model’s weights are initialized with ran‐
dom values and then optimized by your model as it goes through training iterations.
Hyperparameters, on the other hand, refer to any parameters that you, as a model"|Distribution Strategy design pattern; Hyperparameter Tuning design pattern; hyperparameters; model parameters
"SHAP has some built-in visualization methods that make it easier to understand the
resulting attribution values. We’ll use SHAP’s force_plot() method to plot the attri‐
bution values for the first example in our test set with the following code:
shap.force_plot(
explainer.expected_value[0],
shap_values[0][0,:],
x_test.iloc[0,:]
)
explainer.expected_value
In the code above, is our model’s baseline. SHAP calcu‐
lates the baseline as the mean of the model’s output across the dataset we passed
when we created the explainer (in this case, x_train[:100] ), though we could also
force_plot.
pass our own baseline value to The ground truth value for this example
is 14 miles per gallon, and our model predicts 13.16. Our explanation will therefore
explain our model’s prediction of 13.16 with feature attribution values. In this case,
the attribution values are relative to the model’s baseline of 24.16 MPG. The attribu‐
tion values should therefore add up to roughly 11, the difference between the model’s
baseline and the prediction for this example. We can identify the most important fea‐
tures by looking at the ones with the highest absolute value. Figure 7-3 shows the
resulting plot for this example’s attribution values.
<i>Figure</i> <i>7-3.</i> <i>The</i> <i>feature</i> <i>attribution</i> <i>values</i> <i>for</i> <i>one</i> <i>example</i> <i>from</i> <i>our</i> <i>fuel</i> <i>efficiency</i> <i>pre‐</i>
<i>diction</i> <i>model.</i> <i>In</i> <i>this</i> <i>case,</i> <i>the</i> <i>car’s</i> <i>weight</i> <i>is</i> <i>the</i> <i>most</i> <i>significant</i> <i>indicator</i> <i>of</i> <i>MPG</i>
<i>with</i> <i>a</i> <i>feature</i> <i>attribution</i> <i>value</i> <i>of</i> <i>roughly</i> <i>6.</i> <i>Had</i> <i>our</i> <i>model’s</i> <i>prediction</i> <i>been</i> <i>above</i>
<i>the</i> <i>baseline</i> <i>of</i> <i>24.16,</i> <i>we</i> <i>would</i> <i>instead</i> <i>see</i> <i>mostly</i> <i>negative</i> <i>attribution</i> <i>values.</i>
For this example, the most important indicator of fuel efficiency is weight, pushing
our model’s prediction down by about 6 MPG from the baseline. This is followed by
horsepower, displacement, and then the car’s model year. We can get a summary (or
global explanation) of the feature attribution values for the first 10 examples from
our test set with the following:
shap.summary_plot(
shap_values,
feature_names=data.columns.tolist(),
class_names=['MPG']
)
This results in the summary plot shown in Figure 7-4."|baseline; Explainable Predictions design pattern; feature attributions; SHAP
"The externalized model state gets updated every 10 minutes based on a 2-hour rolling
window:
<b>Windowclosetime</b> <b>prediction</b> <b>acceptable_deviation</b>
2010-05-10T06:35:00 -2.8421052631578947 10.48412597725367
2010-05-10T06:45:00 -2.6818181818181817 12.083729926046008
2010-05-10T06:55:00 -2.9615384615384617 11.765962341537781
The code to extract the model parameters shown above is similar to that of the pan‐
das case, but it is done within a Beam pipeline. This allows the code to work in
streaming, but the model state is available only within the context of the sliding win‐
dow. In order to carry out inference on every arriving flight, we need to externalize
the model state (similar to how we export the model weights out to a file in the State‐
less Serving Function pattern to decouple it from the context of the training program
where these weights are computed):
model_external = beam.pvalue.AsSingleton(model_state)
This externalized state can be used to detect whether or not a given flight is an
anomaly:
<b>def</b> is_anomaly(flight, model_external_state):
result = flight.copy()
error = flight['delay'] - model_external_state['prediction']
tolerance = model_external_state['acceptable_deviation']
result['is_anomaly'] = np.abs(error) > tolerance
<b>return</b> result
is_anomaly
The function is then applied to every item in the last pane of the sliding
window:
anomalies = (windowed
| 'latest_slice' >> beam.FlatMap(is_latest_slice)
| 'find_anomaly' >> beam.Map(is_anomaly, model_external))
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The solution suggested above is computationally efficient in the case of high-
throughput data streams but can be improved further if the ML model parameters
can be updated online. This pattern is also applicable to stateful ML models such as
recurrent neural networks and when a stateless model requires stateful input features.
<b>Reducecomputationaloverhead</b>
In the Problem section, we used the following pandas code:
dfw['delay'].rolling('2h').apply(is_anomaly, raw=False);
Whereas, in the Solution section, the Beam code was as follows:"|Apache Beam; Windowed Inference design pattern
"cp_callback = tf.keras.callbacks.ModelCheckpoint(...)
history = model.fit(trainds,
validation_data=evalds,
epochs=15,
batch_size=128,
callbacks=[cp_callback])
However, using epochs on large datasets remains a bad idea. Epochs may be easy to
understand, but the use of epochs leads to bad effects in real-world ML models. To
see why, imagine that you have a training dataset with one million examples. It can
be tempting to simply go through this dataset 15 times (for example) by setting the
number of epochs to 15. There are several problems with this:
• The number of epochs is an integer, but the difference in training time between
processing the dataset 14.3 times and 15 times can be hours. If the model has
converged after having seen 14.3 million examples, you might want to exit and
not waste the computational resources necessary to process 0.7 million more
examples.
• You checkpoint once per epoch, and waiting one million examples between
checkpoints might be way too long. For resilience, you might want to checkpoint
more often.
• Datasets grow over time. If you get 100,000 more examples and you train the
model and get a higher error, is it because you need to do an early stop, or is the
new data corrupt in some way? You can’t tell because the prior training was on
15 million examples and the new one is on 16.5 million examples.
• In distributed, parameter-server training (see “Design Pattern 14: Distribution
Strategy” on page 175) with data parallelism and proper shuffling, the concept of an
epoch is not clear anymore. Because of potentially straggling workers, you can
only instruct the system to train on some number of mini-batches.
<b>Stepsperepoch.</b>
Instead of training for 15 epochs, we might decide to train for
143,000 steps where the batch_size is 100:
NUM_STEPS = 143000
BATCH_SIZE = 100
NUM_CHECKPOINTS = 15
cp_callback = tf.keras.callbacks.ModelCheckpoint(...)
history = model.fit(trainds,
validation_data=evalds,
epochs=NUM_CHECKPOINTS,
steps_per_epoch=NUM_STEPS // NUM_CHECKPOINTS,
batch_size=BATCH_SIZE,
callbacks=[cp_callback])"|Checkpoints design pattern; epochs
"each step. There are also other synchronous distribution strategies within
Keras, such as CentralStorageStrategy and MultiWorkerMirroredStrategy .
MultiWorkerMirroredStrategy enables the distribution to be spread not just on
CentralStorageStrategy
GPUs on a single machine, but on multiple machines. In ,
the model variables are not mirrored; instead, they are placed on the CPU and opera‐
tions are replicated across all local GPUs. So the variable updates only happen in one
place.
When choosing between different distribution strategies, the best option depends on
your computer topology and how fast the CPUs and GPUs can communicate with
one another. Table 4-2 summarizes how the different strategies described here com‐
pare on these criteria.
<i>Table</i> <i>4-2.</i> <i>Choosing</i> <i>between</i> <i>distribution</i> <i>strategies</i> <i>depends</i> <i>on</i> <i>your</i> <i>computer</i> <i>topology</i> <i>and</i>
<i>how</i> <i>fast</i> <i>the</i> <i>CPUs</i> <i>and</i> <i>GPUs</i> <i>can</i> <i>communicate</i> <i>with</i> <i>one</i> <i>another</i>
<b>FasterCPU-GPUconnection</b> <b>FasterGPU-GPUconnection</b>
OnemachinewithmultipleGPUs CentralStorageStrategy MirroredStrategy
MultiplemachineswithmultipleGPUs MultiWorkerMirroredStrategy MultiWorkerMirroredStrategy
<header><largefont><b>Distributed</b></largefont> <largefont><b>Data</b></largefont> <largefont><b>Parallelism</b></largefont> <largefont><b>in</b></largefont> <largefont><b>PyTorch</b></largefont></header>
DistributedDataParallel
In PyTorch, the code always uses whether you have one
GPU or multiple GPUs and whether the model is run on one machine or multiple
machines. Instead, how and where you start the processes and how you wire up sam‐
pling, data loading, and so on determines the distribution strategy.
First, we initialize the process and wait for other processes to start and set up commu‐
nication using:
torch.distributed.init_process_group(backend=""nccl"")
Second, specify the device number by obtaining a rank from the command line.
Rank = 0 is the master process, and 1,2,3,... are the workers:
device = torch.device(""cuda:{}"".format(local_rank))
The model is created as normal in each of the processes, but is sent to this device. A
distributed version of the model that will process its shard of batch is created using
DistributedDataParallel:
model = model.to(device)
ddp_model = DistributedDataParallel(model, device_ids=[local_rank],
output_device=local_rank)
DistributedSampler
The data itself is sharded using a , and each batch of data is also
sent to the device:"|data parallelism; DistributedDataParallel; Distribution Strategy design pattern; GPU; PyTorch; synchronous training
"sampler = DistributedSampler(dataset=trainds)
loader = DataLoader(dataset=trainds, batch_size=batch_size,
sampler=sampler, num_workers=4)
...
<b>for</b> data <b>in</b> train_loader:
features, labels = data[0].to(device), data[1].to(device)
When a PyTorch trainer is launched, it is told the total number of nodes and its own
rank:
python -m torch.distributed.launch --nproc_per_node=4 <b>\</b>
<b>--nnodes=16</b> <b>--node_rank=3</b> --master_addr=""192.168.0.1"" <b>\</b>
--master_port=1234 my_pytorch.py
If the number of nodes is one, we have the equivalent of TensorFlow’s
MirroredStrategy , and if the number of nodes is more than one, we have the equiva‐
MultiWorkerMirroredStrategy.
lent of TensorFlow’s If the number of processes per
OneDeviceStrategy
node and number of nodes are both one, then we have a . Opti‐
mized communication for all these cases is provided if supported by the backend
init_process_group
(NCCL, in this case) passed into .
<b>Asynchronoustraining</b>
In asynchronous training, the workers train on different slices of the input data inde‐
pendently, and the model weights and parameters are updated asynchronously, typi‐
cally through a parameter server architecture. This means that no one worker waits
for updates to the model from any of the other workers. In the parameter-server
architecture, there is a single parameter server that manages the current values of the
model weights, as in Figure 4-16.
As with synchronous training, a mini-batch of data is split among each of the sepa‐
rate workers for each SGD step. Each device performs a forward pass with their por‐
tion of the mini-batch and computes gradients for each parameter of the model.
Those gradients are sent to the parameter server, which performs the parameter
update and then sends the new model parameters back to the worker with another
split of the next mini-batch.
The key difference between synchronous and asynchronous training is that the
parameter server does not do an <i>all-reduce.</i> Instead, it computes the new model
parameters periodically based on whichever gradient updates it received since the last
computation. Typically, asynchronous distribution achieves higher throughput than
synchronous training because a slow worker doesn’t block the progression of training
steps. If a single worker fails, the training continues as planned with the other work‐
ers while that worker reboots. As a result, some splits of the mini-batch may be lost
during training, making it too difficult to accurately keep track of how many epochs
of data have been processed. This is another reason why we typically specify virtual"|asynchronous training; Distribution Strategy design pattern; MirroredStrategy; MultiWorkerMirroredStrategy; OneDeviceStrategy; parameter server architecture; synchronous training; training
"As a related point, when choosing which features to combine for a feature cross, we
would not want to cross two features that are highly correlated. We can think of a
feature cross as combining two features to create an ordered pair. In fact, the term
“cross” of “feature cross” refers to the Cartesian product. If two features are highly
correlated, then the “span” of their feature cross doesn’t bring any new information
to the model. As an extreme example, suppose we had two features, x_1 and x_2,
where x_2 = 5*x_1. Bucketing values for x_1 and x_2 by their sign and creating a fea‐
ture cross will still produce four new boolean features. However, due to the
dependence of x_1 and x_2, two of those four features are actually empty, and the
other two are precisely the two buckets created for x_1.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>4:</b></largefont> <largefont><b>Multimodal</b></largefont> <largefont><b>Input</b></largefont></header>
The Multimodal Input design pattern addresses the problem of representing different
types of data or data that can be expressed in complex ways by concatenating all the
available data representations.
<header><largefont><b>Problem</b></largefont></header>
Typically, an input to a model can be represented as a number or as a category, an
image, or free-form text. Many off-the-shelf models are defined for specific types of
input only—a standard image classification model such as Resnet-50, for example,
does not have the ability to handle inputs other than images.
To understand the need for multimodal inputs, let’s say we’ve got a camera capturing
footage at an intersection to identify traffic violations. We want our model to handle
both image data (camera footage) and some metadata about when the image was cap‐
tured (time of day, day of week, weather, etc.), as depicted in Figure 2-19.
This problem also occurs when training a structured data model where one of the
inputs is free-form text. Unlike numerical data, images and text cannot be fed directly
into a model. As a result, we’ll need to represent image and text inputs in a way our
model can understand (usually using the Embeddings design pattern), then combine
these inputs with other tabular 7 features. For example, we might want to predict a
restaurant patron’s rating based on their review text and other attributes such as what
they paid and whether it was lunch or dinner (see Figure 2-20).
7 Weusetheterm“tabulardata”torefertonumericalandcategoricalinputs,butnotfree-formtext.Youcan
thinkoftabulardataasanythingyoumightcommonlyfindinaspreadsheet.Forexample,valueslikeage,
typeofcar,price,ornumberofhoursworked.Tabulardatadoesnotincludefree-formtextlikedescriptions
orreviews."|Cartesian product; Embedding design pattern; Feature Cross design pattern; Multimodal Input design pattern
"<b>Deterministicinputs</b>
Splitting an ML problem is usually a bad idea, since an ML model can/should learn
combinations of multiple factors. For example:
• If a condition can be known deterministically from the input (holiday shopping
versus weekday shopping), we should just add the condition as one more input
to the model.
• If the condition involves an extrema in just one input (some customers who live
nearby versus far away, with the meaning of near/far needing to be learned from
the data), we can use Mixed Input Representation to handle it.
The Cascade design pattern addresses an unusual scenario for which we do not have
a categorical input, and for which extreme values need to be learned from multiple
inputs.
<b>Singlemodel</b>
The Cascade design pattern should not be used for common scenarios where a single
model will suffice. For example, suppose we are trying to learn a customer’s propen‐
sity to buy. We may think we need to learn different models for people who have
been comparison shopping versus those who aren’t. We don’t really know who has
been comparison shopping, but we can make an educated guess based on the number
of visits, how long the item has been in the cart, and so on. This problem does not
need the Cascade design pattern because it is common enough (a large fraction of
customers will be comparison shopping) that the machine learning model should be
able to learn it implicitly in the course of training. For common scenarios, train a sin‐
gle model.
<b>Internalconsistency</b>
The Cascade is needed when we need to maintain internal consistency amongst the
predictions of multiple models. Note that we are trying to do more than just predict
the unusual activity. We are trying to predict returns, considering that there will be
some reseller activity also. If the task is only to predict whether or not a sale is by a
reseller, we’d use the Rebalancing pattern. The reason to use Cascade is that the
imbalanced label output is needed as an input to subsequent models and is useful in
and of itself.
Similarly, suppose that the reason we are training the model to predict a customer’s
propensity to buy is to make a discounted offer. Whether or not we make the dis‐
counted offer, and the amount of discount, will very often depend on whether this
customer is comparison shopping or not. Given this, we need internal consistency
between the two models (the model for comparison shoppers and the model for pro‐
pensity to buy). In this case, the Cascade design pattern might be needed."|Cascade design pattern; Mixed Input Representation; Rebalancing design pattern
"Saving the full model state so that model training can resume from a point is called
<i>checkpointing,</i> and the saved model files are called <i>checkpoints.</i> How often should we
checkpoint? The model state changes after every batch because of gradient descent.
So, technically, if we don’t want to lose any work, we should checkpoint after every
batch. However, checkpoints are huge and this I/O would add considerable overhead.
Instead, model frameworks typically provide the option to checkpoint at the end of
every epoch. This is a reasonable tradeoff between never checkpointing and check‐
pointing after every batch.
To checkpoint a model in Keras, provide a callback to the fit() method:
checkpoint_path = '{}/checkpoints/taxi'.format(OUTDIR)
cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,
save_weights_only=False,
verbose=1)
history = model.fit(x_train, y_train,
batch_size=64,
epochs=3,
validation_data=(x_val, y_val),
verbose=2,
callbacks=[cp_callback])
With checkpointing added, the training looping becomes what is shown in
Figure 4-6.
<i>Figure</i> <i>4-6.</i> <i>Checkpointing</i> <i>saves</i> <i>the</i> <i>full</i> <i>model</i> <i>state</i> <i>at</i> <i>the</i> <i>end</i> <i>of</i> <i>every</i> <i>epoch.</i>"|checkpointing; checkpoints; Checkpoints design pattern
"your model architecture. Once you are happy with your model’s performance during
evaluation, you’ll likely want to serve your model so that others can access it to make
predictions. We use the term <i>serving</i> to refer to accepting incoming requests and
sending back predictions by deploying the model as a microservice. The serving
infrastructure could be in the cloud, on-premises, or on-device.
The process of sending new data to your model and making use of its output is called
<i>prediction.</i> This can refer both to generating predictions from local models that have
not yet been deployed as well as getting predictions from deployed models. For
deployed models, we’ll refer both to online and batch prediction. <i>Online</i> <i>prediction</i> is
used when you want to get predictions on a few examples in near real time. With
online prediction, the emphasis is on low latency. <i>Batch</i> <i>prediction,</i> on the other
hand, refers to generating predictions on a large set of data offline. Batch prediction
jobs take longer than online prediction and are useful for precomputing predictions
(such as in recommendation systems) and in analyzing your model’s predictions
across a large sample of new data.
The word <i>prediction</i> is apt when it comes to forecasting future values, such as in pre‐
dicting the duration of a bicycle ride or predicting whether a shopping cart will be
abandoned. It is less intuitive in the case of image and text classification models. If an
ML model looks at a text review and outputs that the sentiment is positive, it’s not
really a “prediction” (there is no future outcome). Hence, you will also see word <i>infer‐</i>
<i>ence</i> being used to refer to predictions. The statistical term inference is being repur‐
posed here, but it’s not really about reasoning.
Often, the processes of collecting training data, feature engineering, training, and
evaluating your model are handled separately from the production pipeline. When
this is the case, you’ll reevaluate your solution whenever you decide you have enough
additional data to train a new version of your model. In other situations, you may
have new data being ingested continuously and need to process this data immediately
before sending it to your model for training or prediction. This is known as <i>stream‐</i>
<i>ing.</i> To handle streaming data, you’ll need a multistep solution for performing feature
engineering, training, evaluation, and predictions. Such multistep solutions are called
<i>ML</i> <i>pipelines.</i>
<header><largefont><b>Data</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>Tooling</b></largefont></header>
There are various Google Cloud products we’ll be referencing that provide tooling for
solving data and machine learning problems. These products are merely one option
for implementing the design patterns referenced in this book and are not meant to be
an exhaustive list. All of the products included here are serverless, allowing us to
focus more on implementing machine learning design patterns instead of the infra‐
structure behind them."|inference; low latency; ML pipelines; model evaluation; serverless; streaming
"<i>Figure</i> <i>6-5.</i> <i>Arrival</i> <i>delays</i> <i>at</i> <i>Dallas</i> <i>Fort</i> <i>Worth</i> <i>(DFW)</i> <i>airport</i> <i>on</i> <i>May</i> <i>10–11,</i> <i>2010.</i>
<i>Abnormal</i> <i>arrival</i> <i>delays</i> <i>are</i> <i>marked</i> <i>with</i> <i>a</i> <i>dot.</i>
The arrival delays exhibit considerable variability, but it is still possible to note
unusually large arrival delays (marked by a dot). Note that the definition of “unusual”
varies by context. Early in the morning (left corner of the plot), most flights are on
time, so even the small spike is anomalous. By the middle of the day (after 12 p.m. on
May 10), variability picks up and 25-minute delays are quite common, but a 75-
minute delay is still unusual.
Whether or not a specific delay is anomalous depends on a time context, for example,
on the arrival delays observed over the past two hours. To determine that a delay is
anomalous requires that we first sort the dataframe based on the time (as in the graph
in Figure 6-5 and shown below in pandas):
df = df.sort_values(by='scheduled_time').set_index('scheduled_time')
Then, we need to apply an anomaly detection function to sliding windows of two
hours:
df['delay'].rolling('2h').apply(is_anomaly, raw=False)
is_anomaly
The anomaly detection function, , can be quite sophisticated, but let’s
take the simple case of discarding extrema and calling a data value an anomaly if it is
more than four standard deviations from the mean in the two-hour window:
<b>def</b> is_anomaly(d):
outcome = d[-1] <i>#</i> <i>the</i> <i>last</i> <i>item</i>
<i>#</i> <i>discard</i> <i>min</i> <i>&</i> <i>max</i> <i>value</i> <i>&</i> <i>current</i> <i>(last)</i> <i>item</i>
xarr = d.drop(index=[d.idxmin(), d.idxmax(), d.index[-1]])
prediction = xarr.mean()
acceptable_deviation = 4 * xarr.std()
<b>return</b> np.abs(outcome - prediction) > acceptable_deviation
This works on historical (training) data because the entire dataframe is at hand. Of
course, when running inference on our production model, we will not have the entire
dataframe. In production, we will be receiving flight arrival information one by one,
as each flight arrives. So, all that we will have is a single delay value at a timestamp:"|Windowed Inference design pattern
"<b>def</b> read_dataset(client, <b>row_restriction,</b> batch_size=2048):
...
bqsession = client.read_session(
...
row_restriction=row_restriction)
dataset = bqsession.parallel_read_rows()
<b>return</b> (dataset.prefetch(1).map(features_and_labels)
.shuffle(batch_size*10).batch(batch_size))
client = BigQueryClient()
<b>train_df</b> = read_dataset(client, <b>'Time</b> <b><=</b> <b>144803',</b> 2048)
eval_df = read_dataset(client, <b>'Time</b> <b>></b> <b>144803',</b> 2048)
Another instance where a sequential split of data is needed is when there are high
correlations between successive times. For example, in weather forecasting, the
weather on consecutive days is highly correlated. Therefore, it is not reasonable to
put October 12 in the training dataset and October 13 in the testing dataset because
there will be considerable leakage (imagine, for example, that there is a hurricane on
October 12). Further, weather is highly seasonal, and so it is necessary to have days
from all seasons in all three splits. One way to properly evaluate the performance of a
forecasting model is to use a sequential split but take seasonality into account by
using the first 20 days of every month in the training dataset, the next 5 days in the
validation dataset, and the last 5 days in the testing dataset.
In all these instances, repeatable splitting requires only that we place the logic used to
create the split into version control and ensure that the model version is updated
whenever this logic is changed.
<b>Stratifiedsplit</b>
The example above of how weather patterns are different between different seasons is
an example of a situation where the splitting needs to happen after the dataset is
<i>stratified.</i> We needed to ensure that there were examples of all seasons in each split,
and so we stratified the dataset in terms of months before carrying out the split. We
used the first 20 days of every month in the training dataset, the next 5 days in the
validation dataset, and the last 5 days in the testing dataset. Had we not been con‐
cerned with the correlation between successive days, we could have randomly split
the dates within each month.
The larger the dataset, the less concerned we have to be with stratification. On very
large datasets, the odds are very high that the feature values will be well distributed
among all the splits. Therefore, in large-scale machine learning, the need to stratify
happens quite commonly only in the case of skewed datasets. For example, in the
flights dataset, less than 1% of flights take off before 6 a.m., and so the number of
flights that meet this criterion may be quite small. If it is critical for our business use
case to get the behavior of these flights correct, we should stratify the dataset based
on departure hour and split each stratification evenly."|Repeatable Splitting design pattern; stratified split; test data
"Accurate data labels are just as important as feature accuracy. Your model relies
solely on the ground truth labels in your training data to update its weights and mini‐
mize loss. As a result, incorrectly labeled training examples can cause misleading
model accuracy. For example, let’s say you’re building a sentiment analysis model
and 25% of your “positive” training examples have been incorrectly labeled as “nega‐
tive.” Your model will have an inaccurate picture of what should be considered nega‐
tive sentiment, and this will be directly reflected in its predictions.
To understand data <i>completeness,</i> let’s say you’re training a model to identify cat
breeds. You train the model on an extensive dataset of cat images, and the resulting
model is able to classify images into 1 of 10 possible categories (“Bengal,” “Siamese,”
and so forth) with 99% accuracy. When you deploy your model to production, how‐
ever, you find that in addition to uploading cat photos for classification, many of
your users are uploading photos of dogs and are disappointed with the model’s
results. Because the model was trained only to identify 10 different cat breeds, this is
all it knows how to do. These 10 breed categories are, essentially, the model’s entire
“world view.” No matter what you send the model, you can expect it to slot it into
one of these 10 categories. It may even do so with high confidence for an image that
looks nothing like a cat. Additionally, there’s no way your model will be able to
return “not a cat” if this data and label weren’t included in the training dataset.
Another aspect of data completeness is ensuring your training data contains a varied
representation of each label. In the cat breed detection example, if all of your images
are close-ups of a cat’s face, your model won’t be able to correctly identify an image
of a cat from the side, or a full-body cat image. To look at a tabular data example, if
you are building a model to predict the price of real estate in a specific city but only
include training examples of houses larger than 2,000 square feet, your resulting
model will perform poorly on smaller houses.
The third aspect of data quality is data <i>consistency.</i> For large datasets, it’s common to
divide the work of data collection and labeling among a group of people. Developing
a set of standards for this process can help ensure consistency across your dataset,
since each person involved in this will inevitably bring their own biases to the pro‐
cess. Like data completeness, data inconsistencies can be found in both data features
and labels. For an example of inconsistent features, let’s say you’re collecting
atmospheric data from temperature sensors. If each sensor has been calibrated to dif‐
ferent standards, this will result in inaccurate and unreliable model predictions.
Inconsistencies can also refer to data format. If you’re capturing location data, some
people may write out a full street address as “Main Street” and others may abbreviate
it as “Main St.” Measurement units, like miles and kilometers, can also differ around
the world."|bias; completeness; consistency; ground truth label
"artists and venues, the loan problem on personal income, and the taxi duration on
urban traffic patterns. For these reasons, there are inherent challenges in transferring
the learnings from one tabular model to another.
Although transfer learning is not yet as common on tabular data as it is for image and
text domains, a new model architecture called TabNet presents novel research in this
area. Most tabular models require significant feature engineering when compared
with image and text models. TabNet employs a technique that first uses unsupervised
learning to learn representations for tabular features, and then fine-tunes these
learned representations to produce predictions. In this way, TabNet automates fea‐
ture engineering for tabular models.
<b>Embeddingsofwordsversussentences</b>
In our discussion of text embeddings so far, we’ve referred mostly to <i>word</i> embed‐
dings. Another type of text embedding is <i>sentence</i> embeddings. Where word embed‐
dings represent individual words in a vector space, sentence embeddings represent
entire sentences. Consequently, word embeddings are context agnostic. Let’s see how
this plays out with the following sentence:
<i>“I’ve</i> <i>left</i> <i>you</i> <i>fresh</i> <i>baked</i> <i>cookies</i> <i>on</i> <i>the</i> <i>left</i> <i>side</i> <i>of</i> <i>the</i> <i>kitchen</i> <i>counter.”</i>
Notice that the word <i>left</i> appears twice in that sentence, first as a verb and then as an
adjective. If we were to generate word embeddings for this sentence, we’d get a sepa‐
rate array for each word. With word embeddings, the array for both instances of the
word <i>left</i> would be the same. Using sentence-level embeddings, however, we’d get a
single vector to represent the entire sentence. There are several approaches for gener‐
ating sentence embeddings—from averaging a sentence’s word embeddings to train‐
ing a supervised learning model on a large corpus of text to generate the embeddings.
How does this relate to transfer learning? The latter method—training a supervised
learning model to generate sentence-level embeddings—is actually a form of transfer
learning. This is the approach used by Google’s Universal Sentence Encoder (avail‐
able in TF Hub) and BERT. These methods differ from word embeddings in that they
go beyond simply providing a weight lookup for individual words. Instead, they have
been built by training a model on a large dataset of varied text to understand the
meaning conveyed by <i>sequences</i> of words. In this way, they are designed to be trans‐
ferred to different natural language tasks and can thus be used to build models that
implement transfer learning."|BERT; sentence embeddings; TabNet; tabular data; Transfer Learning design pattern; Universal Sentence Encoder
"Netflix Prize. There is also a lot of theoretical evidence to back up the success demon‐
strated on these real-world challenges.
<b>Increasedtraininganddesigntime</b>
One downside to ensemble learning is increased training and design time. For exam‐
ple, for a stacked ensemble model, choosing the ensemble member models can
require its own level of expertise and poses its own questions: Is it best to reuse the
same architectures or encourage diversity? If we do use different architectures, which
ones should we use? And how many? Instead of developing a single ML model
(which can be a lot of work on its own!), we are now developing <i>k</i> models. We’ve
introduced an additional amount of overhead in our model development, not to
mention maintenance, inference complexity, and resource usage if the ensemble
model is to go into production. This can quickly become impractical as the number
of models in the ensemble increases.
Popular machine learning libraries, like scikit-learn and TensorFlow, provide easy-
to-use implementations for many common bagging and boosting methods, like ran‐
dom forest, AdaBoost, gradient boosting, and XGBoost. However, we should
carefully consider whether the increased overhead associated with an ensemble
method is worth it. Always compare accuracy and resource usage against a linear or
DNN model. Note that distilling (see “Design Pattern 11: Useful Overfitting” on page
141) an ensemble of neural networks can often reduce complexity and improve
performance.
<b>Dropoutasbagging</b>
Techniques like dropout provide a powerful and effective alternative. Dropout is
known as a regularization technique in deep learning but can be also understood as
an approximation to bagging. Dropout in a neural network randomly (with a prescri‐
bed probability) “turns off” neurons of the network for each mini-batch of training,
essentially evaluating a bagged ensemble of exponentially many neural networks.
That being said, training a neural network with dropout is not exactly the same as
bagging. There are two notable differences. First, in the case of bagging, the models
are independent, while when training with dropout, the models share parameters.
Second, in bagging, the models are trained to convergence on their respective train‐
ing set. However, when training with dropout, the ensemble member models would
only be trained for a single training step because different nodes are dropped out in
each iteration of the training loop.
<b>Decreasedmodelinterpretability</b>
Another point to keep in mind is model interpretability. Already in deep learning,
effectively explaining why our model makes the predictions it does can be difficult.
This problem is compounded with ensemble models. Consider, for example, decision"|AdaBoost; decision trees; DNN model; dropout technique; Ensemble design pattern; random forest; regularization; scikit-learn; TensorFlow; XGBoost
"tion task. Another approach is multitask learning that combines both tasks (classifi‐
cation and regression) into a single model using multiple prediction heads. With any
reframing technique, being aware of data limitations or the risk of introducing label
bias is important.
<b>Bucketizedoutputs</b>
The typical approach to reframing a regression task as a classification is to bucketize
the output values. For example, if our model is to be used to indicate when a baby
might need critical care upon birth, the categories in Table 3-1 could be sufficient.
<i>Table</i> <i>3-1.</i> <i>Bucketized</i> <i>outputs</i> <i>for</i> <i>baby</i> <i>weight</i>
<b>Category</b> <b>Description</b>
Highbirthweight Morethan8.8lbs
Averagebirthweight Between5.5lbsand8.8lbs
Lowbirthweight Between3.31lbsand5.5lbs
Verylowbirthweight Lessthan3.31lbs
Our regression model now becomes a multiclass classification. Intuitively, it is easier
to predict one out of four possible categorical cases than to predict a single value
from the continuum of real numbers—just as it would be easier to predict a binary 0
versus 1 target for is_underweight instead of four separate categories high_weight
avg_weight low_weight very_low_weight
versus versus versus . By using categorical
outputs, our model is incentivized less for getting arbitrarily close to the actual out‐
put value since we’ve essentially changed the output label to a range of values instead
of a single real number.
In the notebook accompanying this section, we train both a regression and a multi‐
class classification model. The regression model achieves an RMSE of 1.3 on the vali‐
dation set while the classification model has an accuracy of 67%. Comparing these
two models is difficult since one evaluation metric is RMSE and the other is accuracy.
In the end, the design decision is governed by the use case. If medical decisions are
based on bucketed values, then our model should be a classification using those buck‐
ets. However, if a more precise prediction of baby weight is needed, then it makes
sense to use the regression model.
<b>Otherwaysofcapturinguncertainty</b>
There are other ways to capture uncertainty in regression. A simple approach is to
carry out quantile regression. For example, instead of predicting just the mean, we
can estimate the conditional 10th, 20th, 30th, …, 90th percentile of what needs to be
predicted. Quantile regression is an extension of linear regression. Reframing, on the
other hand, can work with more complex machine learning models."|quantile regression; Reframing design pattern
"<i>Figure</i> <i>4-20.</i> <i>Large</i> <i>batch</i> <i>sizes</i> <i>have</i> <i>been</i> <i>shown</i> <i>to</i> <i>adversely</i> <i>affect</i> <i>the</i> <i>quality</i> <i>of</i> <i>the</i>
<i>final</i> <i>trained</i> <i>model.</i>
Thus, setting the mini-batch size in the context of distributed training is a complex
optimization space of its own, as it affects both statistical accuracy (generalization)
and hardware efficiency (utilization) of the model. Related work, focusing on this
optimization, introduces a layerwise adaptive large batch optimization technique
called LAMB, which has been able to reduce BERT training time from 3 days to just
76 minutes.
<b>MinimizingI/Owaits</b>
GPUs and TPUs can process data much faster than CPUs, and when using dis‐
tributed strategies with multiple accelerators, I/O pipelines can struggle to keep up,
creating a bottleneck to more efficient training. Specifically, before a training step fin‐
ishes, the data for the next step is not available for processing. This is shown in
Figure 4-21. The CPU handles the input pipeline: reading data from storage, prepro‐
cessing, and sending to the accelerator for computation. As distributed strategies
speed up training, more than ever it becomes necessary to have efficient input pipe‐
lines to fully utilize the computing power available.
This can be achieved in a number of ways, including using optimized file formats like
tf.data
TFRecords and building data pipelines using the TensorFlow API. The
tf.data API makes it possible to handle large amounts of data and has built-in trans‐
formations useful for creating flexible, efficient pipelines. For example, tf.data.Data
set.prefetch
overlaps the preprocessing and model execution of a training step so"|batch size; BERT; Distribution Strategy design pattern; GPU; LAMB; TPU
"Looking at the execution details in the BigQuery web console, we see that the entire
query took 35 seconds (see the box marked #1 in Figure 5-1).
<i>Figure</i> <i>5-1.</i> <i>The</i> <i>first</i> <i>two</i> <i>steps</i> <i>of</i> <i>a</i> <i>query</i> <i>to</i> <i>find</i> <i>the</i> <i>five</i> <i>most</i> <i>“positive”</i> <i>complaints</i> <i>in</i>
<i>the</i> <i>Consumer</i> <i>Financial</i> <i>Protection</i> <i>Bureau</i> <i>dataset</i> <i>of</i> <i>consumer</i> <i>complaints.</i>
consumer_complaint_narrative
The first step (see box #2 in Figure 5-1) reads the
column from the BigQuery public dataset where the complaint narrative is not NULL .
From the number of rows highlighted in box #3, we learn that this involves reading
1,582,045 values. The output of this step is written into 10 shards (see box #4 of
Figure 5-1).
The second step reads the data from this shard (note the $12:shard in the query), but
also obtains the file_path and file_contents of the machine learning model
imdb_sentiment
and applies the model to the data in each shard. The way Map‐
Reduce works is that each shard is processed by a worker, so the fact that there are 10
shards indicates that the second step is being done by 10 workers. The original 1.5
million rows would have been stored over many files, and so the first step was likely
to have been processed by as many workers as the number of files that comprised that
dataset.
The remaining steps are shown in Figure 5-2."|Batch Serving design pattern; MapReduce
"builder, can control. They include values like learning rate, number of epochs, num‐
ber of layers in your model, and more.
<b>Manualtuning</b>
Because you can manually select the values for different hyperparameters, your first
instinct might be a trial-and-error approach to finding the optimal combination of
hyperparameter values. This might work for models that train in seconds or minutes,
but it can quickly get expensive on larger models that require significant training
time and infrastructure. Imagine you are training an image classification model that
takes hours to train on GPUs. You settle on a few hyperparameter values to try and
then wait for the results of the first training run. Based on these results, you tweak the
hyperparameters, train the model again, compare the results with the first run, and
then settle on the best hyperparameter values by looking at the training run with the
best metrics.
There are a few problems with this approach. First, you’ve spent nearly a day and
many compute hours on this task. Second, there’s no way of knowing if you’ve
arrived at the optimal combination of hyperparameter values. You’ve only tried two
different combinations, and because you changed multiple values at once, you don’t
know which parameter had the biggest influence on performance. Even with addi‐
tional trials, using this approach will quickly use up your time and compute resources
and may not yield the most optimal hyperparameter values.
We’re using the term <i>trial</i> here to refer to a single training run with
a set of hyperparameter values.
<b>Gridsearchandcombinatorialexplosion</b>
A more structured version of the trial-and-error approach described earlier is known
as <i>grid</i> <i>search.</i> When implementing hyperparameter tuning with grid search, we
choose a list of possible values we’d like to try for each hyperparameter we want to
RandomForestRegressor()
optimize. For example, in scikit-learn’s model, let’s say
max_depth
we want to try the following combination of values for the model’s and
n_estimators hyperparameters:
grid_values = {
'max_depth': [5, 10, 100],
'n_estimators': [100, 150, 200]
}"|grid search; Hyperparameter Tuning design pattern; model parameters; RandomForestRegressor; trials
"net connectivity, it may be slow and expensive to continuously generate predictions
from a model deployed in the cloud.
To convert a trained model into a format that works on edge devices, models often go
through a process known as <i>quantization,</i> where learned model weights are repre‐
sented with fewer bytes. TensorFlow, for example, uses a format called TensorFlow
Lite to convert saved models into a smaller format optimized for serving at the edge.
In addition to quantization, models intended for edge devices may also start out
smaller to fit into stringent memory and processor constraints.
Quantization and other techniques employed by TF Lite significantly reduce the size
and prediction latency of resulting ML models, but with that may come reduced
model accuracy. Additionally, since we can’t consistently rely on edge devices having
connectivity, deploying new model versions to these devices in a timely manner also
presents a challenge.
We can see how these trade-offs play out in practice by looking at the options for
training edge models in Cloud AutoML Vision in Figure 5-9.
<i>Figure</i> <i>5-9.</i> <i>Making</i> <i>trade-offs</i> <i>between</i> <i>accuracy,</i> <i>model</i> <i>size,</i> <i>and</i> <i>latency</i> <i>for</i> <i>models</i>
<i>deployed</i> <i>at</i> <i>the</i> <i>edge</i> <i>in</i> <i>Cloud</i> <i>AutoML</i> <i>Vision.</i>
To account for these trade-offs, we need a solution that balances the reduced size and
latency of edge models against the added sophistication and accuracy of cloud
models."|edge; quantization; Tensorflow Lite; TensorFlow Lite; Two-Phase Predictions design pattern
"tf.transform
The library (which is part of TensorFlow Extended) provides an effi‐
cient way of carrying out transformations over a preprocessing pass through the data
and saving the resulting features and transformation artifacts so that the transforma‐
tions can be applied by TensorFlow Serving during prediction time.
The first step is to define the transformation function. For example, to scale all the
inputs to be zero mean and unit variance and bucketize them, we would create this
preprocessing function (see the full code on GitHub):
<b>def</b> preprocessing_fn(inputs):
outputs = {}
<b>for</b> key <b>in</b> ...:
outputs[key + '_z'] = tft.scale_to_z_score(inputs[key])
outputs[key + '_bkt'] = tft.bucketize(inputs[key], 5)
<b>return</b> outputs
Before training, the raw data is read and transformed using the prior function in
Apache Beam:
transformed_dataset, transform_fn = (raw_dataset |
beam_impl.AnalyzeAndTransformDataset(preprocessing_fn))
transformed_data, transformed_metadata = transformed_dataset
The transformed data is then written out in a format suitable for reading by the train‐
ing pipeline:
transformed_data | tfrecordio.WriteToTFRecord(
PATH_TO_TFT_ARTIFACTS,
coder=example_proto_coder.ExampleProtoCoder(
transformed_metadata.schema))
The Beam pipeline also stores the preprocessing function that needs to be run, along
with any artifacts the function needs, into an artifact in TensorFlow graph format. In
the case above, for example, this artifact would include the mean and variance for
scaling the numbers, and the bucket boundaries for bucketizing numbers. The
training function reads transformed data and, therefore, the transformations do not
have to be repeated within the training loop.
The serving function needs to load in these artifacts and create a Transform layer:
tf_transform_output = tft.TFTransformOutput(PATH_TO_TFT_ARTIFACTS)
tf_transform_layer = tf_transform_output.transform_features_layer()
Then, the serving function can apply the Transform layer to the parsed input features
and invoke the model with the transformed data to calculate the model output:
@tf.function
<b>def</b> serve_tf_examples_fn(serialized_tf_examples):
feature_spec = tf_transform_output.raw_feature_spec()
feature_spec.pop(_LABEL_KEY)
parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)"|Apache Beam; TensorFlow Extended; TensorFlow Serving; Transform design pattern
"pre-trained model as a feature extractor rather than fine-tuning. If you’re retraining
the weights of a model that was likely trained on thousands or millions of examples,
fine-tuning can cause the updated model to overfit to your small dataset and lose the
more general information learned from those millions of examples. Although it
depends on your data and prediction task, when we say “small dataset” here, we’re
referring to datasets with hundreds or a few thousand training examples.
Another factor to take into account when deciding whether to fine-tune is how simi‐
lar your prediction task is to that of the original pre-trained model you’re using.
When the prediction task is similar or a continuation of the previous training, as it
was in our movie review sentiment analysis model, fine-tuning can produce higher-
accuracy results. When the task is different or the datasets are significantly different,
it’s best to freeze all the layers of the pre-trained model instead of fine-tuning them.
Table 4-1 summarizes the key points. 4
<i>Table</i> <i>4-1.</i> <i>Criteria</i> <i>to</i> <i>help</i> <i>choose</i> <i>between</i> <i>feature</i> <i>extraction</i> <i>and</i> <i>fine-tuning</i>
<b>Criterion</b> <b>Featureextraction</b> <b>Fine-tuning</b>
Howlargeisthedataset? Small Large
Isyourpredictiontaskthesameasthatofthepre-trained Differenttasks Sametask,orsimilartaskwithsame
model? classdistributionoflabels
Budgetfortrainingtimeandcomputationalcost Low High
In our text example, the pre-trained model was trained on a corpus of news text but
our use case was sentiment analysis. Because these tasks are different, we should use
the original model as a feature extractor rather than fine-tune it. An example of dif‐
ferent prediction tasks in an image domain might be using our MobileNet model
trained on ImageNet as a basis for doing transfer learning on a dataset of medical
images. Although both tasks involve image classification, the nature of the images in
each dataset are very different.
<b>Focusonimageandtextmodels</b>
You may have noticed that all of the examples in this section focused on image and
text data. This is because transfer learning is primarily for cases where you can apply
a similar task to the same data domain. Models trained with tabular data, however,
cover a potentially infinite number of possible prediction tasks and data types. You
could train a model on tabular data to predict how you should price tickets to your
event, whether or not someone is likely to default on loan, your company’s revenue
next quarter, the duration of a taxi trip, and so forth. The specific data for these tasks
is also incredibly varied, with the ticket problem depending on information about
4 Formoreinformation,see“CS231nConvolutionalNeuralNetworksforVisualRecognition.”"|feature extraction; fine-tuning; tabular data; Transfer Learning design pattern
"<b>if</b> request_json <b>and</b> 'review' <b>in</b> request_json:
review = request_json['review']
outputs = serving_fn(full_text_input=tf.constant([review]))
<b>return</b> outputs['positive_review_logits']
Note that we should be careful to define the serving function as a global variable (or a
singleton class) so that it isn’t reloaded in response to every request. In practice, the
serving function will be reloaded from the export path (on Google Cloud Storage)
only in the case of cold starts.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
The approach of exporting a model to a stateless function and deploying the stateless
function in a web application framework works because web application frameworks
offer autoscaling, can be fully managed, and are language neutral. They are also
familiar to software and business development teams who may not have experience
with machine learning. This also has benefits for agile development—an ML engineer
or data scientist can independently change the model, and all the application devel‐
oper needs to do is change the endpoint they are accessing.
<b>Autoscaling</b>
Scaling web endpoints to millions of requests per second is a well-understood engi‐
neering problem. Rather than building services unique to machine learning, we can
rely on the decades of engineering work that has gone into building resilient web
applications and web servers. Cloud providers know how to autoscale web endpoints
efficiently, with minimal warmup times.
We don’t even need to write the serving system ourselves. Most modern enterprise
machine learning frameworks come with a serving subsystem. For example, Tensor‐
Flow provides TensorFlow Serving and PyTorch provides TorchServe. If we use these
serving subsystems, we can simply provide the exported file and the software takes
care of creating a web endpoint.
<b>Fullymanaged</b>
Cloud platforms abstract away the managing and installation of components like
TensorFlow Serving as well. Thus, on Google Cloud, deploying the serving function
as a REST API is as simple as running this command-line program providing the
location of the SavedModel output:
gcloud ai-platform versions create ${MODEL_VERSION} <b>\</b>
--model ${MODEL_NAME} --origin ${MODEL_LOCATION} <b>\</b>
--runtime-version $TFVERSION
In Amazon’s SageMaker, deployment of a TensorFlow SavedModel is similarly sim‐
ple, and achieved using:"|data scientists; ML engineers; SageMaker; Stateless Serving Function design pattern; TensorFlow Serving; TorchServe
"REGEXP_CONTAINS(tags,
r""(?:keras|tensorflow|matplotlib|pandas|scikit-learn)"")
The output layer of our model would look like the following (full code for this section
is available in the GitHub repository):
keras.layers.Dense(5, activation='sigmoid')
Let’s take the Stack Overflow question <i>“What</i> is the definition of a non-trainable
parameter?” as an input example. Assuming our output indices correspond with the
order of tags in our query, an output for that question might look like this:
[.95, .83, .02, .08, .65]
Our model is 95% confident this question should be tagged Keras, and 83% confident
it should be tagged TensorFlow. When evaluating model predictions, we’ll need to
iterate over every element in the output array and determine how we want to display
those results to our end users. If 80% is our threshold for all tags, we’d show Keras
<i>and</i> TensorFlow associated with this question. Alternatively, maybe we want to
encourage users to add as many tags as possible and we want to show options for any
tag with prediction confidence above 50%.
For examples like this one, where the goal is primarily to suggest possible tags
with less emphasis on getting the tag <i>exactly</i> right, a typical rule of thumb is to
n_specific_tag n_total_examples
use / as a threshold for each class. Here,
n_specific_tag is the number of examples with one tag in the dataset (for example,
“pandas”), and n_total_examples is the total number of examples in the training set
across all tags. This ensures that the model is doing better than guessing a certain
label based on its occurrence in the training dataset.
For a more precise approach to thresholding, consider using S-Cut
or optimizing for your model’s F-measure. Details on both can be
found in this paper. Calibrating the per-label probabilities is often
helpful as well, especially when there are thousands of labels and
you want to consider the top K of them (this is common in search
and ranking problems).
As you’ve seen, multilabel models provide more flexibility in how we parse predic‐
tions and require us to think carefully about the output for each class.
<b>Datasetconsiderations</b>
When dealing with single-label classification tasks, we can ensure our dataset is bal‐
anced by aiming for a relatively equal number of training examples for each class.
Building a balanced dataset is more nuanced for the Multilabel design pattern."|binary classification; Multilabel design pattern; threshold selection
"A common model architecture for accomplishing this is a <i>convolutional</i> <i>neural</i> <i>net‐</i>
<i>work</i> (CNN).
<header><largefont><b>Convolutional</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Network</b></largefont> <largefont><b>Layers</b></largefont></header>
Take a look at Figure 2-22. In this example, we’ve got a 4×4 grid where each square
represents pixel values on our image. We then use max pooling to take the largest
value of each grid and generate a resulting, smaller matrix. By dividing our image into
a grid of tiles, our model is able to extract key insights from each region of an image
at different levels of granularity.
<i>Figure</i> <i>2-22.</i> <i>Max</i> <i>pooling</i> <i>on</i> <i>a</i> <i>single</i> <i>4×4</i> <i>slice</i> <i>of</i> <i>image</i> <i>data.</i>
Figure 2-22 uses a <i>kernel</i> <i>size</i> of (2, 2). Kernel size refers to the size of each chunk of
our image. The number of spaces our filter moves before creating its next chunk, also
known as <i>stride,</i> is 2. Because our stride is equal to the size of our kernel, the chunks
created <i>do</i> <i>not</i> <i>overlap.</i>
While this tiling method preserves more detail than representing images as arrays of
pixel values, quite a bit of information is lost after each pooling step. In the diagram
above, the next pooling step would produce a scalar value of 8, taking our matrix
from 4 ×4 to a single value in just two steps. In a real-world image, you can imagine
how this might bias a model to focus on areas with dominant pixel values while los‐
ing important details that may surround these areas.
How can we build on this idea of splitting images into smaller chunks, while still pre‐
serving important details in images? We’ll do this by making these chunks <i>overlap.</i> If
the example in Figure 2-22 had instead used a stride of 1, the output would instead be
a 3×3 matrix (Figure 2-23)."|bias; kernel size; Multimodal Input design pattern
"dropoff_latitude dropoff_longitude
In Figure 6-2, these are the boxes marked , ,
and so on.
Second, maintain a dictionary of transformed features, and make every transforma‐
Lambda
tion either a Keras Preprocessing layer or a layer. Here, we scale the inputs
using Lambda layers:
transformed = {}
<b>for</b> lon_col <b>in</b> ['pickup_longitude', 'dropoff_longitude']:
transformed[lon_col] = tf.keras.layers.Lambda(
<b>lambda</b> x: (x+78)/8.0,
name='scale_{}'.format(lon_col)
)(inputs[lon_col])
<b>for</b> lat_col <b>in</b> ['pickup_latitude', 'dropoff_latitude']:
transformed[lat_col] = tf.keras.layers.Lambda(
<b>lambda</b> x: (x-37)/8.0,
name='scale_{}'.format(lat_col)
)(inputs[lat_col])
scale_dropoff_latitude scale_drop
In Figure 6-2, these are the boxes marked ,
off_longitude, and so on.
Lambda
We will also have one layer for the Euclidean distance, which is computed
from four of the Input layers (see Figure 6-2):
<b>def</b> euclidean(params):
lon1, lat1, lon2, lat2 = params
londiff = lon2 - lon1
latdiff = lat2 - lat1
<b>return</b> tf.sqrt(londiff*londiff + latdiff*latdiff)
transformed['euclidean'] = tf.keras.layers.Lambda(euclidean, name='euclidean')([
inputs['pickup_longitude'],
inputs['pickup_latitude'],
inputs['dropoff_longitude'],
inputs['dropoff_latitude']
])
Lambda
Similarly, the column to create the hour of day from the timestamp is a layer:
transformed['hourofday'] = tf.keras.layers.Lambda(
<b>lambda</b> x: tf.strings.to_number(tf.strings.substr(x, 11, 2),
out_type=tf.dtypes.int32),
name='hourofday'
)(inputs['pickup_datetime'])
Third, all these transformed layers will be concatenated into a DenseFeatures layer:
dnn_inputs = tf.keras.layers.DenseFeatures(feature_columns.values())(transformed)
Because the constructor for DenseFeatures requires a set of feature columns, we will
have to specify how to take each of the transformed values and convert them into an"|feature columns; Keras; Transform design pattern
"No machine learning model is perfect. To better understand where and how our
model is wrong, the error of an ML model can be broken down into three parts: the
irreducible error, the error due to bias, and the error due to variance. The irreducible
error is the inherent error in the model resulting from noise in the dataset, the fram‐
ing of the problem, or bad training examples, like measurement errors or confound‐
ing factors. Just as the name implies, we can’t do much about <i>irreducible</i> <i>error.</i>
The other two, the bias and the variance, are referred to as the <i>reducible</i> <i>error,</i> and
here is where we can influence our model’s performance. In short, the bias is the
model’s inability to learn enough about the relationship between the model’s features
and labels, while the variance captures the model’s inability to generalize on new,
unseen examples. A model with high bias oversimplifies the relationship and is said
to be <i>underfit.</i> A model with high variance has learned too much about the training
data and is said to be <i>overfit.</i> Of course, the goal of any ML model is to have low bias
and low variance, but in practice, it is hard to achieve both. This is known as the
bias–variance trade-off. We can’t have our cake and eat it too. For example, increas‐
ing model complexity decreases bias but increases variance, while decreasing model
complexity decreases variance but introduces more bias.
Recent work suggests that when using modern machine learning techniques such as
large neural networks with high capacity, this behavior is valid only up to a point. In
observed experiments, there is an “interpolation threshold” beyond which very high
capacity models are able to achieve zero training error as well as low error on unseen
data. Of course, we need much larger datasets in order to avoid overfitting on high-
capacity models.
Is there a way to mitigate this bias–variance trade-off on small- and medium-scale
problems?
<header><largefont><b>Solution</b></largefont></header>
<i>Ensemble</i> <i>methods</i> are meta-algorithms that combine several machine learning mod‐
els as a technique to decrease the bias and/or variance and improve model perfor‐
mance. Generally speaking, the idea is that combining multiple models helps to
improve the machine learning results. By building several models with different
inductive biases and aggregating their outputs, we hope to get a model with better
performance. In this section, we’ll discuss some commonly used ensemble methods,
including bagging, boosting, and stacking.
<b>Bagging</b>
Bagging (short for bootstrap aggregating) is a type of parallel ensembling method and
is used to address high variance in machine learning models. The bootstrap part of
bagging refers to the datasets used for training the ensemble members. Specifically, if
there are <i>k</i> submodels, then there are <i>k</i> separate datasets used for training each"|bagging; bias; bias-variance tradeoff; capacity; Ensemble design pattern; ensemble methods; irreducible error; overfit model; reducible error; underfit model
"Overfitting goes beyond just a batch. From a more holistic perspec‐
tive, overfitting follows the general advice commonly given with
regards to deep learning and regularization. The best fitting model
is a large model that has been properly regularized. In short, if your
deep neural network isn’t capable of overfitting your training data‐
set, you should be using a bigger one. Then, once you have a large
model that overfits the training set, you can apply regularization to
improve the validation accuracy, even though training accuracy
may decrease.
You can test your Keras model code in this way using the tf.data.Dataset you’ve
written for your input pipeline. For example, if your training data input pipeline is
trainds batch()
called , we’ll use to pull a single batch of data. You can find the full
code for this example in the repository accompanying this book:
BATCH_SIZE = 256
single_batch = trainds.batch(BATCH_SIZE).take(1)
Then, when training the model, instead of calling the full trainds dataset inside the
fit()
method, use the single batch that we created:
model.fit(single_batch.repeat(),
validation_data=evalds,
…)
Note that we apply repeat() so that we won’t run out of data when training on that
single batch. This ensures that we take the one batch over and over again while train‐
ing. Everything else (the validation dataset, model code, engineered features, and so
on) remains the same.
Rather than choose an arbitrary sample of the training dataset, we
recommend that you overfit on a small dataset, each of whose
examples has been carefully verified to have correct labels. Design
your neural network architecture such that it is able to learn this
batch of data precisely and get to zero loss. Then take the same net‐
work and train it on the full training dataset.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>12:</b></largefont> <largefont><b>Checkpoints</b></largefont></header>
In Checkpoints, we store the full state of the model periodically so that we have parti‐
ally trained models available. These partially trained models can serve as the final
model (in the case of early stopping) or as the starting points for continued training
(in the cases of machine failure and fine-tuning)."|Checkpoints design pattern; overfitting; regularization; Useful Overfitting design pattern
"The four steps are as follows:
1. Create a FeatureSet . The feature set specifies the entities, features, and source.
2. Add entities and features to the FeatureSet .
FeatureSet
3. Register the . This creates a named feature set within Feast. The fea‐
ture set contains no feature data.
FeatureSet
4. Load feature data into the .
A notebook with the full code for this example can be found in the repository accom‐
panying this book.
<b>CreatingaFeatureSet.</b>
We connect to a Feast deployment by setting up a client with
the Python SDK:
<b>from</b> <b>feast</b> <b>import</b> Client, FeatureSet, Entity, ValueType
<i>#</i> <i>Connect</i> <i>to</i> <i>an</i> <i>existing</i> <i>Feast</i> <i>deployment</i>
client = Client(core_url='localhost:6565')
We can check that the client is connected by printing the existing feature sets with
client.list_feature_sets()
the command . If this is a new deployment, this will
return an empty list. To create a new feature set, call the class FeatureSet and specify
the feature set’s name:
<i>#</i> <i>Create</i> <i>a</i> <i>feature</i> <i>set</i>
taxi_fs = FeatureSet(""taxi_rides"")
<b>AddingentitiesandfeaturestotheFeatureSet.</b> In the context of Feast, FeatureSets
consist of entities and features. Entities are used as keys to look up feature values and
are used to join features between different feature sets when creating datasets for
training or serving. The entity serves as an identifier for whatever relevant character‐
istic you have in the dataset. It is an object that can be modeled and store informa‐
tion. In the context of a ride-hailing or food delivery service, a relevant entity could
customer_id, order_id, driver_id, restaurant_id.
be or In the context of a churn
model, an entity could be a customer_id or segment_id . Here, the entity is the
taxi_id
, a unique identifier for the taxi vendor of each trip.
At this stage, the feature set we created called taxi_rides contains no entities or fea‐
tures. We can use the Feast core client to specify these from a pandas dataframe that
contains the raw data inputs and entities as shown in Table 6-2."|Feast; Feature Store design pattern; FeatureSet
"submodel of the ensemble. Each dataset is constructed by randomly sampling (with
replacement) from the original training dataset. This means there is a high probabil‐
ity that any of the <i>k</i> datasets will be missing some training examples, but also any
dataset will likely have repeated training examples. The aggregation takes place on
the output of the multiple ensemble model members—either an average in the case of
a regression task or a majority vote in the case of classification.
A good example of a bagging ensemble method is the random forest: multiple deci‐
sion trees are trained on randomly sampled subsets of the entire training data, then
the tree predictions are aggregated to produce a prediction, as shown in Figure 3-11.
<i>Figure</i> <i>3-11.</i> <i>Bagging</i> <i>is</i> <i>good</i> <i>for</i> <i>decreasing</i> <i>variance</i> <i>in</i> <i>machine</i> <i>learning</i> <i>model</i> <i>output.</i>
Popular machine learning libraries have implementations of bagging methods. For
example, to implement a random Forest regression in scikit-learn to predict baby
weight from our natality dataset:
<b>from</b> <b>sklearn.ensemble</b> <b>import</b> RandomForestRegressor
<i>#</i> <i>Create</i> <i>the</i> <i>model</i> <i>with</i> <i>50</i> <i>trees</i>
RF_model = RandomForestRegressor(n_estimators=50,
max_features='sqrt',
n_jobs=-1, verbose = 1)
<i>#</i> <i>Fit</i> <i>on</i> <i>training</i> <i>data</i>
RF_model.fit(X_train, Y_train)
Model averaging as seen in bagging is a powerful and reliable method for reducing
model variance. As we’ll see, different ensemble methods combine multiple submo‐
dels in different ways, sometimes using different models, different algorithms, or
even different objective functions. With bagging, the model and algorithms are the
same. For example, with random forest, the submodels are all short decision trees."|bagging; Ensemble design pattern
"model_state <b>AS</b> (
<b>SELECT</b>
scheduled_arrival_time,
arrival_delay,
<b>AVG(arrival_delay)</b> OVER (time_window) <b>AS</b> prediction,
4*STDDEV(arrival_delay) OVER (time_window) <b>AS</b> acceptable_deviation
<b>FROM</b> <b>data</b>
WINDOW time_window <b>AS</b>
(ORDER <b>BY</b> UNIX_SECONDS(TIMESTAMP(scheduled_arrival_time))
RANGE <b>BETWEEN</b> 7200 PRECEDING <b>AND</b> 1 PRECEDING)
)
Finally, we apply the anomaly detection algorithm to each instance:
<b>SELECT</b>
*,
(ABS(arrival_delay - prediction) > acceptable_deviation) <b>AS</b> is_anomaly
<b>FROM</b> model_state
The result looks like Table 6-1, with the arrival delay of 54 minutes marked as an
anomaly given that all the previous flights arrived early.
<i>Table</i> <i>6-1.</i> <i>The</i> <i>result</i> <i>of</i> <i>a</i> <i>BigQuery</i> <i>query</i> <i>determining</i> <i>whether</i> <i>incoming</i> <i>flight</i> <i>data</i>
<i>is</i> <i>an</i> <i>anomaly</i>
<b>scheduled_arrival_time</b> <b>arrival_delay</b> <b>prediction</b> <b>acceptable_deviation</b> <b>is_anomaly</b>
2010-05-01T05:45:00 -18.0 -8.25 62.51399843235114 false
2010-05-01T06:00:00 -13.0 -10.2 56.878818553131005 false
2010-05-01T06:35:00 -1.0 -10.666 51.0790237442599 false
2010-05-01T06:45:00 -9.0 -9.28576 48.86521793473886 false
2010-05-01T07:00:00 <b>54.0</b> -9.25 45.24220532707422 <b>true</b>
Unlike the Apache Beam solution, the efficiency of distributed SQL will allow us to
calculate the 2-hour time window centered on each instance (instead of at a resolu‐
tion of 10-minute windows). However, the drawback is that BigQuery tends to have
relatively high latency (on the order of seconds), and so it cannot be used for real-
time control applications.
<b>Sequencemodels</b>
The Windowed Inference pattern of passing a sliding window of previous instances
to an inference function is useful beyond anomaly detection or even time-series mod‐
els. Specifically, it is useful in any class of models, such as Sequence models, that
require a historical state. For example, a translation model needs to see several suc‐
cessive words before it can carry out the translation so that the translation takes into
account the context of the word. After all, the translation of the words “left,” “Chi‐
cago,” and “road” vary between the sentences “I left Chicago by road” and “Turn left
on Chicago Road.”"|BigQuery; Windowed Inference design pattern
"In addition to these features extracted directly from a question’s title, we could also
represent <i>metadata</i> about the question as features. For example, we could add fea‐
tures representing the number of tags the question had and the day of the week it was
posted. We could then combine these tabular features with our encoded text and feed
both representations into our model using Keras’s Concatenate layer to combine the
BOW-encoded text array with the tabular metadata describing our text.
<b>Multimodalrepresentationofimages</b>
Similar to our analysis of embeddings and BOW encoding for text, there are many
ways to represent image data when preparing it for an ML model. Like raw text,
images cannot be fed directly into a model and need to be transformed into a numer‐
ical format that the model can understand. We’ll start by discussing some common
approaches to representing image data: as pixel values, as sets of tiles, and as sets of
windowed sequences. The Multimodal Input design pattern provides a way to use
more than one representation of an image in our model.
<b>Imagesaspixelvalues.</b>
At their core, images are arrays of pixel values. A black and
white image, for example, contains pixel values ranging from 0 to 255. We could
therefore represent a 28×28-pixel black-and-white image in a model as a 28×28 array
with integer values ranging from 0 to 255. In this section, we’ll be referencing the
MNIST dataset, a popular ML dataset that includes images of handwritten digits.
With the Sequential API, we can represent our MNIST images of pixel values using
a Flatten layer, which flattens the image into a one-dimensional 784 (28 * 28) element
array:
layers.Flatten(input_shape=(28, 28))
For color images, this gets more complex. Each pixel in an RGB color image has three
values—one for red, green, and blue. If our images in the example above were instead
input_shape
color, we’d add a third dimension to the model’s such that it would be:
layers.Flatten(input_shape=(28, 28, 3))
While representing images as arrays of pixel values works well for simple images like
the grayscale ones in the MNIST dataset, it starts to break down when we introduce
images with more edges and shapes throughout. When a network is fed with all of the
pixels in an image at once, it’s hard for it to focus on smaller areas of adjacent pixels
that contain important information.
<b>Imagesastiledstructures.</b> We need a way to represent more complex, real-world
images that will enable our model to extract meaningful details and understand pat‐
terns. If we feed the network only small pieces of an image at a time, it’ll be more
likely to identify things like spatial gradients and edges present in neighboring pixels."|CNN; Flatten layer; Keras; MNIST dataset; Multimodal Input design pattern; pixel values; Sequential API
"<b>Nonlineartransformations</b>
What if our data is skewed and neither uniformly distributed nor distributed like a
bell curve? In that case, it is better to apply a <i>nonlinear</i> <i>transform</i> to the input before
scaling it. One common trick is to take the logarithm of the input value before scaling
it. Other common transformations include the sigmoid and polynomial expansions
(square, square root, cube, cube root, and so on). We’ll know that we have a good
transformation function if the distribution of the transformed value becomes uni‐
form or normally distributed.
Assume that we are building a model to predict the sales of a nonfiction book. One of
the inputs to the model is the popularity of the Wikipedia page corresponding to the
topic. The number of views of pages in Wikipedia is, however, highly skewed and
occupies a large dynamic range (see the left panel of Figure 2-4: the distribution is
highly skewed toward rarely viewed pages, but the most common pages are viewed
tens of millions of times). By taking the logarithm of the views, then taking the fourth
root of this log value and scaling the result linearly, we obtain something that is in the
desired range and somewhat bell-shaped. For details of the code to query the Wikipe‐
dia data, apply these transformations, and generate this plot, refer to the GitHub
repository for this book.
<i>Figure</i> <i>2-4.</i> <i>Left</i> <i>panel:</i> <i>the</i> <i>distribution</i> <i>of</i> <i>the</i> <i>number</i> <i>of</i> <i>views</i> <i>of</i> <i>Wikipedia</i> <i>pages</i> <i>is</i>
<i>highly</i> <i>skewed</i> <i>and</i> <i>occupies</i> <i>a</i> <i>large</i> <i>dynamic</i> <i>range.</i> <i>The</i> <i>second</i> <i>panel</i> <i>demonstrates</i> <i>that</i>
<i>problems</i> <i>can</i> <i>be</i> <i>addressed</i> <i>by</i> <i>transforming</i> <i>the</i> <i>number</i> <i>of</i> <i>views</i> <i>using</i> <i>the</i> <i>logarithm,</i> <i>a</i>
<i>power</i> <i>function,</i> <i>and</i> <i>linear</i> <i>scaling</i> <i>in</i> <i>succession.</i> <i>The</i> <i>third</i> <i>panel</i> <i>shows</i> <i>the</i> <i>effect</i> <i>of</i> <i>his‐</i>
<i>togram</i> <i>equalization</i> <i>and</i> <i>the</i> <i>fourth</i> <i>panel</i> <i>shows</i> <i>the</i> <i>effect</i> <i>of</i> <i>the</i> <i>Box-Cox</i> <i>transform.</i>
It can be difficult to devise a linearizing function that makes the distribution look like
a bell curve. An easier approach is to bucketize the number of views, choosing the"|histogram equalization; nonlinear transformations
"<i>Table</i> <i>2-3.</i> <i>An</i> <i>example</i> <i>of</i> <i>one-hot</i> <i>encoding</i> <i>categorical</i> <i>inputs</i> <i>for</i> <i>the</i> <i>natality</i> <i>dataset</i>
<b>Plurality</b> <b>One-hotencoding</b>
Single(1) [1,0,0,0,0,0]
Multiple(2+) [0,1,0,0,0,0]
Twins(2) [0,0,1,0,0,0]
Triplets(3) [0,0,0,1,0,0]
Quadruplets(4) [0,0,0,0,1,0]
Quintuplets(5) [0,0,0,0,0,1]
When encoded in this way, we need six dimensions to represent each of the different
categories. Six dimensions may not be so bad, but what if we had many, many more
categories to consider?
For example, what if our dataset consisted of customers’ view history of our video
database and our task is to suggest a list of new videos given customers’ previous
video interactions? In this scenario, the customer_id field could have millions of
unique entries. Similarly, the video_id of previously watched videos could contain
thousands of entries as well. One-hot encoding <i>high-cardinality</i> categorical features
like video_ids or customer_ids as inputs to a machine learning model leads to a
sparse matrix that isn’t well suited for a number of machine learning algorithms.
The second problem with one-hot encoding is that it treats the categorical variables
as being <i>independent.</i> However, the data representation for twins should be close to
the data representation for triplets and quite far away from the data representation
for quintuplets. A multiple is most likely a twin, but could be a triplet. As an example,
Table 2-4 shows an alternate representation of the plurality column in a lower
dimension that captures this <i>closeness</i> relationship.
<i>Table</i> <i>2-4.</i> <i>Using</i> <i>lower</i> <i>dimensionality</i> <i>embeddings</i> <i>to</i> <i>represent</i> <i>the</i> <i>plurality</i> <i>column</i>
<i>in</i> <i>the</i> <i>natality</i> <i>dataset.</i>
<b>Plurality</b> <b>Candidateencoding</b>
Single(1) [1.0,0.0]
Multiple(2+) [0.0,0.6]
Twins(2) [0.0,0.5]
Triplets(3) [0.0,0.7]
Quadruplets(4) [0.0,0.8]
Quintuplets(5) [0.0,0.9]
These numbers are arbitrary of course. But is it possible to learn the best possible rep‐
resentation of the plurality column using just two dimensions for the natality prob‐
lem? That is the problem that the Embeddings design pattern solves."|closeness relationships; Embedding design pattern; high cardinality; one-hot encoding
"<header><largefont><b>Problem</b></largefont></header>
The more complex a model is (for example, the more layers and nodes a neural net‐
work has), the larger the dataset that is needed to train it effectively. This is because
more complex models tend to have more tunable parameters. As model sizes
increase, the time it takes to fit one batch of examples also increases. As the data size
increases (and assuming batch sizes are fixed), the number of batches also increases.
Therefore, in terms of computational complexity, this double whammy means that
training will take a long time.
At the time of writing, training an English-to-German translation model on a state-
of-the-art tensor processing unit (TPU) pod on a relatively small dataset takes about
two hours. On real datasets of the sort used to train smart devices, the training can
take several days.
When we have training that takes this long, the chances of machine failure are
uncomfortably high. If there is a problem, we’d like to be able to resume from an
intermediate point, instead of from the very beginning.
<header><largefont><b>Solution</b></largefont></header>
At the end of every epoch, we can save the model state. Then, if the training loop is
interrupted for any reason, we can go back to the saved model state and restart. How‐
ever, when doing this, we have to make sure to save the intermediate model state, not
just the model. What does that mean?
Once training is complete, we save or <i>export</i> the model so that we can deploy it for
inference. An exported model does not contain the entire model state, just the infor‐
mation necessary to create the prediction function. For a decision tree, for example,
this would be the final rules for each intermediate node and the predicted value for
each of the leaf nodes. For a linear model, this would be the final values of the weights
and biases. For a fully connected neural network, we’d also need to add the activation
functions and the weights of the hidden connections.
What data on model state do we need when restoring from a checkpoint that an
exported model does not contain? An exported model does not contain which epoch
and batch number the model is currently processing, which is obviously important in
order to resume training. But there is more information that a model training loop
can contain. In order to carry out gradient descent effectively, the optimizer might be
changing the learning rate on a schedule. This learning rate state is not present in an
exported model. Additionally, there might be stochastic behavior in the model, such
as dropout. This is not captured in the exported model state either. Models like recur‐
rent neural networks incorporate history of previous input values. In general, the full
model state can be many times the size of the exported model."|Checkpoints design pattern; epochs; exported model; TPU; training loop
"tions. We’ll then combine with our 8,000 fraudulent examples, reshuffle the data, and
use this new, smaller dataset to train a model. Here’s how we could implement this
with pandas:
data = pd.read_csv('fraud_data.csv')
<i>#</i> <i>Split</i> <i>into</i> <i>separate</i> <i>dataframes</i> <i>for</i> <i>fraud</i> <i>/</i> <i>not</i> <i>fraud</i>
fraud = data[data['isFraud'] == 1]
not_fraud = data[data['isFraud'] == 0]
<i>#</i> <i>Take</i> <i>a</i> <i>random</i> <i>sample</i> <i>of</i> <i>non</i> <i>fraud</i> <i>rows</i>
not_fraud_sample = not_fraud.sample(random_state=2, frac=.005)
<i>#</i> <i>Put</i> <i>it</i> <i>back</i> <i>together</i> <i>and</i> <i>shuffle</i>
df = pd.concat([not_fraud_sample,fraud])
df = shuffle(df, random_state=2)
Following this, our dataset would contain 25% fraudulent transactions, much more
balanced than the original dataset with only 0.1% in the minority class. It’s worth
experimenting with the exact balance used when downsampling. Here we used a
25/75 split, but different problems might require closer to a 50/50 split to achieve
decent accuracy.
Downsampling is usually combined with the Ensemble pattern, following these steps:
1. Downsample the majority class and use all the instances of the minority class.
2. Train a model and add it to the ensemble.
3. Repeat.
During inference, take the median output of the ensemble models.
We discussed a classification example here, but downsampling can also be applied to
regression models where we’re predicting a numerical value. In this case, taking a
random sample of majority class samples will be more nuanced since the majority
“class” in our data includes a range of values rather than a single label.
<b>Weightedclasses</b>
Another approach to handling imbalanced datasets is to change the <i>weight</i> our model
gives to examples from each class. Note that this is a different use of the term
“weight” than the weights (or parameters) learned by our model during training,
which you cannot set manually. By weighting <i>classes,</i> we tell our model to treat spe‐
cific label classes with more importance during training. We’ll want our model to
assign more weight to examples from the minority class. Exactly how much impor‐
tance your model should give to certain examples is up to you, and is a parameter you
can experiment with."|downsampling; fraud detection; Rebalancing design pattern
"• BOW encoding provides strong signals for the most significant words present in
our vocabulary, while embeddings can identify relationships between words in a
much larger vocabulary.
• If we have text that switches between languages, we can build embeddings (or
BOW encodings) for each one and concatenate them.
• Embeddings can encode the frequency of words in text, where the BOW treats
the presence of each word as a boolean value. Both representations are valuable.
• BOW encoding can identify patterns between reviews that all contain the word
“amazing,” while an embedding can learn to correlate the phrase “not amazing”
with a below-average review. Again, both of these representations are valuable.
<b>Extractingtabularfeaturesfromtext.</b>
In addition to encoding raw text data, there are
often other characteristics of text that can be represented as tabular features. Let’s say
we are building a model to predict whether or not a Stack Overflow question will get
a response. Various factors about the text but unrelated to the exact words themselves
may be relevant to training a model on this task. For example, maybe the length of a
question or the presence of a question mark influences the likelihood of an answer.
However, when we create an embedding, we usually truncate the words to a certain
length. The actual length of a question is lost in that data representation. Similarly,
punctuation is often removed. We can use the Multimodal Input design pattern to
bring back this lost information to the model.
In the following query, we’ll extract some tabular features from the title field of the
Stack Overflow dataset to predict whether or not a question will get an answer:
<b>SELECT</b>
<b>LENGTH(title)</b> <b>AS</b> title_len,
ARRAY_LENGTH(SPLIT(title, "" "")) <b>AS</b> word_count,
ENDS_WITH(title, ""?"") <b>AS</b> ends_with_q_mark,
IF
(answer_count > 0,
1,
0) <b>AS</b> is_answered,
<b>FROM</b>
`bigquery-public-data.stackoverflow.posts_questions`
This results in the following:
<b>Row</b> <b>title_len</b> <b>word_count</b> <b>ends_with_q_mark</b> <b>is_answered</b>
1 84 14 true 0
2 104 16 false 0
3 85 19 true 1
4 88 14 false 1
5 17 3 false 1"|BOW encoding; Multimodal Input design pattern; Stack Overflow
"<b>Pre-trainedembeddings</b>
While we can load a pre-trained model on our own, we can also implement transfer
learning by making use of the many pre-trained models available in TF Hub, a library
of pre-trained models (called modules). These modules span a variety of data
domains and use cases, including classification, object detection, machine translation,
and more. In TensorFlow, you can load these modules as a layer, then add your own
classification layer on top.
To see how TF Hub works, let’s build a model that classifies movie reviews as either
<i>positive</i> or <i>negative.</i> First, we’ll load a pre-trained embedding model trained on a
large corpus of news articles. We can instantiate this model as a hub.KerasLayer :
hub_layer = hub.KerasLayer(
""https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1"",
input_shape=[], dtype=tf.string, trainable=True)
We can stack additional layers on top of this to build our classifier:
model = keras.Sequential([
hub_layer,
keras.layers.Dense(32, activation='relu'),
keras.layers.Dense(1, activation='sigmoid')
])
We can now train this model, passing it our own text dataset as input. The resulting
prediction will be a 1-element array indicating whether our model thinks the given
text is positive or negative.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
To understand why transfer learning works, let’s first look at an analogy. When chil‐
dren are learning their first language, they are exposed to many examples and correc‐
ted if they misidentify something. For example, the first time they learn to identify a
cat, they’ll see their parents point to the cat and say the word <i>cat,</i> and this repetition
strengthens pathways in their brain. Similarly, they are corrected when they say <i>cat</i>
referring to an animal that is not a cat. When the child then learns how to identify a
dog, they don’t need to start from scratch. They can use a similar recognition process
to the one they used for the cat but apply it to a slightly different task. In this way, the
child has built a foundation for learning. In addition to learning new things, they
have also learned <i>how</i> to learn new things. Applying these learning methods to differ‐
ent domains is roughly how transfer learning works, too.
How does this play out in neural networks? In a typical convolutional neural network
(CNN), the learning is hierarchical. The first layers learn to recognize edges and
shapes present in an image. In the cat example, this might mean that the model can
identify areas in an image where the edge of the cat’s body meets the background.
The next layers in the model begin to understand groups of edges—perhaps that"|CNN; TensorFlow hub; Transfer Learning design pattern
"also a metadata layer that simplifies documentation and versioning of different fea‐
ture sets and an API that manages loading and retrieving feature data.
<i>Figure</i> <i>6-12.</i> <i>A</i> <i>feature</i> <i>store</i> <i>provides</i> <i>a</i> <i>bridge</i> <i>between</i> <i>raw</i> <i>data</i> <i>sources</i> <i>and</i> <i>model</i>
<i>training</i> <i>and</i> <i>serving.</i>
The typical workflow of a data or ML engineer is to read raw data (structured or
streaming) from a data source, apply various transformations on the data using their
favorite processing framework, and store the transformed feature within the feature
store. Rather than creating feature pipelines to support a single ML model, the Fea‐
ture Store pattern decouples feature engineering from model development. In partic‐
ular, tools like Apache Beam, Flink, or Spark are often used when building a feature
store since they can handle processing data in batch as well as streaming. This also
reduces the incidence of training–serving skew, since the feature data is populated by
the same feature creation pipelines.
After features are created, they are housed in a data store to be retrieved for training
and serving. For serving feature retrieval, speed is optimized. A model in production
backing some online application may need to produce real-time predictions within
milliseconds, making low latency essential. However, for training, higher latency is
not a problem. Instead the emphasis is on high throughput since historical features
are pulled in large batches for training. A feature store addresses both these use cases
by using different data stores for online and offline feature access. For example, a fea‐
ture store may use Cassandra or Redis as a data store for online feature retrieval, and
Hive or BigQuery for fetching historical, large batch feature sets.
In the end, a typical feature store will house many different feature sets containing
features created from myriad raw data sources. The metadata layer is built in to docu‐
ment feature sets and provide a registry for easy feature discovery and cross collabo‐
ration among teams."|Apache Beam; Apache Flink; Apache Spark; BigQuery; Cassandra; data engineers; Feature Store design pattern; Hive; low latency; ML engineers; Redis
"<i>Table</i> <i>6-2.</i> <i>The</i> <i>taxi</i> <i>ride</i> <i>dataset</i> <i>contains</i> <i>information</i> <i>about</i> <i>taxi</i> <i>rides</i> <i>in</i> <i>New</i> <i>York.</i> <i>The</i>
<i>entity</i> <i>is</i> <i>the</i> <i>taxi_id,</i> <i>a</i> <i>unique</i> <i>identifier</i> <i>for</i> <i>the</i> <i>taxi</i> <i>vendor</i> <i>of</i> <i>each</i> <i>trip</i>
<b>Row</b> <b>pickup_datetime</b> <b>pickup_lat</b> <b>pickup_lon</b> <b>dropoff_lat</b> <b>dropoff_lon</b> <b>num_pass</b> <b>taxi_id</b> <b>fare_amt</b>
1 2020-05-3111:29:48 40.787403 -73.955848 40.723042 -73.993106 2 0 15.3
UTC
2 2011-04-0614:30:00 40.645343 -73.776698 40.71489 -73.987242 2 0 45.0
UTC
3 2020-04-2413:11:06 40.650105 -73.785373 40.638858 -73.9678 2 2 32.1
UTC
4 2020-02-2009:07:00 40.762365 -73.925733 40.740118 -73.986487 2 1 21.3
UTC
<header><largefont><b>Defining</b></largefont> <largefont><b>Streaming</b></largefont> <largefont><b>Data</b></largefont> <largefont><b>Sources</b></largefont> <largefont><b>when</b></largefont> <largefont><b>Creating</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Feature</b></largefont> <largefont><b>Set</b></largefont></header>
Users can define streaming data sources when creating a feature set. Once a feature
set is registered with a source, Feast will automatically start to populate its stores with
data from this source. This is an example of a feature set with a user-provided source
that retrieves streaming data from a Kafka topic:
feature_set = FeatureSet(
name=""stream_feature"",
entities=[
Entity(""taxi_id"", ValueType.INT64)
],
features=[
Feature(""traffic_last_5min"", ValueType.INT64)
],
source=KafkaSource(
brokers=""mybroker:9092"",
topic=""my_feature_topic""
)
)
pickup_datetime
The timestamp here is important since it is necessary to retrieve
batch features and is used to ensure time-correct joins for batch features. To create an
additional feature, such as the Euclidean distance, load the dataset into a pandas data‐
frame and compute the feature:
<i>#</i> <i>Load</i> <i>dataframe</i>
taxi_df = pd.read_csv(""taxi-train.csv"")
<i>#</i> <i>Engineer</i> <i>features,</i> <i>Euclidean</i> <i>distance</i>
taxi_df['euclid_dist'] = taxi_df.apply(compute_dist, axis=1)
.add(...)
We can add entities and features to the feature set with . Alternatively, the
method .infer_fields_from_df(...) will create the entities and features for our"|Feast; Feature Store design pattern; FeatureSet
"<b>Allowanddisallowlists</b>
When we can’t find a way to fix inherent bias in our data or model directly, it’s possi‐
ble to hardcode rules on top of our production model using allow and disallow lists.
This applies mostly to classification or generative models, when there are labels or
words we don’t want our model to return. As an example, gendered words such as
“man” and “woman” were removed from Google Cloud Vision API’s label detection
feature. Because gender cannot be determined by appearance alone, it would have
reinforced unfair biases to return these labels when the model’s prediction is based
solely on visual features. Instead, the Vision API returns “person.” Similarly, the
Smart Compose feature in Gmail avoids the use of gendered pronouns when com‐
pleting sentences such as “I am meeting an investor next week. Do you want to meet
___?”
These allow and disallow lists can be applied in one of two phases in an ML
workflow:
<i>Data</i> <i>collection</i>
When training a model from scratch or using the Transfer Learning design pat‐
tern to add our own classification layer, we can define our model’s label set in the
data collection phase, before a model has been trained.
<i>After</i> <i>training</i>
If we’re relying on a pre-trained model for predictions, and are using the same
labels from that model, an allow and disallow list can be implemented in produc‐
tion—after the model returns a prediction but before those labels are surfaced to
end users. This could also apply to text generation models, where we don’t have
complete control of all possible model outputs.
<b>Dataaugmentation</b>
In addition to the data distribution and representation solutions discussed earlier,
another approach to minimizing model bias is to perform data <i>augmentation.</i> Using
this approach, data is changed before training with the goal of removing potential
sources of bias. One specific type of data augmentation is known as ablation, and is
especially applicable in text models. In a text sentiment analysis model, for example,
we could remove identity terms from text to ensure they don’t influence our model’s
predictions. Building on the ice cream example we used earlier in this section, the
sentence “Mint chip is their best ice cream flavor” would become “BLANK is their
best ice cream flavor” after applying ablation. We’d then replace all other words
throughout the dataset that we didn’t want to influence the model’s sentiment predic‐
tion with the same word (we used BLANK here, but anything not present in the rest
of the text data will work). Note that while this ablation technique works well for
many text models, it’s important to be careful when removing areas of bias from tab‐
ular datasets, as mentioned in the Problem section."|ablation; bias; data augmentation; Fairness Lens design pattern; Smart Compose; Transfer Learning design pattern; Vision API
"A new, synthetic example based on these two actual examples from the dataset might
look like Table 3-4, calculating by the midpoint between each of these column values.
<i>Table</i> <i>3-4.</i> <i>A</i> <i>synthetic</i> <i>example</i> <i>generated</i> <i>from</i> <i>the</i> <i>two</i> <i>minority</i> <i>training</i> <i>examples</i> <i>using</i>
<i>the</i> <i>SMOTE</i> <i>approach</i>
<b>Glucose</b> <b>BloodPressure</b> <b>SkinThickness</b> <b>BMI</b>
165.5 68 17.5 28.4
The SMOTE technique refers primarily to tabular data, but similar logic can be
applied to image datasets. For example, if we’re building a model to distinguish
between Bengal and Siamese cats and only 10% of our dataset contains images of
Bengals, we can generate additional variations of the Bengal cats in our dataset
ImageDataGenerator
through image augmentation using the Keras class. With a few
parameters, this class will generate multiple variations of the same image by rotating,
cropping, adjusting brightness, and more.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
There are a few other alternative solutions for building models with inherently imbal‐
anced datasets, including reframing the problem and handling cases of anomaly
detection. We’ll also explore several important considerations for imbalanced data‐
sets: overall dataset size, the optimal model architectures for different problem types,
and explaining minority class prediction.
<b>ReframingandCascade</b>
Reframing the problem is another approach for handling imbalanced datasets. First,
we might consider switching the problem from classification to regression or vice
versa utilizing the techniques described in the Reframing design pattern section and
training a cascade of models. For example, let’s say we have a regression problem
where the majority of our training data falls within a certain range, with a few outli‐
ers. Assuming we care about predicting outlier values, we could convert this to a clas‐
sification problem by bucketing the majority of the data in one bucket and the
outliers in another.
Imagine we’re building a model to predict baby weight using the BigQuery natality
dataset. Using pandas, we can create a histogram of a sample of the baby weight data
to see the weight distribution:
%%bigquerydf
<b>SELECT</b>
weight_pounds
<b>FROM</b>
`bigquery-public-data.samples.natality`
<b>LIMIT</b> 10000"|cascade; Keras ImageDataGenerator; Rebalancing design pattern; reframing; SMOTE; upsampling
"<header><largefont><b>Checkpoints</b></largefont> <largefont><b>in</b></largefont> <largefont><b>PyTorch</b></largefont></header>
At the time of writing, PyTorch doesn’t support checkpoints directly. However, it
does support externalizing the state of most objects. To implement checkpoints in
PyTorch, ask for the epoch, model state, optimizer state, and any other information
needed to resume training to be serialized along with the model:
torch.save({
'epoch': epoch,
'model_state_dict': <b>model.state_dict(),</b>
'optimizer_state_dict': <b>optimizer.state_dict(),</b>
'loss': loss,
…
}, PATH)
When loading from a checkpoint, you need to create the necessary classes and then
load them from the checkpoint:
model = ...
optimizer = ...
checkpoint = torch.load(PATH)
<b>model.load_state_dict(checkpoint['model_state_dict'])</b>
<b>optimizer.load_state_dict(checkpoint['optimizer_state_dict'])</b>
epoch = checkpoint['epoch']
loss = checkpoint['loss']
This is lower level than TensorFlow but provides the flexibility of storing multiple
models in a checkpoint and choosing which parts of the model state to load or not
load.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
TensorFlow and Keras automatically resume training from a checkpoint if check‐
points are found in the output path. To start training from scratch, therefore, you
have to start from a new output directory (or delete previous checkpoints from the
output directory). This works because enterprise-grade machine learning frameworks
honor the presence of checkpoint files.
Even though checkpoints are designed primarily to support resilience, the availability
of partially trained models opens up a number of other use cases. This is because the
partially trained models are usually more generalizable than the models created in
later iterations. A good intuition of why this occurs can be obtained from the Tensor‐
Flow playground, as shown in Figure 4-7."|checkpoints; Checkpoints design pattern; Keras; PyTorch; TensorFlow
"appointment. Other examples include energy forecasting, customer churn prediction,
financial modeling, weather prediction, and predictive maintenance.
• Feature Store
• Feature Cross
• Embeddings
• Ensemble
• Transform
• Reframing
• Cascade
• Multilabel
• Neutral Class
• Windowed Inference
• Batch Serving
IoT analytics is also a broad category that sits within predictive analytics. IoT models
rely on data collected by internet-connected sensors called IoT devices. Consider a
commercial aircraft that has thousands of sensors on it collecting more than 2 TB of
data per day. Machine learning of IoT sensor device data can provide predictive
models to warn against equipment failure before it happens.
• Feature Store
• Transform
• Reframing
• Hashed Feature
• Cascade
• Neutral Class
• Two-Phase Predictions
• Stateless Serving Function
• Windowed Inference
<header><largefont><b>Recommendation</b></largefont> <largefont><b>Systems</b></largefont></header>
Recommender systems are one of the most widespread applications of machine
learning in business and they often arise whenever users interact with items. Recom‐
mender systems capture features of past behavior and similar users and recommend
items most relevant for a given user. Think of how YouTube will recommend a series
of videos for you to watch based on your watch history, or Amazon may recommend"|IoT analytics; recommendation systems
"<i>Figure</i> <i>3-6.</i> <i>Two</i> <i>common</i> <i>implementations</i> <i>of</i> <i>multitask</i> <i>learning</i> <i>are</i> <i>through</i> <i>hard</i>
<i>parameter</i> <i>sharing</i> <i>and</i> <i>soft</i> <i>parameter</i> <i>sharing.</i>
In this context, we could have two heads to our model: one to predict a regression
output and another to predict classification output. For example, this paper trains a
computer vision model using a classification output of softmax probabilities together
with a regression output to predict bounding boxes. They show that this approach
achieves better performance than related work that trains networks separately for the
classification and localization tasks. The idea is that through parameter sharing, the
tasks are learned simultaneously and the gradient updates from the two loss func‐
tions inform both outputs and result in a more generalizable model.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>6:</b></largefont> <largefont><b>Multilabel</b></largefont></header>
The Multilabel design pattern refers to problems where we can assign <i>more</i> <i>than</i> <i>one</i>
label to a given training example. For neural networks, this design requires changing
the activation function used in the final output layer of the model and choosing how
our application will parse model output. Note that this is different from <i>multiclass</i>
classification problems, where a single example is assigned exactly one label from a
group of many (> 1) possible classes. You may also hear the Multilabel design pattern
referred to as <i>multilabel,</i> <i>multiclass</i> <i>classification</i> since it involves choosing more than
one label from a group of more than one possible class. When discussing this pattern,
we’ll focus primarily on neural networks.
<header><largefont><b>Problem</b></largefont></header>
Often, model prediction tasks involve applying a single classification to a given train‐
ing example. This prediction is determined from <i>N</i> possible classes where <i>N</i> is greater
than 1. In this case, it’s common to use softmax as the activation function for the out‐
put layer. Using softmax, the output of our model is an N-element array, where the
sum of all the values adds up to 1. Each value indicates the probability that a particu‐
lar training example is associated with the class at that index."|multiclass classification problems; Multilabel design pattern; parameter sharing; Reframing design pattern; softmax
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>7</b></largefont></header>
<header><largefont><b>Responsible</b></largefont> <largefont><b>AI</b></largefont></header>
Until this point, we’ve focused on patterns designed to help data and engineering
teams prepare, build, train, and scale models for production use. These patterns
mainly addressed teams directly involved in the ML model development process.
Once a model is in production, its impact extends far beyond the teams who built it.
In this chapter, we’ll discuss the other <i>stakeholders</i> of a model, both those within and
outside of an organization. Stakeholders could include executives whose business
objectives dictate a model’s goals, the end users of a model, auditors, and compliance
regulators.
There are several groups of model stakeholders we’ll be referring to in this chapter:
<i>Model</i> <i>builders</i>
Data scientists and ML researchers directly involved in building ML models.
<i>ML</i> <i>engineers</i>
Members of ML Ops teams directly involved in deploying ML models.
<i>Business</i> <i>decision</i> <i>makers</i>
Decide whether or not to incorporate the ML model into their business processes
or customer-facing applications and will need to evaluate whether the model is
fit for this purpose.
<i>End</i> <i>users</i> <i>of</i> <i>ML</i> <i>systems</i>
Make use of predictions from an ML model. There are many different types of
model end users: customers, employees, and hybrids of these. Examples include a
customer getting a movie recommendation from a model, an employee on a fac‐
tory floor using a visual inspection model to determine whether a product is bro‐
ken, or a medical practitioner using a model to aid in patient diagnosis."|ML engineers; ML researchers; model builders; stakeholders
"hp.Float('learning_rate', .005, .01, sampling='log')),
loss='sparse_categorical_crossentropy',
metrics=['accuracy'])
<b>return</b> model
keras-tuner
The library supports many different optimization algorithms. Here,
we’ll instantiate our tuner with Bayesian optimization and optimize for validation
accuracy:
<b>import</b> <b>kerastuner</b> <b>as</b> <b>kt</b>
tuner = kt.BayesianOptimization(
build_model,
objective='val_accuracy',
max_trials=10
)
The code to run the tuning job looks similar to training our model with fit() . As
this runs, we’ll be able to see the values for the three hyperparameters that were
selected for each trial. When the job completes, we can see the hyperparameter com‐
bination that resulted in the best trial. In Figure 4-23, we can see the example output
for a single trial run using keras-tuner .
<i>Figure</i> <i>4-23.</i> <i>Output</i> <i>for</i> <i>one</i> <i>trial</i> <i>run</i> <i>of</i> <i>hyperparameter</i> <i>tuning</i> <i>with</i> <i>keras-tuner.</i> <i>At</i>
<i>the</i> <i>top,</i> <i>we</i> <i>can</i> <i>see</i> <i>the</i> <i>hyperparameters</i> <i>selected</i> <i>by</i> <i>the</i> <i>tuner,</i> <i>and</i> <i>in</i> <i>the</i> <i>summary</i> <i>sec‐</i>
<i>tion,</i> <i>we</i> <i>see</i> <i>the</i> <i>resulting</i> <i>optimization</i> <i>metric.</i>
In addition to the examples shown here, there is additional functionality provided by
keras-tuner
that we haven’t covered. You can use it to experiment with different
numbers of layers for your model by defining an hp.Int() parameter within a loop,
and you can also provide a fixed set of values for a hyperparameter instead of a range."|Hyperparameter Tuning design pattern
"<i>Regulatory</i> <i>and</i> <i>compliance</i> <i>agencies</i>
People and organizations who need an executive-level summary of how a model
is making decisions from a regulatory compliance perspective. This could
include financial auditors, government agencies, or governance teams within an
organization.
Throughout this chapter, we’ll look at patterns that address a model’s impact on indi‐
viduals and groups outside the team and organization building a model. The <i>Heuris‐</i>
<i>tic</i> <i>Benchmark</i> design pattern provides a way of putting the model’s performance in a
context that end users and decision makers can understand. The <i>Explainable</i> <i>Predic‐</i>
<i>tions</i> pattern provides approaches to improving trust in ML systems by fostering an
understanding of the signals a model is using to make predictions. The <i>Fairness</i> <i>Lens</i>
design pattern aims to ensure that models behave equitably across different subsets of
users and prediction scenarios.
Taken together, the patterns in this chapter fall under the practice of <i>Responsible</i> <i>AI.</i>
This is an area of active research and is concerned with the best ways to build fair‐
ness, interpretability, privacy, and security into AI systems. Recommended practices
for responsible AI include employing a human-centered design approach by engag‐
ing with a diverse set of users and use-case scenarios throughout project develop‐
ment, understanding the limitations of datasets and models, and continuing to
monitor and update ML systems after deployment. Responsible AI patterns are not
limited to the three that we discuss in this chapter—many of the patterns in earlier
chapters (like Continuous Evaluation, Repeatable Splitting, and Neutral Class, to
name a few) provide methods to implement these recommended practices and attain
the goal of building fairness, interpretability, privacy, and security into AI systems.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>28:</b></largefont> <largefont><b>Heuristic</b></largefont> <largefont><b>Benchmark</b></largefont></header>
The Heuristic Benchmark pattern compares an ML model against a simple, easy-to-
understand heuristic in order to explain the model’s performance to business deci‐
sion makers.
<header><largefont><b>Problem</b></largefont></header>
Suppose a bicycle rental agency wishes to use the expected duration of rentals to
build a dynamic pricing solution. After training an ML model to predict the duration
of a bicycle’s rental period, they evaluate the model on a test dataset and determine
that the mean absolute error (MAE) of the trained ML model is 1,200 seconds. When
they present this model to the business decision makers, they will likely be asked: “Is
an MAE of 1,200 seconds good or bad?” This is a question we need to be ready to
handle whenever we develop a model and present it to business stakeholders. If we
train an image classification model on items in a product catalog and the mean"|Continued Model Evaluation design pattern; Explainable Predictions design pattern; Fairness Lens design pattern; Heuristic Benchmark design pattern; MAE (mean absolute error); mean absolute error (MAE); Neutral Class design pattern; Repeatable Splitting design pattern; responsible AI
"Synchronous and asynchronous training each have their advantages, and disadvan‐
tages and choosing between the two often comes down to hardware and network
limitations.
Synchronous training is particularly vulnerable to slow devices or poor network con‐
nection because training will stall waiting for updates from all workers. This means
synchronous distribution is preferable when all devices are on a single host and there
are fast devices (for example, TPUs or GPUs) with strong links. On the other hand,
asynchronous distribution is preferable if there are many low-power or unreliable
workers. If a single worker fails or stalls in returning a gradient update, it won’t stall
the training loop. The only limitation is I/O constraints.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Large, complex neural networks require massive amounts of training data to be effec‐
tive. Distributed training schemes drastically increase the throughput of data pro‐
cessed by these models and can effectively decrease training times from weeks to
hours. Sharing resources between workers and parameter server tasks leads to a dra‐
matic increase in data throughput. Figure 4-17 compares the throughput of training
setups.5
data, in this case images, with different distribution Most notable is that
throughput increases with the number of worker nodes and, even though parameter
servers perform tasks not related to the computation done on the GPU’s workers,
splitting the workload among more machines is the most advantageous strategy.
In addition, data parallelization decreases time to convergence during training. In a
similar study, it was shown that increasing workers leads to minimum loss much
faster. 6 Figure 4-18 compares the time to minimum for different distribution strate‐
gies. As the number of workers increases, the time to minimum training loss dramat‐
ically decreases, showing nearly a 5× speed up with 8 workers as opposed to just 1.
5 VictorCamposetal.,“Distributedtrainingstrategiesforacomputervisiondeeplearningalgorithmonadis‐
tributedGPUcluster,”InternationalConferenceonComputationalScience,ICCS2017,June12–14,2017.
6 Ibid."|asynchronous training; data parallelism; Distribution Strategy design pattern; synchronous training; training
"<b>RunningthepipelineonCloudAIPlatform</b>
We can run the TFX pipeline on Cloud AI Platform Pipelines, which will manage
low-level details of the infrastructure for us. To deploy a pipeline to AI Platform, we
package our pipeline code as a Docker container and host it on Google Container
Registry (GCR).6 Once our containerized pipeline code has been pushed to GCR,
we’ll create the pipeline using the TFX CLI:
tfx pipeline create \
--pipeline-path=kubeflow_dag_runner.py \
--endpoint='your-pipelines-dashboard-url' \
--build-target-image='gcr.io/your-pipeline-container-url'
In the command above, endpoint corresponds with the URL of our AI Platform Pipe‐
lines dashboard. When that completes, we’ll see the pipeline we just created in our
pipelines dashboard. The create command creates a pipeline <i>resource</i> that we can
invoke by creating a run:
tfx run create --pipeline-name='your-pipeline-name' --endpoint='pipeline-url'
After running this command, we’ll be able to see a graph that updates in real time as
our pipeline moves through each step. From the Pipelines dashboard, we can further
examine individual steps to see any artifacts they generate, metadata, and more. We
can see an example of the output for an individual step in Figure 6-8.
We could train our model directly in our containerized pipeline on GKE, but TFX
provides a utility for using Cloud AI Platform Training as part of our process. TFX
also has an extension for deploying our trained model to AI Platform Prediction.
We’ll utilize both of these integrations in our pipeline. AI Platform Training lets us
take advantage of specialized hardware for training our models, such as GPUs or
TPUs, in a cost-effective way. It also provides an option to use distributed training,
which can accelerate training time and minimize training cost. We can track individ‐
ual training jobs and their output within the AI Platform console.
6 NotethatinordertorunTFXpipelinesonAIPlatform,youcurrentlyneedtohostyourcodeonGCRand
can’tuseanothercontainerregistryservicelikeDockerHub."|Cloud AI Platform Pipelines; Cloud AI Platform Training; Docker container; GKE; Google Container Registry; GPU; TFX; TPU; Workflow Pipeline design pattern
"<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
In addition to data parallelism, there are other aspects of distribution to consider,
such as model parallelism, other training accelerators—(such as TPUs) and other
considerations (such as I/O limitations and batch size).
<b>Modelparallelism</b>
In some cases, the neural network is so large it cannot fit in the memory of a single
device; for example, Google’s Neural Machine Translation has billions of parameters.
devices,7
In order to train models this big, they must be split up over multiple as
shown in Figure 4-19. This is called <i>model</i> <i>parallelism.</i> By partitioning parts of a net‐
work and their associated computations across multiple cores, the computation and
memory workload is distributed across multiple devices. Each device operates over
the same mini-batch of data during training, but carries out computations related
only to their separate components of the model.
<i>Figure</i> <i>4-19.</i> <i>Model</i> <i>parallelism</i> <i>partitions</i> <i>the</i> <i>model</i> <i>over</i> <i>multiple</i> <i>devices.</i>
7 JeffreyDeanetal.“LargeScaleDistributedDeepNetworks,”NIPSProceedings(2012)."|Distribution Strategy design pattern; model parallelism; Neural Machine Translation
"Figure 7-15. This looks at one feature in the data (height) and breaks down the
model’s false negative rate for each possible categorical value of that feature.
<i>Figure</i> <i>7-15.</i> <i>Comparing</i> <i>a</i> <i>model’s</i> <i>false</i> <i>negative</i> <i>rate</i> <i>over</i> <i>different</i> <i>subsets</i> <i>of</i> <i>data.</i>
From the Fairness Indicators Python package, TFMA can also be used as a stand‐
alone tool that works with both TensorFlow and non-TensorFlow models.
<b>Automatingdataevaluation</b>
The fairness evaluation methods we discussed in the Solution section focused on
manual, interactive data and model analysis. This type of analysis is important, espe‐
cially in the initial phases of model development. As we operationalize our model and
shift our focus to maintaining and improving it, finding ways to automate fairness
evaluation will improve efficiency and ensure that fairness is integrated throughout
our ML process. We can do this through “Design Pattern 18: Continued Model Eval‐
uation” on page 220 discussed in Chapter 5, or with “Design Pattern 25: Workflow
Pipeline” on page 282 in Chapter 6 using components like those provided by TFX for
data analysis and model evaluation."|bias; Continued Model Evaluation design pattern; Fairness Lens design pattern; TFMA; Workflow Pipeline design pattern
"<i>Figure</i> <i>5-3.</i> <i>A</i> <i>confusion</i> <i>matrix</i> <i>shows</i> <i>all</i> <i>pairs</i> <i>of</i> <i>ground</i> <i>truth</i> <i>labels</i> <i>and</i> <i>predictions</i> <i>so</i>
<i>you</i> <i>can</i> <i>explore</i> <i>your</i> <i>model</i> <i>performance</i> <i>within</i> <i>different</i> <i>classes.</i>
<b>Continuousevaluation</b>
We should make sure the output table also captures the model version and the time‐
stamp of prediction requests so that we can use the same table for continuous evalua‐
tion of two different model versions for comparing metrics between the models. For
example, if we deploy a newer version of our model, called swivel_v2, that is trained
on more recent data or has different hyperparameters, we can compare their perfor‐
mance by slicing the evaluation dataframe according to the model version:
df_v1 = df_evals[df_evals.version == ""swivel""]
df_v2 = df_evals[df_evals.version == ""swivel_v2""]
Similarly, we can create evaluation slices in time, focusing only on model predictions
within the last month or the last week:
today = pd.Timestamp.now(tz='UTC')
one_month_ago = today - pd.DateOffset(months=1)
one_week_ago = today - pd.DateOffset(weeks=1)
df_prev_month = df_evals[df_evals.time >= one_month_ago]
df_prev_week = df_evals[df_evals.time >= one_week_ago]
To carry out the above evaluations continuously, the notebook (or a containerized
form) can be scheduled. We can set it up to trigger a model retraining if the evalua‐
tion metric falls below some threshold."|Continued Model Evaluation design pattern; ground truth label
"Function” on page 201 (introduced in Chapter 5) explained how to export a trained
model as a stateless function for serving in production. This is especially useful when
model inputs require preprocessing to transform data sent by the client into the for‐
mat the model expects.
To handle requirements for different groups of model end users, we can define multi‐
ple serving functions when we export our model. These serving functions are part of
<i>one</i> exported model version, and this model is deployed to a single REST endpoint.
In TensorFlow, serving functions are implemented using model <i>signatures,</i> which
define the input and output format a model is expecting. We can define multiple
@tf.function
serving functions using the decorator and pass each function an input
signature.
In the application code where we invoke our deployed model, we would determine
which serving function to use based on the data sent from the client. For example, a
request such as:
{""signature_name"": <b>""get_genre"",</b> ""instances"": … }
would be sent to the exported signature called get_genre , whereas a request like:
{""signature_name"": <b>""get_genre_with_explanation"",</b> ""instances"": … }
would be sent to the exported signature called get_genre_with_explanation .
Deploying multiple signatures can, therefore, solve the backward compatibility prob‐
lem. However, there is a significant difference—there is only one model, and when
that model is deployed, all the signatures are simultaneously updated. In our original
example of changing the model from providing just one genre to providing multiple
genres, the model architecture changed. The multiple-signature approach wouldn’t
work with that example since we have two different models. The multiple-signature
solution is also not appropriate when we wish to keep different versions of the model
separate and deprecate the older version over time.
Using multiple signatures is better than using multiple versions if you wish to main‐
tain <i>both</i> model signatures going forward. In the scenario where there are some cli‐
ents who simply want the best answer and other clients who want both the best
answer and an explanation, there is an added benefit to updating all the signatures
with a newer model instead of having to update versions one by one every time the
model is retrained and redeployed.
What are some scenarios where we might want to maintain both versions of the
model? With a text classification model, we may have some clients that need to send
raw text to the model, and others that are able to transform raw text into matrices
before getting a prediction. Based on the request data from the client, the model
framework can determine which serving function to use. Passing text embedding
matrices to a model is less expensive than preprocessing raw text, so this is an exam‐"|Model Versioning design pattern; model
"`bigquery-public-data.samples.natality`
<b>LIMIT</b> 10000
The resulting model will cluster our data into four groups. Once the model has been
created, we can then generate predictions on new data and look at that prediction’s
distance from existing clusters. If the distance is high, we can flag the data point as an
anomaly. To generate a cluster prediction on our model, we can run the following
query, passing it a made-up average example from the dataset:
<b>SELECT</b>
*
<b>FROM</b>
ML.PREDICT (MODEL `project-name.dataset-name.baby_weight`,
(
<b>SELECT</b>
7.0 <b>as</b> weight_pounds,
28 <b>as</b> mother_age,
40 <b>as</b> gestation_weeks
)
)
The query results in Table 3-6 show us the distance between this data point and the
model’s generated clusters, called centroids.
<i>Table</i> <i>3-6.</i> <i>The</i> <i>distance</i> <i>between</i> <i>our</i> <i>average</i> <i>weight</i> <i>example</i> <i>data</i> <i>point</i> <i>and</i> <i>each</i> <i>of</i> <i>the</i>
<i>clusters</i> <i>generated</i> <i>by</i> <i>our</i> <i>k-means</i> <i>model</i>
<b>CENTROID_ID</b> <b>NEAREST_CENTROIDS_DISTANCE.CENTROID_ID</b> <b>NEAREST_CENTROIDS_DISTANCE.DISTANCE</b>
4 4 0.29998627812137374
1 1.2370167418282159
2 1.376651161584178
3 1.6853517159990536
This example clearly fits into centroid 4, as seen by the small distance (.29).
We can compare this to the results we get if we send an outlier, underweight example
to the model, as shown in Table 3-7.
<i>Table</i> <i>3-7.</i> <i>The</i> <i>distance</i> <i>between</i> <i>our</i> <i>underweight</i> <i>example</i> <i>data</i> <i>point</i> <i>and</i> <i>each</i> <i>of</i> <i>the</i>
<i>clusters</i> <i>generated</i> <i>by</i> <i>our</i> <i>k-means</i> <i>model</i>
<b>CENTROID_ID</b> <b>NEAREST_CENTROIDS_DISTANCE.CENTROID_ID</b> <b>NEAREST_CENTROIDS_DISTANCE.DISTANCE</b>
3 3 3.061985789261998
4 3.3124603501734966
2 4.330205096751425
1 4.658614918595627"|anomaly detection; centroid; Rebalancing design pattern
"The same problem of high cardinality and dependent data also occurs in images and
text. Images consist of thousands of pixels, which are not independent of one
another. Natural language text is drawn from a vocabulary in the tens of thousands of
words, and a word like walk is closer to the word run than to the word book .
<header><largefont><b>Solution</b></largefont></header>
The Embeddings design pattern addresses the problem of representing high-
cardinality data densely in a lower dimension by passing the input data through an
embedding layer that has trainable weights. This will map the high-dimensional, cat‐
egorical input variable to a real-valued vector in some low-dimensional space. The
weights to create the dense representation are learned as part of the optimization of
the model (see Figure 2-5). In practice, these embeddings end up capturing closeness
relationships in the input data.
<i>Figure</i> <i>2-5.</i> <i>The</i> <i>weights</i> <i>of</i> <i>an</i> <i>embedding</i> <i>layer</i> <i>are</i> <i>learned</i> <i>as</i> <i>parameters</i> <i>during</i>
<i>training.</i>
Because embeddings capture closeness relationships in the input
data in a lower-dimensional representation, we can use an embed‐
ding layer as a replacement for clustering techniques (e.g., cus‐
tomer segmentation) and dimensionality reduction methods like
principal components analysis (PCA). Embedding weights are
determined in the main model training loop, thus saving the need
to cluster or do PCA beforehand.
The weights in the embedding layer would be learned as part of the gradient descent
procedure when training the natality model.
At the end of training, the weights of the embedding layer might be such that the
encoding for the categorical variables is as shown in Table 2-5."|closeness relationships; Embedding design pattern; PCA
"In regards to labeling inconsistencies, let’s return to the text sentiment example. In
this case, it’s likely people will not always agree on what is considered positive and
negative when labeling training data. To solve this, you can have multiple people
labeling each example in your dataset, then take the most commonly applied label for
each item. Being aware of potential labeler bias, and implementing systems to
account for it, will ensure label consistency throughout your dataset. We’ll explore
the concept of bias in the “Design Pattern 30: Fairness Lens” on page 343 in Chapter 7.
<i>Timeliness</i> in data refers to the latency between when an event occurred and when it
was added to your database. If you’re collecting data on application logs, for example,
an error log might take a few hours to show up in your log database. For a dataset
recording credit card transactions, it might take one day from when the transaction
occurred before it is reported in your system. To deal with timeliness, it’s useful to
record as much information as possible about a particular data point, and make sure
that information is reflected when you transform your data into features for a
machine learning model. More specifically, you can keep track of the timestamp of
when an event occurred and when it was added to your dataset. Then, when perform‐
ing feature engineering, you can account for these differences accordingly.
<header><largefont><b>Reproducibility</b></largefont></header>
In traditional programming, the output of a program is reproducible and guaranteed.
For example, if you write a Python program that reverses a string, you know that an
input of the word “banana” will always return an output of “ananab.” Similarly, if
there’s a bug in your program causing it to incorrectly reverse strings containing
numbers, you could send the program to a colleague and expect them to be able to
reproduce the error with the same inputs you used (unless the bug has something to
do with the program maintaining some incorrect internal state, differences in archi‐
tecture such as floating point precision, or differences in execution such as
threading).
Machine learning models, on the other hand, have an inherent element of random‐
ness. When training, ML model weights are initialized with random values. These
weights then converge during training as the model iterates and learns from the data.
Because of this, the same model code given the same training data will produce
slightly different results across training runs. This introduces a challenge of reprodu‐
cibility. If you train a model to 98.1% accuracy, a repeated training run is not guaran‐
teed to reach the same result. This can make it difficult to run comparisons across
experiments.
In order to address this problem of repeatability, it’s common to set the random
seed value used by your model to ensure that the same randomness will be
applied each time you run training. In TensorFlow, you can do this by running
tf.random.set_seed(value) at the beginning of your program."|bias; consistency; labeling; repeatability; reproducibility; TensorFlow; timeliness
"model.save(EXPORT_PATH,
signatures={'serving_default': nokey_prediction,
'keyed_prediction': keyed_prediction
})
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Why can’t the server just assign keys to the inputs it receives? For online prediction,
it is possible for servers to assign unique request IDs that lack any semantic informa‐
tion. For batch prediction, the problem is that the inputs need to be associated with
the outputs, so the server assigning a unique ID is not enough since it can’t be joined
back to the inputs. What the server has to do is to assign keys to the inputs it receives
before it invokes the model, use the keys to order the outputs, and then remove the
keys before sending along the outputs. The problem is that ordering is computation‐
ally very expensive in distributed data processing.
In addition, there are a couple of other situations where client-supplied keys are use‐
ful—asynchronous serving and evaluation. Given these two situations, it is preferable
that what constitutes a key becomes use case specific and needs to be identifiable.
Therefore, asking clients to supply a key makes the solution simpler.
<b>Asynchronousserving</b>
Many production machine learning models these days are neural networks, and neu‐
ral networks involve matrix multiplications. Matrix multiplication on hardware like
GPUs and TPUs is more efficient if you can ensure that the matrices are within cer‐
tain size ranges and/or multiples of a certain number. It can, therefore, be helpful to
accumulate requests (up to a maximum latency of course) and handle the incoming
requests in chunks. Since the chunks will consist of interleaved requests from multi‐
ple clients, the key, in this case, needs to have some sort of client identifier as well.
<b>Continuousevaluation</b>
If you are doing continuous evaluation, it can be helpful to log metadata about the
prediction requests so that you can monitor whether performance drops across the
board, or only in specific situations. Such slicing is made much easier if the key iden‐
tifies the situation in question. For example, suppose that we need to apply a Fairness
Lens (see Chapter 7) to ensure that our model’s performance is fair across different
customer segments (age of customer and/or race of customer, for example). The
model will not use the customer segment as an input, but we need to evaluate the
performance of the model sliced by the customer segment. In such cases, having the
customer segment(s) be embedded in the key (an example key might be 35-Black-
Male-34324323) makes slicing easier.
An alternate solution is to have the model ignore unrecognized inputs and send back
not just the prediction outputs but also all inputs, including the unrecognized ones."|asynchronous serving; batch prediction; continuous evaluation; Keyed Predictions design pattern; keys; online prediction
"<header><largefont><b>Stateless</b></largefont> <largefont><b>Functions</b></largefont></header>
A stateless function is a function whose outputs are determined purely by its inputs.
This function, for example, is stateless:
<b>def</b> stateless_fn(x):
<b>return</b> 3*x + 15
Another way to think of a stateless function is as an immutable object, where the
weights and biases are stored as constants:
<b>class</b> <b>Stateless:</b>
<b>def</b> <b>__init__(self):</b>
self.weight = 3
self.bias = 15
<b>def</b> <b>__call__(self,</b> x):
<b>return</b> self.weight*x + self.bias
A function that maintains a counter of the number of times it has been invoked and
returns a different value depending on whether the counter is odd or even is an
example of a function that is stateful, not stateless:
<b>class</b> <b>State:</b>
<b>def</b> <b>__init__(self):</b>
self.counter = 0
<b>def</b> <b>__call__(self,</b> x):
self.counter += 1
<b>if</b> self.counter % 2 == 0:
<b>return</b> 3*x + 15
<b>else:</b>
<b>return</b> 3*x - 15
Invoking stateless_fn(3) or Stateless()(3) always returns 24, whereas
a = State()
and then invoking
a(3)
returns a value that rocks between −6 and 24. The counter in this case is the state of
the function, and the output depends on both the input (x) and the state (counter) .
The state is typically maintained using class variables (as in our example) or using
global variables.
Because stateless components don’t have any state, they can be shared by multiple cli‐
ents. Servers typically create an instance pool of stateless components and use them
to service client requests as they come in. On the other hand, stateful components will
need to represent each client’s conversational state. The life cycle of stateless compo‐
nents needs to be managed by the server. For example, they need to be initialized on
the first request and destroyed when the client terminates or times out. Because of
these factors, stateless components are highly scalable, whereas stateful components"|stateful vs. stateless components; stateless functions; Stateless Serving Function design pattern
"<b>Pre-trainedmodels</b>
The Cascade is also needed when we wish to reuse the output of a pre-trained model
as an input into our model. For example, let’s say we are building a model to detect
authorized entrants to a building so that we can automatically open the gate. One of
the inputs to our model might be the license plate of the vehicle. Instead of using the
security photo directly in our model, we might find it simpler to use the output of an
optical character recognition (OCR) model. It is critical that we recognize that OCR
systems will have errors, and so we should not train our model with perfect license
plate information. Instead, we should train the model on the actual output of the
OCR system. Indeed, because different OCR models will behave differently and have
different errors, it is necessary to retrain the model if we change the vendor of our
OCR system.
A common scenario of using a pre-trained model as the first step
of a pipeline is using an object-detection model followed by a fine-
grained image classification model. For example, the object-
detection model might find all handbags in the image, an
intermediate step might crop the image to the bounding boxes of
the detected objects, and the subsequent model might identify the
type of handbag. We recommend using a Cascade so that the entire
pipeline can be retrained whenever the object-detection model is
updated (such as with a new version of the API).
<b>ReframinginsteadofCascade</b>
Note that in our example problem, we were trying to predict the likelihood that an
item would be returned, and so this was a classification problem. Suppose instead we
wish to predict hourly sales amounts. Most of the time, we will serve just retail buy‐
ers, but once in a while (perhaps four or five times a year), we will have a wholesale
buyer.
This is notionally a regression problem of predicting daily sales amounts where we
have a confounding factor in the form of wholesale buyers. Reframing the regression
problem to be a classification problem of different sales amounts might be a better
approach. Although it will involve training a classification model for each sales
amount bucket, it avoids the need to get the retail versus wholesale classification
correct.
<b>Regressioninraresituations</b>
The Cascade design pattern can be helpful when carrying out regression when some
values are much more common than others. For example, we might want to predict
the quantity of rainfall from a satellite image. It might be the case that on 99% of the"|Cascade design pattern; OCR (optical character recognition); optical character recognition (OCR)
"all-reduce. TPU pods have high-speed interconnect, so we tend to not worry about
communication overhead within a pod (a pod consists of thousands of TPUs). In
addition, there is lots of memory available on-disk, which means that it is possible to
preemptively fetch data and make less-frequent calls to the CPU. As a result, you
should use much higher batch sizes to take full advantage of high-memory, high-
interconnect chips like TPUs.
In terms of distributed training, TPUStrategy allows you to run distributed training
jobs on TPUs. Under the hood, TPUStrategy is the same as MirroredStrategy
although TPUs have their own implementation of the all-reduce algorithm.
Using TPUStrategy is similar to using the other distribution strategies in Tensor‐
Flow. One difference is you must first set up a TPUClusterResolver , which points to
the location of the TPUs. TPUs are currently available to use for free in Google
Colab, and there you don’t need to specify any arguments for tpu_address :
cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
tpu=tpu_address)
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)
<b>Choosingabatchsize</b>
Another important factor to consider is batch size. Particular to synchronous data
parallelism, when the model is particularly large, it’s better to decrease the total num‐
ber of training iterations because each training step requires the updated model to be
shared among different workers, causing a slowdown for transfer time. Thus, it’s
important to increase the mini-batch size as much as possible so that the same per‐
formance can be met with fewer steps.
However, it has been shown that very large batch sizes adversely affect the rate at
which stochastic gradient descent converges as well as the quality of the final solu‐
tion.8 Figure 4-20 shows that increasing the batch size alone ultimately causes the
top-1 validation error to increase. In fact, they argue that linearly scaling the learning
rate as a function of the large batch size is necessary to maintain a low validation
error while decreasing the time of distributed training.
8 PriyaGoyaletal.,“Accurate,LargeMinibatchSGD:TrainingImageNetin1Hour”(2017),arXiv:
1706.02677v2[cs.CV]."|batch size; Distribution Strategy design pattern; TPAClusterResolver; TPUStrategy
"<b>return</b> <b>{'key':</b> <b>key,</b> 'babyweight': output}
This model is then saved as discussed in the Stateless Serving Function design
pattern:
model.save(EXPORT_PATH,
signatures={'serving_default': keyed_prediction})
<b>Addingkeyedpredictioncapabilitytoanexistingmodel</b>
Note that the code above works even if the original model was not saved with a serv‐
ing function. Simply load the model using tf.saved_model.load() , attach a serving
function, and use the code snippet above, as shown in Figure 5-14.
<i>Figure</i> <i>5-14.</i> <i>Load</i> <i>a</i> <i>SavedModel,</i> <i>attach</i> <i>a</i> <i>nondefault</i> <i>serving</i> <i>function,</i> <i>and</i> <i>save</i> <i>it.</i>
When doing so, it is preferable to provide a serving function that replicates the older,
no-key behavior:
<i>#</i> <i>Serving</i> <i>function</i> <i>that</i> <i>does</i> <i>not</i> <i>require</i> <i>a</i> <i>key</i>
@tf.function(input_signature=[{
'is_male': tf.TensorSpec([None,], dtype=tf.string, name='is_male'),
'mother_age': tf.TensorSpec([None,], dtype=tf.float32,
name='mother_age'),
'plurality': tf.TensorSpec([None,], dtype=tf.string, name='plurality'),
'gestation_weeks': tf.TensorSpec([None,], dtype=tf.float32,
name='gestation_weeks')
}])
<b>def</b> nokey_prediction(inputs):
output = model(inputs) <i>#</i> <i>invoke</i> <i>model</i>
<b>return</b> <b>{'babyweight':</b> output}
keyed_prediction
Use the previous behavior as the default and add the as a new
serving function:"|Keyed Predictions design pattern; keys
"To illustrate the Two-Phase Predictions pattern, let’s employ a general-purpose audio
recognition dataset from Kaggle. The dataset contains around 9,000 audio samples of
familiar sounds with a total of 41 label categories, including “cello,” “knock,” “tele‐
phone,” “trumpet,” and more. The first phase of our solution will be a model that
predicts whether or not the given sound is a musical instrument. Then, for sounds
that the first model predicts are an instrument, we’ll get a prediction from a model
deployed in the cloud to predict the specific instrument from a total of 18 possible
options. Figure 5-10 shows the two-phased flow for this example.
<i>Figure</i> <i>5-10.</i> <i>Using</i> <i>the</i> <i>Two-Phase</i> <i>Predictions</i> <i>pattern</i> <i>to</i> <i>identify</i> <i>instrument</i> <i>sounds.</i>
To build each of these models, we’ll convert the audio data to spectrograms, which
are visual representations of sound. This will allow us to use common image model
architectures along with the Transfer Learning design pattern to solve this problem.
See Figure 5-11 for a spectrogram of a saxophone audio clip from our dataset."|Kaggle; Two-Phase Predictions design pattern
"Another alternative is to use the agreement among human labelers as the weight of a
pattern during training. Thus, if 5 experts agree on a diagnosis, the training pattern
gets a weight of 1, while if the experts are split 3 to 2, the weight of the pattern might
be only 0.6. This allows us to train a binary classifier, but overweight the classifier
toward the “sure” cases. The drawback to this approach is that when the probability
output by the model is 0.5, it is unclear whether it is because this reflects a situation
where there was insufficient training data, or whether it is a situation where human
experts disagree. Using a neutral class to capture areas of disagreement allows us to
disambiguate the two situations.
<b>Customersatisfaction</b>
The need for a neutral class also arises with models that attempt to predict customer
satisfaction. If the training data consists of survey responses where customers grade
their experience on a scale of 1 to 10, it might be helpful to bucket the ratings into
three categories: 1 to 4 as bad, 8 to 10 as good, and 5 to 7 is neutral. If, instead, we
attempt to train a binary classifier by thresholding at 6, the model will spend too
much effort trying to get essentially neutral responses correct.
<b>Asawaytoimproveembeddings</b>
Suppose we are creating a pricing model for flights and wish to predict whether or
not a customer will buy a flight at a certain price. To do this, we can look at historical
transactions of flight purchases and abandoned shopping carts. However, suppose
many of our transactions also include purchases by consolidators and travel agents—
these are people who have contracted fares, and so the fares for them were not
actually set dynamically. In other words, they don’t pay the currently displayed price.
We could throw away all the nondynamic purchases and train the model only on cus‐
tomers who made the decision to buy or not buy based on the price being displayed.
However, such a model will miss all the information held in the destinations that the
consolidator or travel agent was interested in at various times—this will affect things
like how airports and hotels are embedded. One way to retain that information while
not affecting the pricing decision is to use a neutral class for these transactions.
<b>Reframingwithneutralclass</b>
Suppose we are training an automated trading system that makes trades based on
whether it expects a security to go up or down in price. Because of stock market vola‐
tility and the speed with which new information is reflected in stock prices, trying to
trade on small predicted ups and downs is likely to lead to high trading costs and
poor profits over time."|binary classifier; Neutral Class design pattern
"<i>Figure</i> <i>7-10.</i> <i>The</i> <i>What-If</i> <i>Tool’s</i> <i>“Datapoint</i> <i>editor,”</i> <i>where</i> <i>we</i> <i>can</i> <i>see</i> <i>how</i> <i>our</i> <i>data</i> <i>is</i>
<i>split</i> <i>by</i> <i>label</i> <i>class</i> <i>and</i> <i>inspect</i> <i>features</i> <i>for</i> <i>individual</i> <i>examples</i> <i>from</i> <i>our</i> <i>dataset.</i>
There are many options for customizing the visualization in the Datapoint editor,
and doing this can help us understand how our dataset is split across different slices.
Keeping the same color-coding by label, if we select the agency_code column from
the Binning | Y-Axis drop-down, the tool now shows a chart of how balanced our
data is with regard to the agency underwriting each application’s loan. This is shown
in Figure 7-11. Assuming these 1,000 datapoints are a good representation of the rest
of our dataset, there are a few instances of potential bias revealed in Figure 7-11:
<i>Data</i> <i>representation</i> <i>bias</i>
The percentage of HUD applications <i>not</i> approved is higher than other agencies
represented in our data. A model will likely learn this, causing it to predict “not
approved” more frequently for applications originating through HUD.
<i>Data</i> <i>collection</i> <i>bias</i>
We may not have enough data on loans originating from FRS, OCC, FDIC, or
NCUA to accurately use agency_code as a feature in our model. We should make
sure the percentage of applications for each agency in our dataset reflects real-
world trends. For example, if a similar number of loans go through FRS and
HUD, we should have an equal number of examples for each of those agencies in
our dataset."|data collection bias; data representation bias; Fairness Lens design pattern; What-If Tool
"The ML life cycle consists of three stages, as shown in Figure 8-2: discovery, develop‐
ment, and deployment. There is a canonical order to the individual steps of each
stage. However, these steps are completed in an iterative manner and earlier steps
may be revisited depending on the outcomes and insights gathered from later stages.
<b>Discovery</b>
Machine learning exists as a tool to solve a problem. The discovery stage of an ML
project begins with defining the business use case (Step 1 of Figure 8-2). This is a cru‐
cial time for business leaders and ML practitioners to align on the specifics of the
problem and develop an understanding of what ML can and cannot do to achieve
that goal.
It is important to keep sight of the business value through each stage of the life cycle.
Many choices and design decisions must be made throughout the various stages, and
often there is no single “right” answer. Rather, the best option is determined by how
the model will be used in support of the business goal. While a feasible goal for a
research project could be to eke out 0.1% more accuracy on a benchmark dataset, this
is not acceptable in industry. For a production model built for a corporate organiza‐
tion, success is governed by factors more closely tied to the business, like improving
customer retention, optimizing business processes, increasing customer engagement,
or decreasing churn rates. There could also be indirect factors related to the business
use case that influence development choices, like speed of inference, model size, or
model interpretability. Any machine learning project should begin with a thorough
understanding of the business opportunity and how a machine learning model can
make a tangible improvement on current operations.
A successful discovery stage requires collaboration between the business domain
experts as well as machine learning experts to assess the viability of an ML approach.
It is crucial to have someone who understands the business and the data collaborat‐
ing with teams that understand the technical challenges and the engineering effort
that would be involved. If the overall investment of development resources outweighs
the value to the organization, then it is not a worthwhile solution. It is possible that
the technical overhead and cost of resources for productionization exceed the benefit
provided by a model that ultimately improves churn prediction by only 0.1%. Or
maybe not. If an organization’s customer base has 1 billion people, then 0.1% is still 1
million happier customers.
During the discovery phase, it is important to outline the business objectives and
scope for the task. This is also the time to determine which metrics will be used to
measure or define success. Success can look different for different organizations, or
even within different groups of the same organization. See, for example, the discus‐
sion on multiple objectives in “Common Challenges in Machine Learning” on page
11 in Chapter 1. Creating well-defined metrics and key performance indicators"|KPI; ML life cycle
"car.3
fuel efficiency of a In scikit-learn, we can get the learned coefficients of a linear
regression model with the following:
model = LinearRegression().fit(x_train, y_train)
coefficients = model.coef_
The resulting coefficients for each feature in our model are shown in Figure 7-1.
<i>Figure</i> <i>7-1.</i> <i>The</i> <i>learned</i> <i>coefficients</i> <i>from</i> <i>our</i> <i>linear</i> <i>regression</i> <i>fuel</i> <i>efficiency</i> <i>model,</i>
<i>which</i> <i>predicts</i> <i>a</i> <i>car’s</i> <i>miles</i> <i>per</i> <i>gallon.</i> <i>We</i> <i>used</i> <i>get_dummies()</i> <i>from</i> <i>pandas</i> <i>to</i> <i>convert</i>
<i>the</i> <i>origin</i> <i>feature</i> <i>to</i> <i>a</i> <i>boolean</i> <i>column</i> <i>since</i> <i>it</i> <i>is</i> <i>categorical.</i>
The coefficients show us the relationship between each feature and the model’s out‐
put, predicted miles per gallon (MPG). For example, from these coefficients, we can
conclude that for each additional cylinder in a car, our model’s predicted MPG will
decrease. Our model has also learned that as new cars are introduced (denoted by the
“model year” feature), they often have higher fuel efficiency. We can learn much
more about the relationships between our model’s features and output from these
coefficients than we could from the learned weights of a hidden layer in a deep neural
network. This is why models like the one demonstrated above are often referred to as
<i>interpretable</i> <i>by</i> <i>design.</i>
3 ThemodeldiscussedhereistrainedonapublicUCIdataset."|Explainable Predictions design pattern
"examples from the training dataset. In Figure 7-8, we can see the example-based
explanations for a drawing of french fries that the model successfully recognized.
<i>Figure</i> <i>7-8.</i> <i>Example-based</i> <i>explanations</i> <i>from</i> <i>the</i> <i>game</i> <i>Quick,</i> <i>Draw!</i> <i>showing</i> <i>how</i> <i>the</i>
<i>model</i> <i>correctly</i> <i>predicted</i> <i>“french</i> <i>fries”</i> <i>for</i> <i>the</i> <i>given</i> <i>drawing</i> <i>through</i> <i>examples</i> <i>from</i>
<i>the</i> <i>training</i> <i>dataset.</i>
<b>Limitationsofexplanations</b>
Explainability represents a significant improvement in understanding and interpret‐
ing models, but we should be cautious about placing too much trust in our model’s
explanations, or assuming they provide perfect insight into a model. Explanations in
any form are a direct reflection of our training data, model, and selected baseline.
That is to say, we can’t expect our explanations to be high quality if our training data‐
set is an inaccurate representation of the groups reflected by our model, or if the
baseline we’ve chosen doesn’t work well for the problem we’re solving.
Additionally, the relationship that explanations can identify between a model’s fea‐
tures and output is representative only of our data and model, and not necessarily the"|counterfactual analysis; example-based explanation; Explainable Predictions design pattern
"there are two edges that meet toward the top-left corner of the image. A CNN’s final
layers can then piece together these groups of edges, developing an understanding of
different features in the image. In the cat example, the model might be able to iden‐
tify two triangular shapes toward the top of the image and two oval shapes below
them. As humans, we know that these triangular shapes are ears and the oval shapes
are eyes.
We can visualize this process in Figure 4-14, from research by Zeiler and Fergus on
deconstructing CNNs to understand the different features that were activated
throughout each layer of the model. For each layer in a five-layer CNN, this shows an
image’s feature map for a given layer alongside the actual image. This lets us see how
the model’s perception of an image progresses as it moves throughout the network.
Layers 1 and 2 recognize only edges, layer 3 begins to recognize objects, and layers 4
and 5 can understand focal points within the entire image.
Remember, though, that to our model, these are simply groupings of pixel values. It
doesn’t know that the triangular and oval shapes are ears and eyes—it only knows to
associate specific groupings of features with the labels it has been trained on. In this
way, the model’s process of learning what groupings of features make up a cat isn’t
<i>much</i> different from learning the groups of features that are part of other objects, like
a table, a mountain, or even a celebrity. To a model, these are all just different combi‐
nations of pixel values, edges, and shapes."|CNN; Transfer Learning design pattern
"<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
When developing machine learning models, there is an implicit assumption that the
train, validation, and test data come from the same distribution, as shown in
Figure 5-4. When we deploy models to production, this assumption implies that
future data will be similar to past data. However, once the model is in production “in
the wild,” this static assumption on the data may no longer be valid. In fact, many
production ML systems encounter rapidly changing, nonstationary data, and models
become stale over time, which negatively impacts the quality of predictions.
<i>Figure</i> <i>5-4.</i> <i>When</i> <i>developing</i> <i>a</i> <i>machine</i> <i>learning</i> <i>model,</i> <i>the</i> <i>train,</i> <i>validation,</i> <i>and</i> <i>test</i>
<i>data</i> <i>come</i> <i>from</i> <i>the</i> <i>same</i> <i>data</i> <i>distribution.</i> <i>However,</i> <i>once</i> <i>the</i> <i>model</i> <i>is</i> <i>deployed,</i> <i>that</i>
<i>distribution</i> <i>can</i> <i>change,</i> <i>severely</i> <i>affecting</i> <i>model</i> <i>performance.</i>
Continuous model evaluation provides a framework to evaluate a deployed model’s
performance <i>exclusively</i> on new data. This allows us to detect model staleness as early
as possible. This information helps determine how frequently to retrain a model or
when to replace it with a new version entirely.
By capturing prediction inputs and outputs and comparing with ground truth, it’s
possible to quantifiably track model performance or measure how different model
versions perform with A/B testing in the current environment, without regard to how
the versions performed in the past.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The goal of continuous evaluation is to provide a means to monitor model perfor‐
mance and keep models in production fresh. In this way, continuous evaluation pro‐
vides a trigger for when to retrain the model. In this case, it is important to consider
tolerance thresholds for model performance, the trade-offs they pose, and the role of
scheduled retraining. There are also techniques and tools, like TFX, to help detect
data and concept drift preemptively by monitoring input data distributions directly."|Continued Model Evaluation design pattern; ground truth label
"<header><largefont><b>Solution</b></largefont></header>
First, we identify a column that captures the correlation relationship between rows.
In our airline delay dataset, this is the date column. Then, we use the last few digits of
a hash function on that column to split the data. For the airline delay problem, we
date
can use the Farm Fingerprint hashing algorithm on the column to split the
available data into training, validation, and testing datasets.
For more on the Farm Fingerprint algorithm, support for other
frameworks and languages, and the relationship between hashing
and cryptography, please see “Design Pattern 1: Hashed Feature”
on page 32 in Chapter 2. In particular, open source wrappers of the
Farm Hash algorithm are available in a number of languages
(including Python), and so this pattern can be applied even if data
is not in a data warehouse that supports a repeatable hash out of
the box.
date
Here is how to split the dataset based on the hash of the column:
<b>SELECT</b>
airline,
departure_airport,
departure_schedule,
arrival_airport,
arrival_delay
<b>FROM</b>
`bigquery-samples`.airline_ontime_data.flights
<b>WHERE</b>
<b>ABS(MOD(FARM_FINGERPRINT(date),</b> 10)) < 8 <i>--</i> <i>80%</i> <i>for</i> <i>TRAIN</i>
To split on the date column, we compute its hash using the FARM_FINGERPRINT func‐
tion and then use the modulo function to find an arbitrary 80% subset of the rows.
FARM_FINGERPRINT
This is now repeatable—because the function returns the same
value any time it is invoked on a specific date, we can be sure we will get the same
80% of data each time. As a result, all the flights on any given date will belong to the
same split—train, validation, or test. This is repeatable regardless of the random seed.
arrival_airport
If we want to split our data by (so that 80% of airports are in the
training dataset, perhaps because we are trying to predict something about airport
arrival_airport date
amenities), we would compute the hash on instead of .
< 8
It is also straightforward to get the validation data: change the in the query above
to =8 , and for testing data, change it to =9 . This way, we get 10% of samples in valida‐
tion and 10% in testing.
date
What are the considerations for choosing the column to split on? The column
has to have several characteristics for us to be able to use it as the splitting column:"|Farm Fingerprint hashing algorithm; random seed; Repeatable Splitting design pattern
"Each step involves weight updates based on a single mini-batch of data, and this
allows us to stop at 14.3 epochs. This gives us much more granularity, but we have to
define an “epoch” as 1/15th of the total number of steps:
steps_per_epoch=NUM_STEPS // NUM_CHECKPOINTS,
This is so that we get the right number of checkpoints. It works as long as we make
sure to repeat the trainds infinitely:
trainds = trainds.repeat()
repeat() num_epochs
The is needed because we no longer set , so the number of
epochs defaults to one. Without the repeat() , the model will exit once the training
patterns are exhausted after reading the dataset once.
<b>Retrainingwithmoredata.</b>
What happens when we get 100,000 more examples? Easy!
We add it to our data warehouse but do not update the code. Our code will still want
to process 143,000 steps, and it will get to process that much data, except that 10% of
the examples it sees are newer. If the model converges, great. If it doesn’t, we know
that these new data points are the issue because we are not training longer than we
were before. By keeping the number of steps constant, we have been able to separate
out the effects of new data from training on more data.
Once we have trained for 143,000 steps, we restart the training and run it a bit longer
(say, 10,000 steps), and as long as the model continues to converge, we keep training
it longer. Then, we update the number 143,000 in the code above (in reality, it will be
a parameter to the code) to reflect the new number of steps.
This all works fine, until you want to do hyperparameter tuning. When you do
hyperparameter tuning, you will want to want to change the batch size. Unfortu‐
nately, if you change the batch size to 50, you will find yourself training for half the
time because we are training for 143,000 steps, and each step is only half as long as
before. Obviously, this is no good.
<b>Virtualepochs.</b>
The answer is to keep the total number of training examples shown to
the model (not number of steps; see Figure 4-12) constant:
NUM_TRAINING_EXAMPLES = 1000 * 1000
STOP_POINT = 14.3
TOTAL_TRAINING_EXAMPLES = int(STOP_POINT * NUM_TRAINING_EXAMPLES)
BATCH_SIZE = 100
NUM_CHECKPOINTS = 15
steps_per_epoch = (TOTAL_TRAINING_EXAMPLES //
(BATCH_SIZE*NUM_CHECKPOINTS))
cp_callback = tf.keras.callbacks.ModelCheckpoint(...)
history = model.fit(trainds,
validation_data=evalds,
epochs=NUM_CHECKPOINTS,
steps_per_epoch=steps_per_epoch,"|Checkpoints design pattern; epochs; hyperparameter tuning
"<header><largefont><b>Solution</b></largefont></header>
The solution consists of the following steps:
1. Export the model into a format that captures the mathematical core of the model
and is programming language agnostic.
2. In the production system, the formula consisting of the “forward” calculations of
the model is restored as a stateless function.
3. The stateless function is deployed into a framework that provides a REST end‐
point.
<b>Modelexport</b>
The first step of the solution is to export the model into a format (TensorFlow uses
SavedModel, but ONNX is another choice) that captures the mathematical core of
the model. The entire model state (learning rate, dropout, short-circuit, etc.) doesn’t
need to be saved—just the mathematical formula required to compute the output
from the inputs. Typically, the trained weight values are constants in the mathemati‐
cal formula.
In Keras, this is accomplished by:
model.save('export/mymodel')
The SavedModel format relies on protocol buffers for a platform-neutral, efficient
restoration mechanism. In other words, the model.save() method writes the model
.pb
as a protocol buffer (with the extension ) and externalizes the trained weights,
vocabularies, and so on into other files in a standard directory structure:
<i>export/.../variables/variables.data-00000-of-00001</i>
<i>export/.../assets/tokens.txt</i>
<i>export/.../saved_model.pb</i>
<b>InferenceinPython</b>
In a production system, the model’s formula is restored from the protocol buffer and
other associated files as a stateless function that conforms to a specific model signa‐
ture with input and output variable names and data types.
We can use the TensorFlow saved_model_cli tool to examine the exported files to
determine the signature of the stateless function that we can use in serving:
saved_model_cli show --dir ${export_path} <b>\</b>
--tag_set serve --signature_def serving_default
This outputs:"|ONNX; SavedModel; saved_model_cli; Stateless Serving Function design pattern
"Take the following made-up comment as an example: “Mint chip is their best ice
cream flavor, hands down.” If we were to replace “Mint chip” with “Rocky road,” the
comment should be labeled with the same toxicity score (ideally 0). Similarly, if the
comment were instead, “Mint chip is the worst. If you like this flavor you’re an idiot,”
we’d expect a higher toxicity score, and that score should be the same any time we
replace “Mint chip” with a different flavor name. We’ve used ice cream in this exam‐
ple, but it’s easy to imagine how this would play out with more controversial identity
terms, especially in a human-centered dataset—a concept known as counterfactual
fairness.
<b>Aftertraining</b>
Even with rigorous data analysis, bias may find its way into a trained model. This can
happen as a result of a model’s architecture, optimization metrics, or data bias that
wasn’t identified before training. To solve for this, it’s important to evaluate our
model from a fairness perspective and dig deeper into metrics other than overall
model accuracy. The goal of this post-training analysis is to understand the trade-offs
between model accuracy and the effects a model’s predictions will have on different
groups.
The What-If Tool is one such option for post-model analysis. To demonstrate how to
use it on a trained model, we’ll build on our mortgage dataset example. Based on our
previous analysis, we’ve refined the dataset to only include loans for the purpose of
refinancing or home purchases,10 and trained an XGBoost model to predict whether
or not an application will be approved. Because we’re using XGBoost, we converted
get_dummies()
all categorical features into boolean columns using the pandas
method.
We’ll make a few additions to our What-If Tool initialization code above, this time
passing in a function that calls our trained model, along with configs specifying our
label column and the name for each label:
<b>def</b> custom_fn(examples):
df = pd.DataFrame(examples, columns=columns)
preds = bst.predict_proba(df)
<b>return</b> preds
config_builder = (WitConfigBuilder(test_examples, columns)
.set_custom_predict_fn(custom_fn)
.set_target_feature('mortgage_status')
.set_label_vocab(['denied', 'approved']))
WitWidget(config_builder, height=800)
10 Therearemanymorepre-trainingoptimizationsthatcouldbemadeonthisdataset.We’vechosenjustone
hereasademoofwhat’spossible."|bias; Fairness Lens design pattern; What-If Tool
"<header><largefont><b>Models</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Frameworks</b></largefont></header>
At its core, <i>machine</i> <i>learning</i> is a process of building models that learn from data.
This is in contrast to traditional programming where we write explicit rules that tell
programs how to behave. Machine learning <i>models</i> are algorithms that learn patterns
from data. To illustrate this point, imagine we are a moving company and need to
estimate moving costs for potential customers. In traditional programming, we might
solve this with an if statement:
<b>if</b> num_bedrooms == 2 <b>and</b> num_bathrooms == 2:
estimate = 1500
<b>elif</b> num_bedrooms == 3 <b>and</b> sq_ft > 2000:
estimate = 2500
You can imagine how this will quickly get complicated as we add more variables
(number of large furniture items, amount of clothing, fragile items, and so on) and
try to handle edge cases. More to the point, asking for all this information ahead of
time from customers can cause them to abandon the estimation process. Instead, we
can train a machine learning model to estimate moving costs based on past data on
previous households our company has moved.
Throughout the book, we primarily use feed-forward neural network models in our
examples, but we’ll also reference linear regression models, decision trees, clustering
models, and others. <i>Feed-forward</i> <i>neural</i> <i>networks,</i> which we will commonly shorten
as <i>neural</i> <i>networks,</i> are a type of machine learning algorithm whereby multiple layers,
each with many neurons, analyze and process information and then send that infor‐
mation to the next layer, resulting in a final layer that produces a prediction as out‐
put. Though they are in no way identical, neural networks are often compared to the
neurons in our brain because of the connectivity between nodes and the way they are
able to generalize and form new predictions from the data they process. Neural net‐
works with more than one <i>hidden</i> <i>layer</i> (layers other than the input and output layer)
are classified as <i>deep</i> <i>learning</i> (see Figure 1-1).
Machine learning models, regardless of how they are depicted visually, are mathe‐
matical functions and can therefore be implemented from scratch using a numerical
software package. However, ML engineers in industry tend to employ one of several
open source frameworks designed to provide intuitive APIs for building models. The
majority of our examples will use <i>TensorFlow,</i> an open source machine learning
framework created by Google with a focus on deep learning models. Within the
TensorFlow library, we’ll be using the <i>Keras</i> API in our examples, which can be
imported through tensorflow.keras. Keras is a higher-level API for build
ing neural networks
. While Keras supports many backends, we’ll be using its
TensorFlow backend. In other examples, we’ll be using <i>scikit-learn,</i> <i>XGBoost,</i> and
<i>PyTorch,</i> which are other popular open source frameworks that provide utilities for
preparing your data, along with APIs for building linear and deep models. Machine"|deep learning; hidden layers; Keras; machine learning models; machine learning; neural networks; TensorFlow
"ntrain = 8*len(df)//10 <i>#</i> <i>80%</i> <i>of</i> <i>data</i> <i>for</i> <i>training</i>
lm = linear_model.LogisticRegression()
lm = lm.fit(df.loc[:ntrain-1, ['jaundice', 'ulcers']],
df[label][:ntrain])
acc = lm.score(df.loc[ntrain:, ['jaundice', 'ulcers']],
df[label][ntrain:])
If we create three classes, and put all the randomly assigned prescriptions into that
class, we get, as expected, perfect (100%) accuracy. The purpose of the synthetic data
was to illustrate that, provided there is random assignment at work, the Neutral Class
design pattern can help us avoid losing model accuracy because of arbitrarily labeled
data.
<b>Intherealworld</b>
In real-world situations, things may not be precisely random as in the synthetic data‐
set, but the arbitrary assignment paradigm still holds. For example, one minute after
a baby is born, the baby is assigned an “Apgar score,” a number between 1 and 10,
with 10 being a baby that has come through the birthing process perfectly.
Consider a model that is trained to predict whether or not a baby will come through
the birthing process healthily, or will require immediate attention (the full code is on
GitHub):
CREATE OR REPLACE MODEL mlpatterns.neutral_2classes
OPTIONS(model_type='logistic_reg', input_label_cols=['health']) AS
SELECT
IF(apgar_1min >= 9, 'Healthy', 'NeedsAttention') AS health,
plurality,
mother_age,
gestation_weeks,
ever_born
FROM `bigquery-public-data.samples.natality`
WHERE apgar_1min <= 10
We are thresholding the Apgar score at 9 and treating babies whose Apgar score is 9
or 10 as healthy, and babies whose Apgar score is 8 or lower as requiring attention.
The accuracy of this binary classification model when trained on the natality dataset
and evaluated on held-out data is 0.56.
Yet, assigning an Apgar score involves a number of relatively subjective assessments,
and whether a baby is assigned 8 or 9 often reduces to matters of physician prefer‐
ence. Such babies are neither perfectly healthy, nor do they need serious medical
intervention. What if we create a neutral class to hold these “marginal” scores? This
requires creating three classes, with an Apgar score of 10 defined as healthy, scores of
8 to 9 defined as neutral, and lower scores defined as requiring attention:
CREATE OR REPLACE MODEL mlpatterns.neutral_3classes
OPTIONS(model_type='logistic_reg', input_label_cols=['health']) AS"|Neutral Class design pattern
"the Euclidean distance, and extract the hour of day from the timestamp. We have to
carefully design the model graph (see Figure 6-2), keeping the Transform concept
firmly in mind. As we walk through the code below, notice how we set things up so
that we clearly design three separate layers in our Keras model—the Inputs layer, the
DenseFeatures
Transform layer, and a layer.
<i>Figure</i> <i>6-2.</i> <i>The</i> <i>model</i> <i>graph</i> <i>for</i> <i>the</i> <i>taxi</i> <i>fare</i> <i>estimation</i> <i>problem</i> <i>in</i> <i>Keras.</i>
First, make every input to the Keras model an Input layer (the full code is in a note‐
book on GitHub):
inputs = {
colname : tf.keras.layers.Input(
name=colname, shape=(), dtype='float32')
<b>for</b> colname <b>in</b> ['pickup_longitude', 'pickup_latitude',
'dropoff_longitude', 'dropoff_latitude']
}"|feature columns; Keras; Transform design pattern
"Another way to separate out the programming language and framework used for
transformation of the features from the language used to write the model is to carry
out the preprocessing in containers and use these custom containers as part of both
the training and serving. This is discussed in “Design Pattern 25: Workflow Pipeline”
on page 282 and is adopted in practice by Kubeflow Serving.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>22:</b></largefont> <largefont><b>Repeatable</b></largefont> <largefont><b>Splitting</b></largefont></header>
To ensure that sampling is repeatable and reproducible, it is necessary to use a well-
distributed column and a deterministic hash function to split the available data into
training, validation, and test datasets.
<header><largefont><b>Problem</b></largefont></header>
Many machine learning tutorials will suggest splitting data randomly into training,
validation, and test datasets using code similar to the following:
df = pd.DataFrame(...)
rnd = np.random.rand(len(df))
train = df[ rnd < 0.8 ]
valid = df[ rnd >= 0.8 & rnd < 0.9 ]
test = df[ rnd >= 0.9 ]
Unfortunately, this approach fails in many real-world situations. The reason is that it
is rare that the rows are independent. For example, if we are training a model to pre‐
dict flight delays, the arrival delays of flights on the same day will be highly correlated
with one another. This leads to leakage of information between the training and test‐
ing dataset when some of the flights on any particular day are in the training dataset
and some other flights on the same day are in the testing dataset. This leakage due to
correlated rows is a frequently occurring problem, and one that we have to avoid
when doing machine learning.
rand
In addition, the function orders data differently each time it is run, so if we run
the program again, we will get a different 80% of rows. This can play havoc if we are
experimenting with different machine learning models with the goal of choosing the
best one—we need to compare the model performance on the same test dataset. In
order to address this, we need to set the random seed in advance or store the data
after it is split. Hardcoding how the data is to be split is not a good idea because,
when carrying out techniques like jackknifing, bootstrapping, cross-validation, and
hyperparameter tuning, we will need to change this data split and do so in a way that
allows us to do individual trials.
For machine learning, we want lightweight, repeatable splitting of the data that works
regardless of programming language or random seeds. We also want to ensure that
correlated rows fall into the same split. For example, we do not want flights on Janu‐
ary 2, 2019 in the test dataset if flights on that day are in the training dataset."|random seed; Repeatable Splitting design pattern; test data; Transform design pattern
"<b>Feast</b>
As an example of this pattern in action, consider Feast, which is an open source fea‐
ture store for machine learning developed by Google Cloud and Gojek. It is built
around Google Cloud services using Big Query for offline model training and Redis
for low-latency, online serving (Figure 6-13). Apache Beam is used for feature cre‐
ation, which allows for consistent data pipelines for both batch and streaming.
<i>Figure</i> <i>6-13.</i> <i>High-level</i> <i>architecture</i> <i>of</i> <i>the</i> <i>Feast</i> <i>feature</i> <i>store.</i> <i>Feast</i> <i>is</i> <i>built</i> <i>around</i>
<i>Google</i> <i>BigQuery,</i> <i>Redis,</i> <i>and</i> <i>Apache</i> <i>Beam.</i>
To see how this works in practice, we’ll use a public BigQuery dataset containing
information about taxi rides in New York City.7 Each row of the table contains a
timestamp of the pickup, the pickup latitude and longitude, the dropoff latitude and
longitude, the number of passengers, and the cost of the taxi ride. The goal of the ML
model will be to predict the cost of the taxi ride, denoted fare_amount , using these
characteristics.
This model benefits from engineering additional features from the raw data. For
example, since taxi rides are based on the distance and duration of the trip, pre-
7 ThedataisavailableintheBigQuerytable:bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016."|Feast; Feature Store design pattern; low latency
"augmentations to generate additional samples. We’ll also look at approaches for
<i>reframing</i> the problem: changing it to a regression task, analyzing our model’s error
values for each example, or clustering.
<b>Choosinganevaluationmetric</b>
For imbalanced datasets like the one in our fraud detection example, it’s best to use
metrics like precision, recall, or F-measure to get a complete picture of how our
model is performing. <i>Precision</i> measures the percentage of positive classifications that
were correct out of all positive predictions made by the model. Conversely, <i>recall</i>
measures the proportion of actual positive examples that were identified correctly by
the model. The biggest difference between these two metrics is the denominator used
to calculate them. For precision, the denominator is the total number of positive class
predictions made by our model. For recall, it is the number of <i>actual</i> positive class
examples present in our dataset.
A perfect model would have both precision and recall of 1.0, but in practice, these
two measures are often at odds with each other. The <i>F-measure</i> is a metric that ranges
from 0 to 1 and takes both precision and recall into account. It is calculated as:
2 * (precision * recall / (precision + recall))
Let’s return to the fraud detection use case to see how each of these metrics plays out
in practice. For this example, let’s say our test set contains a total of 1,000 examples,
50 of which should be labeled as fraudulent transactions. For these examples, our
model predicts 930/950 nonfraudulent examples correctly, and 15/50 fraudulent
examples correctly. We can visualize these results in Figure 3-19.
<i>Figure</i> <i>3-19.</i> <i>Sample</i> <i>predictions</i> <i>for</i> <i>a</i> <i>fraud</i> <i>detection</i> <i>model.</i>
In this case, our model’s precision is 15/35 (42%), recall is 15/50 (30%), and F-
measure is 35%. These do a much better job capturing our model’s inability to
correctly identify fraudulent transactions compared to accuracy, which is 945/1000
(94.5%). Therefore, for models trained on imbalanced datasets, metrics other than
accuracy are preferred. In fact, accuracy may even go down when optimizing for"|fraud detection; precision; Rebalancing design pattern; recall
"create_bucket = CreateBucketComponent(
bucket_name='my-bucket')
<b>IntegratingCI/CDwithpipelines</b>
In addition to invoking pipelines via the dashboard or programmatically via the CLI
or API, chances are we’ll want to automate runs of our pipeline as we productionize
the model. For example, we may want to invoke our pipeline whenever a certain
amount of new training data is available. Or we might want to trigger a pipeline run
when the source code for the pipeline changes. Adding CI/CD to our Workflow Pipe‐
line can help connect trigger events to pipeline runs.
There are many managed services available for setting up triggers to run a pipeline
when we want to retrain a model on new data. We could use a managed scheduling
service to invoke our pipeline on a schedule. Alternatively, we could use a serverless
event-based service like Cloud Functions to invoke our pipeline when new data is
added to a storage location. In our function, we could specify conditions—like a
threshold for the amount of new data added to necessitate retraining—for creating a
new pipeline run. Once enough new training data is available, we can instantiate a
pipeline run for retraining and redeploying the model as demonstrated in Figure 6-9.
<i>Figure</i> <i>6-9.</i> <i>A</i> <i>CI/CD</i> <i>workflow</i> <i>using</i> <i>Cloud</i> <i>Functions</i> <i>to</i> <i>invoke</i> <i>a</i> <i>pipeline</i> <i>when</i> <i>enough</i>
<i>new</i> <i>data</i> <i>is</i> <i>added</i> <i>to</i> <i>a</i> <i>storage</i> <i>location.</i>"|CI/CD; Cloud Functions; TFX; Workflow Pipeline design pattern
"Another reason to use a framework like Apache Beam is if the client code needs to
maintain state. A common reason that the client needs to maintain state is if one of
the inputs to the ML model is a time-windowed average. In that case, the client code
has to carry out moving averages of the incoming stream of data and supply the mov‐
ing average to the ML model.
Imagine that we are building a comment moderation system and we wish to reject
people who comment more than two times a day about a specific person. For exam‐
ple, the first two times that a commenter writes something about President Obama,
we will let it go but block all attempts by that commenter to mention President
Obama for the rest of the day. This is an example of postprocessing that needs to
maintain state because we need a counter of the number of times that each com‐
menter has mentioned a particular celebrity. Moreover, this counter needs to be over
a rotating time period of 24 hours.
We can do this using a distributed data processing framework that can maintain
state. Enter Apache Beam. Invoking an ML model to identify mentions of a celebrity
and tying them to a canonical knowledge graph (so that a mention of Obama and a
mention of President Obama both tie to <i>en.wikipedia.org/wiki/Barack_Obama)</i> from
Apache Beam can be accomplished using the following (see this notebook in GitHub
for complete code):
| beam.Map(lambda x : nlp.Document(x, type='PLAIN_TEXT'))
| nlp.AnnotateText(features)
| beam.Map(parse_nlp_result)
parse_nlp_result Annotate
where parses the JSON request that goes through the
Text transform which, beneath the covers, invokes an NLP API.
<b>Cachedresultsofbatchserving</b>
We discussed batch serving as a way to invoke a model over millions of items when
the model is normally served online using the Stateless Serving Function design pat‐
tern. Of course, it is possible for batch serving to work even if the model does not
support online serving. What matters is that the machine learning framework doing
inference is capable of taking advantage of embarrassingly parallel processing.
Recommendation engines, for example, need to fill out a sparse matrix consisting of
every user–item pair. A typical business might have 10 million all-time users and
10,000 items in the product catalog. In order to make a recommendation for a user,
recommendation scores have to be computed for each of the 10,000 items, ranked,
and the top 5 presented to the user. This is not feasible to do in near real time off a
serving function. Yet, the near real-time requirement means that simply using batch
serving will not work either.
In such cases, use batch serving to precompute recommendations for all 10 million
users:"|batch serving; Batch Serving design pattern; time-windowed average
"<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>29:</b></largefont> <largefont><b>Explainable</b></largefont> <largefont><b>Predictions</b></largefont></header>
The Explainable Predictions design pattern increases user trust in ML systems by
providing users with an understanding of how and why models make certain predic‐
tions. While models such as decision trees are interpretable by design, the architec‐
ture of deep neural networks makes them inherently difficult to explain. For all
models, it is useful to be able to interpret predictions in order to understand the com‐
binations of features influencing model behavior.
<header><largefont><b>Problem</b></largefont></header>
When evaluating a machine learning model to determine whether it’s ready for pro‐
duction, metrics like accuracy, precision, recall, and mean squared error only tell one
piece of the story. They provide data on how <i>correct</i> a model’s predictions are relative
to ground truth values in the test set, but they carry no insight on <i>why</i> a model
arrived at those predictions. In many ML scenarios, users may be hesitant to accept a
model’s prediction at face value.
To understand this, let’s look at a model that predicts the severity of diabetic retinop‐
athy (DR) from an image of a retina. 1 The model returns a softmax output, indicating
the probability that an individual image belongs to 1 of 5 categories denoting the
severity of DR in the image—ranging from 1 (no DR present) to 5 (proliferative DR,
the worst form). Let’s imagine that for a given image, the model returns 95% confi‐
dence that the image contains proliferative DR. This may seem like a high-
confidence, accurate result, but if a medical professional is relying solely on this
model output to provide a patient diagnosis, they still have no insight into <i>how</i> the
model arrived at this prediction. Maybe the model identified the correct regions in
the image that are indicative of DR, but there’s also a chance the model’s prediction is
based on pixels in the image that show no indication of the disease. As an example,
maybe some images in the dataset contain doctor notes or annotations. The model
could be incorrectly using the presence of an annotation to make its prediction,
rather than the diseased areas in the image.2 In the model’s current form, there is no
way to attribute the prediction to regions in an image, making it difficult for the doc‐
tor to trust the model.
Medical imaging is just one example—there are many industries, scenarios, and
model types where a lack of insight into a model’s decision-making process can lead
to problems with user trust. If an ML model is used to predict an individual’s credit
score or other financial health metric, people will likely want to know why they
1 DRisaneyeconditionaffectingmillionsofpeoplearoundtheworld.Itcanleadtoblindness,butifcaught
early,itcanbesuccessfullytreated.Tolearnmoreandfindthedataset,seehere.
2 Explanationswereusedtoidentifyandcorrectforannotationspresentinradiologyimagesinthisstudy."|bias; Explainable Predictions design pattern
"<i>Table</i> <i>3-5.</i> <i>The</i> <i>percentage</i> <i>of</i> <i>each</i> <i>weight</i> <i>class</i> <i>present</i> <i>in</i> <i>the</i> <i>natality</i> <i>dataset</i>
<b>weight</b> <b>num_examples</b> <b>percent_of_dataset</b>
Average 123781044 0.8981
Underweight 9649724 0.07
Overweight 4395995 0.0319
For demo purposes, we’ll take 100,000 examples from each class to train a model on
an updated, balanced dataset:
<b>SELECT</b>
is_male,
gestation_weeks,
mother_age,
weight_pounds,
weight
<b>FROM</b> (
<b>SELECT</b>
*,
ROW_NUMBER() OVER (PARTITION <b>BY</b> weight <b>ORDER</b> <b>BY</b> RAND()) <b>AS</b> row_num
<b>FROM</b> (
<b>SELECT</b>
is_male,
gestation_weeks,
mother_age,
weight_pounds,
<b>CASE</b>
<b>WHEN</b> weight_pounds < 5.5 <b>THEN</b> ""underweight""
<b>WHEN</b> weight_pounds > 9.5 <b>THEN</b> ""overweight""
<b>ELSE</b>
""average""
<b>END</b>
<b>AS</b> weight,
<b>FROM</b>
`bigquery-public-data.samples.natality`
<b>LIMIT</b>
4000000) )
<b>WHERE</b>
row_num < 100000
We can save the results of that query to a table, and with a more balanced dataset, we
can now train a classification model to label babies as “underweight,” “average,” or
“overweight”:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL
`project.dataset.baby_weight_classification` <b>OPTIONS(model_type='logistic_reg',</b>
input_label_cols=['weight']) <b>AS</b>
<b>SELECT</b>
is_male,
weight_pounds,
mother_age,"|cascade; Rebalancing design pattern; reframing
"<b>Multimodalfeaturerepresentationsandmodelinterpretability</b>
Deep learning models are inherently difficult to explain. If we build a model that ach‐
ieves 99% accuracy, we still don’t know exactly <i>how</i> our model is making predictions
and consequently, if the way it’s making those predictions is correct. For example,
let’s say we train a model on images of petri dishes taken in a lab that achieves a high
accuracy score. These images also contain annotations from the scientist who took
the pictures. What we don’t know is that the model is incorrectly using the annota‐
tions to make its predictions, rather than the contents of the petri dishes.
There are several techniques for explaining image models that can highlight the pix‐
els that signaled a model’s prediction. When we combine multiple data representa‐
tions in a single model, however, these features become dependent on one another.
As a result, it can be difficult to explain how the model is making predictions.
Explainability is covered in Chapter 7.
<header><largefont><b>Summary</b></largefont></header>
In this chapter, we learned different approaches to representing data for our model.
We started by looking at how to handle numerical inputs, and how scaling these
inputs can speed up model training time and improve accuracy. Then we explored
how to do feature engineering on categorical inputs, specifically with one-hot encod‐
ing and using arrays of categorical values.
Throughout the rest of the chapter, we discussed four design patterns for represent‐
ing data. The first was the <i>Hashed</i> <i>Feature</i> design pattern, which involves encoding
categorical inputs as unique strings. We explored a few different approaches to hash‐
ing using the airport dataset in BigQuery. The second pattern we looked at in this
chapter was <i>Embeddings,</i> a technique for representing high-cardinality data such as
inputs with many possible categories or text data. Embeddings represent data in mul‐
tidimensional space, where the dimension is dependent on our data and prediction
task. Next we looked at <i>Feature</i> <i>Crosses,</i> an approach that joins two features to extract
relationships that may not have been easily captured by encoding the features on
their own. Finally, we looked at <i>Multimodal</i> <i>Input</i> representations by addressing the
problem of how to combine inputs of different types into the same model, and how a
single feature can be represented multiple ways.
This chapter focused on preparing <i>input</i> data for our models. In the next chapter,
we’ll focus on model <i>output</i> by diving into different approaches for representing our
prediction task."|deep learning; Multimodal Input design pattern
"double_val: 40.78923797607422
}
}
fields {
key: ""taxi_rides:dropoff_lon""
value {
double_val: -73.96871948242188
}
…
To make an online prediction for this example, we pass the field values from the
online_features predict_df
object returned in as a pandas dataframe called to
model.predict :
predict_df = pd.DataFrame.from_dict(online_features_dict)
model.predict(predict_df)
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Feature stores work because they decouple feature engineering from feature usage,
allowing feature development and creation to occur independently from the con‐
sumption of features during model development. As features are added to the feature
store, they become available immediately for both training and serving and are stored
in a single location. This ensures consistency between model training and serving.
For example, a model served as a customer-facing application may receive only 10
input values from a client, but those 10 inputs may need to be transformed into many
more features via feature engineering before being sent to a model. Those engineered
features are maintained within the feature store. It is crucial that the pipeline for
retrieving features during development is the same as when serving the model. A fea‐
ture store ensures that consistency (Figure 6-16).
Feast accomplishes this by using Beam on the backend for feature ingestion pipelines
that write feature values into the feature sets, and uses Redis and BigQuery for online
6-17).8
and offline (respectively) feature retrieval (Figure As with any feature store,
the ingestion pipeline also handles partial failure or race conditions that might cause
some data to be in one storage but not the other.
8 SeetheGojekblog,“Feast:BridgingMLModelsandData.”"|Feast; Feature Store design pattern
"<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>1:</b></largefont> <largefont><b>Hashed</b></largefont> <largefont><b>Feature</b></largefont></header>
The Hashed Feature design pattern addresses three possible problems associated with
categorical features: incomplete vocabulary, model size due to cardinality, and cold
start. It does so by grouping the categorical features and accepting the trade-off of
collisions in the data representation.
<header><largefont><b>Problem</b></largefont></header>
One-hot encoding a categorical input variable requires knowing the vocabulary
beforehand. This is not a problem if the input variable is something like the language
a book is written in or the day of the week that traffic level is being predicted.
What if the categorical variable in question is something like the hospital_id of
physician_id
where the baby is born or the of the person delivering the baby? Cate‐
gorical variables like these pose a few problems:
• Knowing the vocabulary requires extracting it from the training data. Due to
random sampling, it is possible that the training data does not contain all the
possible hospitals or physicians. The vocabulary might be <i>incomplete.</i>
• The categorical variables have <i>high</i> <i>cardinality.</i> Instead of having feature vectors
with three languages or seven days, we have feature vectors whose length is in the
thousands to millions. Such feature vectors pose several problems in practice.
They involve so many weights that the training data may be insufficient. Even if
we can train the model, the trained model will require a lot of space to store
because the entire vocabulary is needed at serving time. Thus, we may not be able
to deploy the model on smaller devices.
• After the model is placed into production, new hospitals might be built and new
physicians hired. The model will be unable to make predictions for these, and so
a separate serving infrastructure will be required to handle such <i>cold-start</i>
problems.
Even with simple representations like one-hot encoding, it is worth
anticipating the cold-start problem and explicitly reserving all
zeros for out-of-vocabulary inputs.
As a concrete example, let’s take the problem of predicting the arrival delay of a
flight. One of the inputs to the model is the departure airport. There were, at the time
the dataset was collected, 347 airports in the United States:"|cold start; Hashed Feature design pattern; high cardinality; vocabulary
"<i>Figure</i> <i>3-4.</i> <i>The</i> <i>precision</i> <i>of</i> <i>the</i> <i>multiclass</i> <i>classification</i> <i>is</i> <i>controlled</i> <i>by</i> <i>the</i> <i>width</i> <i>of</i> <i>the</i>
<i>bins</i> <i>for</i> <i>the</i> <i>label.</i>
The sharpness of the PDF indicates the precision of the task as a regression. A
sharper PDF indicates a smaller standard deviation of the output distribution while a
wider PDF indicates a larger standard deviation and thus more variance. For a very
sharp density function, it’s better to stick with a regression model (see Figure 3-5)."|PDF; Reframing design pattern
"Now that we have a working model in production, let’s imagine that our data science
team decides to change the model from XGBoost to TensorFlow since it results in
improved accuracy and gives them access to additional tooling in the TensorFlow
ecosystem. The model has the same input and output format, but its architecture and
exported asset format has changed. Instead of a <i>.bst</i> file, our model is now in the
TensorFlow SavedModel format. Ideally we can keep our underlying model assets
separate from our application frontend—this will allow application developers to
focus on our application’s functionality, rather than a change in model formatting
that won’t affect the way end users interact with the model. This is where model ver‐
sioning can help. We’ll deploy our TensorFlow model as a second version under the
flight_delay_prediction
same model resource. End users can upgrade to the new
version for improved performance simply by changing the version name in the API
endpoint.
To deploy our second version, we’ll export the model and copy it to a new subdirec‐
tory in the bucket we used previously. We can use the same deploy command as
above, replacing the version name with v2 and pointing to the Cloud Storage location
of the new model. As shown in Figure 6-19, we’re now able to see both deployed ver‐
sions in our Cloud console.
<i>Figure</i> <i>6-19.</i> <i>The</i> <i>dashboard</i> <i>for</i> <i>managing</i> <i>models</i> <i>and</i> <i>versions</i> <i>in</i> <i>the</i> <i>Cloud</i> <i>AI</i> <i>Plat‐</i>
<i>form</i> <i>console.</i>
Notice that we’ve also set <i>v2</i> as the new default version, so that if users don’t specify a
version, they’ll get a response from <i>v2.</i> Since the input and output format of our
model are the same, clients can upgrade without worrying about breaking changes."|Model Versioning design pattern
"One way to solve this problem is to train a classification model to first classify trips
based on whether they are Long or Typical (the full code is in the code repository of
this book):
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL mlpatterns.classify_trips
<b>TRANSFORM(</b>
trip_type,
<b>EXTRACT</b> (HOUR <b>FROM</b> start_date) <b>AS</b> start_hour,
<b>EXTRACT</b> (DAYOFWEEK <b>FROM</b> start_date) <b>AS</b> day_of_week,
start_station_name,
subscriber_type,
...
)
<b>OPTIONS(model_type='logistic_reg',</b>
auto_class_weights=True,
input_label_cols=['trip_type']) <b>AS</b>
<b>SELECT</b>
start_date, start_station_name, subscriber_type, ...
IF(duration_sec > 3600*4, <b>'Long',</b> <b>'Typical')</b> <b>AS</b> trip_type
<b>FROM</b> `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`
It can be tempting to simply split the training dataset into two parts based on the
actual duration of the rental and train the next two models, one on Long rentals and
the other on Typical rentals. The problem is that the classification model just dis‐
cussed will have errors. Indeed, evaluating the model on a held-out portion of the San
Francisco bicycle data shows that the accuracy of the model is only around 75% (see
Figure 3-16). Given this, training a model on a perfect split of the data will lead to
tears.
<i>Figure</i> <i>3-16.</i> <i>The</i> <i>accuracy</i> <i>of</i> <i>a</i> <i>classification</i> <i>model</i> <i>to</i> <i>predict</i> <i>atypical</i> <i>behavior</i> <i>is</i>
<i>unlikely</i> <i>to</i> <i>be</i> <i>100%.</i>
Instead, after training this classification model, we need to use the predictions of this
model to create the training dataset for the next set of models. For example, we could
create the training dataset for the model to predict the distance of Typical rentals
using:"|Cascade design pattern
"computing the distance between the pickup and dropoff is a useful feature. Once this
feature is computed on the dataset, we can store it within a feature set for future use.
<b>AddingfeaturedatatoFeast.</b> FeatureSets. FeatureSet
Data is stored in Feast using A
contains the data schema and data source information, whether it is coming from a
FeatureSets
pandas dataframe or a streaming Kafka topic. are how Feast knows
where to source the data it needs for a feature, how to ingest it, and some basic char‐
acteristics about the data types. Groups of features can be ingested and stored
together, and feature sets provide efficient storage and logical namespacing of data
within these stores.
Once our feature set is registered, Feast will start an Apache Beam job to populate the
feature store with data from the source. A feature set is used to generate both offline
and online feature stores, which ensures developers train and serve their model with
the same data. Feast ensures that the source data complies with the expected schema
of the feature set.
There are four steps to ingest feature data into Feast, as shown in Figure 6-14.
<i>Figure</i> <i>6-14.</i> <i>There</i> <i>are</i> <i>four</i> <i>steps</i> <i>to</i> <i>ingesting</i> <i>feature</i> <i>data</i> <i>into</i> <i>Feast:</i> <i>create</i> <i>a</i> <i>Feature‐</i>
<i>Set,</i> <i>add</i> <i>entities</i> <i>and</i> <i>features,</i> <i>register</i> <i>the</i> <i>FeatureSet,</i> <i>and</i> <i>ingest</i> <i>feature</i> <i>data</i> <i>into</i> <i>the</i>
<i>FeatureSet.</i>"|Feast; Feature Store design pattern; FeatureSet
"While continuous evaluation provides a post hoc way of monitoring a deployed
model, it is also valuable to monitor the new data that is received during serving and
preemptively identify changes in data distributions.
TFX’s Data Validation is a useful tool to accomplish this. TFX is an end-to-end plat‐
form for deploying machine learning models open sourced by Google. The Data Val‐
idation library can be used to compare the data examples used in training with those
collected during serving. Validity checks detect anomalies in the data, training-
serving skew, or data drift. TensorFlow Data Validation creates data visualizations
using Facets, an open source visualization tool for machine learning. The Facets
Overview gives a high-level look at the distributions of values across various features
and can uncover several common and uncommon issues like unexpected feature val‐
ues, missing feature values, and training-serving skew.
<b>Estimatingretraininginterval</b>
A useful and relatively cheap tactic to understand how data and concept drift affect
your model is to train a model using only stale data and assess the performance of
that model on more current data (Figure 5-8). This mimics the continued model
evaluation process in an offline environment. That is, collect data from six months or
a year ago and go through the usual model development workflow, generating fea‐
tures, optimizing hyperparameters, and capturing relevant evaluation metrics. Then,
compare those evaluation metrics against the model predictions for more recent data
collected from only a month prior. How much worse does your stale model perform
on the current data? This gives a good estimate of the rate at which a model’s perfor‐
mance falls off over time and how often it might be necessary to retrain."|concept drift; Continued Model Evaluation design pattern; data drift; data validation; Facets; TensorFlow Data Validation; TFX
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>4</b></largefont></header>
<header><largefont><b>Model</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Patterns</b></largefont></header>
Machine learning models are usually trained iteratively, and this iterative process is
informally called the <i>training</i> <i>loop.</i> In this chapter, we discuss what the typical train‐
ing loop looks like, and catalog a number of situations in which you might want to do
something different.
<header><largefont><b>Typical</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Loop</b></largefont></header>
Machine learning models can be trained using different types of optimization. Deci‐
sion trees are often built node by node based on an information gain measure. In
genetic algorithms, the model parameters are represented as genes, and the optimiza‐
tion method involves techniques that are based on evolutionary theory. However, the
most common approach to determining the parameters of machine learning models
is <i>gradient</i> <i>descent.</i>
<header><largefont><b>Stochastic</b></largefont> <largefont><b>Gradient</b></largefont> <largefont><b>Descent</b></largefont></header>
On large datasets, gradient descent is applied to mini-batches of the input data to
train everything from linear models and boosted trees to deep neural networks
(DNNs) and support vector machines (SVMs). This is called <i>stochastic</i> <i>gradient</i>
<i>descent</i> <i>(SGD),</i> and extensions of SGD (such as Adam and Adagrad) are the de facto
optimizers used in modern-day machine learning frameworks.
Because SGD requires training to take place iteratively on small batches of the train‐
ing dataset, training a machine learning model happens in a loop. SGD finds a mini‐
mum, but is not a closed-form solution, and so we have to detect whether the model
convergence has happened. Because of this, the error (called the <i>loss)</i> on the training
dataset has to be monitored. Overfitting can happen if the model complexity is higher
than can be afforded by the size and coverage of the dataset. Unfortunately, you"|decision trees; DNN model; SGD; SVM; training loop
"scikit-learn supports an alternative to grid search called
RandomizedSearchCV
that implements <i>random</i> <i>search.</i> Instead of
trying every possible combination of hyperparameters from a set,
you determine the number of times you’d like to randomly sample
values for each hyperparameter. To implement random search in
scikit-learn, we’d create an instance of RandomizedSearchCV and
pass it a dict similar to grid_values above, specifying <i>ranges</i>
instead of specific values. Random search runs faster than grid
search since it doesn’t try every combination in your set of possible
values, but it is very likely that the optimal set of hyperparameters
will not be among the ones randomly selected.
For robust hyperparameter tuning, we need a solution that scales and learns from
previous trials to find an optimal combination of hyperparameter values.
<header><largefont><b>Solution</b></largefont></header>
keras-tuner
The library implements Bayesian optimization to do hyperparameter
search directly in Keras. To use keras-tuner , we define our model inside a function
hp hp
that takes a hyperparameter argument, here called . We can then use through‐
out the function wherever we want to include a hyperparameter, specifying the
hyperparameter’s name, data type, the value range we’d like to search, and how much
to increment it each time we try a new one.
Instead of hardcoding the hyperparameter value when we define a layer in our Keras
model, we define it using a hyperparameter variable. Here, we want to tune the num‐
ber of neurons in the first hidden layer of our neural network:
keras.layers.Dense(hp.Int('first_hidden', 32, 256, step=32), activation='relu')
first_hidden
is the name we’ve given this hyperparameter, 32 is the minimum value
we’ve defined for it, 256 is the maximum, and 32 is the amount we should increment
this value by within the range we’ve defined. If we were building an MNIST classifi‐
keras-tuner
cation model, the full function that we’d pass to might look like the
following:
<b>def</b> build_model(hp):
model = keras.Sequential([
keras.layers.Flatten(input_shape=(28, 28)),
keras.layers.Dense(
hp.Int('first_hidden', 32, 256, step=32), activation='relu'),
keras.layers.Dense(
hp.Int('second_hidden', 32, 256, step=32), activation='relu'),
keras.layers.Dense(10, activation='softmax')
])
model.compile(
optimizer=tf.keras.optimizers.Adam("|grid search; Hyperparameter Tuning design pattern; Keras; random search; RandomizedSearchCV; scikit-learn
"training–serving skew, but not feature reusability. There are also some alternative
uses of a feature store that we have not yet detailed, such as how a feature store han‐
dles data from different sources and data arriving at different cadences.
<b>Alternativeimplementations</b>
Many large technology companies, like Uber, LinkedIn, Airbnb, Netflix, and Com‐
cast, host their own version of a feature store, though the architectures and tools vary.
Uber’s Michelangelo Palette is built around Spark/Scala using Hive for offline feature
creation and Cassandra for online features. Hopsworks provides another open source
feature store alternative to Feast and is built around dataframes using Spark and pan‐
das with Hive for offline and MySQL Cluster for online feature access. Airbnb built
their own feature store as part of their production ML framework called Zipline. It
uses Spark and Flink for feature engineering jobs and Hive for feature storage.
Whichever tech stack is used, the primary components of the feature store are the
same:
• A tool to process large feature engineering jobs quickly, such as Spark, Flink or
Beam.
• A storage component for housing the feature sets that are created, such as Hive,
cloud storage (Amazon S3, Google Cloud Storage), BigQuery, Redis, BigTable,
and/or Cassandra. The combination that Feast uses (BigQuery and Redis) is opti‐
mized for offline versus online (low-latency) feature retrieval.
• A metadata layer to record feature version information, documentation, and fea‐
ture registry to simplify discovery and sharing of feature sets.
• An API for ingesting and retrieving features to/from the feature store.
<b>Transformdesignpattern</b>
If feature engineering code is not the same during training and inference, there is a
risk that the two code sources will not be consistent. This leads to training–serving
skew, and model predictions may not be reliable since the features may not be the
same. Feature stores get around this problem by having their feature engineering jobs
write feature data to both an online and an offline database. And, while a feature
store itself doesn’t perform the feature transformations, it provides a way to separate
the upstream feature engineering steps from model serving and provide point in time
correctness.
The Transform design pattern discussed in this chapter also provides a way to keep
feature transformations separate and reproducible. For example, tf.transform can
be used to preprocess data using exactly the same code for both training a model and
serving predictions in production, thus eliminating training–serving skew. This
ensures that training and serving feature engineering pipelines are consistent."|Apache Beam; Apache Flink; Apache Spark; BigQuery; BigTable; Cassandra; Feast; Feature Store design pattern; Hive; Hopsworks; MySQL Cluster; Redis; training-serving skew; Transform design pattern
"When computing the similarity of plurality categories as one-hot encoded vectors, we
obtain the identity matrix since each category is treated as a distinct feature (see
Table 2-6).
<i>Table</i> <i>2-6.</i> <i>When</i> <i>features</i> <i>are</i> <i>one-hot</i> <i>encoded,</i> <i>the</i> <i>similarity</i> <i>matrix</i> <i>is</i> <i>just</i> <i>the</i>
<i>identity</i> <i>matrix</i>
<b>Single(1)</b> <b>Multiple(2+)</b> <b>Twins(2)</b> <b>Triplets(3)</b> <b>Quadruplets(4)</b> <b>Quintuplets(5)</b>
<b>Single(1)</b> 1 0 0 0 0 0
<b>Multiple(2+)</b> - 1 0 0 0 0
<b>Twins(2)</b> - - 1 0 0 0
<b>Triplets(3)</b> - - - 1 0 0
<b>Quadruplets(4)</b> - - - - 1 0
<b>Quintuplets(5)</b> - - - - - 1
However, once the plurality is embedded into two dimensions, the similarity measure
becomes nontrivial, and important relationships between the different categories
emerge (see Table 2-7).
<i>Table</i> <i>2-7.</i> <i>When</i> <i>the</i> <i>features</i> <i>are</i> <i>embedded</i> <i>in</i> <i>two</i> <i>dimensions,</i> <i>the</i> <i>similarity</i> <i>matrix</i> <i>gives</i>
<i>us</i> <i>more</i> <i>information</i>
<b>Single(1)</b> <b>Multiple(2+)</b> <b>Twins(2)</b> <b>Triplets(3)</b> <b>Quadruplets(4)</b> <b>Quintuplets(5)</b>
<b>Single(1)</b> 1 0.92 0.61 0.57 0.06 0.1
<b>Multiple(2+)</b> - 1 0.86 0.83 0.43 0.48
<b>Twins(2)</b> - 1 0.99 0.82 0.85
<b>Triplets(3)</b> - 1 0.85 0.88
<b>Quadruplets(4)</b> - 1 0.99
<b>Quintuplets(5)</b> - - - - - 1
Thus, a learned embedding allows us to extract inherent similarities between two sep‐
arate categories and, given there is a numeric vector representation, we can precisely
quantify the similarity between two categorical features.
This is easy to visualize with the natality dataset, but the same principle applies when
customer_ids
dealing with embedded into 20-dimensional space. When applied to
our customer dataset, embeddings allow us to retrieve similar customers to a given
customer_id and make suggestions based on similarity, such as which videos they are
likely to watch, as shown in Figure 2-10. Furthermore, these user and item embed‐
dings can be combined with other features when training a separate machine learning
model. Using pre-trained embeddings in machine learning models is referred to as
<i>transfer</i> <i>learning.</i>"|Embedding design pattern; embeddings
"<i>Table</i> <i>7-3.</i> <i>Descriptions</i> <i>of</i> <i>different</i> <i>types</i> <i>of</i> <i>data</i> <i>bias</i>
<b>Definition</b> <b>Considerationsforanalysis</b>
Datadistribution Datathatdoesn’tcontainan
• Doesthedatacontainabalancedsetofexamplesacrossallrelevant
bias equalrepresentationofall
demographicslices(gender,age,race,religion,etc.)?
possiblegroupsthatwilluse
• Doeseachlabelinthedatacontainabalancedsplitofallpossible
themodelinproduction
variationsofthislabel?(E.g.,theshoeexampleintheProblem
section.)
Data Datathatiswellbalanced,
• Forclassificationmodels,arelabelsbalancedacrossrelevantfeatures?
representation butdoesn’trepresent
Forexample,inadatasetintendedforcreditworthinessprediction,
bias differentslicesofdata
doesthedatacontainanequalrepresentationacrossgender,race,and
equally otheridentitycharacteristicsofpeoplemarkedasunlikelytopayback
aloan?
• Istherebiasinthewaydifferentdemographicgroupsarerepresented
inthedata?Thisisespeciallyrelevantformodelspredictingsentiment
oraratingvalue.
• Istheresubjectivebiasintroducedbydatalabelers?
Once we’ve examined our data and corrected for bias, we should take these same
considerations into account when splitting our data into training, test, and validation
sets. That is to say, once our full dataset is balanced, it’s essential that our train, test,
and validation splits maintain the same balance. Returning to our shoe image exam‐
ple, let’s imagine we’ve improved our dataset to include varied images of 10 types of
shoes. The training set should contain a similar percentage of each type of shoe as the
test and validation sets. This will ensure that our model reflects and is being evaluated
on real-world scenarios.
To see what this dataset analysis looks like in practice, we’ll use the What-If Tool on
the mortgage dataset introduced above. This will let us visualize the current balance
of our data across various slices. The What-If Tool works both with and without a
model. Since we haven’t built our model yet, we can initialize the What-If Tool
widget by passing it only our data:
config_builder = WitConfigBuilder(test_examples, column_names)
WitWidget(config_builder)
In Figure 7-10, we can see what the tool looks like when it loads when passed 1,000
examples from our dataset. The first tab is called the “Datapoint editor,” which pro‐
vides an overview of our data and lets us inspect individual examples. In this visuali‐
zation, our data points are colored by the label—whether or not a mortgage
application was approved. An individual example is also highlighted, and we can see
the feature values associated with it."|Fairness Lens design pattern; What-If Tool
"<header><largefont><b>How</b></largefont> <largefont><b>Bag</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Words</b></largefont> <largefont><b>Works</b></largefont></header>
The first step in BOW encoding is choosing our vocabulary size, which will include
the top <i>N</i> most frequently occurring words in our text corpus. In theory, our vocabu‐
lary size could be equal to the number of unique words in our entire dataset. How‐
ever, this would lead to very large input arrays of mostly zeros, since many words
could be unique to a single question. Instead, we’ll want to choose a vocabulary size
small enough to include key, recurring words that convey meaning for our prediction
task, but big enough that our vocabulary isn’t limited to words found in nearly every
question (like “the,” “is,” “and,” etc.).
Each input to our model will then be an array the size of our vocabulary. This BOW
representation therefore entirely disregards words that aren’t included in our
vocabulary. There isn’t a magic number or percentage for choosing vocabulary size—
it’s helpful to try a few and see which performs best on our model.
To understand BOW encoding, let’s first look at a simplified example. For this exam‐
ple, let’s say we’re predicting the tag of a Stack Overflow question from a list of three
possible tags: “pandas,” “keras,” and “matplotlib.” To keep things simple, assume our
vocabulary consists of only the 10 words listed below:
dataframe
layer
series
graph
column
plot
color
axes
read_csv
activation
This list is our <i>word</i> <i>index,</i> and every input we feed into our model will be a 10-
element array where each index corresponds with one of the words listed above. For
example, a 1 in the first index of an input array means a particular question contains
the word <i>dataframe.</i> To understand BOW encoding from the perspective of our
model, imagine we’re learning a new language and the 10 words above are the only
words we know. Every “prediction” we make will be based solely on the presence or
absence of these 10 words and will disregard any words outside this list.
Therefore, given question title, “How to plot dataframe bar graph,” how will we
transform it into a BOW representation? First, let’s take note of the words in this sen‐
tence that appear in our vocabulary: <i>plot,</i> <i>dataframe,</i> and <i>graph.</i> The other words in
this sentence will be ignored by the bag of words approach. Using our word index
above, this sentence becomes:
[ 1 0 0 1 0 1 0 0 0 0 ]"|BOW encoding; Multimodal Input design pattern; Stack Overflow; vocabulary; word index
"<header><largefont><b>Heuristic</b></largefont> <largefont><b>Benchmarks</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>Baselines</b></largefont></header>
How do model baselines relate to the Heuristic Benchmark design pattern? A heuris‐
tic benchmark is meant to be a starting point for summarizing a model at a global
level, often before implementing explainability. When using explainability, the type
of baseline we choose (informative or uninformative) and the way we calculate it is
up to us. The techniques outlined in the Heuristic Benchmark pattern could also be
used to determine a model’s baseline for use with an explainability method.
Both heuristic benchmarks and model baselines provide a framework for answering
the question, “Why did the model do X as compared to Y?” Heuristic benchmarks are
a first step in model analysis, and represent one possible approach for calculating a
baseline. When we use the term <i>baseline</i> in this section, we’re referring specifically to
the value used as a point of reference in explainability methods.
<b>SHAP</b>
The open source library SHAP provides a Python API for getting feature attributions
on many types of models, and is based on the concept of Shapley Value introduced in
Table 7-2. To determine feature attribution values, SHAP calculates how much
adding or removing each feature contributes to a model’s prediction output. It per‐
forms this analysis across many different combinations of feature values and model
output.
SHAP is framework-agnostic and works with models trained on image, text, or tabu‐
lar data. To see how SHAP works in practice, we’ll use the fuel efficiency dataset ref‐
erenced previously. This time, we’ll build a deep model with the Keras Sequential
API:
model = tf.keras.Sequential([
tf.keras.layers.Dense(16, input_shape=(len(x_train.iloc[0])),
tf.keras.layers.Dense(16, activation='relu'),
tf.keras.layers.Dense(1)
])
DeepExplainer
To use SHAP, we’ll first create a object by passing it our model and a
subset of examples from our training set. Then we’ll get the attribution values for the
first 10 examples in our test set:
<b>import</b> <b>shap</b>
explainer = shap.DeepExplainer(model, x_train[:100])
attribution_values = explainer.shap_values(x_test.values[:10])"|baseline; Explainable Predictions design pattern; feature attributions; heuristic benchmark; informative baseline; SHAP; Shapley Value
"<b>Labelbias</b>
Recommendation systems like matrix factorization can be reframed in the context of
neural networks, both as a regression or classification. One advantage to this change
of context is that a neural network framed as a regression or classification model can
incorporate many more additional features outside of just the user and item embed‐
dings learned in matrix factorization. So it can be an appealing alternative.
However, it is important to consider the nature of the target label when reframing the
problem. For example, suppose we reframed our recommendation model to a classi‐
fication task that predicts the likelihood a user will click on a certain video thumb‐
nail. This seems like a reasonable reframing since our goal is to provide content a
user will select and watch. But be careful. This change of label is not actually in line
with our prediction task. By optimizing for user clicks, our model will inadvertently
promote click bait and not actually recommend content of use to the user.
Instead, a more advantageous label would be video watch time, reframing our recom‐
mendation as a regression instead. Or perhaps we can modify the classification objec‐
tive to predict the likelihood that a user will watch at least half the video clip. There is
often more than one suitable approach, and it is important to consider the problem
holistically when framing a solution.
Be careful when changing the label and training task of your
machine learning model, as it can inadvertently introduce label
bias into your solution. Consider again the example of video rec‐
ommendation we discussed in “Why It Works” on page 82.
<b>Multitasklearning</b>
One alternative to reframing is multitask learning. Instead of trying to choose
between regression or classification, do both! Generally speaking, multitask learning
refers to any machine learning model in which more than one loss function is opti‐
mized. This can be accomplished in many different ways, but the two most common
forms of multi task learning in neural networks is through hard parameter sharing
and soft parameter sharing.
Parameter sharing refers to the parameters of the neural network being shared
between the different output tasks, such as regression and classification. Hard param‐
eter sharing occurs when the hidden layers of the model are shared between all the
output tasks. In soft parameter sharing, each label has its own neural network with its
own parameters, and the parameters of the different models are encouraged to be
similar through some form of regularization. Figure 3-6 shows the typical architec‐
ture for hard parameter sharing and soft parameter sharing."|label bias; parameter sharing; recommendation systems; Reframing design pattern
"hp.Choice()
For more complex models, this parameter could be used to experiment
with different types of layers, like BasicLSTMCell and BasicRNNCell . keras-tuner
runs in any environment where you can train a Keras model.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Although grid and random search are more efficient than a trial-and-error approach
to hyperparameter tuning, they quickly become expensive for models requiring sig‐
nificant training time or having a large hyperparameter search space.
Since both machine learning models themselves and the process of hyperparameter
search are optimization problems, it would follow that we would be able to use an
approach that <i>learns</i> to find the optimal hyperparameter combination within a given
range of possible values just like our models learn from training data.
We can think of hyperparameter tuning as an outer optimization loop (see
Figure 4-24) where the inner loop consists of typical model training. Even though we
depict neural networks as the model whose parameters are being optimized, this sol‐
ution is applicable to other types of machine learning models. Also, although the
more common use case is to choose a single best model from all potential hyperpara‐
meters, in some cases, the hyperparameter framework can be used to generate a fam‐
ily of models that can act as an ensemble (see the discussion of the Ensembles pattern
in Chapter 3).
<i>Figure</i> <i>4-24.</i> <i>Hyperparameter</i> <i>tuning</i> <i>can</i> <i>be</i> <i>thought</i> <i>of</i> <i>as</i> <i>an</i> <i>outer</i> <i>optimization</i> <i>loop.</i>"|grid search; Hyperparameter Tuning design pattern; random search
"<i>Figure</i> <i>2-23.</i> <i>Using</i> <i>overlapping</i> <i>windows</i> <i>for</i> <i>max</i> <i>pooling</i> <i>on</i> <i>a</i> <i>4×4</i> <i>pixel</i> <i>grid.</i>
We could then transform this into a 2×2 grid (Figure 2-24).
<i>Figure</i> <i>2-24.</i> <i>Transforming</i> <i>the</i> <i>3×3</i> <i>grid</i> <i>into</i> <i>2×2</i> <i>with</i> <i>sliding</i> <i>windows</i> <i>and</i> <i>max</i> <i>pool‐</i>
<i>ing.</i>
We end with a final scalar value of 127. While the end value is the same, you can see
how the intermediate steps preserved more detail from the original matrix.
Keras provides convolution layers to build models that split images into smaller, win‐
dowed chunks. Let’s say we’re building a model to classify 28×28 color images as
either “dog” or “cat.” Since these images are color, each image will be represented as a
28×28×3-dimensional array, since each pixel has three color channels. Here’s how
we’d define the inputs to this model using a convolution layer and the Sequential
API:
Conv2D(filters=16, kernel_size=3, activation='relu', input_shape=(28,28,3))
In this example, we’re dividing our input images into 3×3 chunks before passing
them through a max pooling layer. Building a model architecture that splits images"|Keras; Multimodal Input design pattern; Sequential API
"include_top=True
Notice that we’ve set , which means we’re loading the full VGG
model, including the output layer. For ImageNet, the model classifies images into
1,000 different classes, so the output layer is a 1,000-element array. Let’s look at the
model.summary()
output of to understand which layer will be used as the bottleneck.
For brevity, we’ve left out some of the middle layers here:
Model: ""vgg19""
_________________________________________________________________
Layer (type) Output Shape Param <i>#</i>
=================================================================
input_3 (InputLayer) [(None, 224, 224, 3)] 0
_________________________________________________________________
block1_conv1 (Conv2D) (None, 224, 224, 64) 1792
...more layers here...
_________________________________________________________________
block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808
_________________________________________________________________
block5_conv4 (Conv2D) (None, 14, 14, 512) 2359808
_________________________________________________________________
block5_pool (MaxPooling2D) (None, 7, 7, 512) 0
_________________________________________________________________
flatten (Flatten) (None, 25088) 0
_________________________________________________________________
fc1 (Dense) (None, 4096) 102764544
_________________________________________________________________
fc2 (Dense) (None, 4096) 16781312
_________________________________________________________________
predictions (Dense) (None, 1000) 4097000
=================================================================
Total params: 143,667,240
Trainable params: 143,667,240
Non-trainable params: 0
_________________________________________________________________
As you can see, the VGG model accepts images as a 224×224×3-pixel array. This 128-
element array is then passed through successive layers (each of which may change the
dimensionality of the array) until it is flattened into a 25,088×1-dimensional array in
flatten
the layer called . Finally, it is fed into the output layer, which returns a 1,000-
element array (for each class in ImageNet). In this example, we’ll choose the
block5_pool layer as the bottleneck layer when we adapt this model to be trained on
our medical histology images. The bottleneck layer produces a 7×7×512-dimensional
array, which is a low-dimensional representation of the input image. It has retained
enough of the information from the input image to be able to classify it. When we
apply this model to our medical image classification task, we hope that the informa‐
tion distillation will be sufficient to successfully carry out classification on our
dataset."|bottleneck layer; ImageNet; Transfer Learning design pattern; VGG
"Additionally, in scikit-learn, many utility functions for shuffling your data also allow
you to set a random seed value:
<b>from</b> <b>sklearn.utils</b> <b>import</b> shuffle
data = shuffle(data, random_state=value)
Keep in mind that you’ll need to use the same data <i>and</i> the same random seed when
training your model to ensure repeatable, reproducible results across different
experiments.
Training an ML model involves several artifacts that need to be fixed in order to
ensure reproducibility: the data used, the splitting mechanism used to generate data‐
sets for training and validation, data preparation and model hyperparameters, and
variables like the batch size and learning rate schedule.
Reproducibility also applies to machine learning framework dependencies. In addi‐
tion to manually setting a random seed, frameworks also implement elements of ran‐
domness internally that are executed when you call a function to train your model. If
this underlying implementation changes between different framework versions,
repeatability is not guaranteed. As a concrete example, if one version of a frame‐
work’s train() method makes 13 calls to rand() , and a newer version of the same
framework makes 14 calls, using different versions between experiments will cause
slightly different results, even with the same data and model code. Running ML
workloads in containers and standardizing library versions can help ensure repeata‐
bility. Chapter 6 introduces a series of patterns for making ML processes
reproducible.
Finally, reproducibility can refer to a model’s training environment. Often, due to
large datasets and complexity, many models take a significant amount of time to
train. This can be accelerated by employing distribution strategies like data or model
parallelism (see Chapter 5). With this acceleration, however, comes an added chal‐
lenge of repeatability when you rerun code that makes use of distributed training.
<header><largefont><b>Data</b></largefont> <largefont><b>Drift</b></largefont></header>
While machine learning models typically represent a static relationship between
inputs and outputs, data can change significantly over time. Data drift refers to the
challenge of ensuring your machine learning models stay relevant, and that model
predictions are an accurate reflection of the environment in which they’re being used.
For example, let’s say you’re training a model to classify news article headlines into
categories like “politics,” “business,” and “technology.” If you train and evaluate your
model on historical news articles from the 20th century, it likely won’t perform as
well on current data. Today, we know that an article with the word “smartphone” in
the headline is probably about technology. However, a model trained on historical
data would have no knowledge of this word. To solve for drift, it’s important to"|data drift; machine learning framework; reproducibility; scikit-learn
"the MD5 hash is not deterministic and not unique—it is a one-way hash and will
have many unexpected collisions.
In the Hashed Feature design pattern, we have to use a fingerprint hashing algorithm
and not a cryptographic hashing algorithm. This is because the goal of a fingerprint
function is to produce a deterministic and unique value. If you think about it, this is a
key requirement of preprocessing functions in machine learning, since we need to
apply the same function during model serving and get the same hashed value. A fin‐
gerprint function does not produce a uniformly distributed output. Cryptographic
algorithms such as MD5 or SHA1 do produce uniformly distributed output, but they
are not deterministic and are purposefully made to be computationally expensive.
Therefore, a cryptographic hash is not usable in a feature engineering context where
the hashed value computed for a given input during prediction has to be the same as
the hash computed during training, and where the hash function should not slow
down the machine learning model.
The reason that MD5 is not deterministic is that a “salt” is typically
added to the string to be hashed. The salt is a random string added
to each password to ensure that even if two users happen to use the
same password, the hashed value in the database will be different.
This is needed to thwart attacks based on “rainbow tables,” which
are attacks that rely on dictionaries of commonly chosen pass‐
words and that compare the hash of the known password against
hashes in the database. As computational power has increased, it is
possible to carry out a brute-force attack on every possible salt as
well, and so modern cryptographic implementations do their hash‐
ing in a loop to increase the computational expense. Even if we
were to turn off the salt and reduce the number of iterations to one,
the MD5 hash is only one way. It won’t be unique.
The bottom line is that we need to use a fingerprint hashing algorithm, and we need
to modulo the resulting hash.
<b>Orderofoperations</b>
Note that we do the modulo first, and then the absolute value:
<b>CREATE</b> <b>TEMPORARY</b> <b>FUNCTION</b> hashed(airport STRING, numbuckets INT64) <b>AS</b> (
<b>ABS(MOD(FARM_FINGERPRINT(airport),</b> numbuckets))
);
The order of ABS, MOD, and FARM_FINGERPRINT in the preceding snippet is important
INT64
because the range of is not symmetric. Specifically, its range is between
–9,223,372,036,854,775,808 and 9,223,372,036,854,775,807 (both inclusive).
So, if we were to do:"|cryptographic algorithms; fingerprint hashing algorithm; Hashed Feature design pattern; salt
"<b>Arrayofcategoricalvariables</b>
Sometimes, the input data is an array of categories. If the array is of fixed length, we
can treat each array position as a separate feature. But often, the array will be of vari‐
able length. For example, one of the inputs to the natality model might be the type of
previous births to this mother:
[Induced, Induced, Natural, Cesarean]
Obviously, the length of this array will vary in each row because there are different
numbers of older siblings for each baby.
Common idioms to handle arrays of categorical variables include the following:
• <i>Counting</i> the number of occurrences of each vocabulary item. So, the representa‐
[2, 1, 1]
tion for our example would be assuming that the vocabulary is
Induced, Natural , and Cesarean (in that order). This is now a fixed-length
array of numbers that can be flattened and used in positional order. If we have an
array where an item can occur only once (for example, of languages a person
speaks), or if the feature just indicates presence and not count (such as whether
the mother has ever had a Cesarean operation), then the count at each position is
0 or 1, and this is called <i>multi-hot</i> <i>encoding.</i>
• To avoid large numbers, the <i>relative</i> <i>frequency</i> can be used instead of the count.
The representation for our example would be [0.5, 0.25, 0.25] instead of [2,
1, 1] . Empty arrays (first-born babies with no previous siblings) are represented
[0, 0, 0].
as In natural language processing, the relative frequency of a word
overall is normalized by the relative frequency of documents that contain the
word to yield TF-IDF (short for term frequency–inverse document frequency).
TF-IDF reflects how unique a word is to a document.
• If the array is ordered in a specific way (e.g., in order of time), representing the
input array by the last three items. Arrays shorter than three are padded with
missing values.
• Representing the array by bulk statistics, e.g., the length of the array, the mode
(most common entry), the median, the 10th/20th/… percentile, etc.
Of these, the counting/relative-frequency idiom is the most common. Note that both
of these are a generalization of one-hot encoding—if the baby had no older siblings,
the representation would be [0, 0, 0] , and if the baby had one older sibling who
[0, 1, 0]
was born in a natural birth, the representation would be .
Having seen simple data representations, let’s discuss design patterns that help with
data representation."|categorical inputs; idioms; multi-hot encoding; relative frequency; vocabulary
"<i>data</i> <i>parallelism</i> and <i>model</i> <i>parallelism.</i> In data parallelism, computation is split
across different machines and different workers train on different subsets of the
training data. In model parallelism, the model is split and different workers carry out
the computation for different parts of the model. In this section, we’ll focus on data
tf.distrib
parallelism and show implementations in TensorFlow using the
ute.Strategy library. We’ll discuss model parallelism in “Trade-Offs and Alterna‐
tives” on page 183.
To implement data parallelism, there must be a method in place for different workers
to compute gradients and share that information to make updates to the model
parameters. This ensures that all workers are consistent and each gradient step works
to train the model. Broadly speaking, data parallelism can be carried out either syn‐
chronously or asynchronously.
<b>Synchronoustraining</b>
In synchronous training, the workers train on different slices of input data in parallel
and the gradient values are aggregated at the end of each training step. This is per‐
formed via an <i>all-reduce</i> algorithm. This means that each worker, typically a GPU,
has a copy of the model on device and, for a single stochastic gradient descent (SGD)
step, a mini-batch of data is split among each of the separate workers. Each device
performs a forward pass with their portion of the mini-batch and computes gradients
for each parameter of the model. These locally computed gradients are then collected
from each device and aggregated (for example, averaged) to produce a single gradient
update for each parameter. A central server holds the most current copy of the model
parameters and performs the gradient step according to the gradients received from
the multiple workers. Once the model parameters are updated according to this
aggregated gradient step, the new model is sent back to the workers along with
another split of the next mini-batch, and the process repeats. Figure 4-15 shows a
typical all-reduce architecture for synchronous data distribution.
As with any parallelism strategy, this introduces additional overhead to manage tim‐
ing and communication between workers. Large models could cause I/O bottlenecks
as data is passed from the CPU to the GPU during training, and slow networks could
also cause delays.
In TensorFlow, tf.distribute.MirroredStrategy supports synchronous dis‐
tributed training across multiple GPUs on the same machine. Each model parameter
is mirrored across all workers and stored as a single conceptual variable called
MirroredVariable
. During the all-reduce step, all gradient tensors are made available
on each device. This helps to significantly reduce the overhead of synchronization.
There are also various other implementations for the all-reduce algorithm available,
many of which use NVIDIA NCCL."|all-reduce algorithm; data parallelism; Distribution Strategy design pattern; GPU; Mirrored Variable; SGD; synchronous training; TensorFlow
"sampled in the training phase. This can be problematic for using ML to solve PDEs
that are defined on unbounded domains, since it would be impossible to capture a
representative sample for training.
<b>Distillingknowledgeofneuralnetwork</b>
Another situation where overfitting is warranted is in distilling, or transferring
knowledge, from a large machine learning model into a smaller one. Knowledge dis‐
tillation is useful when the learning capacity of the large model is not fully utilized. If
that is the case, the computational complexity of the large model may not be neces‐
sary. However, it is also the case that training smaller models is harder. While the
smaller model has enough capacity to represent the knowledge, it may not have
enough capacity to learn the knowledge efficiently.
The solution is to train the smaller model on a large amount of generated data that is
labeled by the larger model. The smaller model learns the soft output of the larger
model, instead of actual labels on real data. This is a simpler problem that can be
learned by the smaller model. As with approximating a numerical function by a
machine learning model, the aim is for the smaller model to faithfully represent the
predictions of the larger machine learning model. This second training step can
employ Useful Overfitting.
<b>Overfittingabatch</b>
In practice, training neural networks requires a lot of experimentation, and a practi‐
tioner must make many choices, from the size and architecture of the network to the
choice of the learning rate, weight initializations, or other hyperparameters.
Overfitting on a small batch is a good sanity check both for the model code as well as
the data input pipeline. Just because the model compiles and the code runs without
errors doesn’t mean you’ve computed what you think you have or that the training
objective is configured correctly. A complex enough model <i>should</i> be able to overfit
on a small enough batch of data, assuming everything is set up correctly. So, if you’re
not able to overfit a small batch with any model, it’s worth rechecking your model
code, input pipeline, and loss function for any errors or simple bugs. Overfitting on a
batch is a useful technique when training and troubleshooting neural networks."|Deep Galerkin Method; overfitting; Useful Overfitting design pattern
"This allows the client to match inputs to outputs, but is more expensive in terms of
bandwidth and client-side computation.
Because high-performance servers will support multiple clients, be backed by a clus‐
ter, and batch up requests to gain performance benefits, it’s better to plan ahead for
this—ask that clients supply keys with every prediction and for clients to specify keys
that will not cause a collision with other clients.
<header><largefont><b>Summary</b></largefont></header>
In this chapter, we looked at techniques for operationalizing machine learning mod‐
els to ensure they are resilient and can scale to handle production load. Each resil‐
ience pattern we discussed relates to the deployment and serving steps in a typical
ML workflow.
We started this chapter by looking at how to encapsulate your trained machine learn‐
ing model as a stateless function using the <i>Stateless</i> <i>Serving</i> <i>Function</i> design pattern.
A serving function decouples your model’s training and deployment environments
by defining a function that performs inference on an exported version of your model,
and is deployed to a REST endpoint. Not all production models require immediate
prediction results, as there are situations where you need to send a large batch of data
to your model for prediction but don’t need results right away. We saw how the
<i>Batch</i> <i>Serving</i> design pattern solves this by utilizing distributed data processing infra‐
structure designed to run many model prediction requests asynchronously as a back‐
ground job, with output written to a specified location.
Next, with the <i>Continued</i> <i>Model</i> <i>Evaluation</i> design pattern, we looked at an approach
to verifying that your deployed model is still performing well on new data. This pat‐
tern addresses the problems of data and concept drift by regularly evaluating your
model and using these results to determine if retraining is necessary. In the
<i>Two-Phase</i> <i>Predictions</i> design pattern, we solved for specific use cases where models
need to be deployed at the edge. When you can break a problem into two logical
parts, this pattern first creates a simpler model that can be deployed on-device. This
edge model is connected to a more complex model hosted in the cloud. Finally, in the
<i>Keyed</i> <i>Prediction</i> design pattern, we discussed why it can be beneficial to supply a
unique key with each example when making prediction requests. This ensures that
your client associates each prediction output with the correct input example.
In the next chapter, we’ll look at <i>reproducibility</i> patterns. These patterns address chal‐
lenges associated with the inherent randomness present in many aspects of machine
learning and focus on enabling reliable, consistent results each time a machine learn‐
ing process runs."|continuous evaluation; Keyed Predictions design pattern; keys
"<i>Figure</i> <i>4-10.</i> <i>In</i> <i>the</i> <i>ideal</i> <i>situation,</i> <i>validation</i> <i>error</i> <i>does</i> <i>not</i> <i>increase.</i> <i>Instead,</i> <i>both</i> <i>the</i>
<i>training</i> <i>loss</i> <i>and</i> <i>validation</i> <i>error</i> <i>plateau.</i>
If early stopping is not carried out, and only the training loss is used to decide con‐
vergence, then we can avoid having to set aside a separate testing dataset. Even if we
are not doing early stopping, displaying the progress of the model training can be
helpful, particularly if the model takes a long time to train. Although the performance
and progress of the model training is normally monitored on the validation dataset
during the training loop, it is for visualization purposes only. Since we don’t have to
take any action based on metrics being displayed, we can carry out visualization on
the test dataset.
The reason that using regularization might be better than early stopping is that regu‐
larization allows you to use the entire dataset to change the weights of the model,
whereas early stopping requires you to waste 10% to 20% of your dataset purely to
decide when to stop training. Other methods to limit overfitting (such as dropout
and using models with lower complexity) are also good alternatives to early stopping.
In addition, recent research indicates that double descent happens in a variety of
machine learning problems, and therefore it is better to train longer rather than risk a
suboptimal solution by stopping early.
<b>Twosplits.</b>
Isn’t the advice in the regularization section in conflict with the advice in
the previous sections on early stopping or checkpoint selection? Not really.
We recommend that you split your data into two parts: a training dataset and an
evaluation dataset. The evaluation dataset plays the part of the test dataset during"|checkpoint selection; Checkpoints design pattern; regularization
"<i>Figure</i> <i>5-11.</i> <i>The</i> <i>image</i> <i>representation</i> <i>(spectrogram)</i> <i>of</i> <i>a</i> <i>saxophone</i> <i>audio</i> <i>clip</i> <i>from</i>
<i>our</i> <i>training</i> <i>dataset.</i> <i>Code</i> <i>for</i> <i>converting</i> <i>.wav</i> <i>files</i> <i>to</i> <i>spectrograms</i> <i>can</i> <i>be</i> <i>found</i> <i>in</i> <i>the</i>
<i>GitHub</i> <i>repository.</i>
<b>Phase1:Buildingtheofflinemodel</b>
The first model in our Two-Phase Predictions solution should be small enough that it
can be loaded on a mobile device for quick inference without relying on internet con‐
nectivity. Building on the instrument example introduced above, we’ll provide an
example of the first prediction phase by building a binary classification model opti‐
mized for on-device inference.
The original sound dataset has 41 labels for different types of audio clips. Our first
model will only have two labels: “instrument” or “not instrument.” We’ll build our
model using the MobileNetV2 model architecture trained on the ImageNet dataset.
MobileNetV2 is available directly in Keras and is an architecture optimized for mod‐
els that will be served on-device. For our model, we’ll freeze the MobileNetV2
weights and load it <i>without</i> the top so that we can add our own binary classification
output layer:
mobilenet = tf.keras.applications.MobileNetV2(
input_shape=((128,128,3)),
include_top=False,
weights='imagenet'
)
mobilenet.trainable = False
If we organize our spectrogram images into directories with the corresponding label
name, we can use Keras’s ImageDataGenerator class to create our training and vali‐
dation datasets:"|ImageDataGenerator; Keras; MobileNetV2; Two-Phase Predictions design pattern
"should always have control of the retraining of your model to better understand and
debug the model in the production.
<i>Figure</i> <i>5-5.</i> <i>Setting</i> <i>a</i> <i>higher</i> <i>threshold</i> <i>for</i> <i>model</i> <i>performance</i> <i>ensures</i> <i>a</i> <i>higher-quality</i>
<i>model</i> <i>in</i> <i>production</i> <i>but</i> <i>will</i> <i>require</i> <i>more</i> <i>frequent</i> <i>retraining</i> <i>jobs,</i> <i>which</i> <i>can</i> <i>be</i> <i>costly.</i>
<b>Scheduledretraining</b>
Continuous evaluation provides a crucial signal for knowing when it’s necessary to
retrain your model. This process of retraining is often carried out by fine-tuning the
previous model using any newly collected training data. Where continued evaluation
may happen every day, scheduled retraining jobs may occur only every week or every
month (Figure 5-6).
Once a new version of the model is trained, its performance is compared against the
current model version. The updated model is deployed as a replacement only if it
outperforms the previous model with respect to a test set of current data.
<i>Figure</i> <i>5-6.</i> <i>Continuous</i> <i>evaluation</i> <i>provides</i> <i>model</i> <i>evaluation</i> <i>each</i> <i>day</i> <i>as</i> <i>new</i> <i>data</i> <i>is</i>
<i>collected.</i> <i>Periodic</i> <i>retraining</i> <i>and</i> <i>model</i> <i>comparison</i> <i>provides</i> <i>evaluation</i> <i>at</i> <i>discrete</i>
<i>time</i> <i>points.</i>
So how often should you schedule retraining? The timeline for retraining will depend
on the business use case, prevalence of new data, and the cost (in time and money) of
executing the retraining pipeline. Sometimes, the time horizon of the model naturally
determines when to schedule retraining jobs. For example, if the goal of the model is
to predict next quarter’s earnings, since you will get new ground truth labels only"|Continued Model Evaluation design pattern; fine-tuning
"examples from our minority class and 100 different, randomly selected values from
our majority class. The bagging technique illustrated in Figure 3-11 would work well
for this approach.
In addition to combining these data-centric approaches, we can also adjust the thres‐
hold for our classifier to optimize for precision or recall depending on our use case. If
we care more that our model is correct whenever it makes a positive class prediction,
we’d optimize our prediction threshold for recall. This can apply in any situation
where we want to avoid false positives. Alternatively, if it is more costly to <i>miss</i> a
potential positive classification even when we might get it wrong, we optimize our
model for recall.
<b>Choosingamodelarchitecture</b>
Depending on our prediction task, there are different model architectures to consider
when solving problems with the Rebalancing design pattern. If we’re working with
tabular data and building a classification model for anomaly detection, research has
shown that decision tree models perform well on these types of tasks. Tree-based
models also work well on problems involving small and imbalanced datasets.
XGBoost, scikit-learn, and TensorFlow all have methods for implementing decision
tree models.
We can implement a binary classifier in XGBoost with the following code:
<i>#</i> <i>Build</i> <i>the</i> <i>model</i>
model = xgb.XGBClassifier(
objective='binary:logistic'
)
<i>#</i> <i>Train</i> <i>the</i> <i>model</i>
model.fit(
train_data,
train_labels
)
We can use downsampling and class weights in each of these frameworks to further
optimize our model using the Rebalancing design pattern. For example, to add
XGBClassifier scale_pos_weight
weighted classes to our above, we’d add a param‐
eter, calculated based on the balance of classes in our dataset.
If we’re detecting anomalies in time-series data, long short-term memory (LSTM)
models work well for identifying patterns present in sequences. Clustering models are
also an option for tabular data with imbalanced classes. For imbalanced datasets with
image input, use deep learning architectures with downsampling, weighted classes,
upsampling, or a combination of these techniques. For text data, however, generating
synthetic data is less straightforward, and it’s best to rely on downsampling and
weighted classes."|anomaly detection; decision trees; downsampling; LSTM; Rebalancing design pattern; scikit-learn; TensorFlow; XGBoost
"Explainable AI also works in AutoML Tables, a tool for training
and deploying tabular data models. AutoML Tables handles data
preprocessing and selects the best model for our data, which means
we don’t need to write any model code. Feature attributions
through Explainable AI are enabled by default for models trained
in AutoML Tables, and both global and instance-level explanations
are provided.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
While explanations provide important insight into how a model is making decisions,
they are only as good as the model’s training data, the quality of your model, and the
chosen baseline. In this section, we’ll discuss some limitations of explainability, along
with some alternatives to feature attributions.
<b>Dataselectionbias</b>
It’s often said that machine learning is “garbage in, garbage out.” In other words, a
model is only as good as the data used to train it. If we train an image model to iden‐
tify 10 different cat breeds, those 10 cat breeds are all it knows. If we show the model
an image of a dog, all it can do is try to classify the dog into 1 of the 10 cat categories
it’s been trained on. It might even do so with high confidence. That is to say, models
are a direct representation of their training data.
If we don’t catch data imbalances before training a model, explainability methods like
feature attributions can help bring data selection bias to light. As an example, say
we’re building a model to predict the type of boat present in an image. Let’s say it
correctly labels an image from our test set as “kayak,” but using feature attributions,
we find that the model is relying on the boat’s paddle to predict “kayak” rather than
the shape of the boat. This is a signal that our dataset might not have enough varia‐
tion in training images for each class—we’ll likely need to go back and add more
images of kayaks at different angles, both with and without paddles.
<b>Counterfactualanalysisandexample-basedexplanations</b>
In addition to feature attributions—described in the Solution section—there are
many other approaches to explaining the output of ML models. This section is not
meant to provide an exhaustive list of all explainability techniques, as this area is
quickly evolving. Here, we will briefly describe two other approaches: counterfactual
analysis and example-based explanations.
Counterfactual analysis is an instance-level explainability technique that refers to
finding examples from our dataset with similar features that resulted in different pre‐
dictions from our model. One way to do this is through the What-If Tool, an open
source tool for evaluating and visualizing the output of ML models. We’ll provide a"|AutoML Tables; bias; counterfactual analysis; example-based explanation; Explainable AI; Explainable Predictions design pattern; feature attributions; What-If Tool
"<i>Figure</i> <i>2-15.</i> <i>The</i> <i>feature</i> <i>cross</i> <i>introduces</i> <i>four</i> <i>new</i> <i>boolean</i> <i>features.</i>
A feature cross of these bucketized features introduces four new boolean features for
our model:
AC where x_1 >= 0 and x_2 >= 0
BC where x_1 < 0 and x_2 >= 0
AD where x_1 >= 0 and x_2 < 0
BD where x_1 < 0 and x_2 < 0
Each of these four boolean features (AC, BC, AD, and BD) would get its own weight
when training the model. This means we can treat each quadrant as its own feature.
Since the original dataset was split perfectly by the buckets we created, a feature cross
of A and B is able to linearly separate the dataset.
But this is just an illustration. What about real-world data? Consider a public dataset
of yellow cab rides in New York City (see Table 2-8). 5
<i>Table</i> <i>2-8.</i> <i>A</i> <i>preview</i> <i>of</i> <i>the</i> <i>public</i> <i>New</i> <i>York</i> <i>City</i> <i>taxi</i> <i>dataset</i> <i>in</i> <i>BigQuery</i>
<b>pickup_datetime</b> <b>pickuplon</b> <b>pickuplat</b> <b>dropofflon</b> <b>dropofflat</b> <b>passengers</b> <b>fare_amount</b>
2014-05–1715:15:00UTC -73.99955 40.7606 -73.99965 40.72522 1 31
2013–12-0915:03:00UTC -73.99095 40.749772 -73.870807 40.77407 1 34.33
2013-04–1808:48:00UTC -73.973102 40.785075 -74.011462 40.708307 1 29
2009–11-0506:47:00UTC -73.980313 40.744282 -74.015285 40.711458 1 14.9
2009-05-2109:47:06UTC -73.901887 40.764021 -73.901795 40.763612 1 12.8
5 Thefeature_cross.ipynbnotebookinthebook’srepositoryofthisbookwillhelpyoufollowthediscussion
better."|BigQuery; Feature Cross design pattern
"<header><largefont><b>Solution</b></largefont></header>
To handle the problems that come with scaling machine learning processes, we can
make each step in our ML workflow a separate, containerized service. Containers
guarantee that we’ll be able to run the same code in different environments, and that
we’ll see consistent behavior between runs. These individual containerized steps
together are then chained together to make a <i>pipeline</i> that can be run with a REST
API call. Because pipeline steps run in containers, we can run them on a development
laptop, with on-premises infrastructure, or with a hosted cloud service. This pipeline
workflow allows team members to build out pipeline steps independently. Containers
also provide a reproducible way to run an entire pipeline end to end, since they guar‐
antee consistency among library dependency versions and runtime environments.
Additionally, because containerizing pipeline steps allows for a separation of con‐
cerns, individual steps can use different runtimes and language versions.
There are many tools for creating pipelines with both on-premise and cloud options
available, including Cloud AI Platform Pipelines, TensorFlow Extended (TFX),
Kubeflow Pipelines (KFP), MLflow, and Apache Airflow. To demonstrate the Work‐
flow Pipeline design pattern here, we’ll define our pipeline with TFX and run it on
Cloud AI Platform Pipelines, a hosted service for running ML pipelines on Google
Cloud using Google Kubernetes Engine (GKE) as the underlying container infra‐
structure.
Steps in TFX pipelines are known as <i>components,</i> and both pre-built and customiza‐
ble components are available. Typically, the first component in a TFX pipeline is one
that ingests data from an external source. This is referred to as an ExampleGen com‐
ponent where example refers to the machine learning terminology for a labeled
ExampleGen
instance used for training. components allow you to source data from
CSV files, TFRecords, BigQuery, or a custom source. The BigQueryExampleGen com‐
ponent, for example, lets us connect data stored in BigQuery to our pipeline by speci‐
fying a query that will fetch the data. Then it will store that data as TFRecords in a
GCS bucket so that it can be used by the next component. This is a component we
customize by passing it a query. These ExampleGen components address the data col‐
lection phase of an ML workflow outlined in Figure 6-6.
The next step of this workflow is data validation. Once we’ve ingested data, we can
pass it to other components for transformation or analysis before training a model.
The StatisticsGen component takes data ingested from an ExampleGen step and
generates summary statistics on the provided data. The SchemaGen outputs the infer‐
SchemaGen
red schema from our ingested data. Utilizing the output of , the
ExampleValidator performs anomaly detection on our dataset and checks for signs"|Apache Airflow; BigQueryExampleGen component; Cloud AI Platform Pipelines; components; containers; ExampleGen components; ExampleValidator; GKE; Kubeflow Pipelines; MLflow; pipeline; SchemaGen; StatisticsGen component; TensorFlow Extended; TFX; Transform component; Workflow Pipeline design pattern
"The benefit of one versus rest is that we can use it with model architectures that can
only do binary classification, like SVMs. It may also help with rare categories since
the model will be performing only one classification task at a time on each input, and
it is possible to apply the Rebalancing design pattern. The disadvantage of this
approach is the added complexity of training many different classifiers, requiring us
to build our application in a way that generates predictions from each of these mod‐
els rather than having just one.
To summarize, use the Multilabel design pattern when your data falls into any of the
following classification scenarios:
• A single training example can be associated with mutually exclusive labels.
• A single training example can have many hierarchical labels.
• Labelers describe the same item in different ways, and each interpretation is
accurate.
When implementing a multilabel model, ensure combinations of overlapping labels
are well represented in your dataset, and consider the threshold values you’re willing
to accept for each possible label in your model. Using a sigmoid output layer is the
most common approach for building models that can handle multilabel classifica‐
tion. Additionally, sigmoid output can also be applied to binary classification tasks
where a training example can have only one out of two possible labels.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>7:</b></largefont> <largefont><b>Ensembles</b></largefont></header>
The Ensembles design pattern refers to techniques in machine learning that combine
multiple machine learning models and aggregate their results to make predictions.
Ensembles can be an effective means to improve performance and produce predic‐
tions that are better than any single model.
<header><largefont><b>Problem</b></largefont></header>
Suppose we’ve trained our baby weight prediction model, engineering special fea‐
tures and adding additional layers to our neural network so that the error on our
training set is nearly zero. Excellent, you say! However, when we look to use our
model in production at the hospital or evaluate performance on the hold out test set,
our predictions are all wrong. What happened? And, more importantly, how can we
fix it?"|binary classification; Ensemble design pattern; labels; Multilabel design pattern; sigmoid
"flow Pipeline design pattern lets others run and monitor our entire ML workflow
from end to end in both on-premises and cloud environments, while still being able
to debug the output of individual steps. Containerizing each step of the pipeline
ensures that others will be able to reproduce both the environment we used to build it
and the entire workflow captured in the pipeline. This also allows us to potentially
reproduce the environment months later to support regulatory needs. With TFX and
AI Platform Pipelines, the dashboard also gives us a UI for tracking the output arti‐
facts produced from every pipeline execution. This is discussed further in “Trade-
Offs and Alternatives” on page 315.
Additionally, with each pipeline component in its own container, different team
members can build and test separate pieces of a pipeline in parallel. This allows for
faster development and minimizes the risks associated with a more monolithic ML
process where steps are inextricably linked to one another. The package dependencies
and code required to build out the data preprocessing step, for example, may be sig‐
nificantly different than those for model deployment. By building these steps as part
of a pipeline, each piece can be built in a separate container with its own dependen‐
cies and incorporated into a larger pipeline when completed.
To summarize, the Workflow Pipeline pattern gives us the benefits that come with a
directed acyclic graph (DAG), along with the pre-built components that come with
pipeline frameworks like TFX. Because the pipeline is a DAG, we have the option of
executing individual steps or running an entire pipeline from end to end. This also
gives us logging and monitoring for each step of the pipeline across different runs,
and allows for tracking artifacts from each step and pipeline execution in a central‐
ized place. Pre-built components provide standalone, ready-to-use steps for common
components of ML workflows, including training, evaluation, and inference. These
components run as individual containers wherever we choose to run our pipeline.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The main alternative to using a pipeline framework is to run the steps of our ML
workflow using a makeshift approach for keeping track of the notebooks and output
associated with each step. Of course, there is some overhead involved in converting
the different pieces of our ML workflow into an organized pipeline. In this section,
we’ll look at some variations and extensions of the Workflow Pipeline design pattern:
creating containers manually, automating a pipeline with tools for continuous inte‐
gration and continuous delivery (CI/CD), processes for moving from a development
to production workflow pipeline, and alternative tools for building and orchestrating
pipelines. We’ll also explore how to use pipelines for metadata tracking."|CI/CD; DAG; TFX; Workflow Pipeline design pattern
"<header><largefont><b>Model</b></largefont> <largefont><b>Parallelism</b></largefont> <largefont><b>or</b></largefont> <largefont><b>Data</b></largefont> <largefont><b>Parallelism?</b></largefont></header>
A priori, neither scheme is better than the other. Each has its own benefits. Typically,
the model architecture determines whether it is better to use data parallelism or
model parallelism.
In particular, model parallelism improves efficiency when the amount of computa‐
tion per neuron activity is high, such as in wide models with many fully connected
layers. This is because it is the neuron value that is being communicated between dif‐
ferent components of the model. Outside of the training paradigm, model parallelism
provides an added benefit for serving very large models where low latency is needed.
Distributing the computation of a large model across multiple devices can vastly
reduce the overall computation time when making online predictions.
On the other hand, data parallelism is more efficient when the amount of computa‐
tion per weight is high, such as when there are convolutional layers involved. This is
because it is the model weights (and their gradient updates) that are being passed
between different workers.
Depending on the scale of your model and problem, it may be necessary to exploit
both. Mesh TensorFlow is a library optimized for distributed deep learning that com‐
bines synchronous data parallelism with model parallelism. It is implemented as a
layer over TensorFlow and allows tensors to be easily split across different dimen‐
sions. Splitting across the batch layer is synonymous with data parallelism, while
splitting over any other dimension—for example, a dimension representing the size
of a hidden layer—achieves model parallelism.
<b>ASICsforbetterperformanceatlowercost</b>
Another way to speed up the training process is by accelerating the underlying hard‐
ware, such as by using application-specific integrated circuits (ASICs). In machine
learning, this refers to hardware components designed specifically to optimize per‐
formance on the types of large matrix computations at the heart of the training loop.
TPUs in Google Cloud are ASICs that can be used for both model training and mak‐
ing predictions. Similarly, Microsoft Azure offers the Azure FPGA (field-
programmable gate array), which is also a custom machine learning chip like the
ASIC except that it can be reconfigured over time. These chips are able to vastly min‐
imize the time to accuracy when training large, complex neural network models. A
model that takes two weeks to train on GPUs can converge in hours on TPUs.
There are other advantages to using custom machine learning chips. For example, as
accelerators (GPUs, FPGAs, TPUs, and so on) have gotten faster, I/O has become a
significant bottleneck in ML training. Many training processes waste cycles waiting to
read and move data to the accelerator and waiting for gradient updates to carry out"|ASIC; Azure; data parallelism; Distribution Strategy design pattern; field-programmable gate array (FPGA); FPGA (field-programmable gate array); GPU; Mesh TensorFlow; model parallelism; TPU
"For example, we can train a model to infer whether or not a baby will require atten‐
tion by training a logistic regression model on the natality dataset:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL
mlpatterns.neutral_3classes <b>OPTIONS(model_type='logistic_reg',</b>
input_label_cols=['health']) <b>AS</b>
<b>SELECT</b>
IF
(apgar_1min = 10,
'Healthy',
IF
(apgar_1min >= 8,
'Neutral',
'NeedsAttention')) <b>AS</b> health,
plurality,
mother_age,
gestation_weeks,
ever_born
<b>FROM</b>
`bigquery-public-data.samples.natality`
<b>WHERE</b>
apgar_1min <= 10
Once the model is trained, we can carry out prediction using SQL:
<b>SELECT</b> * <b>FROM</b> ML.PREDICT(MODEL mlpatterns.neutral_3classes,
(SELECT
2 <b>AS</b> plurality,
32 <b>AS</b> mother_age,
41 <b>AS</b> gestation_weeks,
1 <b>AS</b> ever_born
)
)
However, BigQuery is primarily for distributed data processing. While it was great
for training the ML model on gigabytes of data, using such a system to carry out
inference on a single row is not the best fit—latencies can be as high as a second or
ML.PREDICT
two. Rather, the functionality is more appropriate for batch serving.
In order to carry out online prediction, we can ask BigQuery to export the model as a
TensorFlow SavedModel:
bq extract -m --destination_format=ML_TF_SAVED_MODEL <b>\</b>
mlpatterns.neutral_3classes gs://${BUCKET}/export/baby_health
Now, we can deploy the SavedModel into a serving framework like Cloud AI Plat‐
form that supports SavedModel to get the benefits of low-latency, autoscaled ML
model serving. See the notebook in GitHub for the complete code.
Even if this ability to export the model as a SavedModel did not exist, we could have
extracted the weights, written a mathematical model to carry out the linear model,
containerized it, and deployed the container image into a serving platform."|SavedModel; Stateless Serving Function design pattern
"class_weights
In Keras, we can pass a parameter to our model when we train it with
fit() . The parameter class_weights is a dict, mapping each class to the weight
Keras should assign to examples from that class. But how should we determine the
exact weights for each class? The class weight values should relate to the balance of
each class in our dataset. For example, if the minority class accounts for only 0.1% of
the dataset, a reasonable conclusion is that our model should treat examples from
that class with 1000× more weight than the majority class. In practice, it’s common to
divide this weight value by 2 for each class so that the average weight of an example is
<i>1.0.</i> Therefore, given a dataset with 0.1% of values representing the minority class, we
could calculate the class weights with the following code:
num_minority_examples = 1
num_majority_examples = 999
total_examples = num_minority_examples + num_majority_examples
minority_class_weight = 1/(num_minority_examples/total_examples)/2
majority_class_weight = 1/(num_majority_examples/total_examples)/2
<i>#</i> <i>Pass</i> <i>the</i> <i>weights</i> <i>to</i> <i>Keras</i> <i>in</i> <i>a</i> <i>dict</i>
<i>#</i> <i>The</i> <i>key</i> <i>is</i> <i>the</i> <i>index</i> <i>of</i> <i>each</i> <i>class</i>
keras_class_weights = {0: majority_class_weight, 1: minority_class_weight}
We’d then pass these weights to our model during training:
model.fit(
train_data,
train_labels,
class_weight=keras_class_weights
)
In BigQuery ML, we can set AUTO_CLASS_WEIGHTS = True in the OPTIONS block when
creating our model to have different classes weighted based on their frequency of
occurrence in the training data.
While it can be helpful to follow a heuristic of class balance for setting class weights,
the business application of a model might also dictate the class weights we choose to
assign. For example, let’s say we have a model classifying images of defective prod‐
ucts. If the cost of shipping a defective product is 10 times that of incorrectly classify‐
ing a normal product, we would choose 10 as the weight for our minority class."|BigQuery ML; downsampling; Keras; Rebalancing design pattern
"texts_to_sequences
We can then invoke this mapping with the method of our
tokenizer. This maps each sequence of words in the text input being represented
(here, we assume that they are titles of articles) to a sequence of tokens corresponding
to each word as in Figure 2-7:
integerized_titles = tokenizer.texts_to_sequences(titles_df.title)
<i>Figure</i> <i>2-7.</i> <i>Using</i> <i>the</i> <i>tokenizer,</i> <i>each</i> <i>title</i> <i>is</i> <i>mapped</i> <i>to</i> <i>a</i> <i>sequence</i> <i>of</i> <i>integer</i>
<i>index</i> <i>values.</i>
The tokenizer contains other relevant information that we will use later for creating
an embedding layer. In particular, VOCAB_SIZE captures the number of elements of
MAX_LEN
the index lookup table and contains the maximum length of the text strings
in the dataset:
VOCAB_SIZE = len(tokenizer.index_word)
MAX_LEN = max(len(sequence) <b>for</b> sequence <b>in</b> integerized_titles)
Before creating the model, it is necessary to preprocess the titles in the dataset. We’ll
need to pad the elements of our title to feed into the model. Keras has the helper
pad_sequence
functions for that on the top of the tokenizer methods. The function
create_sequences takes both titles as well as the maximum sentence length as input
and returns a list of the integers corresponding to our tokens padded to the sentence
maximum length:
<b>from</b> <b>tensorflow.keras.preprocessing.sequence</b> <b>import</b> pad_sequences
<b>def</b> create_sequences(texts, max_len=MAX_LEN):
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences,
max_len,
padding='post')
<b>return</b> padded_sequences
Next, we’ll build a deep neural network (DNN) model in Keras that implements a
simple embedding layer to transform the word integers into dense vectors. The Keras
Embedding layer can be thought of as a map from the integer indices of specific words
to dense vectors (their embeddings). The dimensionality of the embedding is deter‐
mined by output_dim . The argument input_dim indicates the size of the vocabulary,"|DNN model; Embedding design pattern; Keras; text embeddings; tokenization
"sigmoid = [.8, .9, .2, .5]
softmax = [.7, .1, .15, .05]
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
There are several special cases to consider when following the Multilabel design pat‐
tern and using sigmoid output. Next, we’ll explore how to structure models that have
two possible label classes, how to make sense of sigmoid results, and other important
considerations for Multilabel models.
<b>Sigmoidoutputformodelswithtwoclasses</b>
There are two types of models where the output can belong to two possible classes:
• Each training example can be assigned <i>only</i> <i>one</i> class. This is also called <i>binary</i>
<i>classification</i> and is a special type of multiclass classification problem.
• Some training examples could belong to <i>both</i> classes. This is a type of <i>multilabel</i>
<i>classification</i> problem.
Figure 3-8 shows the distinction between these classifications.
<i>Figure</i> <i>3-8.</i> <i>Understanding</i> <i>the</i> <i>distinction</i> <i>between</i> <i>multiclass,</i> <i>multilabel,</i> <i>and</i> <i>binary</i>
<i>classification</i> <i>problems.</i>"|binary classification; multilabel classification; Multilabel design pattern; sigmoid activation
"want to deploy models to production that have 95% accuracy or higher. When newly
available data triggers a pipeline run and trains an updated model, we can add logic
to check the output of our evaluation component to execute the deployment compo‐
nent if the accuracy is above our threshold, or end the pipeline run if not. Both Air‐
flow and Kubeflow Pipelines, discussed previously in this section, provide APIs for
pipeline orchestration.
<b>LineagetrackinginMLpipelines</b>
One additional feature of pipelines is using them for tracking model metadata and
artifacts, also known as <i>lineage</i> <i>tracking.</i> Each time we invoke a pipeline, a series of
artifacts is generated. These artifacts could include dataset summaries, exported
models, model evaluation results, metadata on specific pipeline invocations, and
more. Lineage tracking lets us visualize the history of our model versions along with
other associated model artifacts. In AI Platform Pipelines, for example, we can use
the pipelines dashboard to see which data a model version was trained on, broken
down both by data schema and date. Figure 6-11 shows the Lineage Explorer dash‐
board for a TFX pipeline running on AI Platform. This allows us to track the input
and output artifacts associated with a particular model.
<i>Figure</i> <i>6-11.</i> <i>The</i> <i>Lineage</i> <i>Explorer</i> <i>section</i> <i>of</i> <i>the</i> <i>AI</i> <i>Platform</i> <i>Pipelines</i> <i>dashboard</i> <i>for</i> <i>a</i>
<i>TFX</i> <i>pipeline.</i>
One benefit of using lineage tracking to manage artifacts generated during our pipe‐
line run is that it supports both cloud-based and on-premises environments. This
gives us flexibility in where models are trained and deployed, and where model meta‐
data is stored. Lineage tracking is also an important aspect of making ML pipelines
reproducible, since it allows for comparisons between metadata and artifacts from
different pipeline runs."|lineage tracking; model evaluation; Workflow Pipeline design pattern
"tion” on page 201. The serving infrastructure is usually designed as a microservice
that offloads the heavy computation (such as with deep convolutional neural net‐
works) to high-performance hardware such as tensor processing units (TPUs) or
graphics processing units (GPUs) and minimizes the inefficiency associated with
multiple software layers.
However, there are circumstances where predictions need to be carried out asynchro‐
nously over large volumes of data. For example, determining whether to reorder a
stock-keeping unit (SKU) might be an operation that is carried out hourly, not every
time the SKU is bought at the cash register. Music services might create personalized
daily playlists for every one of their users and push them out to those users. The per‐
sonalized playlist is not created on-demand in response to every interaction that the
user makes with the music software. Because of this, the ML model needs to make
predictions for millions of instances at a time, not one instance at a time.
Attempting to take a software endpoint that is designed to handle one request at a
time and sending it millions of SKUs or billions of users will overwhelm the ML
model.
<header><largefont><b>Solution</b></largefont></header>
The Batch Serving design pattern uses a distributed data processing infrastructure
(MapReduce, Apache Spark, BigQuery, Apache Beam, and so on) to carry out ML
inference on a large number of instances asynchronously.
In the discussion on the Stateless Serving Function design pattern, we trained a text
classification model to output whether a review was positive or negative. Let’s say
that we want to apply this model to every complaint that has ever been made to the
United States Consumer Finance Protection Bureau (CFPB).
We can load the Keras model into BigQuery as follows (complete code is available in
a notebook in GitHub):
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL mlpatterns.imdb_sentiment
<b>OPTIONS(model_type='tensorflow',</b> model_path='gs://.../*')
Where normally, one would train a model using data in BigQuery, here we are simply
loading an externally trained model. Having done that, though, it is possible to use
BigQuery to carry out ML predictions. For example, the SQL query.
<b>SELECT</b> * <b>FROM</b> <b>ML.PREDICT(MODEL</b> mlpatterns.imdb_sentiment,
(SELECT 'This was very well done.' <b>AS</b> reviews)
)
returns a positive_review_probability of 0.82.
Using a distributed data processing system like BigQuery to carry out one-off predic‐
tions is not very efficient. However, what if we want to apply the machine learning"|Batch Serving design pattern; BigQuery; distributed data processing infrastructure
"Consider the situation shown in Figure 4-2. Observations collected from the physical
environment are used as inputs (or initial starting conditions) for a physics-based
model that carries out iterative, numerical calculations to calculate the precise state of
the system. Suppose all the observations have a finite number of possibilities (for
example, temperature will be between 60°C and 80°C in increments of 0.01°C). It is
then possible to create a training dataset for the machine learning system consisting
of the complete input space and calculate the labels using the physical model.
<i>Figure</i> <i>4-2.</i> <i>One</i> <i>situation</i> <i>when</i> <i>it</i> <i>is</i> <i>acceptable</i> <i>to</i> <i>overfit</i> <i>is</i> <i>when</i> <i>the</i> <i>entire</i> <i>domain</i>
<i>space</i> <i>of</i> <i>observations</i> <i>can</i> <i>be</i> <i>tabulated</i> <i>and</i> <i>a</i> <i>physical</i> <i>model</i> <i>capable</i> <i>of</i> <i>computing</i> <i>the</i>
<i>precise</i> <i>solution</i> <i>is</i> <i>available.</i>
The ML model needs to learn this precisely calculated and nonoverlapping lookup
table of inputs to outputs. Splitting such a dataset into a training dataset and an eval‐
uation dataset is counterproductive because we would then be expecting the model to
learn parts of the input space it will not have seen in the training dataset.
<header><largefont><b>Solution</b></largefont></header>
In this scenario, there is no “unseen” data that needs to be generalized to, since all
possible inputs have been tabulated. When building a machine learning model to
learn such a physics model or dynamical system, there is no such thing as overfitting.
The basic machine learning training paradigm is slightly different. Here, there is
some physical phenomenon that you are trying to learn that is governed by an under‐
lying PDE or system of PDEs. Machine learning merely provides a data-driven"|overfit model; PDE; physics-based model; Useful Overfitting design pattern
"<b>One-hotencoding</b>
The simplest method of mapping categorical variables while ensuring that the vari‐
ables are independent is <i>one-hot</i> <i>encoding.</i> In our example, the categorical input vari‐
able would be converted into a three-element feature vector using the following
mapping:
<b>Categoricalinput</b> <b>Numericfeature</b>
English [1.0,0.0,0.0]
Chinese [0.0,1.0,0.0]
German [0.0,0.0,1.0]
One-hot encoding requires us to know the <i>vocabulary</i> of the categorical input before‐
hand. Here, the vocabulary consists of three tokens (English, Chinese, and German),
and the length of the resulting feature is the size of this vocabulary.
<header><largefont><b>Dummy</b></largefont> <largefont><b>Coding</b></largefont> <largefont><b>or</b></largefont> <largefont><b>One-Hot</b></largefont> <largefont><b>Encoding?</b></largefont></header>
Technically, a 2-element feature vector is enough to provide a unique mapping for a
vocabulary of size 3:
<b>Categoricalinput</b> <b>Numericfeature</b>
English [0.0,0.0]
Chinese [1.0,0.0]
German [0.0,1.0]
This is called <i>dummy</i> <i>coding.</i> Because dummy coding is a more compact representa‐
tion, it is preferred in statistical models that perform better when the inputs are line‐
arly independent.
Modern machine learning algorithms, though, don’t require their inputs to be line‐
arly independent and use methods such as L1 regularization to prune redundant
inputs. The additional degree of freedom allows the framework to transparently han‐
dle a missing input in production as all zeros:
<b>Categoricalinput</b> <b>Numericfeature</b>
English [1.0,0.0,0.0]
Chinese [0.0,1.0,0.0]
German [0.0,0.0,1.0]
(missing) [0.0,0.0,0.0]
Therefore, many machine learning frameworks often support only one-hot encoding."|categorical inputs; dummy coding; one-hot encoding; vocabulary
"to set up monitoring to check for changes in these upstream data sources. Lastly, it is
important to set up systems to monitor prediction distributions and, when possible,
measure the quality of those predictions in the production environment.
Upon completion of the monitoring step, it can be beneficial to revisit the business
use case and objectively, accurately assess how the machine learning model has influ‐
enced business performance. Likely, this will lead to new insights and the start of new
ML projects, and the life cycle begins again.
<header><largefont><b>AI</b></largefont> <largefont><b>Readiness</b></largefont></header>
We find that different organizations working on building machine learning solutions
are at different stages of AI Readiness. According to a white paper published by Goo‐
gle Cloud, a company’s maturity in incorporating AI into the business can typically
be characterized into three phases: tactical, strategic, and transformational. Machine
learning tools in these three phases go from involving primarily manual development
in the tactical phase, to using pipelines in the strategic phase, to being fully automa‐
ted in the transformational phase.
<b>Tacticalphase:Manualdevelopment</b>
The tactical phase of AI Readiness is often seen in organizations just beginning to
explore the potential for AI to deliver, with focus on short-term projects. Here, the
AI/ML use cases tend to be more narrow, focusing more on proofs of concept or pro‐
totypes; a direct link to the business goals may not always be clear. In this stage,
organizations recognize the promise of advanced analytics work, but the execution is
driven primarily by individual contributors or outsourced entirely to partners; access
to large-scale, quality datasets within the organization can be difficult.
Typically, in this phase, there is no process to scale solutions consistently, and the ML
tools used (see Figure 8-4) are developed on an ad hoc basis. Data is warehoused off‐
line or in isolated data islands and accessed manually for data exploration and analy‐
sis. There are no tools in place to automate the various phases of the ML
development cycle and there is little attention paid to developing repeatable processes
of the workflow. This makes it difficult to share assets within members of the organi‐
zation, and there is no dedicated hardware for development.
The extent of MLOps is limited to a repository of trained models, and there is little
distinction between testing and production environments where the final model may
be deployed as an API-based solution."|AI readiness; ML life cycle; MLOps
"solution to the discrete problem on a spatio-temporal grid of the original domain.
However, when the dimension of the problem becomes large, this mesh-based
approach fails dramatically due to the curse of dimensionality because the mesh spac‐
ing of the grid must be small enough to capture the smallest feature size of the solu‐
tion. So, to achieve 10× higher resolution of an image requires 10,000× more
compute power, because the mesh grid must be scaled in four dimensions accounting
for space and time.
However, it is possible to use machine learning (rather than Monte Carlo methods)
to select the sampling points to create data-driven discretizations of PDEs. In the
paper ""Learning data-driven discretizations for PDEs,” Bar-Sinai et al. demonstrate
the effectiveness of this approach. The authors use a low-resolution grid of fixed
points to approximate a solution via a piecewise polynomial interpolation using stan‐
dard finite-difference methods as well as one obtained from a neural network. The
solution obtained from the neural network vastly outperforms the numeric simula‐
tion in minimizing the absolute error, in some places achieving a 102 order of magni‐
tude improvement. While increasing the resolution requires substantially more
compute power using finite-difference methods, the neural network is able to main‐
tain high performance with only marginal additional cost. Techniques like the Deep
Galerkin Method can then use deep learning to provide a mesh-free approximation
of the solution to the given PDE. In this way, solving the PDE is reduced to a chained
optimization problem (see “Design Pattern 8: Cascade ” on page 108).
<header><largefont><b>Deep</b></largefont> <largefont><b>Galerkin</b></largefont> <largefont><b>Method</b></largefont></header>
The Deep Galerkin Method is a deep learning algorithm for solving partial differen‐
tial equations. The algorithm is similar in spirit to Galerkin methods used in the field
of numeric analysis, where the solution is approximated using a neural network
instead of a linear combination of basis functions.
<b>Unboundeddomains</b>
The Monte Carlo and data-driven discretization methods both assume that sampling
the entire input space, even if imperfectly, is possible. That’s why the ML model was
treated as an interpolation between known points.
Generalization and the concern of overfitting become difficult to ignore whenever we
are unable to sample points in the full domain of the function—for example, for
functions with unbounded domains or projections along a time axis into the future.
In these settings, it is important to consider overfitting, underfitting, and
generalization error. In fact, it’s been shown that although techniques like the Deep
Galerkin Method do well on regions that are well sampled, a function that is learned
this way does not generalize well on regions outside the domain that were not"|Deep Galerkin Method; mesh-free approximation; Monte Carlo approach; neural networks; PDE; Useful Overfitting design pattern
"In this query, the 700 is 70*10 and 560 is 70*8. The first modulo operation picks 1 in
70 rows and the second modulo operation picks 8 in 10 of those rows.
For validation data, you’d replace < 560 by the appropriate range:
<b>ABS(MOD(FARM_FINGERPRINT(date),</b> 70)) = 0
<b>AND</b> <b>ABS(MOD(FARM_FINGERPRINT(date),</b> 700)) <b>BETWEEN</b> <b>560</b> <b>AND</b> <b>629</b>
In the preceding code, our one million flights come from only 1/70th of the days in
the dataset. This may be precisely what we want—for example, we may be modeling
the full spectrum of flights on a particular day when experimenting with the smaller
dataset. However, if what we want is 1/70th of the flights on any particular day, we’d
have to use RAND() and save the result as a new table for repeatability. From this
FARM_FINGERPRINT().
smaller table, we can sample 80% of dates using Because this
new table is only one million rows and only for experimentation, the duplication may
be acceptable.
<b>Sequentialsplit</b>
In the case of time-series models, a common approach is to use sequential splits of
data. For example, to train a demand forecasting model where we train a model on
the past 45 days of data to predict demand over the next 14 days, we’d train the
model (full code) by pulling the necessary data:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL ch09eu.numrentals_forecast
<b>OPTIONS(model_type='ARIMA',</b>
time_series_data_col='numrentals',
time_series_timestamp_col='date') <b>AS</b>
<b>SELECT</b>
<b>CAST(EXTRACT(date</b> <b>from</b> start_date) <b>AS</b> <b>TIMESTAMP)</b> <b>AS</b> date
, <b>COUNT(*)</b> <b>AS</b> numrentals
<b>FROM</b>
`bigquery-public-data`.london_bicycles.cycle_hire
<b>GROUP</b> <b>BY</b> date
<b>HAVING</b> date <b>BETWEEN</b>
DATE_SUB(CURRENT_DATE(), INTERVAL 45 <b>DAY)</b> <b>AND</b> <b>CURRENT_DATE()</b>
Such a sequential split of data is also necessary in fast-moving environments even if
the goal is not to predict the future value of a time series. For example, in a fraud-
detection model, bad actors adapt quickly to the fraud algorithm, and the model has
to therefore be continually retrained on the latest data to predict future fraud. It is
not sufficient to generate the evaluation data from a random split of the historical
dataset because the goal is to predict behavior that the bad actors will exhibit in the
future. The indirect goal is the same as that of a time-series model in that a good
model will be able to train on historical data and predict future fraud. The data has to
be split sequentially in terms of time to correctly evaluate this. For example (full
code):"|Farm Fingerprint hashing algorithm; fraud detection; Repeatable Splitting design pattern
"input to the neural network. We might use them as is, one-hot encode them, or
choose to bucketize the numbers. For simplicity, let’s just use them all as is:
feature_columns = {
colname: tf.feature_column.numeric_column(colname)
<b>for</b> colname <b>in</b> ['pickup_longitude', 'pickup_latitude',
'dropoff_longitude', 'dropoff_latitude']
}
feature_columns['euclidean'] = \
tf.feature_column.numeric_column('euclidean')
Once we have a DenseFeatures input layer, we can build the rest of our Keras model
as usual:
h1 = tf.keras.layers.Dense(32, activation='relu', name='h1')(dnn_inputs)
h2 = tf.keras.layers.Dense(8, activation='relu', name='h2')(h1)
output = tf.keras.layers.Dense(1, name='fare')(h2)
model = tf.keras.models.Model(inputs, output)
model.compile(optimizer='adam', loss='mse', metrics=['mse'])
The complete example is on GitHub.
Inputs
Notice how we set things up so that the first layer of the Keras model was .
The second layer was the Transform layer. The third layer was the DenseFeatures
layer that combined them. After this sequence of layers, the usual model architecture
Transform
starts. Because the layer is part of the model graph, the usual Serving
Function and Batch Serving solutions (see Chapter 5) will work as is.
<b>Efficienttransformationswithtf.transform</b>
One drawback to the above approach is that the transformations will be carried out
during each iteration of training. This is not such a big deal if all we are doing is scal‐
ing by known constants. But what if our transformations are more computationally
expensive? What if we want to scale using the mean and variance, in which case, we
need to pass through all the data first to compute these variables?
It is helpful to differentiate between <i>instance-level</i> transformations
that can be part of the model directly (where the only drawback is
applying them on each training iteration) and <i>dataset-level</i> trans‐
formations, where we need a full pass to compute overall statistics
or the vocabulary of a categorical variable. Such dataset-level trans‐
formations cannot be part of the model and have to be applied as a
scalable preprocessing step, which produces the Transform, cap‐
turing the logic and the artifacts (mean, variance, vocabulary, and
so on) to be attached to the model. For dataset-level transforma‐
tf.transform.
tions, use"|dataset-level transformations; feature columns; instance-level transformations; Keras; Transform design pattern
"Both Azure and AWS have similar model versioning services avail‐
able. On Azure, model deployment and versioning is available with
Azure Machine Learning. In AWS, these services are available in
SageMaker.
An ML engineer deploying a new version of a model as an ML model endpoint may
want to use an API gateway such as Apigee that determines which model version to
call. There are various reasons for doing this, including split testing a new version.
For split testing, maybe they want to test a model update with a randomly selected
group of 10% of application users to track how it affects their overall engagement
with the app. The API gateway determines which deployed model version to call
given a user’s ID or IP address.
With multiple model versions deployed, AI Platform allows for performance moni‐
toring and analytics across versions. This lets us trace errors to a specific version,
monitor traffic, and combine this with additional data we’re collecting in our
application.
<header><largefont><b>Versioning</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Handle</b></largefont> <largefont><b>Newly</b></largefont> <largefont><b>Available</b></largefont> <largefont><b>Data</b></largefont></header>
In addition to handling changes to our model itself, another reason to use versioning
is when new training data becomes available. Assuming this new data follows the
same schema used to train the original model, it’s important to keep track of <i>when</i>
the data was captured for each newly trained version. One approach to tracking this is
to encode the timestamp range of each training dataset in the name of a model ver‐
sion. For example, if the latest version of a model is trained on data from 2019, we
v20190101_20191231
could name the version .
We can use this approach in combination with “Design Pattern 18: Continued Model
Evaluation” on page 220 (discussed in Chapter 5) to determine when to take older
model versions offline, or how far back training data should go. Continuous evalua‐
tion might help us determine that our model performs best when trained on data
from the past two years. This could then inform the versions we decide to remove,
and how much data to use when training newer versions."|Apigee; AWS Lambda; Azure; Azure Machine Learning; Continued Model Evaluation design pattern; ML engineers; Model Versioning design pattern; SageMaker
"<i>unsupervised</i> <i>learning,</i> you do not know the labels for your data in advance, and the
goal is to build a model that can find natural groupings of your data (called <i>cluster‐</i>
<i>ing),</i> compress the information content (dimensionality <i>reduction),</i> or find associa‐
tion rules. The majority of this book will focus on supervised learning because the
vast majority of machine learning models used in production are supervised.
With supervised learning, problems can typically be defined as either classification or
regression. <i>Classification</i> models assign your input data a label (or labels) from a dis‐
crete, predefined set of categories. Examples of classification problems include deter‐
mining the type of pet breed in an image, tagging a document, or predicting whether
or not a transaction is fraudulent. <i>Regression</i> models assign continuous, numerical
values to your inputs. Examples of regression models include predicting the duration
of a bike trip, a company’s future revenue, or the price of a product.
<header><largefont><b>Data</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Feature</b></largefont> <largefont><b>Engineering</b></largefont></header>
Data is at the heart of any machine learning problem. When we talk about <i>datasets,</i>
we’re referring to the data used for training, validating, and testing a machine learn‐
ing model. The bulk of your data will be <i>training</i> <i>data:</i> the data fed to your model
during the training process. <i>Validation</i> <i>data</i> is data that is held out from your train‐
ing set and used to evaluate how the model is performing after each training <i>epoch</i>
(or pass through the training data). The performance of the model on the validation
data is used to decide when to stop the training run, and to choose <i>hyperparameters,</i>
such as the number of trees in a random forest model. <i>Test</i> <i>data</i> is data that is not
used in the training process at all and is used to evaluate how the trained model per‐
forms. Performance reports of the machine learning model must be computed on the
independent test data, rather than the training or validation tests. It’s also important
that the data be split in such a way that all three datasets (training, test, validation)
have similar statistical properties.
The data you use to train your model can take many forms depending on the model
type. We define <i>structured</i> <i>data</i> as numerical and categorical data. Numerical data
includes integer and float values, and categorical data includes data that can be divi‐
ded into a finite set of groups, like type of car or education level. You can also think
of structured data as data you would commonly find in a spreadsheet. Throughout
the book, we’ll use the term <i>tabular</i> <i>data</i> interchangeably with structured data.
<i>Unstructured</i> <i>data,</i> on the other hand, includes data that cannot be represented as
neatly. This typically includes free-form text, images, video, and audio.
Numeric data can often be fed directly to a machine learning model, where other data
requires various <i>data</i> <i>preprocessing</i> before it’s ready to be sent to a model. This pre‐
processing step typically includes scaling numerical values, or converting nonnumer‐
ical data into a numerical format that can be understood by your model. Another"|categorical data; classification models; data preprocessing; datasets; epochs; feature engineering; hyperparameters; numerical data; regression models; structured data; tabular data; test data; training data; unstructured data; validation data
"The departure time was an example of a skewed feature. In an imbalanced classifica‐
tion problem (such as fraud detection, where the number of fraud examples is quite
small), we might want to stratify the dataset by the label and split each stratification
evenly. This is also important if we have a multilabel problem and some of the labels
are rarer than others. These are discussed in “Design Pattern 10: Rebalancing ” on
page 122 in Chapter 3.
<b>Unstructureddata</b>
Although we have focused in this section on structured data, the same principles
apply to unstructured data such as images, video, audio, or free-form text as well. Just
use the metadata to carry out the split. For example, if videos taken on the same day
are correlated, use a video’s capture date from its metadata to split the videos among
independent datasets. Similarly, if text reviews from the same person tend to be cor‐
user_id
related, use the Farm Fingerprint of the of the reviewer to repeatedly split
reviews among the datasets. If the metadata is not available or there is no correlation
between instances, encode the image or video using Base64 encoding and compute
the fingerprint of the encoding.
A natural way to split text datasets might be to use the hash of the text itself for split‐
ting. However, this is akin to a random split and does not address the problem of cor‐
relations between reviews. For example, if a person uses the word “stunning” a lot in
their negative reviews or if a person rates all Star Wars movies as bad, their reviews
are correlated. Similarly, a natural way to split image or audio datasets might be to
use the hash of the filename for splitting, but it does not address the problem of
correlations between images or videos. It is worth thinking carefully about the best
way to split a dataset. In our experience, many problems with poor performance of
ML can be addressed by designing the data split (and data collection) with potential
correlations in mind.
When computing embeddings or pre-training autoencoders, we should make sure to
first split the data and perform these pre-computations on the training dataset only.
Because of this, splitting should not be done on the embeddings of the images, vid‐
eos, or text unless these embeddings were created on a completely separate dataset."|Farm Fingerprint hashing algorithm; fraud detection; Repeatable Splitting design pattern; unstructured data
"<i>Figure</i> <i>7-5.</i> <i>The</i> <i>feature</i> <i>attributions</i> <i>returned</i> <i>from</i> <i>Explainable</i> <i>AI</i> <i>for</i> <i>an</i> <i>ImageNet</i>
<i>model</i> <i>deployed</i> <i>to</i> <i>AI</i> <i>Platform.</i> <i>On</i> <i>the</i> <i>left</i> <i>is</i> <i>the</i> <i>original</i> <i>image.</i> <i>The</i> <i>IG</i> <i>attributions</i>
<i>are</i> <i>shown</i> <i>in</i> <i>the</i> <i>middle,</i> <i>and</i> <i>the</i> <i>XRAI</i> <i>attributions</i> <i>are</i> <i>shown</i> <i>on</i> <i>the</i> <i>right.</i> <i>The</i> <i>key</i>
<i>below</i> <i>shows</i> <i>what</i> <i>the</i> <i>regions</i> <i>in</i> <i>XRAI</i> <i>correspond</i> <i>to—lighter</i> <i>regions</i> <i>are</i> <i>the</i> <i>most</i>
<i>important,</i> <i>and</i> <i>darker</i> <i>areas</i> <i>represent</i> <i>the</i> <i>least</i> <i>important</i> <i>regions.</i>
<i>Figure</i> <i>7-6.</i> <i>As</i> <i>part</i> <i>of</i> <i>a</i> <i>study</i> <i>by</i> <i>Rory</i> <i>Sayres</i> <i>and</i> <i>colleagues</i> <i>in</i> <i>2019,</i> <i>different</i> <i>groups</i> <i>of</i>
<i>ophthalmologists</i> <i>were</i> <i>asked</i> <i>to</i> <i>evaluate</i> <i>the</i> <i>degree</i> <i>of</i> <i>DR</i> <i>on</i> <i>an</i> <i>image</i> <i>in</i> <i>three</i> <i>scenar‐</i>
<i>ios:</i> <i>the</i> <i>image</i> <i>by</i> <i>itself</i> <i>without</i> <i>model</i> <i>predictions,</i> <i>the</i> <i>image</i> <i>with</i> <i>model</i> <i>predictions,</i>
<i>and</i> <i>the</i> <i>image</i> <i>with</i> <i>predictions</i> <i>and</i> <i>pixel</i> <i>attributions</i> <i>(shown</i> <i>here).</i> <i>We</i> <i>can</i> <i>see</i> <i>how</i>
<i>pixel</i> <i>attributions</i> <i>can</i> <i>help</i> <i>increase</i> <i>confidence</i> <i>in</i> <i>the</i> <i>model’s</i> <i>prediction.</i>"|Explainable Predictions design pattern; feature attributions
"<i>#</i> <i>rename</i> <i>to</i> <i>avoid</i> <i>'unique</i> <i>layer</i> <i>name'</i> <i>issue</i>
layer._name = 'ensemble_' + str(i+1) + '_' + layer.name
We create the ensemble model stitching together the components using the Keras
functional API:
member_inputs = [model.input <b>for</b> model <b>in</b> members]
<i>#</i> <i>concatenate</i> <i>merge</i> <i>output</i> <i>from</i> <i>each</i> <i>model</i>
member_outputs = [model.output <b>for</b> model <b>in</b> members]
merge = layers.concatenate(member_outputs)
hidden = layers.Dense(10, activation='relu')(merge)
ensemble_output = layers.Dense(1, activation='relu')(hidden)
ensemble_model = Model(inputs=member_inputs, outputs=ensemble_output)
<i>#</i> <i>plot</i> <i>graph</i> <i>of</i> <i>ensemble</i>
tf.keras.utils.plot_model(ensemble_model, show_shapes=True,
to_file='ensemble_graph.png')
<i>#</i> <i>compile</i>
ensemble_model.compile(loss='mse', optimizer='adam', metrics=['mse'])
In this example, the secondary model is a dense neural network with two hidden lay‐
ers. Through training, this network learns how to best combine the results of the
ensemble members when making predictions.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Model averaging methods like bagging work because typically the individual models
that make up the ensemble model will not all make the same errors on the test set. In
an ideal situation, each individual model is off by a random amount, so when their
results are averaged, the random errors cancel out, and the prediction is closer to the
correct answer. In short, there is wisdom in the crowd.
Boosting works well because the model is punished more and more according to the
residuals at each iteration step. With each iteration, the ensemble model is encour‐
aged to get better and better at predicting those hard-to-predict examples. Stacking
works because it combines the best of both bagging and boosting. The secondary
model can be thought of as a more sophisticated version of model averaging.
<b>Bagging</b>
More precisely, suppose we’ve trained <i>k</i> neural network regression models and aver‐
error_i
age their results to create an ensemble model. If each model has error on
each example, where error_i is drawn from a zero-mean multivariate normal
distribution with variance var and covariance cov , then the ensemble predictor will
have an error:
ensemble_error = 1./k * np.sum([error_1, error_2,...,error_k])"|bagging; boosting; Ensemble design pattern; stacking
"<header><largefont><b>Solution</b></largefont></header>
With the Two-Phase Predictions design pattern, we split our problem into two parts.
We start with a smaller, cheaper model that can be deployed on-device. Because this
model typically has a simpler task, it can accomplish this task on-device with rela‐
tively high accuracy. This is followed by a second, more complex model deployed in
the cloud and triggered only when needed. Of course, this design pattern requires
you to have a problem that can be split into two parts with varying levels of complex‐
ity. One example of such a problem is smart devices like Google Home, which are
activated by a wake word and can then answer questions and respond to commands
related to setting alarms, reading the news, and interacting with integrated devices
like lights and thermostats. Google Home, for example, is activated by saying “OK
Google” or “Hey Google.” Once the device recognizes a wake word, users can ask
more complex questions like, “Can you schedule a meeting with Sara at 10 a.m.?”
This problem can be broken into two distinct parts: an initial model that listens for a
wake word, and a more complex model that can understand and respond to any
other user query. Both models will perform audio recognition. The first model,
however, will only need to perform binary classification: does the sound it just heard
match the wake word or not? Although this model is simpler in complexity, it needs
to be constantly running, which will be expensive if it’s deployed to the cloud. The
second model will require audio recognition <i>and</i> natural language understanding in
order to parse the user’s query. This model only needs to run when a user asks a
question, but places more emphasis on high accuracy. The Two-Phase Predictions
pattern can solve this by deploying the wake word model on-device and the more
complex model in the cloud.
In addition to this smart device use case, there are many other situations where the
Two-Phase Predictions pattern can be employed. Let’s say you work on a factory
floor where many different machines are running at a given time. When a machine
stops working correctly, it typically makes a noise that can be associated with a mal‐
function. There are different noises corresponding with each distinct machine and
the different ways a machine could be broken. Ideally, you can build a model to flag
problematic noises and identify what they mean. With Two-Phase Predictions, you
could build one offline model to detect anomalous sounds. A second cloud model
could then be used to identify whether the usual sound is indicative of some malfunc‐
tioning condition.
You could also use the Two-Phase Predictions pattern for an image-based scenario.
Let’s say you have cameras deployed in the wild to identify and track endangered spe‐
cies. You can have one model on the device that detects whether the latest image cap‐
tured contains an endangered animal. If it does, this image can then be sent to a
cloud model that determines the specific type of animal in the image."|Two-Phase Predictions design pattern
"<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
We will often find that a heuristic benchmark is useful beyond the primary purpose
of explaining model performance. In some cases, the heuristic benchmark might
require special data collection. Finally, there are instances where a heuristic bench‐
mark may be insufficient because the comparison itself needs context.
<b>Developmentcheck</b>
It is often the case that a heuristic benchmark proves useful beyond explaining the
performance of ML models. During development, it can also help with diagnosing
problems with a particular model approach.
For example, say that we are building a model to predict the duration of rentals and
our benchmark is a lookup table of average rental duration given the station name
and whether or not it is peak commute hour:
<b>CREATE</b> <b>TEMPORARY</b> <b>FUNCTION</b> is_peak_hour(start_date <b>TIMESTAMP)</b> <b>AS</b>
<b>EXTRACT(DAYOFWEEK</b> <b>FROM</b> start_date) <b>BETWEEN</b> 2 <b>AND</b> 6 <i>--</i> <i>weekday</i>
<b>AND</b> (
<b>EXTRACT(HOUR</b> <b>FROM</b> start_date) <b>BETWEEN</b> 6 <b>AND</b> 10
<b>OR</b>
<b>EXTRACT(HOUR</b> <b>FROM</b> start_date) <b>BETWEEN</b> 15 <b>AND</b> 18)
;
<b>SELECT</b>
start_station_name,
is_peak_hour(start_date) <b>AS</b> is_peak,
<b>AVG(duration)</b> <b>AS</b> predicted_duration,
<b>FROM</b> `bigquery-public-data.london_bicycles.cycle_hire`
<b>GROUP</b> <b>BY</b> 1, 2
As we develop our model, it is a good idea to compare the performance of our ML
model against this benchmark. In order to do this, we will be evaluating model per‐
formance on different stratifications of the evaluation dataset. Here, the evaluation
start_station_name is_peak
dataset will be stratified by and . By doing so, we can
easily diagnose whether our model is overemphasizing the busy, popular stations and
ignoring infrequent stations in the training data. If that is happening, we can experi‐
ment with increasing model complexity or balancing the dataset to overweight less
popular stations.
<b>Humanexperts</b>
We recommended that in classification problems like diagnosing eye disease—where
the work is carried out by human experts—that the benchmark would involve a panel
of such experts. By having three or more physicians examine each image, it is possible
to identify the extent to which human physicians make errors and compare the error
rate of the model against that of human experts. In the case of such image"|heuristic benchmark; Heuristic Benchmark design pattern; labeling
"Another reason for a model’s performance to degrade over time is data drift. We
introduced the problem of data drift in “Common Challenges in Machine Learning”
on page 11 in Chapter 1. Data drift refers to any change that has occurred to the data
being fed to your model for prediction as compared to the data that was used for
training. Data drift can occur for a number of reasons: the input data schema changes
at the source (for example, fields are added or deleted upstream), feature distribu‐
tions change over time (for example, a hospital might start to see more younger
adults because a ski resort opened nearby), or the meaning of the data changes even if
the structure/schema hasn’t (for example, whether a patient is considered “over‐
weight” may change over time). Software updates could introduce new bugs or the
business scenario changes and creates a new product label previously not available in
the training data. ETL pipelines for building, training, and predicting with ML mod‐
els can be brittle and opaque, and any of these changes would have drastic effects on
the performance of your model.
Model deployment is a continuous process, and to solve for concept drift or data
drift, it is necessary to update your training dataset and retrain your model with fresh
data to improve predictions. But how do you know when retraining is necessary?
And how often should you retrain? Data preprocessing and model training can be
costly both in time and money and each step of the model development cycle adds
additional overhead of development, monitoring, and maintenance.
<header><largefont><b>Solution</b></largefont></header>
The most direct way to identify model deterioration is to continuously monitor your
model’s predictive performance over time, and assess that performance with the same
evaluation metrics you used during development. This kind of continuous model
evaluation and monitoring is how we determine whether the model, or any changes
we’ve made to the model, are working as they should.
<b>Concept</b>
Continuous evaluation of this kind requires access to the raw prediction request data
and the predictions the model generated as well as the ground truth, all in the same
place. Google Cloud AI Platform provides the ability to configure the deployed
model version so that the online prediction input and output are regularly sampled
and saved to a table in BigQuery. In order to keep the service performant to a large
number of requests per second, we can customize how much data is sampled by spec‐
ifying a percentage of the number of input requests. In order to measure performance
metrics, it is necessary to combine this saved sample of predictions against the
ground truth.
In most situations, it may take time before the ground truth labels become available.
For example, for a churn model, it may not be known until the next subscription"|Cloud AI Platform; Continued Model Evaluation design pattern
"<i>Figure</i> <i>4-15.</i> <i>In</i> <i>synchronous</i> <i>training,</i> <i>each</i> <i>worker</i> <i>holds</i> <i>a</i> <i>copy</i> <i>of</i> <i>the</i> <i>model</i> <i>and</i> <i>com‐</i>
<i>putes</i> <i>gradients</i> <i>using</i> <i>a</i> <i>slice</i> <i>of</i> <i>the</i> <i>training</i> <i>data</i> <i>mini-batch.</i>
To implement this mirrored strategy in Keras, you first create an instance of the mir‐
rored distribution strategy, then move the creation and compiling of the model inside
the scope of that instance. The following code shows how to use MirroredStrategy
when training a three-layer neural network:
mirrored_strategy = tf.distribute.MirroredStrategy()
<b>with</b> mirrored_strategy.scope():
model = tf.keras.Sequential([tf.keras.layers.Dense(32, input_shape=(5,)),
tf.keras.layers.Dense(16, activation='relu'),
tf.keras.layers.Dense(1)])
model.compile(loss='mse', optimizer='sgd')
By creating the model inside this scope, the parameters of the model are created as
mirrored variables instead of regular variables. When it comes to fitting the model on
the dataset, everything is performed exactly the same as before. The model code stays
the same! Wrapping the model code in the distribution strategy scope is all you need
MirroredStrategy
to do to enable distributed training. The handles replicating the
model parameters on the available GPUs, aggregating gradients, and more. To train
fit() evaluate()
or evaluate the model, we just call or as usual:
model.fit(train_dataset, epochs=2)
model.evaluate(train_dataset)
During training, each batch of the input data is divided equally among
the multiple workers. For example, if you are using two GPUs, then a batch
size of 10 will be split among the 2 GPUs, with each receiving 5 training examples"|CentralStorageStrategy; Distribution Strategy design pattern; GPU; Keras; MirroredStrategy; MultiWorkerMirroredStrategy; synchronous training
"<b>Boosting</b>
Boosting is another Ensemble technique. However, unlike bagging, boosting ulti‐
mately constructs an ensemble model with <i>more</i> capacity than the individual member
models. For this reason, boosting provides a more effective means of reducing bias
than variance. The idea behind boosting is to iteratively build an ensemble of models
where each successive model focuses on learning the examples the previous model
got wrong. In short, boosting iteratively improves upon a sequence of weak learners
taking a weighted average to ultimately yield a strong learner.
f_0
At the start of the boosting procedure, a simple base model is selected. For a
regression task, the base model could just be the average target value: f_0 =
np.mean(Y_train) delta_1
. For the first iteration step, the residuals are measured
and approximated via a separate model. This residual model can be anything, but
typically it isn’t very sophisticated; we’d often use a weak learner like a decision tree.
The approximation provided by the residual model is then added to the current pre‐
diction, and the process continues.
After many iterations, the residuals tend toward zero and the prediction gets better
and better at modeling the original training dataset. Notice that in Figure 3-12 the
residuals for each element of the dataset decrease with each successive iteration.
<i>Figure</i> <i>3-12.</i> <i>Boosting</i> <i>converts</i> <i>weak</i> <i>learners</i> <i>into</i> <i>strong</i> <i>learners</i> <i>by</i> <i>iteratively</i> <i>improv‐</i>
<i>ing</i> <i>the</i> <i>model</i> <i>prediction.</i>
Some of the more well-known boosting algorithms are AdaBoost, Gradient Boosting
Machines, and XGBoost, and they have easy-to-use implementations in popular
machine learning frameworks like scikit-learn or TensorFlow."|AdaBoost; boosting; Ensemble design pattern; Gradient Boosting Machines; XGBoost
"<b>Savingpredictions</b>
Once the model is deployed, we can set up a job to save a sample of the prediction
requests—the reason to save a sample, rather than all requests, is to avoid unnecessa‐
rily slowing down the serving system. We can do this in the Continuous Evaluation
section of the Google Cloud AI Platform (CAIP) console by specifying the LabelKey
(the column that is the output of the model, which in our case will be source since we
ScoreKey
are predicting the source of the article), a in the prediction outputs (a
numeric value, which in our case is confidence), and a table in BigQuery where a
portion of the online prediction requests are stored. In our example code, the table is
txtcls_eval.swivel
called . Once this has been configured, whenever online predic‐
tions are made, CAIP streams the model name, the model version, the timestamp of
the prediction request, the raw prediction input, and the model’s output to the speci‐
fied BigQuery table, as shown in Table 5-1.
<i>Table</i> <i>5-1.</i> <i>A</i> <i>proportion</i> <i>of</i> <i>the</i> <i>online</i> <i>prediction</i> <i>requests</i> <i>and</i> <i>the</i> <i>raw</i> <i>prediction</i> <i>output</i> <i>is</i>
<i>saved</i> <i>to</i> <i>a</i> <i>table</i> <i>in</i> <i>BigQuery</i>
<b>Row</b> <b>model</b> <b>model_version</b> <b>time</b> <b>raw_data</b> <b>raw_prediction</b> <b>groundtruth</b>
1 txtcls swivel 2020-06-10 {""instances”:[{""text”: {""predictions”:[{""source”: null
01:40:32UTC “AstronautsDockWith “github”,“confidence”:
SpaceStationAfter 0.9994275569915771}]}
HistoricSpaceX
Launch""}]}
2 txtcls swivel 2020-06-10 {""instances”:[{""text”: {""predictions”:[{""source”: null
01:37:46UTC “SenateConfirmsFirst “nytimes”,“confidence”:
BlackAirForce 0.9989787340164185}]}
Chief""}]}
3 txtcls swivel 2020-06-09 {""instances”:[{""text”: {""predictions”:[{""source”: null
21:21:47UTC “AnativeMacapp “github”,“confidence”:
wrapperforWhatsApp 0.745254397392273}]}
Web""}]}
<b>Capturinggroundtruth</b>
It is also necessary to capture the ground truth for each of the instances sent to the
model for prediction. This can be done in a number of ways depending on the use
case and data availability. One approach would be to use a human labeling service—
all instances sent to the model for prediction, or maybe just the ones for which the
model has marginal confidence, are sent out for human annotation. Most cloud pro‐
viders offer some form of a human labeling service to enable labeling instances at
scale in this way.
Ground truth labels can also be derived from how users interact with the model and
its predictions. By having users take a specific action, it is possible to obtain implicit
feedback for a model’s prediction or to produce a ground truth label. For example,"|Cloud AI Platform; confidence; Continued Model Evaluation design pattern; ground truth label; labeling
"meal_type
, so we can turn this into a one-hot encoding and will represent dinner as
[ 0, 0, 1 ]. With this categorical feature represented as an array, we can now combine
it with meal_total by adding the price of the meal as the fourth element of the array:
0, 0, 1, 30.5
[ ].
The Embeddings design pattern is a common approach to encoding text for machine
learning models. If our model had only text, we could represent it as an embedding
tf.keras
layer using the following code:
<b>from</b> <b>tensorflow.keras</b> <b>import</b> Sequential
<b>from</b> <b>tensorflow.keras.layers</b> <b>import</b> Embedding
model = Sequential()
model.add(Embedding(batch_size, 64, input_length=30))
Here, we need to flatten the embedding8 in order to concatenate with the meal_type
and meal_total :
model.add(Flatten())
We could then use a series of Dense layers to transform that very large array9 into
smaller ones, ending with our output that is an array of, say, three numbers:
model.add(Dense(3, activation=""relu""))
We now need to concatenate these three numbers, which form the sentence embed‐
ding of the review with the earlier inputs: [0, 0, 1, 30.5, 0.75, -0.82, 0.45].
To do this, we’ll use the Keras functional API and apply the same steps. Layers built
with the functional API are callable, enabling us to chain them together starting with
an Input layer . 10 To make use of this, we’ll first define both our embedding and tab‐
ular layers:
embedding_input = Input(shape=(30,))
embedding_layer = Embedding(batch_size, 64)(embedding_input)
embedding_layer = Flatten()(embedding_layer)
embedding_layer = Dense(3, activation='relu')(embedding_layer)
tabular_input = Input(shape=(4,))
tabular_layer = Dense(32, activation='relu')(tabular_input)
Note that we’ve defined the Input pieces of both of these layers as their own vari‐
Model
ables. This is because we need to pass Input layers when we build a with the
8 Whenwepassanencoded30-wordarraytoourmodel,theKeraslayerwilltransformitintoa64-
dimensionalembeddingrepresentation,sowe’llhavea[64×30]matrixrepresentingthereview.
9 Thestartingpointisanarraythatis1,920numbers.
10 See02_data_representation/mixed_representation.ipynbinthecoderepositoryofthisbookforthefullmodel
code."|Dense layers; Embedding design pattern; Keras; Multimodal Input design pattern
"In addition to image autoencoders, recent work has focused on applying deep learn‐
ing techniques for structured data. TabNet is a deep neural network specifically
designed to learn from tabular data and can be trained in an unsupervised manner.
By modifying the model to have an encoder-decoder structure, TabNet works as an
autoencoder on tabular data, which allows the model to learn embeddings from
structured data via a feature transformer.
<b>Contextlanguagemodels</b>
Is there an auxiliary learning task that works for text? Context language models like
Word2Vec and masked language models like Bidirectional Encoding Representations
from Transformers (BERT) change the learning task to a problem so that there is no
scarcity of labels.
Word2Vec is a well-known method for constructing an embedding using shallow
neural networks and combining two techniques—Continuous Bag of Words
(CBOW) and a skip-gram model—applied to a large corpus of text, such as Wikipe‐
dia. While the goal of both models is to learn the context of a word by mapping input
word(s) to the target word(s) with an intermediate embedding layer, an auxiliary goal
is achieved that learns low-dimensional embeddings that best capture the context of
words. The resulting word embeddings learned through Word2Vec capture the
semantic relationships between words so that, in the embedding space, the vector
representations maintain meaningful distance and directionality (Figure 2-12).
<i>Figure</i> <i>2-12.</i> <i>Word</i> <i>embeddings</i> <i>capture</i> <i>semantic</i> <i>relationships.</i>
BERT is trained using a masked language model and next sentence prediction. For a
masked language model, words are randomly masked from text and the model
guesses what the missing word(s) are. Next sentence prediction is a classification task
where the model predicts whether or not two sentences followed each other in the
original text. So any corpus of text is suitable as a labeled dataset. BERT was initially
trained on all of the English Wikipedia and BooksCorpus. Despite learning on these
auxiliary tasks, the learned embeddings from BERT or Word2Vec have proven very
powerful when used on other downstream training tasks. The word embeddings"|autoencoders; BERT; CBOW; context language models; Embedding design pattern; skip-gram model; TabNet; Word2Vec
"We don’t need to store the vocabulary because the transformation code is independ‐
ent of the actual data value and the core of the model only deals with num_buckets
inputs, not the full vocabulary.
It is true that hashing is lossy—since we have 347 airports, an average of 35 airports
will get the same hash bucket code if we hash it into 10 buckets. When the alternative
is to discard the variable because it is too wide, though, a lossy encoding is an accept‐
able compromise.
<b>Coldstart</b>
The cold-start situation is similar to the out-of-vocabulary situation. If a new airport
gets added to the system, it will initially get the predictions corresponding to other
airports in the hash bucket. As an airport gets popular, there will be more flights from
that airport. As long as we periodically retrain the model, its predictions will start to
reflect arrival delays from the new airport. This is discussed in more detail in the
“Design Pattern 18: Continued Model Evaluation” on page 220 in Chapter 5.
By choosing the number of hash buckets such that each bucket gets about five entries,
we can ensure that any bucket will have reasonable initial results.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Most design patterns involve some kind of a trade-off, and the Hashed Feature design
pattern is no exception. The key trade-off here is that we lose model accuracy.
<b>Bucketcollision</b>
The modulo part of the Hashed Feature implementation is a lossy operation. By
choosing a hash bucket size of 100, we are choosing to have 3–4 airports share a
bucket. We are explicitly compromising on the ability to accurately represent the data
(with a fixed vocabulary and one-hot encoding) in order to handle out-of-vocabulary
inputs, cardinality/model size constraints, and cold-start problems. It is not a free
lunch. Do not choose Hashed Feature if you know the vocabulary beforehand, if the
vocabulary size is relatively small (in the thousands is acceptable for a dataset with
millions of examples), and if cold start is not a concern.
Note that we cannot simply increase the number of buckets to an extremely high
number hoping to avoid collisions altogether. Even if we raise the number of buckets
to 100,000 with only 347 airports, the probability that at least two airports share the
same hash bucket is 45%—unacceptably high (see Table 2-2). Therefore, we should
use Hashed Features only if we are willing to tolerate multiple categorical inputs
sharing the same hash bucket value."|cold start; hash buckets; Hashed Feature design pattern; vocabulary
"• When a flight arrives, its arrival delay can be classified as anomalous or not using
the externalized model state—it is not necessary to have the full list of flights over
the past 2 hours.
We can use Apache Beam for streaming pipelines because then, the same code will
work on both the historical data and on newly arriving data. In Apache Beam, the
sliding window is set up as follows (full code is on GitHub):
windowed = (data
| 'window' >> beam.WindowInto(
beam.window.SlidingWindows(2 * 60 * 60, 10*60))
The model is updated by combining all the flight data collected over the past two
ModelFn
hours and passing it to a function that we call :
model_state = (windowed
| 'model' >> beam.transforms.CombineGlobally(ModelFn()))
ModelFn
updates the internal model state with flight information. Here, the internal
model state will consist of a pandas dataframe that is updated with the flights in the
window:
<b>class</b> <b>ModelFn(beam.CombineFn):</b>
<b>def</b> create_accumulator(self):
<b>return</b> pd.DataFrame()
<b>def</b> add_input(self, df, window):
<b>return</b> df.append(window, ignore_index=True)
Every time the window is closed, the output is extracted. The output here (we refer to
it as externalized model state) consists of the model parameters:
<b>def</b> extract_output(self, df):
<b>if</b> len(df) < 1:
<b>return</b> {}
orig = df['delay'].values
xarr = np.delete(orig, [np.argmin(orig), np.argmax(orig)])
<b>return</b> {
'prediction': np.mean(xarr),
'acceptable_deviation': 4 * np.std(xarr)
}"|Apache Beam; Windowed Inference design pattern
"model = Model(model_data=MODEL_LOCATION, role='SomeRole')
predictor = model.deploy(initial_instance_count=1,
instance_type='ml.c5.xlarge')
With a REST endpoint in place, we can send a prediction request as a JSON with the
form:
{""instances"":
[
{""reviews"": ""The film is based on a prize-winning novel.""},
{""reviews"": ""The film is fast moving and has several great action scenes.""},
{""reviews"": ""The film was very boring. I walked out half-way.""}
]
}
We get back the predicted values also wrapped in a JSON structure:
{""predictions"": [{ <b>""positive_review_logits"":</b> [0.6965846419334412]},
{""positive_review_logits"": [1.6177300214767456]},
{""positive_review_logits"": [-0.754359781742096]}]}
By allowing clients to send JSON requests with multiple instances
in the request, called <i>batching,</i> we are allowing clients to trade off
the higher throughput associated with fewer network calls against
the increased parallelization if they send more requests with fewer
instances per request.
Besides batching, there are other knobs and levers to improve per‐
formance or lower cost. Using a machine with more powerful
GPUs, for example, typically helps to improve the performance of
deep learning models. Choosing a machine with multiple accelera‐
tors and/or threads helps improve the number of requests per sec‐
ond. Using an autoscaling cluster of machines can help lower cost
on spiky workloads. These kinds of tweaks are often done by the
ML/DevOps team; some are ML-specific, some are not.
<b>Language-neutral</b>
Every modern programming language can speak REST, and a discovery service is
provided to autogenerate the necessary HTTP stubs. Thus, Python clients can invoke
the REST API as follows. Note that there is nothing framework specific in the code
below. Because the cloud service abstracts the specifics of our ML model, we don’t
need to provide any references to Keras or TensorFlow:
credentials = GoogleCredentials.get_application_default()
api = discovery.build(""ml"", ""v1"", credentials = credentials,
discoveryServiceUrl = ""https://storage.googleapis.com/cloud-
ml/discovery/ml_v1_discovery.json"")
request_data = {""instances"":
["|batching; JSON; Stateless Serving Function design pattern
"To deploy a model to AI Platform with explanations, we first need to create a meta‐
data file that will be used by the explanation service to calculate feature attributions.
This metadata is provided in a JSON file and includes information on the baseline
we’d like to use and the parts of the model we want to explain. To simplify this
process, Explainable AI provides an SDK that will generate metadata via the follow‐
ing code:
<b>from</b> <b>explainable_ai_sdk.metadata.tf.v2</b> <b>import</b> SavedModelMetadataBuilder
model_dir = 'path/to/savedmodel/dir'
model_builder = SavedModelMetadataBuilder(model_dir)
model_builder.set_image_metadata('input_tensor_name')
model_builder.save_metadata(model_dir)
This code didn’t specify a model baseline, which means it’ll use the default (for image
models, this is a black and white image). We can optionally add an input_baselines
parameter to set_image_metadata to specify a custom baseline. Running the
save_metadata
method above creates an <i>explanation_metadata.json</i> file in a
model directory (the full code is in the GitHub repository).
When using this SDK via AI Platform Notebooks, we also have the option to generate
explanations locally within a notebook instance without deploying our model to the
load_model_from_local_path method
cloud. We can do this via the .
With our exported model and the <i>explanation_metadata.json</i> file in a Storage bucket,
we’re ready to create a new model version. When we do this, we specify the explana‐
tion method we’d like to use.
To deploy our model to AI Platform, we can copy our model directory to a Cloud
Storage bucket and use the gcloud CLI to create a model version. AI Platform has
three possible explanation methods to choose from:
<i>Integrated</i> <i>Gradients</i> <i>(IG)</i>
This implements the method introduced in the IG paper and works with any dif‐
ferentiable TensorFlow model—image, text, or tabular. For image models
deployed on AI Platform, IG returns an image with highlighted pixels, indicating
the regions that signaled the models prediction.
<i>Sampled</i> <i>Shapley</i>
Based on the Sampled Shapley paper, this uses an approach similar to the open
source SHAP library. On AI Platform, we can use this method with tabular and
text TensorFlow models. Because IG works only with differentiable models,
AutoML Tables uses Sampled Shapley to calculate feature attributions for all
models."|AI Platform Notebooks; baseline; Explainable Predictions design pattern; feature attributions; IG; Sampled Shapley; SHAP
"In some circumstances, it can be helpful to treat a numeric input as categorical and
map it to a one-hot encoded column:
<i>When</i> <i>the</i> <i>numeric</i> <i>input</i> <i>is</i> <i>an</i> <i>index</i>
For example, if we are trying to predict traffic levels and one of our inputs is the
day of the week, we could treat the day of the week as numeric (1, 2, 3, …, 7), but
it is helpful to recognize that the day of the week here is not a continuous scale
but really just an index. It is better to treat it as categorical (Sunday, Monday, …,
Saturday) because the indexing is arbitrary. Should the week start on Sunday (as
in the USA), Monday (as in France), or Saturday (as in Egypt)?
<i>When</i> <i>the</i> <i>relationship</i> <i>between</i> <i>input</i> <i>and</i> <i>label</i> <i>is</i> <i>not</i> <i>continuous</i>
What should tip the scale toward treating day of the week as a categorical feature
is that traffic levels on Friday are not affected by those on Thursday and
Saturday.
<i>When</i> <i>it</i> <i>is</i> <i>advantageous</i> <i>to</i> <i>bucket</i> <i>the</i> <i>numeric</i> <i>variable</i>
In most cities, traffic levels depend on whether it is the weekend, and this can
vary by location (Saturday and Sunday in most of the world, Thursday and Fri‐
day in some Islamic countries). It would be helpful to then treat day of the week
as a boolean feature (weekend or weekday). Such a mapping where the number
of distinct inputs (here, seven) is greater than the number of distinct feature val‐
ues (here, two) is called bucketing. Commonly, bucketing is done in terms of
mother_age
ranges—for example, we might bucket into ranges that break at 20,
25, 30, etc. and treat each of these bins as categorical, but it should be realized
that this loses the ordinal nature of mother_age .
<i>When</i> <i>we</i> <i>want</i> <i>to</i> <i>treat</i> <i>different</i> <i>values</i> <i>of</i> <i>the</i> <i>numeric</i> <i>input</i> <i>as</i> <i>being</i> <i>independent</i>
<i>when</i> <i>it</i> <i>comes</i> <i>to</i> <i>their</i> <i>effect</i> <i>on</i> <i>the</i> <i>label</i>
For example, the weight of a baby depends on the plurality2 of the delivery since
twins and triplets tend to weigh less than single births. So, a lower-weight baby, if
part of a triplet, might be healthier than a twin baby with the same weight. In this
case, we might map the plurality to a categorical variable, since a categorical vari‐
able allows the model to learn independent tunable parameters for the different
values of plurality. Of course, we can do this only if we have enough examples of
twins and triplets in our dataset.
2 Iftwins,thepluralityis2.Iftriplets,thepluralityis3."|bucketing; categorical inputs; one-hot encoding; vocabulary
"• Data governance is made difficult if each ML project computes features from
sensitive data differently.
• Ad hoc features aren’t easily shared between teams or across projects. In many
organizations, the same raw data is used by multiple teams, but separate teams
may define features differently and there is no easy access to feature documenta‐
tion. This also hinders effective cross-collaboration of teams, leading to siloed
work and unnecessarily duplicated effort.
• Ad hoc features used for training and serving are inconsistent—i.e., training–
serving skew. Training is typically done using historical data with batch features
that are created offline. However, serving is typically carried out online. If the
feature pipeline for training differs at all from the pipeline used in production for
serving (for example, different libraries, preprocessing code, or languages), then
we run the risk of training–serving skew.
• Productionizing features is difficult. When moving to production, there is no
standardized framework to serve features for online ML models and to serve
batch features for offline model training. Models are trained offline using fea‐
tures created in batch processes, but when served in production, these features
are often created with an emphasis on low latency and less on high throughput.
The framework for feature generation and storage is not flexible to handle both
of these scenarios.
In short, the ad hoc approach to feature engineering slows model development and
leads to duplicated effort and work stream inefficiency. Furthermore, feature creation
is inconsistent between training and inference, running the risk of training–serving
skew or data leakage by accidentally introducing label information into the model
input pipeline.
<header><largefont><b>Solution</b></largefont></header>
The solution is to create a shared feature store, a centralized location to store and
document feature datasets that will be used in building machine learning models and
can be shared across projects and teams. The feature store acts as the interface
between the data engineer’s pipelines for feature creation and the data scientist’s
workflow building models using those features (Figure 6-12). This way, there is a
central repository to house precomputed features, which speeds development time
and aids in feature discovery. This also allows the basic software engineering princi‐
ples of versioning, documentation, and access control to be applied to the features
that are created.
A typical feature store is built with two key design characteristics: tooling to process
large feature data sets quickly, and a way to store features that supports both low-
latency access (for inference) and large batch access (for model training). There is"|Feature Store design pattern; low latency
"input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
input_details
For the MobileNetV2 binary classification model we trained above,
looks like the following:
[{'dtype': numpy.float32,
'index': 0,
'name': 'mobilenetv2_1.00_128_input',
'quantization': (0.0, 0),
'quantization_parameters': {'quantized_dimension': 0,
'scales': array([], dtype=float32),
'zero_points': array([], dtype=int32)},
'shape': array([ 1, 128, 128, 3], dtype=int32),
'shape_signature': array([ 1, 128, 128, 3], dtype=int32),
'sparsity_parameters': {}}]
We’ll then pass the first image from our validation batch to the loaded TF Lite model
for prediction, invoke the interpreter, and get the output:
input_data = np.array([image_batch[21]], dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
<b>print(output_data)</b>
The resulting output is a sigmoid array with a single value in the [0,1] range indicat‐
ing whether or not the given input sound is an instrument.
Depending on how costly it is to call your cloud model, you can
change what metric you’re optimizing for when you train the on-
device model. For example, you might choose to optimize for pre‐
cision over recall if you care more about avoiding false positives.
With our model now working on-device, we can get fast predictions without having
to rely on internet connectivity. If the model is confident that a given sound is not an
instrument, we can stop here. If the model predicts “instrument,” it’s time to proceed
by sending the audio clip to a more complex cloud-hosted model.
<header><largefont><b>What</b></largefont> <largefont><b>Models</b></largefont> <largefont><b>Are</b></largefont> <largefont><b>Suitable</b></largefont> <largefont><b>on</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Edge?</b></largefont></header>
How should you determine whether a model is a good fit for the edge? There are a
few considerations related to model size, complexity, and available hardware. As a
general rule of thumb, smaller, less complex models are better optimized for running
on-device. This is because edge models are constrained by the available device stor‐
age. Often, when models are scaled down—through quantization or other techniques
—this is done at the expense of accuracy. As such, models with a simpler prediction"|edge; MobileNetV2; quantization; TF Lite Interpreter; Two-Phase Predictions design pattern
"rent production model (which we were able to train with one million examples), the
evaluation dataset here might hold hundreds of thousands of examples. We can then
compute the standard deviation of the evaluation metric over the 25 subsets, repeat
this on different evaluation sizes, and graph this standard deviation against the evalu‐
ation size. The resulting graph will be something like Figure 6-3.
<i>Figure</i> <i>6-3.</i> <i>Determine</i> <i>the</i> <i>number</i> <i>of</i> <i>evaluation</i> <i>examples</i> <i>needed</i> <i>by</i> <i>evaluating</i> <i>the</i>
<i>production</i> <i>model</i> <i>on</i> <i>subsets</i> <i>of</i> <i>varying</i> <i>sizes</i> <i>and</i> <i>tracking</i> <i>the</i> <i>variability</i> <i>of</i> <i>the</i> <i>evalua‐</i>
<i>tion</i> <i>metric</i> <i>by</i> <i>the</i> <i>size</i> <i>of</i> <i>the</i> <i>subset.</i> <i>Here,</i> <i>the</i> <i>standard</i> <i>deviation</i> <i>starts</i> <i>to</i> <i>plateau</i> <i>at</i>
<i>around</i> <i>2,000</i> <i>examples.</i>
From Figure 6-3, we see that the number of evaluation examples needs to be at least
2,000, and is ideally 3,000 or more. Let’s assume for the rest of this discussion that we
choose to evaluate on 2,500 examples.
The training set would contain the remaining 2,500 new examples (the amount of
new data available after withholding 2,500 for evaluation) augmented by some num‐
ber of older examples that have been bridged to match the new schema. How do we
know how many older examples we need? We don’t. This is a hyperparameter that
we will have to tune. For example, on the tip problem, using grid search, we see from"|Bridged Schema design pattern
"ImageNet dataset contains 1,000 labels, our resulting model will <i>only</i> return 8 possi‐
ble classes that we’ll specify, as opposed to the thousands of labels present in
ImageNet.
Loading a pre-trained model and using it to get classifications on
the <i>original</i> <i>labels</i> that model was trained on is not transfer learn‐
ing. Transfer learning is going one step further, replacing the final
layers of the model with your own prediction task.
The VGG model we’ve loaded will be our base model. We’ll need to add a few layers
to flatten the output of our bottleneck layer and feed this flattened output into an 8-
element softmax array:
global_avg_layer = tf.keras.layers.GlobalAveragePooling2D()
feature_batch_avg = global_avg_layer(feature_batch)
prediction_layer = tf.keras.layers.Dense(8, activation='softmax')
prediction_batch = prediction_layer(feature_batch_avg)
Sequential,
Finally, we can use the API to create our new transfer learning model as
a stack of layers:
histology_model = keras.Sequential([
vgg_model,
global_avg_layer,
prediction_layer
])
Let’s take note of the output of model.summary() on our transfer learning model:
_________________________________________________________________
Layer (type) Output Shape Param #
=================================================================
vgg19 (Model) (None, 4, 4, 512) 20024384
_________________________________________________________________
global_average_pooling2d (Gl (None, 512) 0
_________________________________________________________________
dense (Dense) (None, 8) 4104
=================================================================
Total params: 20,028,488
Trainable params: 4,104
Non-trainable params: 20,024,384
_________________________________________________________________
The important piece here is that the only trainable parameters are the ones <i>after</i> our
bottleneck layer. In this example, the bottleneck layer is the feature vectors from the
VGG model. After compiling this model, we can train it using our dataset of histol‐
ogy images."|bottleneck layer; Transfer Learning design pattern; VGG
"Examples of good heuristic benchmarks and situations where we might employ them
are shown in Table 7-1. Example code for the implementations of these heuristic
benchmarks is in the GitHub repository of this book.
<i>Table</i> <i>7-1.</i> <i>Heuristic</i> <i>benchmarks</i> <i>for</i> <i>a</i> <i>few</i> <i>selected</i> <i>scenarios</i> <i>(see</i> <i>code</i> <i>in</i> <i>GitHub)</i>
<b>Scenario</b> <b>Heuristicbenchmark</b> <b>Exampletask</b> <b>Implementationforexampletask</b>
Regressionproblemwhere Meanormedianvalueof Timeintervalbeforea Predictthatitwilltake2,120seconds
featuresandinteractions thelabelvalueoverthe questiononStack always.
betweenfeaturesarenot trainingdata. Overflowisanswered. 2,120secondsisthemediantimeto
wellunderstoodbythe Choosethemedianif firstanswerovertheentiretraining
business. therearealotofoutliers. dataset.
Binaryclassificationproblem Overallfractionof Whetherornotan Predict0.36astheoutputprobability
wherefeaturesand positivesinthetraining acceptedanswerin forallanswers.
interactionsbetween data. StackOverflowwillbe 0.36isthefractionofacceptedanswers
featuresarenotwell edited. overallthatareedited.
understoodbythebusiness.
Multilabelclassification Distributionofthelabel Countryfromwhicha Predict0.03forFrance,0.08forIndia,
problemwherefeaturesand valueoverthetraining StackOverflow andsoon.
interactionsbetween data. questionwillbe Thesearethefractionsofanswers
featuresarenotwell answered. writtenbypeoplefromFrance,India,
understoodbythebusiness. andsoon.
Regressionproblemwhere Linearregressionbased Predicttaxifare Fare=$4.64perkilometer.
thereisasingle,very onwhatis,intuitively, amountgivenpickup The$4.64iscomputedfromthe
important,numericfeature. thesinglemostimportant anddropofflocations. trainingdataoveralltrips.
feature. Thedistancebetween
thetwopointsis,
intuitively,akey
feature.
Regressionproblemwith Lookuptablewherethe Predictdurationof Lookuptableofaveragerentalduration
oneortwoimportant rowsandcolumns bicyclerental. fromeachstationbasedonpeakhour
features.Thefeaturescould correspondtothekey Here,thetwokey versusnonpeakhour.
benumericorcategorical features(discretizedif featuresarethe
butshouldbecommonly necessary)andthe stationthatthebicycle
usedheuristics. predictionforeachcellis isbeingrentedfrom
theaveragelabelinthat andwhetherornotit
cellestimatedoverthe ispeakhoursfor
trainingdata. commuting.
Classificationproblemwith Asabove,exceptthatthe Predictwhethera Foreachtag,computethefractionof
oneortwoimportant predictionforeachcellis StackOverflow questionsthatareansweredwithinone
features.Thefeaturescould thedistributionoflabels questionwillget day.
benumericorcategorical. inthatcell. answeredwithinone
Ifthegoalistopredicta day.
singleclass,computethe Themostimportant
modeofthelabelineach featurehereisthe
cell. primarytag."|binary classification; heuristic benchmark; Heuristic Benchmark design pattern; linear models; multilabel classification; regression models
"there is only one precisely calculable output. There is no overlap between different
examples in the training dataset. For this reason, we can toss out concerns about gen‐
eralization. We <i>want</i> our ML model to fit the training data as perfectly as possible, to
“overfit.”
This is counter to the typical approach of training an ML model where considerations
of bias, variance, and generalization error play an important role. Traditional training
says that it is possible for a model to learn the training data “too well,” and that train‐
ing your model so that the train loss function is equal to zero is more of a red flag
than cause for celebration. Overfitting of the training dataset in this way causes the
model to give misguided predictions on new, unseen data points. The difference here
is that we know in advance there won’t be unseen data, thus the model is approxi‐
mating a solution to a PDE over the full input spectrum. If your neural network is
able to learn a set of parameters where the loss function is zero, then that parameter
set determines the actual solution of the PDE in question.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
If all possible inputs can be tabulated, then as shown by the dotted curve in
Figure 4-4, an overfit model will still make the same predictions as the “true” model if
all possible input points are trained for. So overfitting is not a concern. We have to
take care that inferences are made on rounded-off values of the inputs, with the
rounding determined by the resolution with which the input space was gridded.
<i>Figure</i> <i>4-4.</i> <i>Overfitting</i> <i>is</i> <i>not</i> <i>a</i> <i>concern</i> <i>if</i> <i>all</i> <i>possible</i> <i>input</i> <i>points</i> <i>are</i> <i>trained</i> <i>for</i>
<i>because</i> <i>predictions</i> <i>are</i> <i>the</i> <i>same</i> <i>with</i> <i>both</i> <i>curves.</i>
Is it possible to find a model function that gets arbitrarily close to the true labels? One
bit of intuition as to why this works comes from the Uniform Approximation Theo‐
rem of deep learning, which, loosely put, states that any function (and its derivatives)
can be approximated by a neural network with at least one hidden layer and any
“squashing” activation function, like sigmoid. This means that no matter what
function we are given, so long as it’s relatively well behaved, there exists a neural"|Uniform Approximation Theorem; Useful Overfitting design pattern
"functional API. Next, we’ll create a concatenated layer, feed that into our output
layer, and finally create the model by passing in the original Input layers we defined
above:
merged_input = keras.layers.concatenate([embedding_layer, tabular_layer])
merged_dense = Dense(16)(merged_input)
output = Dense(1)(merged_dense)
model = Model(inputs=[embedding_input, tabular_input], outputs=output)
merged_dense = Dense(16, activation='relu')(merged_input)
output = Dense(1)(merged_dense)
model = Model(inputs=[embedding_input, tabular_input], outputs=output)
Now we have a single model that accepts the multimodal input.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
As we just saw, the Multimodal Input design pattern explores how to represent <i>differ‐</i>
<i>ent</i> <i>input</i> <i>formats</i> in the same model. In addition to mixing different <i>types</i> of data, we
may also want to represent the <i>same</i> <i>data</i> <i>in</i> <i>different</i> <i>ways</i> to make it easier for our
model to identify patterns. For example, we may have a ratings field that is on an
ordinal scale of 1 star to 5 stars, and treat that ratings field as both numeric and cate‐
gorical. Here, we are referring to <i>multimodal</i> <i>inputs</i> as both:
• Combining different types of data, like images + metadata
• Representing complex data in multiple ways
We’ll start by exploring how tabular data can be represented in different ways, and
then we’ll look at text and image data.
<b>Tabulardatamultipleways</b>
To see how we can represent tabular data in different ways for the same model, let’s
return to the restaurant review example. We’ll imagine instead that rating is an <i>input</i>
to our model and we’re trying to predict the review’s usefulness (how many people
liked the review). As an input, the rating can be represented both as an integer value
ranging from 1 to 5 and as a categorical feature. To represent rating categorically, we
can bucket it. The way we bucket the data is up to us and dependent on our dataset
and use case. To keep things simple, let’s say we want to create two buckets: “good”
and “bad.” The “good” bucket includes ratings of 4 and 5, and “bad” includes 3 and
below. We can then create a boolean value to encode the rating buckets and concate‐
nate both the integer and boolean into a single array (full code is on GitHub)."|Embedding design pattern; Keras; Multimodal Input design pattern; multimodal inputs; tabular data
"<header><largefont><b>Training</b></largefont> <largefont><b>Design</b></largefont> <largefont><b>Patterns</b></largefont></header>
The design patterns covered in this chapter all have to do with modifying the typical
training loop in some way. In <i>Useful</i> <i>Overfitting,</i> we forgo the use of a validation or
testing dataset because we want to intentionally overfit on the training dataset. In
<i>Checkpoints,</i> we store the full state of the model periodically, so that we have access to
partially trained models. When we use checkpoints, we usually also use <i>virtual</i>
fit()
<i>epochs,</i> wherein we decide to carry out the inner loop of the function, not on
the full training dataset but on a fixed number of training examples. In <i>Transfer</i>
<i>Learning,</i> we take part of a previously trained model, freeze the weights, and incorpo‐
rate these nontrainable layers into a new model that solves the same problem, but on
a smaller dataset. In <i>Distribution</i> <i>Strategy,</i> the training loop is carried out at scale over
multiple workers, often with caching, hardware acceleration, and parallelization.
Finally, in <i>Hyperparameter</i> <i>Tuning,</i> the training loop is itself inserted into an optimi‐
zation method to find the optimal set of model hyperparameters.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>11:</b></largefont> <largefont><b>Useful</b></largefont> <largefont><b>Overfitting</b></largefont></header>
Useful Overfitting is a design pattern where we forgo the use of generalization mech‐
anisms because we want to intentionally overfit on the training dataset. In situations
where overfitting can be beneficial, this design pattern recommends that we carry out
machine learning without regularization, dropout, or a validation dataset for early
stopping.
<header><largefont><b>Problem</b></largefont></header>
The goal of a machine learning model is to generalize and make reliable predictions
on new, unseen data. If your model <i>overfits</i> the training data (for example, it contin‐
ues to decrease the training error beyond the point at which validation error starts to
increase), then its ability to generalize suffers and so do your future predictions.
Introductory machine learning textbooks advise avoiding overfitting by using early
stopping and regularization techniques.
Consider, however, a situation of simulating the behavior of physical or dynamical
systems like those found in climate science, computational biology, or computational
finance. In such systems, the time dependence of observations can be described by a
mathematical function or set of partial differential equations (PDEs). Although the
equations that govern many of these systems can be formally expressed, they don’t
have a closed-form solution. Instead, classical numerical methods have been devel‐
oped to approximate solutions to these systems. Unfortunately, for many real-world
applications, these methods can be too slow to be used in practice."|PDE; regularization; training loop; Useful Overfitting design pattern
"gestation_weeks,
weight
<b>FROM</b>
`project.dataset.baby_weight`
Another approach is to use the Cascade pattern, training three separate regression
models for each class. Then, we can use our multidesign pattern solution by passing
our initial classification model an example and using the result of that classification
to decide which regression model to send the example to for numeric prediction.
<b>Anomalydetection</b>
There are two approaches to handling regression models for imbalanced datasets:
• Use the model’s error on a prediction as a signal.
• Cluster incoming data and compare the distance of each new data point to exist‐
ing clusters.
To better understand each solution, let’s say we’re training a model on data collected
by a sensor to predict temperature in the future. In this case, we’d need the model
output to be a numerical value.
For the first approach—using error as a signal—after training a model, we would
then compare the model’s predicted value with the actual value for the current point
in time. If there was a significant difference between the predicted and actual current
value, we could flag the incoming data point as an anomaly. Of course, this requires a
model trained with good accuracy on enough historical data to rely on its quality for
future predictions. The main caveat for this approach is that it requires us to have
new data readily available, so that we can compare the incoming data with the
model’s prediction. As a result, it works best for problems involving streaming or
time-series data.
In the second approach—clustering data—we start by building a model with a clus‐
tering algorithm, a modeling technique that organizes our data into clusters. Cluster‐
ing is an <i>unsupervised</i> <i>learning</i> method, meaning it looks for patterns in the dataset
without any knowledge of ground truth labels. A common clustering algorithm is k-
means, which we can implement with BigQuery ML. The following shows how to
train a k-means model on the BigQuery natality dataset using three features:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL
`project-name.dataset-name.baby_weight` <b>OPTIONS(model_type='kmeans',</b>
num_clusters=4) <b>AS</b>
<b>SELECT</b>
weight_pounds,
mother_age,
gestation_weeks
<b>FROM</b>"|anomaly detection; BigQuery ML; cascade; Rebalancing design pattern; reframing; unsupervised learning
"operations around training and deploying those models. ML engineers help build
production systems to handle updating models, model versioning, and serving pre‐
dictions to end users.
The smaller the data science team at a company and the more agile the team is, the
more likely it is that the same person plays multiple roles. If you are in such a situa‐
tion, it is very likely that you read the above three descriptions and saw yourself parti‐
ally in all three categories. You might commonly start out a machine learning project
as a data engineer and build data pipelines to operationalize the ingest of data. Then,
you transition to the data scientist role and build the ML model(s). Finally, you put
on the ML engineer hat and move the model to production. In larger organizations,
machine learning projects may move through the same phases, but different teams
might be involved in each phase.
Research scientists, data analysts, and developers may also build and use AI models,
but these job roles are not a focus audience for this book.
<i>Research</i> <i>scientists</i> focus primarily on finding and developing new algorithms to
advance the discipline of ML. This could include a variety of subfields within
machine learning, like model architectures, natural language processing, computer
vision, hyperparameter tuning, model interpretability, and more. Unlike the other
roles discussed here, research scientists spend most of their time prototyping and
evaluating new approaches to ML, rather than building out production ML systems.
<i>Data</i> <i>analysts</i> evaluate and gather insights from data, then summarize these insights
for other teams within their organization. They tend to work in SQL and spread‐
sheets, and use business intelligence tools to create data visualizations to share their
findings. Data analysts work closely with product teams to understand how their
insights can help address business problems and create value. While data analysts
focus on identifying trends in existing data and deriving insights from it, data scien‐
tists are concerned with using that data to generate future predictions and in auto‐
mating or scaling out the generation of insights. With the increasing democratization
of machine learning, data analysts can upskill themselves to become data scientists.
<i>Developers</i> are in charge of building production systems that enable end users to
access ML models. They are often involved in designing the APIs that query models
and return predictions in a user-friendly format via a web or mobile application. This
could involve models hosted in the cloud, or models served on-device. Developers
utilize the model serving infrastructure implemented by ML Engineers to build appli‐
cations and user interfaces for surfacing predictions to model users.
Figure 1-2 illustrates how these different roles work together throughout an organiza‐
tion’s machine learning model development process."|data analysts; developers; research scientists; roles
"similar words together, identify relationships between words, and understand syntac‐
tic elements of text. While representing text through word embeddings most closely
mirrors how humans innately understand language, there are additional text repre‐
sentations that can maximize our model’s ability to perform a given prediction task.
In this section, we’ll look at the bag of words approach to representing text, along
with extracting tabular features from text.
To demonstrate text data representation, we’ll be referencing a dataset that contains
the text of millions of questions and answers from Stack Overflow,11 along with
metadata about each post. For example, the following query will give us a subset of
questions tagged as either “keras,” “matplotlib,” or “pandas,” along with the number
of answers each question received:
<b>SELECT</b>
title,
answer_count,
<b>REPLACE(tags,</b> ""|"", "","") <b>as</b> tags
<b>FROM</b>
`bigquery-public-data.stackoverflow.posts_questions`
<b>WHERE</b>
REGEXP_CONTAINS( tags, r""(?:keras|matplotlib|pandas)"")
The query results in the following output:
<b>Row</b> <b>title</b> <b>answer_count</b> <b>tags</b>
1 Buildinganewcolumninapandas 6 python,python-2.7,pandas,replace,nested-loops
dataframebymatchingstringvaluesina
list
2 Extractingspecificselectedcolumnstonew 6 python,pandas,chained-assignment
DataFrameasacopy
3 WheredoIcalltheBatchNormalization 7 python,keras,neural-network,data-
functioninKeras? science,batch-normalization
4 UsingExcellikesolverinPythonorSQL 8 python,sql,numpy,pandas,solver
When representing text using the bag of words (BOW) approach, we imagine each
text input to our model as a bag of Scrabble tiles, with each tile containing a single
word instead of a letter. BOW does not preserve the order of our text, but it does
detect the presence or absence of certain words in each piece of text we send to our
model. This approach is a type of multi-hot encoding where each text input is con‐
verted into an array of 1s and 0s. Each index in this BOW array corresponds to a
word from our vocabulary.
11 ThisdatasetisavailableinBigQuery:bigquery-public-data.stackoverflow.posts_questions."|BOW encoding; Multimodal Input design pattern; Stack Overflow; vocabulary
"<header><largefont><b>Scale</b></largefont></header>
The challenge of scaling is present throughout many stages of a typical machine
learning workflow. You’ll likely encounter scaling challenges in data collection and
preprocessing, training, and serving. When ingesting and preparing data for a
machine learning model, the size of the dataset will dictate the tooling required for
your solution. It is often the job of data engineers to build out data pipelines that can
scale to handle datasets with millions of rows.
For model training, ML engineers are responsible for determining the necessary
infrastructure for a specific training job. Depending on the type and size of the data‐
set, model training can be time consuming and computationally expensive, requiring
infrastructure (like GPUs) designed specifically for ML workloads. Image models, for
instance, typically require much more training infrastructure than models trained
entirely on tabular data.
In the context of model serving, the infrastructure required to support a team of data
scientists getting predictions from a model prototype is entirely different from the
infrastructure necessary to support a production model getting millions of prediction
requests every hour. Developers and ML engineers are typically responsible for han‐
dling the scaling challenges associated with model deployment and serving prediction
requests.
Most of the ML patterns in this book are useful without regard to organizational
maturity. However, several of the patterns in Chapters 6 and 7 address resilience and
reproducibility challenges in different ways, and the choice between them will often
come down to the use case and the ability of your organization to absorb complexity.
<header><largefont><b>Multiple</b></largefont> <largefont><b>Objectives</b></largefont></header>
Though there is often a single team responsible for building a machine learning
model, many teams across an organization will make use of the model in some way.
Inevitably, these teams may have different ideas of what defines a successful model.
To understand how this may play out in practice, let’s say you’re building a model to
identify defective products from images. As a data scientist, your goal may be to min‐
imize your model’s cross-entropy loss. The product manager, on the other hand, may
want to reduce the number of defective products that are misclassified and sent to
customers. Finally, the executive team’s goal might be to increase revenue by 30%.
Each of these goals vary in what they are optimizing for, and balancing these differing
needs within an organization can present a challenge.
As a data scientist, you could translate the product team’s needs into the context of
your model by saying false negatives are five times more costly than false positives.
Therefore, you should optimize for recall over precision to satisfy this when"|data engineers; data scientists; developers; ML engineers; scaling
"To see just how much data is required to train high-accuracy models, we can look at
ImageNet, a database of over 14 million labeled images. ImageNet is frequently used
as a benchmark for evaluating machine learning frameworks on various hardware. As
an example, the MLPerf benchmark suite uses ImageNet to compare the time it took
for various ML frameworks running on different hardware to reach 75.9% classifica‐
tion accuracy. In the v0.7 MLPerf Training results, a TensorFlow model running on a
Google TPU v3 took around 30 seconds to reach this target accuracy. 2 With more
training time, models can reach even higher accuracy on ImageNet. However, this is
largely due to ImageNet’s size. Most organizations with specialized prediction prob‐
lems don’t have nearly as much data available.
Because use cases like the image and text examples described above involve particu‐
larly specialized data domains, it’s also not possible to use a general-purpose model to
successfully identify bone fractures or diagnose diseases. A model that is trained on
ImageNet might be able to label an x-ray image as <i>x-ray</i> or <i>medical</i> <i>imaging</i> but is
unlikely to be able to label it as a <i>broken</i> <i>femur.</i> Because such models are often trained
on a wide variety of high-level label categories, we wouldn’t expect them to under‐
stand conditions present in the images that are specific to our dataset. To handle this,
we need a solution that allows us to build a custom model using only the data we
have available and with the labels that we care about.
<header><largefont><b>Solution</b></largefont></header>
With the Transfer Learning design pattern, we can take a model that has been trained
on the same type of data for a similar task and apply it to a specialized task using our
own custom data. By “same type of data,” we mean the same data modality—images,
text, and so forth. Beyond just the broad category like images, it is also ideal to use a
model that has been pre-trained on the same types of images. For example, use a
model that has been pre-trained on photographs if you are going to use it for photo‐
graph classification and a model that has been pre-trained on remotely sensed
imagery if you are going to use it to classify satellite images. By <i>similar</i> <i>task,</i> we’re
referring to the problem being solved. To do transfer learning for image classifica‐
tion, for example, it is better to start with a model that has been trained for image
classification, rather than object detection.
Continuing with the example, let’s say we’re building a binary classifier to determine
whether an image of an x-ray contains a broken bone. We only have 200 images of
each class: <i>broken</i> and <i>not</i> <i>broken.</i> This isn’t enough to train a high-quality model
from scratch, but it is sufficient for transfer learning. To solve this with transfer
learning, we’ll need to find a model that has already been trained on a large dataset to
2 MLPerfv0.7TrainingClosedResNet.Retrievedfromwww.mlperf.org23September2020,entry0.7-67.
MLPerfnameandlogoaretrademarks.Seewww.mlperf.orgformoreinformation."|GPU; ImageNet; Transfer Learning design pattern
"<i>Figure</i> <i>2-6.</i> <i>The</i> <i>tokenizer</i> <i>creates</i> <i>a</i> <i>lookup</i> <i>table</i> <i>that</i> <i>maps</i> <i>each</i> <i>word</i> <i>to</i> <i>an</i> <i>index.</i>
The tokenization is a lookup table that maps each word in our vocabulary to an
index. We can think of this as a one-hot encoding of each word where the tokenized
index is the location of the nonzero element in the one-hot encoding. This requires a
full pass over the entire dataset (let’s assume these consist of titles of articles 4 ) to cre‐
ate the lookup table and can be done in Keras. The complete code can be found in the
repository for this book:
<b>from</b> <b>tensorflow.keras.preprocessing.text</b> <b>import</b> Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(titles_df.title)
Here we can use the Tokenizer class in the <i>keras.preprocessing.text</i> library. The call to
fit_on_texts
creates a lookup table that maps each of the words found in our titles
to an index. By calling tokenizer.index_word, we can examine this lookup table
directly:
tokenizer.index_word
{1: 'the',
2: 'a',
3: 'to',
4: 'for',
5: 'in',
6: 'of',
7: 'and',
8: 's',
9: 'on',
10: 'with',
11: 'show',
...
4 ThisdatasetisavailableinBigQuery:bigquery-public-data.hacker_news.stories."|Embedding design pattern; Keras; text embeddings; tokenization
"Before proceeding with the tools outlined in this section, it’s worth
analyzing both the dataset and prediction task to determine
whether there is potential for problematic bias. This requires look‐
ing closer at <i>who</i> will be impacted by a model, and <i>how</i> those
groups will be impacted. If problematic bias seems likely, the tech‐
nical approaches outlined in this section provide a good starting
point for mitigating this type of bias. If, on the other hand, the
skew in the dataset contains naturally occurring bias that will not
have adverse effects on different groups of people, “Design Pattern
10: Rebalancing ” on page 122 in Chapter 3 provides solutions for
handling data that is inherently imbalanced.
Throughout this section, we’ll be referencing a public dataset of US mortgage appli‐
cations. Loan agencies in the US are required to report information on an individual
application, like the type of loan, the applicant’s income, the agency handling the
loan, and the status of the application. We will train a loan application approval
model on this dataset in order to demonstrate different aspects of fairness. To our
knowledge, this dataset is not used as is by any loan agency to train ML models, and
so the fairness red flags we raise are only hypothetical.
We’ve created a subset of this dataset and done some preprocessing to turn this into a
binary classification problem—whether an application was approved or denied. In
Figure 7-9, we can see a preview of the dataset.
<i>Figure</i> <i>7-9.</i> <i>A</i> <i>preview</i> <i>of</i> <i>a</i> <i>few</i> <i>columns</i> <i>from</i> <i>the</i> <i>US</i> <i>mortgage</i> <i>application</i> <i>dataset</i> <i>refer‐</i>
<i>enced</i> <i>throughout</i> <i>this</i> <i>section.</i>
<b>Beforetraining</b>
Because ML models are a direct representation of the data used to train them, it’s
possible to mitigate a significant amount of bias <i>before</i> building or training a model
by performing thorough data analysis, and using the results of this analysis to adjust
our data. In this phase, focus on identifying data collection or data representation
bias, outlined in the Problem section. Table 7-3 shows some questions to consider for
each type of bias depending on data type."|bias; Fairness Lens design pattern; problematic bias
"<i>Figure</i> <i>6-16.</i> <i>A</i> <i>feature</i> <i>store</i> <i>ensures</i> <i>the</i> <i>feature</i> <i>engineering</i> <i>pipelines</i> <i>are</i> <i>consistent</i>
<i>between</i> <i>model</i> <i>training</i> <i>and</i> <i>serving.</i> <i>See</i> <i>also</i> <i>https://docs.feast.dev/.</i>
<i>Figure</i> <i>6-17.</i> <i>Feast</i> <i>uses</i> <i>Beam</i> <i>on</i> <i>the</i> <i>backend</i> <i>for</i> <i>feature</i> <i>ingestion</i> <i>and</i> <i>Redis</i> <i>and</i> <i>Big‐</i>
<i>Query</i> <i>for</i> <i>online</i> <i>and</i> <i>offline</i> <i>feature</i> <i>retrieval.</i>
Different systems may produce data at different rates, and a feature store is flexible
enough to handle those different cadences, both for ingestion and during retrieval
(Figure 6-18). For example, sensor data could be produced in real time, arriving every"|Feast; Feature Store design pattern; low latency
"when a user chooses one of the proposed alternate routes in Google Maps, the chosen
route serves as an implicit ground truth. More explicitly, when a user rates a recom‐
mended movie, this is a clear indication of the ground truth for a model that is built
to predict user ratings in order to surface recommendations. Similarly, if the model
allows the user to change the prediction, for example, as in medical settings when a
doctor is able to change a model’s suggested diagnosis, this provides a clear signal for
the ground truth.
It is important to keep in mind how the feedback loop of model
predictions and capturing ground truth might affect training data
down the road. For example, suppose you’ve built a model to pre‐
dict when a shopping cart will be abandoned. You can even check
the status of the cart at routine intervals to create ground truth
labels for model evaluation. However, if your model suggests a user
will abandon their shopping cart and you offer them free shipping
or some discount to influence their behavior, then you’ll never
know if the original model prediction was correct. In short, you’ve
violated the assumptions of the model evaluation design and will
need to determine ground truth labels some other way. This task of
estimating a particular outcome under a different scenario is
referred to as counterfactual reasoning and often arises in use cases
like fraud detection, medicine, and advertising where a model’s
predictions likely lead to some intervention that can obscure learn‐
ing the actual ground truth for that example.
<b>Evaluatingmodelperformance</b>
Initially, the groundtruth column of the txtcls_eval.swivel table in BigQuery is
left empty. We can provide the ground truth labels once they are available by updat‐
ing the value directly with a SQL command. Of course, we should make sure the
ground truth is available before we run an evaluation job. Note that the ground truth
adheres to the same JSON structure as the prediction output from the model:
<b>UPDATE</b>
txtcls_eval.swivel
<b>SET</b>
groundtruth = '{""predictions"": [{""source"": ""techcrunch""}]}'
<b>WHERE</b>
raw_data = '{""instances"":
[{""text"": ""YouTube introduces Video Chapters to help navigate longer
videos""}]}'
To update more rows, we’d use a MERGE statement instead of an UPDATE . Once the
ground truth has been added to the table, it’s possible to easily examine the text input
and your model’s prediction and compare with the ground truth as in Table 5-2:"|Continued Model Evaluation design pattern; counterfactual reasoning; fraud detection; ground truth label
"<i>Figure</i> <i>6-8.</i> <i>Output</i> <i>of</i> <i>the</i> <i>schema_gen</i> <i>component</i> <i>for</i> <i>an</i> <i>ML</i> <i>pipeline.</i> <i>The</i> <i>top</i> <i>menu</i>
<i>bar</i> <i>shows</i> <i>the</i> <i>data</i> <i>available</i> <i>for</i> <i>each</i> <i>individual</i> <i>pipeline</i> <i>step.</i>
One advantage of building a pipeline with TFX or Kubeflow Pipe‐
lines is that we are not locked into Google Cloud. We can run the
same code we’re demonstrating here with Google’s AI Platform
Pipelines on Azure ML Pipelines, Amazon SageMaker, or on-
premises.
To implement a training step in TFX, we’ll use the Trainer component and pass it
information on the training data to use as model input, along with our model train‐
ing code. TFX provides an extension for running the training step on AI Platform
tfx.extensions.google_cloud_ai_platform.trainer
that we can use by importing
and providing details on our AI Platform training configuration. This includes our
project name, region, and GCR location of the container with training code.
Pusher
Similarly, TFX also has an AI Platform component for deploying trained
models to AI Platform Prediction. In order to use the Pusher component with AI
Platform, we provide details on the name and version of our model, along with a
serving function that tells AI Platform the format of input data it should expect for
our model. With that, we have a complete pipeline that ingests data, analyzes it, runs
data transformation, and finally trains and deploys the model using AI Platform.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Without running our ML code as a pipeline, it would be difficult for others to reliably
reproduce our work. They’d need to take our preprocessing, model development,
training, and serving code and try to replicate the same environment where we ran it
while taking into account library dependencies, authentication, and more. If there is
logic controlling the selection of downstream components based on the output of
upstream components, that logic will also have to be reliably replicated. The Work‐"|AI Platform Pipelines; AI Platform Prediction; AI Platform pusher component; Azure ML Pipelines; containers; Kubeflow Pipelines; SageMaker; TFX; Trainer component; Workflow Pipeline design pattern
"received a particular score. Was it a late payment? Too many lines of credit? Short
credit history? Maybe the model is relying solely on demographic data to make its
predictions, and subsequently introducing bias into the model without our knowl‐
edge. With only the score, there is no way to know how the model arrived at its
prediction.
In addition to model end users, another group of stakeholders are those involved
with regulatory and compliance standards for ML models, since models in certain
industries may require auditing or additional transparency. Stakeholders involved in
auditing models will likely need a higher-level summary of how the model is arriving
at its predictions in order to justify its use and impact. Metrics like accuracy are not
useful in this case—without insight into <i>why</i> a model makes the predictions it does,
its use may become problematic.
Finally, as data scientists and ML engineers, we can only improve our model quality
to a certain degree without an understanding of the features it’s relying on to make
predictions. We need a way to verify that models are performing in the way we
expect. For example, let’s say we are training a model on tabular data to predict
whether a flight will be delayed. The model is trained on 20 features. Under the hood,
maybe it’s relying only on 2 of those 20 features, and if we removed the rest, we could
significantly improve our system’s performance. Or maybe each of those 20 features
is necessary to achieve the degree of accuracy we need. Without more details on what
the model is using, it’s difficult to know.
<header><largefont><b>Solution</b></largefont></header>
To handle the inherent unknowns in ML, we need a way to understand how models
work under the hood. Techniques for understanding and communicating how and
why an ML model makes predictions is an area of active research. Also called inter‐
pretability or model understanding, explainability is a new and rapidly evolving field
within ML, and can take a variety of forms depending on a model’s architecture and
the type of data it is trained on. Explainability can also help reveal bias in ML models,
which we cover when discussing the Fairness Lens pattern in this chapter. Here, we’ll
focus on explaining deep neural networks using feature attributions. To understand
this in context, first we’ll look at explainability for models with less complex
architectures.
Simpler models like decision trees are more straightforward to explain than deep
models since they are often <i>interpretable</i> <i>by</i> <i>design.</i> This means that their learned
weights provide direct insight into how the model is making predictions. If we have a
linear regression model with independent, numeric input features, the weights may
sometimes be interpretable. Take for example a linear regression model that predicts"|data scientists; decision trees; explainability; Explainable Predictions design pattern; interpretable by design; ML engineers; stakeholders
"advantageous for your model to also pass through a client-supplied key. This is called
the Keyed Predictions design pattern, and it is a necessity to scalably implement sev‐
eral of the design patterns discussed in this chapter.
<header><largefont><b>Problem</b></largefont></header>
If your model is deployed as a web service and accepts a single input, then it is quite
clear which output corresponds to which input. But what if your model accepts a file
with a million inputs and sends back a file with a million output predictions?
You might think that it should be obvious that the first output instance corresponds
to the first input instance, the second output instance to the second input instance,
etc. However, with a 1:1 relationship, it is necessary for each server node to process
the full set of inputs serially. It would be much more advantageous if you use a dis‐
tributed data processing system and farm out instances to multiple machines, collect
all the resulting outputs, and send them back. The problem with this approach is that
the outputs are going to be jumbled. Requiring that the outputs be ordered the same
way poses scalability challenges, and providing the outputs in an unordered manner
requires the clients to somehow know which output corresponds to which input.
This same problem occurs if your online serving system accepts an array of instances
as discussed in the Stateless Serving Function pattern. The problem is that processing
a large number of instances locally will lead to hot spots. Server nodes that receive
only a few requests will be able to keep up, but any server node that receives a partic‐
ularly large array will start to fall behind. These hot spots will force you to make your
server machines more powerful than they need to be. Therefore, many online serving
systems will impose a limit on the number of instances that can be sent in one
request. If there is no such limit, or if the model is so computationally expensive that
requests with fewer instances than this limit can overload the server, you will run into
the problem of hot spots. Therefore, any solution to the batch serving problem will
also address the problem of hot spots in online serving.
<header><largefont><b>Solution</b></largefont></header>
The solution is to use pass-through keys. Have the client supply a key associated with
each input. For example (see Figure 5-13), suppose your model is trained with three
inputs (a, b, c), shown on the left, to produce the output d, shown on the right. Make
your clients supply (k, a, b, c) to your model where k is a key with a unique identifier.
The key could be as simple as numbering the input instances 1, 2, 3, …, etc. Your
model will then return (k, d), and so the client will be able to figure out which output
instance corresponds to which input instance."|Keyed Predictions design pattern; keys
"database?1
model to every complaint in the CFPB We can simply adapt the query
above, making sure to alias the consumer_complaint_narrative column in the inner
SELECT as the reviews to be assessed:
<b>SELECT</b> * <b>FROM</b> ML.PREDICT(MODEL mlpatterns.imdb_sentiment,
(SELECT consumer_complaint_narrative <b>AS</b> reviews
<b>FROM</b> `bigquery-public-data`.cfpb_complaints.complaint_database
<b>WHERE</b> consumer_complaint_narrative <b>IS</b> <b>NOT</b> <b>NULL</b>
)
)
The database has more than 1.5 million complaints, but they get processed in about
30 seconds, proving the benefits of using a distributed data processing framework.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
The Stateless Serving Function design pattern is set up for low-latency serving to sup‐
port thousands of simultaneous queries. Using such a framework for occasional or
periodic processing of millions of items can get quite expensive. If these requests are
not latency-sensitive, it is more cost effective to use a distributed data processing
architecture to invoke machine learning models on millions of items. The reason is
that invoking an ML model on millions of items is an embarrassingly parallel prob‐
lem—it is possible to take the million items, break them down into 1,000 groups of
1,000 items each, send each group of items to a machine, then combine the results.
The result of the machine learning model on item number 2,000 is completely inde‐
pendent of the result of the machine learning model on item number 3,000, and so it
is possible to divide up the work and conquer it.
Take, for example, the query to find the five most positive complaints:
<b>WITH</b> all_complaints <b>AS</b> (
<b>SELECT</b> * <b>FROM</b> ML.PREDICT(MODEL mlpatterns.imdb_sentiment,
(SELECT consumer_complaint_narrative <b>AS</b> reviews
<b>FROM</b> `bigquery-public-data`.cfpb_complaints.complaint_database
<b>WHERE</b> consumer_complaint_narrative <b>IS</b> <b>NOT</b> <b>NULL</b>
)
)
)
<b>SELECT</b> * <b>FROM</b> all_complaints
<b>ORDER</b> <b>BY</b> positive_review_probability <b>DESC</b> <b>LIMIT</b> 5
1 Curiouswhata“positive”complaintlookslike?Hereyougo:
“IgetphonecallsmorningXXXXandnight.Ihavetoldthemtostopsomanycallsbuttheystillcallevenon
Sundayinthemorning.IhadtwocallsinarowonaSundaymorningfromXXXXXXXX.Ireceivednine
callsonSaturday.Ireceiveaboutnineduringtheweekdayeverydayaswell.
Theonlyhintthatthecomplainerisunhappyisthattheyhaveaskedthecallerstostop.Otherwise,therestof
thestatementsmightwellbeaboutsomeonebraggingabouthowpopulartheyare!”"|Batch Serving design pattern; low latency
"Dataflow). It is possible to take a TensorFlow model and import it into BigQuery for
batch serving. It is also possible to take a trained BigQuery ML model and export it as
a TensorFlow SavedModel for online serving. This two-way compatibility enables
users of Google Cloud to hit any point in the spectrum of latency–hroughput
trade-off.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>18:</b></largefont> <largefont><b>Continued</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>Evaluation</b></largefont></header>
The Continued Model Evaluation design pattern handles the common problem of
needing to detect and take action when a deployed model is no longer fit-for-
purpose.
<header><largefont><b>Problem</b></largefont></header>
So, you’ve trained your model. You collected the raw data, cleaned it up, engineered
features, created embedding layers, tuned hyperparameters, the whole shebang.
You’re able to achieve 96% accuracy on your hold-out test set. Amazing! You’ve even
gone through the painstaking process of deploying your model, taking it from a
Jupyter notebook to a machine learning model in production, and are serving predic‐
tions via a REST API. Congratulations, you’ve done it. You’re finished!
Well, not quite. Deployment is not the end of a machine learning model’s life cycle.
How do you know that your model is working as expected in the wild? What if there
are unexpected changes in the incoming data? Or the model no longer produces
accurate or useful predictions? How will these changes be detected?
The world is dynamic, but developing a machine learning model usually creates a
static model from historical data. This means that once the model goes into produc‐
tion, it can start to degrade and its predictions can grow increasingly unreliable. Two
of the main reasons models degrade over time are concept drift and data drift.
Concept drift occurs whenever the relationship between the model inputs and target
have changed. This often happens because the underlying assumptions of your model
have changed, such as models trained to learn adversarial or competitive behavior
like fraud detection, spam filters, stock market trading, online ad bidding, or cyberse‐
curity. In these scenarios, a predictive model aims to identify patterns that are char‐
acteristic of desired (or undesired) activity, while the adversary learns to adapt and
may modify their behavior as circumstances change. Think for example of a model
developed to detect credit card fraud. The way people use credit cards has changed
over time and thus the common characteristics of credit card fraud have also
changed. For instance, when “Chip and Pin” technology was introduced, fraudulent
transactions began to move more online. As fraudulent behavior adapted, the perfor‐
mance of a model that had been developed before this technology would suddenly
begin to suffer and model predictions would be less accurate."|batch serving; Batch Serving design pattern; concept drift; Continued Model Evaluation design pattern; data drift; fraud detection
"<i>Figure</i> <i>6-15.</i> <i>Feature</i> <i>data</i> <i>can</i> <i>be</i> <i>retrieved</i> <i>either</i> <i>offline,</i> <i>using</i> <i>historical</i> <i>features</i> <i>for</i>
<i>model</i> <i>training,</i> <i>or</i> <i>online,</i> <i>for</i> <i>serving.</i>
These deployments are accessed via a separate online and batch client:
_feast_online_client = Client(serving_url='localhost:6566')
_feast_batch_client = Client(serving_url='localhost:6567',
core_url='localhost:6565')
<b>Batchserving.</b>
For training a model, historical feature retrieval is backed by BigQuery
.get_batch_features(...)
and accessed using with the batch serving client. In this
case, we provide Feast with a pandas dataframe containing the entities and time‐
stamps that feature data will be joined to. This allows Feast to produce a point-in-
time correct dataset based on the features that have been requested:
<i>#</i> <i>Create</i> <i>a</i> <i>entity</i> <i>df</i> <i>of</i> <i>all</i> <i>entities</i> <i>and</i> <i>timestamps</i>
<b>entity_df</b> = pd.DataFrame(
{
""datetime"": taxi_df.datetime,
""taxi_id"": taxi_df.taxi_id,
}
)
To retrieve historical features, the features in the feature set are referenced by the fea‐
ture set name and the feature name, separated by a colon—for example,
taxi_rides:pickup_lat
:
FS_NAME = taxi_rides
model_features = ['pickup_lat',
'pickup_lon',
'dropoff_lat',
'dropoff_lon',
'num_pass',"|Feast; Feature Store design pattern
"mind. Is it a tennis shoe? Loafer? Flip flop? What about a stiletto? Let’s imagine that
we live in a climate that is warm year-round and most of the people we know wear
sandals all the time. When we think of a shoe, a sandal is the first thing that comes to
mind. As a result, we collect a diverse representation of sandal images with different
types of straps, sole thicknesses, colors, and more. We contribute these to the larger
clothing dataset, and when we test the model on a test set of images of our friend’s
shoes, it reaches 95% accuracy on the “shoe” label. The model looks promising, but
problems arise when our colleagues from different locations test the model on images
of their heels and sneakers. For their images, the label “shoe” is not returned at all.
This shoe example demonstrates bias in the training data distribution, and although
it may seem oversimplified, this type of bias occurs frequently in production settings.
Data distribution bias happens when the data we collect doesn’t accurately reflect the
entire population who will use our model. If our dataset is human-centered, this type
of bias can be especially evident if our dataset fails to include an equal representation
of ages, races, genders, religions, sexual orientations, and other identity characteris‐
tics.8
Even when our dataset does appear balanced with respect to these identity character‐
istics, it is still subject to bias in the way these groups are represented in the data.
Suppose we are training a sentiment analysis model to classify restaurant reviews on a
scale of 1 (extremely negative) to 5 (extremely positive). We’ve taken care to get a bal‐
anced representation of different types of restaurants in the data. However, it turns
out that the majority of reviews for seafood restaurants are positive, whereas most of
the vegetarian restaurant reviews are negative. This data representation bias will be
directly represented by our model. Whenever new reviews are added for vegetarian
restaurants, they’ll have a much higher chance of being classified as negative, which
could then influence someone’s likelihood to visit one of these restaurants in the
future. This is also known as <i>reporting</i> <i>bias,</i> since the dataset (here, the “reported”
data) doesn’t accurately reflect the real world.
A common fallacy when dealing with data bias issues is that removing the areas of
bias from a dataset will fix the problem. Let’s say we’re building a model to predict
the likelihood someone will default on a loan. If we find the model is treating people
of different races unfairly, we might assume this could be fixed by simply removing
race as a feature from the dataset. The problem with this is that, due to systemic bias,
characteristics like race and gender are often reflected implicitly in other features like
zip code or income. This is known as <i>implicit</i> or <i>proxy</i> <i>bias.</i> Removing obvious fea‐
8 Foramoredetailedlookonhowraceandgenderbiascanfindtheirwayintoimageclassificationmodels,see
JoyBuolamwiniandTimmitGebru,“GenderShades:IntersectionalAccuracyDisparitiesinCommercial
GenderClassification”,ProceedingsofMachineLearningResearch81(2018):1-15."|bias; data distribution bias; Fairness Lens design pattern; problematic bias; reporting bias
"• Rows at the same date tend to be correlated—again, this is the key reason why we
want to ensure that all rows on the same date are in the same split.
date
• is not an input to the model even though it is used as a criteria for splitting.
Features extracted from date such as day of week or hour of day can be inputs,
but we can’t use an actual input as the field with which to split because the
date
trained model will not have seen 20% of the possible input values for the
column if we use 80% of the data for training.
date
• There have to be enough values. Since we are computing the hash and find‐
ing the modulo with respect to 10, we need at least 10 unique hash values. The
more unique values we have, the better. To be safe, a rule of thumb is to shoot for
3–5× the denominator for the modulo, so in this case, we want 40 or so unique
dates.
• The label has to be well distributed among the dates. If it turns out that all the
delays happened on January 1 and there were no delays the rest of the year, this
wouldn’t work since the split datasets will be skewed. To be safe, look at a graph
and make sure that all three splits have a similar distribution of labels. To be
extra safe, ensure that the distributions of label by departure delay and other
input values are similar across the three datasets.
We can automate checking whether the label distributions are sim‐
ilar across the three datasets by using the Kolomogorov–Smirnov
test: just plot the cumulative distribution functions of the label in
the three datasets and find the maximum distance between each
pair. The smaller the maximum distance, the better the split.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Let’s look at a couple of variants of how we might do repeatable splitting and discuss
the pros and cons of each. Let’s also examine how to extend this idea to do repeatable
sampling, not just splitting.
<b>Singlequery</b>
We don’t need three separate queries to generate training, validation, and test splits.
We can do it in a single query as follows:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> <b>TABLE</b> mydataset.mytable <b>AS</b>
<b>SELECT</b>
airline,
departure_airport,
departure_schedule,
arrival_airport,
arrival_delay,"|feature extraction; Repeatable Splitting design pattern
"Training a machine learning model requires a substantial amount of work, but to
fully realize the value of that effort, the model must run in production to support the
business efforts it was designed to improve. There are several approaches that achieve
this goal and deployment can look different among different organizations depend‐
ing on the use case. For example, productionized ML assets could take the form of
interactive dashboards, static notebooks, code that is wrapped in a reusable library, or
web services endpoints.
There are many considerations and design decisions for productionizing models. As
before, many of the decisions that are made during the discovery stage guide this step
as well. How should model retraining be managed? Will input data need to stream
in? Should training happen on new batches of data or in real time? What about
model inference? Should we plan for one-off batch inference jobs each week or do we
need to support real-time prediction? Are there special throughput or latency issues
to consider? Is there a need to handle spiky workloads? Is low latency a priority? Is
network connectivity an issue? The design patterns in Chapter 5 touch on some of
the issues that arise when operationalizing an ML model.
These are important considerations, and this final stage tends to be the largest hurdle
for many businesses, as it can require strong coordination among different parts of
the organization and integration of a variety of technical components. This difficulty
is also in part due to the fact that productionization requires integrating a new pro‐
cess, one that relies on the machine learning model, into an existing system. This can
involve dealing with legacy systems that were developed to support a single approach,
or there could be complex change control and production processes to navigate
within the organization. Also, many times, existing systems do not have a mechanism
for supporting predictions coming from a machine learning model, so new applica‐
tions and workflows must be developed. It is important to anticipate these challenges,
and developing a comprehensive solution requires significant investment from the
business operations side to make the transition as easy as possible and increase the
speed to market.
The next step of the deployment stage is to operationalize the model (Step 9 in
Figure 8-2). This field of the practice is typically referred to as MLOps (ML Opera‐
tions) and covers aspects related to automating, monitoring, testing, managing, and
maintaining machine learning models in production. It is a necessary component for
any company hoping to scale the number of machine learning–driven applications
within their organization.
One of the key characteristics of operationalized models is automated workflow pipe‐
lines. The development stage of the ML life cycle is a multistep process. Building
pipelines to automate these steps enables more efficient workflows and repeatable
processes that improve future model development, and allows for increased agility in
solving problems that arise. Today, open source tools like Kubeflow provide this"|ML life cycle; MLOps; productionizing models
"tures with potential bias like race and gender can often be worse than leaving them
in, since it makes it harder to identify and correct instances of bias in the model.
When collecting and preparing data, another area where bias can be introduced is in
the way the data is labeled. Teams often outsource labeling of large datasets, but it’s
important to take care in understanding how labelers can introduce bias to a dataset,
especially if the labeling is subjective. This is known as <i>experimenter</i> <i>bias.</i> Imagine
we’re building a sentiment analysis model, and we have outsourced the labeling to a
group of 20 people—it’s their job to label each piece of text on a scale from 1 (nega‐
tive) to 5 (positive). This type of analysis is extremely subjective and can be influ‐
enced by one’s culture, upbringing, and many other factors. Before using this data to
train our model, we should ensure this group of 20 labelers reflects a diverse popula‐
tion.
In addition to data, bias can also be introduced during model training by the objec‐
tive function we choose. For example, if we optimize our model for overall accuracy,
this may not accurately reflect model performance across all slices of data. In cases
where datasets are inherently imbalanced, using accuracy as our only metric may
miss cases where our model is underperforming or making unfair decisions on
minority classes in our data.
Throughout this book, we’ve seen that ML has the power to improve productivity,
add business value, and automate tasks that were previously manual. As data scien‐
tists and ML engineers, we have a shared responsibility to ensure the models we build
don’t have adverse effects on the populations that use them.
<header><largefont><b>Solution</b></largefont></header>
To handle problematic bias in machine learning, we need solutions both for identify‐
ing areas of harmful bias in data before training a model, and evaluating our trained
model through a fairness lens. The Fairness Lens design pattern provides approaches
for building datasets and models that treat all groups of users equally. We’ll demon‐
strate techniques for both types of analysis using the What-If Tool, an open source
tool for dataset and model evaluation that can be run from many Python notebook
environments."|bias; experimenter bias; Fairness Lens design pattern; implicit bias; labeling; model evaluation; problematic bias; proxy bias
"windowed = (data
| 'window' >> beam.WindowInto(
beam.window.SlidingWindows(2 * 60 * 60, 10*60))
model_state = (windowed
| 'model' >> beam.transforms.CombineGlobally(ModelFn()))
There are meaningful differences between the rolling window in pandas and the slid‐
ing window in Apache Beam because of how often the is_anomaly function is called
and how often the model parameters (mean and standard deviation) need to be com‐
puted. These are discussed below.
<b>Perelementversusoveratimeinterval.</b> is_anomaly
In the pandas code, the function is
being called on every instance in the dataset. The anomaly detection code computes
the model parameters and applies it immediately to the last item in the window. In
the Beam pipeline, the model state is also created on every sliding window, but the
sliding window in this case is based on time. Therefore, the model parameters are
computed just once every 10 minutes.
The anomaly detection itself is carried out on every instance:
anomalies = (windowed
| 'latest_slice' >> beam.FlatMap(is_latest_slice)
| 'find_anomaly' >> beam.Map(is_anomaly, model_external))
Notice that this carefully separates out computationally expensive training from com‐
putationally cheap inference. The computationally expensive part is carried out only
once every 10 minutes while allowing every instance to be classified as being an
anomaly or not.
<b>High-throughputdatastreams.</b>
Data volumes keep increasing, and much of that
increase in data volume is due to real-time data. Consequently, this pattern has to be
applied to high-throughput data streams—streams where the number of elements
can be in excess of thousands of items per second. Think, for example, of click‐
streams from websites or streams of machine activity from computers, wearable devi‐
ces, or cars.
The suggested solution using a streaming pipeline is advantageous in that it avoids
retraining the model at every instance, something that the pandas code in the Prob‐
lem statement does. However, the suggested solution gives back those gains by creat‐
ing an in-memory dataframe of all the records received. If we receive 5,000 items a
second, then the in-memory dataframe over 10 minutes will contain 3 million
records. Because there are 12 sliding windows that will need to be maintained at any
point in time (10-minute windows, each over 2 hours), the memory requirements can
become considerable.
Storing all the received records in order to compute the model parameters at the end
of the window can become problematic. When the data stream is high throughput, it"|Apache Beam; Windowed Inference design pattern
"{""reviews"": ""The film is based on a prize-winning novel.""},
{""reviews"": ""The film is fast moving and has several great action scenes.""},
{""reviews"": ""The film was very boring. I walked out half-way.""}
]
}
parent = ""projects/{}/models/imdb"".format(""PROJECT"", ""v1"")
response = api.projects().predict(body = request_data,
name = parent).execute()
The equivalent of the above code can be written in many languages (we show Python
because we assume you are somewhat familiar with it). At the time that this book is
being written, developers can access the Discovery API from Java, PHP, .NET, Java‐
Script, Objective-C, Dart, Ruby, Node.js, and Go.
<b>Powerfulecosystem</b>
Because web application frameworks are so widely used, there is a lot of tooling avail‐
able to measure, monitor, and manage web applications. If we deploy the ML model
to a web application framework, the model can be monitored and throttled using
tools that software reliability engineers (SREs), IT administrators, and DevOps per‐
sonnel are familiar with. They do not have to know anything about machine learning.
Similarly, your business development colleagues know how to meter and monetize
web applications using API gateways. They can carry over that knowledge and apply
it to metering and monetizing machine learning models.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
As the joke by David Wheeler goes, the solution to any problem in computer science
is to add an extra level of indirection. Introduction of an exported stateless function
specification provides that extra level of indirection. The Stateless Serving Function
design pattern allows us to change the serving signature to provide extra functional‐
ity, like additional pre- and postprocessing, beyond what the ML model does. In fact,
it is possible to use this design pattern to provide multiple endpoints for a model.
This design pattern can also help with creating low-latency, online prediction for
models that are trained on systems, such as data warehouses, that are typically associ‐
ated with long-running queries.
<b>Customservingfunction</b>
The output layer of our text classification model is a Dense layer whose output is in
the range (-∞,∞):
model.add(tf.keras.layers.Dense(1, name='positive_review_logits'))
Our loss function takes this into account:"|custom serving function; low latency; model; software reliability engineer (SRE); SRE (software reliability engineer); Stateless Serving Function design pattern; Wheeler
"Because they both represent features in reduced dimensionality, bottleneck layers are
conceptually similar to embeddings. For example, in an autoencoder model with an
encoder-decoder architecture, the bottleneck layer <i>is</i> an embedding. In this case, the
bottleneck serves as the middle layer of the model, mapping the original input data to
a lower-dimensionality representation, which the decoder (the second half of the net‐
work) uses to map the input back to its original, higher-dimensional representation.
To see a diagram of the bottleneck layer in an autoencoder, refer to Figure 2-13 in
Chapter 2.
An embedding layer is essentially a lookup table of weights, mapping a particular fea‐
ture to some dimension in vector space. The main difference is that the weights in an
embedding layer can be trained, whereas all the layers leading up to and including the
bottleneck layer have their weights frozen. In other words, the entire network up to
and including the bottleneck layer is nontrainable, and the weights in the layers after
the bottleneck are the only trainable layers in the model.
It’s also worth noting that pre-trained embeddings can be used in
the Transfer Learning design pattern. When you build a model that
includes an embedding layer, you can either utilize an existing
(pre-trained) embedding lookup, or train your own embedding
layer from scratch.
To summarize, transfer learning is a solution you can employ to solve a similar prob‐
lem on a smaller dataset. Transfer learning always makes use of a bottleneck layer
with nontrainable, frozen weights. Embeddings are a type of data representation.
Ultimately, it comes down to purpose. If the purpose is to train a similar model, you
would use transfer learning. Consequently, if the purpose is to represent an input
image more concisely, you would use an embedding. The code might be exactly the
same.
<b>Implementingtransferlearning</b>
You can implement transfer learning in Keras using one of these two methods:
• Loading a pre-trained model on your own, removing the layers after the bottle‐
neck, and adding a new final layer with your own data and labels
• Using a pre-trained TensorFlow Hub module as the foundation for your transfer
learning task
Let’s start by looking at how to load and use a pre-trained model on your own. For
this, we’ll build on the VGG model example we introduced earlier. Note that VGG is
a model architecture, whereas ImageNet is the data it was trained on. Together, these
make up the pre-trained model we’ll be using for transfer learning. Here, we’re using
transfer learning to classify colorectal histology images. Whereas the original"|bottleneck layer; Keras; TensorFlow hub; Transfer Learning design pattern; VGG
"By training this model architecture on a massive dataset of image/caption pairs, the
encoder learns an efficient vector representation for images. The decoder learns how
to translate this vector to a text caption. In this sense, the encoder becomes an
Image2Vec embedding machine.
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
The embedding layer is just another hidden layer of the neural network. The weights
are then associated to each of the high-cardinality dimensions, and the output is
passed through the rest of the network. Therefore, the weights to create the embed‐
ding are learned through the process of gradient descent just like any other weights in
the neural network. This means that the resulting vector embeddings represent the
most efficient low-dimensional representation of those feature values with respect to
the learning task.
While this improved embedding ultimately aids the model, the embeddings them‐
selves have inherent value and allow us to gain additional insight into our dataset.
Consider again the customer video dataset. By only using one-hot encoding, any two
separate users, user_i and user_j, will have the same similarity measure. Similarly, the
dot product or cosine similarity for any two distinct six-dimensional one-hot
encodings of birth plurality would have zero similarity. This makes sense since the
one-hot encoding is essentially telling our model to treat any two different birth plu‐
ralities as separate and unrelated. For our dataset of customers and video watches, we
lose any notion of similarity between customers or videos. But this doesn’t feel quite
right. Two different customers or videos likely do have similarities between them.
The same goes for birth plurality. The occurrence of quadruplets and quintuplets
likely affects the birthweight in a statistically similar way as opposed to single child
birthweights (see Figure 2-9).
<i>Figure</i> <i>2-9.</i> <i>By</i> <i>forcing</i> <i>our</i> <i>categorical</i> <i>variable</i> <i>into</i> <i>a</i> <i>lower-dimensional</i> <i>embedding</i>
<i>space,</i> <i>we</i> <i>can</i> <i>also</i> <i>learn</i> <i>relationships</i> <i>between</i> <i>the</i> <i>different</i> <i>categories.</i>"|Embedding design pattern
"<b>SELECT</b>
model,
model_version,
time,
REGEXP_EXTRACT(raw_data, r'.*""text"": ""(.*)""') <b>AS</b> text,
REGEXP_EXTRACT(raw_prediction, r'.*""source"": ""(.*?)""') <b>AS</b> prediction,
REGEXP_EXTRACT(raw_prediction, r'.*""confidence"": (0.\d{2}).*') <b>AS</b> confidence,
REGEXP_EXTRACT(groundtruth, r'.*""source"": ""(.*?)""') <b>AS</b> groundtruth,
<b>FROM</b>
txtcls_eval.swivel
<i>Table</i> <i>5-2.</i> <i>Once</i> <i>ground</i> <i>truth</i> <i>is</i> <i>available,</i> <i>it</i> <i>can</i> <i>be</i> <i>added</i> <i>to</i> <i>the</i> <i>original</i> <i>BigQuery</i> <i>table</i>
<i>and</i> <i>the</i> <i>performance</i> <i>of</i> <i>the</i> <i>model</i> <i>can</i> <i>be</i> <i>evaluated</i>
<b>Row</b> <b>model</b> <b>model_version</b> <b>time</b> <b>text</b> <b>prediction</b> <b>confidence</b> <b>groundtruth</b>
1 txtcls swivel 2020-06-10 AnativeMacapp github 0.77 github
01:38:13UTC wrapperforWhatsApp
Web
2 txtcls swivel 2020-06-10 SenateConfirmsFirst nytimes 0.99 nytimes
01:37:46UTC BlackAirForceChief
3 txtcls swivel 2020-06-10 AstronautsDockWith github 0.99 nytimes
01:40:32UTC SpaceStationAfter
HistoricSpaceXLaunch
4 txtcls swivel 2020-06-09 YouTubeintroduces techcrunch 0.77 techcrunch
21:21:44UTC VideoChapterstomake
iteasiertonavigate
longervideos
With this information accessible in BigQuery, we can load the evaluation table into a
df_evals
dataframe, , and directly compute evaluation metrics for this model version.
Since this is a multiclass classification, we can compute the precision, recall, and F1-
score for each class. We can also create a confusion matrix, which helps to analyze
where model predictions within certain categorical labels may suffer. Figure 5-3
shows the confusion matrix comparing this model’s predictions with the ground
truth."|confusion matrix; Continued Model Evaluation design pattern; ground truth label
"type: INTEGER
minValue: 8
maxValue: 32
scaleType: UNIT_LINEAR_SCALE
Instead of using a config file to define these values, you can also do
this using the AI Platform Python API.
In order to do this, we’ll need to add an argument parser to our code that will specify
the arguments we defined in the file above, then refer to these hyperparameters
where they appear throughout our model code.
nn.Sequential
Next, we’ll build our model using PyTorch’s API with the SGD opti‐
mizer. Since our model predicts baby weight as a float, this will be a regression
model. We specify each of our hyperparameters using the args variable, which con‐
tains the variables defined in our argument parser:
<b>import</b> <b>torch.nn</b> <b>as</b> <b>nn</b>
model = nn.Sequential(nn.Linear(num_features, args.hidden_layer_size),
nn.ReLU(),
nn.Linear(args.hidden_layer_size, 1))
optimizer = torch.optim.SGD(model.parameters(), lr=args.lr,
momentum=args.momentum)
At the end of our model training code, we’ll create an instance of HyperTune(), and
tell it the metric we’re trying to optimize. This will report the resulting value of our
optimization metric after each training run. It’s important that whichever optimiza‐
tion metric we choose is calculated on our test or validation datasets, and not our
training dataset:
<b>import</b> <b>hypertune</b>
hpt = hypertune.HyperTune()
val_mse = 0
num_batches = 0
criterion = nn.MSELoss()
<b>with</b> torch.no_grad():
<b>for</b> i, (data, label) <b>in</b> enumerate(validation_dataloader):
num_batches += 1
y_pred = model(data)
mse = criterion(y_pred, label.view(-1,1))
val_mse += mse.item()"|Hyperparameter Tuning design pattern; PyTorch
"simpler to implement since every card payment from the old data will have the exact
same value (the 4-element array [0, 0.1, 0.3, 0.6]). We can update the older data in
one line of code, rather than writing a script to generate random numbers as in the
probabilistic method. It is also computationally much less expensive.
<b>Augmenteddata</b>
In order to maximize use of the newer data, make sure to use only two splits of the
data, which is discussed in “Design Pattern 12: Checkpoints” on page 149 in Chap‐
ter 4. Let’s say that we have 1 million examples available with the old schema, but
only 5,000 examples available with the new schema. How should we create the train‐
ing and evaluation datasets?
Let’s take the evaluation dataset first. It is important to realize that the purpose of
training an ML model is to make predictions on unseen data. The unseen data in our
case will be exclusively data that matches the new schema. Therefore, we need to set
aside a sufficient number of examples from the new data to adequately evaluate gen‐
eralization performance. Perhaps we need 2,000 examples in our evaluation dataset in
order to be confident that the model will perform well in production. The evaluation
dataset will not contain any older examples that have been bridged to match the
newer schema.
How do we know whether we need 1,000 examples in the evaluation dataset or 2,000?
To estimate this number, compute the evaluation metric of the current production
model (which was trained on the old schema) on subsets of its evaluation dataset and
determine how large the subset has to be before the evaluation metric is consistent.
Computing the evaluation metric on different subsets could be done as follows (as
usual, the full code is on GitHub in the code repository for this book):
<b>for</b> subset_size <b>in</b> range(100, 5000, 100):
sizes.append(subset_size)
<i>#</i> <i>compute</i> <i>variability</i> <i>of</i> <i>the</i> <i>eval</i> <i>metric</i>
<i>#</i> <i>at</i> <i>this</i> <i>subset</i> <i>size</i> <i>over</i> <i>25</i> <i>tries</i>
scores = []
<b>for</b> x <b>in</b> range(1, 25):
indices = np.random.choice(N_eval,
size=subset_size, replace=False)
scores.append(
model.score(df_eval[indices],
df_old.loc[N_train+indices, 'tip'])
)
score_mean.append(np.mean(scores))
score_stddev.append(np.std(scores))
In the code above, we are trying out evaluation sizes of 100, 200, …, 5,000. At each
subset size, we are evaluating the model 25 times, each time on a different, randomly
sampled subset of the full evaluation set. Because this is the evaluation set of the cur‐"|Bridged Schema design pattern
"For example, suppose we are trying to train a model to predict the likelihood that a
customer will return an item that they have purchased. If we train a single model, the
resellers’ return behavior will be lost because there are millions of retail buyers (and
retail transactions) and only a few thousand resellers. We don’t really know at the
time that a purchase is being made whether this is a retail buyer or a reseller. How‐
ever, by monitoring other marketplaces, we have identified when items bought from
us are subsequently being resold, and so our training dataset has a label that identifies
a purchase as having been done by a reseller.
One way to solve this problem is to overweight the reseller instances when training
the model. This is suboptimal because we need to get the more common retail buyer
use case as correct as possible. We do not want to trade off a lower accuracy on the
retail buyer use case for a higher accuracy on the reseller use case. However, retail
buyers and resellers behave very differently; for example, while retail buyers return
items within a week or so, resellers return items only if they are unable to sell them,
and so the returns may take place after several months. The business decision of
stocking inventory is different for likely returns from retail buyers versus resellers.
Therefore, it is necessary to get both types of returns as accurate as possible. Simply
overweighting the reseller instances will not work.
An intuitive way to address this problem is by using the Cascade design pattern. We
break the problem into four parts:
1. Predicting whether a specific transaction is by a reseller
2. Training one model on sales to retail buyers
3. Training the second model on sales to resellers
4. In production, combining the output of the three separate models to predict
return likelihood for every item purchased and the probability that the transac‐
tion is by a reseller
This allows for the possibility of different decisions on items likely to be returned
depending on the type of buyer and ensures that the models in steps 2 and 3 are as
accurate as possible on their segment of the training data. Each of these models is rel‐
atively easy to train. The first is simply a classifier, and if the unusual activity is
extremely rare, we can use the Rebalancing pattern to address it. The next two mod‐
els are essentially classification models trained on different segments of the training
data. The combination is deterministic since we choose which model to run based on
whether the activity belonged to a reseller.
The problem comes during prediction. At prediction time, we don’t have true labels,
just the output of the first classification model. Based on the output of the first model,
we will have to determine which of the two sales models we invoke. The problem is
that we are training on labels, but at inference time, we will have to make decisions"|Cascade design pattern; prediction; Rebalancing design pattern
"A complete training example for the natality problem is shown below, with a feature
cross of the is_male and plurality columns used as a feature; see the full code in this
book’s repository:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL babyweight.natality_model_feat_eng
<b>TRANSFORM(weight_pounds,</b>
is_male,
plurality,
gestation_weeks,
mother_age,
<b>CAST(mother_race</b> <b>AS</b> string) <b>AS</b> mother_race,
ML.FEATURE_CROSS(
STRUCT(
is_male,
plurality)
<b>)</b> <b>AS</b> gender_X_plurality)
<b>OPTIONS</b>
(MODEL_TYPE='linear_reg',
INPUT_LABEL_COLS=['weight_pounds'],
DATA_SPLIT_METHOD=""NO_SPLIT"") <b>AS</b>
<b>SELECT</b>
*
<b>FROM</b>
babyweight.babyweight_data_train
TrThe Transform pattern (see Chapter 6) is being used here when
engineering features of the natality model. This also allows the
model to “remember” to carry out the feature cross of the input
data fields during prediction.
When we have enough data, the Feature Cross pattern allows models to become sim‐
pler. On the natality dataset, the RMSE for the evaluation set for a linear model with
the Feature Cross pattern is 1.056. Alternatively, training a deep neural network in
BigQuery ML on the same dataset with no feature crosses yields an RMSE of 1.074.
There is a slight improvement in our performance despite using a much simpler lin‐
ear model, and the training time is also drastically reduced.
<b>FeaturecrossesinTensorFlow</b>
is_male plurality
To implement a feature cross using the features and in Tensor‐
tf.feature_column.crossed_column crossed_column
Flow, we use . The method
takes two arguments: a list of the feature keys to be crossed and the hash bucket size.
hash_bucket_size
Crossed features will be hashed according to so it should be large
is_male
enough to comfortably decrease the likelihood of collisions. Since the input
can take 3 values (True, False, Unknown) and the plurality input can take 6 values
(Single(1), Twins(2), Triplets(3), Quadruplets(4), Quintuplets(5), Multiple(2+)),"|BigQuery ML; Feature Cross design pattern; TensorFlow; Transform design pattern
"have windows on two walls? What about your least-favorite room? According to
Alexander:
Rooms lit on two sides, with natural light, create less glare around people and objects;
this lets us see things more intricately; and most important, it allows us to read in
detail the minute expressions that flash across people’s faces….
Having a name for this pattern saves architects from having to continually rediscover
this principle. Yet where and how you get two light sources in any specific local con‐
dition is up to the architect’s skill. Similarly, when designing a balcony, how big
should it be? Alexander recommends 6 feet by 6 feet as being enough for 2 (mis‐
matched!) chairs and a side table, and 12 feet by 12 feet if you want both a covered
sitting space and a sitting space in the sun.
Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides brought the idea to
software by cataloging 23 object-oriented design patterns in a 1994 book entitled
<i>Design</i> <i>Patterns:</i> <i>Elements</i> <i>of</i> <i>Reusable</i> <i>Object-Oriented</i> <i>Software</i> (Addison-Wesley,
1995). Their catalog includes patterns such as Proxy, Singleton, and Decorator and
led to lasting impact on the field of object-oriented programming. In 2005 the Asso‐
ciation of Computing Machinery (ACM) awarded their annual Programming Lan‐
guages Achievement Award to the authors, recognizing the impact of their work “on
programming practice and programming language design.”
Building production machine learning models is increasingly becoming an engineer‐
ing discipline, taking advantage of ML methods that have been proven in research
settings and applying them to business problems. As machine learning becomes more
mainstream, it is important that practitioners take advantage of tried-and-proven
methods to address recurring problems.
One benefit of our jobs in the customer-facing part of Google Cloud is that it brings
us in contact with a wide variety of machine learning and data science teams and
individual developers from around the world. At the same time, we each work closely
with internal Google teams solving cutting-edge machine learning problems. Finally,
we have been fortunate to work with the TensorFlow, Keras, BigQuery ML, TPU, and
Cloud AI Platform teams that are driving the democratization of machine learning
research and infrastructure. All this gives us a rather unique perspective from which
to catalog the best practices we have observed these teams carrying out.
This book is a catalog of design patterns or repeatable solutions to commonly occur‐
ring problems in ML engineering. For example, the Transform pattern (Chapter 6)
enforces the separation of inputs, features, and transforms and makes the transfor‐
mations persistent in order to simplify moving an ML model to production. Simi‐
larly, Keyed Predictions, in Chapter 5, is a pattern that enables the large-scale
distribution of batch predictions, such as for recommendation models."|Alexander; BigQuery; Cloud AI Platform; design patterns; Design Patterns: Elements of Reusable ObjectOriented Software; Gamma; Helm; Johnson; Keras; Keyed Predictions design pattern; Light on Two Sides of Every Room pattern; Six-Foot Balcony pattern; TensorFlow; TPU; Transform design pattern; Vlissides
"bucket. In these cases, it simplifies later machine learning to additionally store the
embeddings of the text columns or of the images as array-type columns. Doing so
will enable the easy incorporation of such unstructured data into machine learning
models.
To create text embeddings, we can load a pre-trained model such as Swivel from
TensorFlow Hub into BigQuery. The full code is on GitHub:
CREATE OR REPLACE MODEL advdata.swivel_text_embed
OPTIONS(model_type='tensorflow', model_path='gs://BUCKET/swivel/*')
Then, use the model to transform the natural language text column into an embed‐
ding array and store the embedding lookup into a new table:
CREATE OR REPLACE TABLE advdata.comments_embedding AS
SELECT
output_0 <b>as</b> comments_embedding,
comments
FROM ML.PREDICT(MODEL advdata.swivel_text_embed,(
SELECT comments, LOWER(comments) AS sentences
FROM `bigquery-public-data.noaa_preliminary_severe_storms.wind_reports`
))
It is now possible to join against this table to get the text embedding for any com‐
ment. For image embeddings, we can similarly transform image URLs into embed‐
dings and load them into the data warehouse.
Precomputing features in this manner is an example of the “Design Pattern 26: Fea‐
ture Store” on page 295 (see Chapter 6).
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>3:</b></largefont> <largefont><b>Feature</b></largefont> <largefont><b>Cross</b></largefont></header>
The Feature Cross design pattern helps models learn relationships between inputs
faster by explicitly making each combination of input values a separate feature.
<header><largefont><b>Problem</b></largefont></header>
Consider the dataset in Figure 2-14 and the task of creating a binary classifier that
separates the + and − labels.
Using only the <i>x_1</i> and <i>x_2</i> coordinates, it is not possible to find a linear boundary
that separates the + and − classes.
This means that to solve this problem, we have to make the model more complex,
perhaps by adding more layers to the model. However, a simpler solution exists."|BigQuery; data warehouses; Embedding design pattern; Feature Cross design pattern; Swivel; TensorFlow hub
"approximate the complex target than a regression model. At inference time, the
model then predicts a collection of probabilities corresponding to these potential out‐
puts. That is, we obtain a discrete PDF giving the relative likelihood of any specific
weight. Of course, care has to be taken here—classification models can be hugely
uncalibrated (such as the model being overly confident and wrong).
<b>Changingtheobjective</b>
In some scenarios, reframing a classification task as a regression could be beneficial.
For example, suppose we had a large movie database with customer ratings on a scale
from 1 to 5, for all movies that the user had watched and rated. Our task is to build a
machine learning model that will be used to serve recommendations to our users.
Viewed as a classification task, we could consider building a model that takes as input
user_id
a , along with that user’s previous video watches and ratings, and predicts
which movie from our database to recommend next. However, it is possible to
reframe this problem as a regression. Instead of the model having a categorical out‐
put corresponding to a movie in our database, our model could instead carry out
multitask learning, with the model learning a number of key characteristics (such as
income, customer segment, and so on) of users who are likely to watch a given movie.
Reframed as a regression task, the model now predicts the user-space representation
for a given movie. To serve recommendations, we choose the set of movies that are
closest to the known characteristics of a user. In this way, instead of the model pro‐
viding the probability that a user will like a movie as in a classification, we would get
a cluster of movies that have been watched by users like this user.
By reframing the classification problem of recommending movies to be a regression
of user characteristics, we gain the ability to easily adapt our recommendation model
to recommend trending videos, or classic movies, or documentaries without having
to train a separate classification model each time.
This type of model approach is also useful when the numerical representation has an
intuitive interpretation; for example, a latitude and longitude pair can be used instead
of urban area predictions. Suppose we wanted to predict which city will experience
the next viral outbreak or which New York neighborhood will have a real estate pric‐
ing surge. It could be easier to predict the latitude and longitude and choose the city
or neighborhood closest to that location, rather than predicting the city or neighbor‐
hood itself.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
There is rarely just one way to frame a problem, and it is helpful to be aware of any
trade-offs or alternatives of a given implementation. For example, bucketizing the
output values of a regression is an approach to reframing the problem as a classifica‐"|reframing as regression; Reframing design pattern
"<b>Predictionlibrary</b>
Instead of deploying the serving function as a microservice that can be invoked via a
REST API, it is possible to implement the prediction code as a library function. The
library function would load the exported model the first time it is called, invoke
model.predict() with the provided input, and return the result. Application devel‐
opers who need to predict with the library can then include the library with their
applications.
A library function is a better alternative than a microservice if the model cannot be
called over a network either because of physical reasons (there is no network connec‐
tivity) or because of performance constraints. The library function approach also
places the computational burden on the client, and this might be preferable from a
TensorFlow.js
budgetary standpoint. Using the library approach with can avoid
cross-site problems when there is a desire to have the model running in a browser.
The main drawback of the library approach is that maintenance and updates of the
model are difficult—all the client code that uses the model will have to be updated to
use the new version of the library. The more commonly a model is updated, the more
attractive a microservices approach becomes. A secondary drawback is that the
library approach is restricted to programming languages for which libraries are writ‐
ten, whereas the REST API approach opens up the model to applications written in
pretty much any modern programming language.
The library developer should take care to employ a threadpool and use parallelization
to support the necessary throughput. However, there is usually a limit to the scalabil‐
ity achievable with this approach.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>17:</b></largefont> <largefont><b>Batch</b></largefont> <largefont><b>Serving</b></largefont></header>
The Batch Serving design pattern uses software infrastructure commonly used for
distributed data processing to carry out inference on a large number of instances all
at once.
<header><largefont><b>Problem</b></largefont></header>
Commonly, predictions are carried one at a time and on demand. Whether or not a
credit card transaction is fraudulent is determined at the time a payment is being
processed. Whether or not a baby requires intensive care is determined when the
baby is examined immediately after birth. Therefore, when you deploy a model into
an ML serving framework, it is set up to process one instance, or at most a few thou‐
sands of instances, embedded in a single request.
The serving framework is architected to process an individual request synchronously
and as quickly as possible, as discussed in “Design Pattern 16: Stateless Serving Func‐"|Batch Serving design pattern; GPU; library function; prediction; REST API; Stateless Serving Function design pattern; TPU
"<b>Aggregatefeature</b>
In cases where the distribution of a categorical variable is skewed or where the num‐
ber of buckets is so small that bucket collisions are frequent, we might find it helpful
to add an aggregate feature as an input to our model. For example, for every airport,
we could find the probability of on-time flights in the training dataset and add it as a
feature to our model. This allows us to avoid losing the information associated with
individual airports when we hash the airport codes. In some cases, we might be able
to avoid using the airport name as a feature entirely, since the relative frequency of
on-time flights might be sufficient.
<b>Hyperparametertuning</b>
Because of the trade-offs with bucket collision frequency, choosing the number of
buckets can be difficult. It very often depends on the problem itself. Therefore, we
recommend that you treat the number of buckets as a hyperparameter that is tuned:
- <b>parameterName:</b> nbuckets
type: INTEGER
minValue: 10
maxValue: 20
scaleType: UNIT_LINEAR_SCALE
Make sure that the number of buckets remains within a sensible range of the cardin‐
ality of the categorical variable being hashed.
<b>Cryptographichash</b>
What makes the Hashed Feature lossy is the modulo part of the implementation.
What if we were to avoid the modulo altogether? After all, the farm fingerprint has a
fixed length (an INT64 is 64 bits), and so it can be represented using 64 feature val‐
ues, each of which is 0 or 1. This is called <i>binary</i> <i>encoding.</i>
However, binary encoding does not solve the problem of out-of-vocabulary inputs or
cold start (only the problem of high cardinality). In fact, the bitwise coding is a red
herring. If we don’t do a modulo, we can get a unique representation by simply
encoding the three characters that form the IATA code (thus using a feature of length
3*26=78). The problem with this representation is immediately obvious: airports
whose names start with the letter O have nothing in common when it comes to their
flight delay characteristics—the encoding has created a <i>spurious</i> <i>correlation</i> between
airports that start with the same letter. The same insight holds in binary space as well.
Because of this, we do not recommend binary encoding of farm fingerprint values.
Binary encoding of an MD5 hash will not suffer from this spurious correlation prob‐
lem because the output of an MD5 hash is uniformly distributed, and so the resulting
bits will be uniformly distributed. However, unlike the Farm Fingerprint algorithm,"|binary encoding; Hashed Feature design pattern; hyperparameter tuning; MD5 hash; spurious correlation
"The static method discussed in the main solution, of assigning a priori frequencies, is
also an imputation method. We assume that the categorical variable is distributed
according to a frequency chart (that we estimate from the training data) and impute
the mean one-hot encoded value (according to that frequency distribution) to the
“missing” categorical variable.
Do we know any other way to estimate unknown values given some examples? Of
course! Machine learning. What we can do is to train a cascade of models (see
“Design Pattern 8: Cascade ” on page 108 in Chapter 3). The first model uses what‐
ever new examples we have to train a machine learning model to predict the card
type. If the original tips model had five inputs, this model will have four inputs. The
fifth input (the payment type) will be the label for this model. Then, the output of the
first model will be used to train the second model.
In practice, the Cascade pattern adds too much complexity for something that is
meant to be a temporary workaround until you have enough new data. The static
method is effectively the simplest machine learning model—it’s the model we would
get if we had uninformative inputs. We recommend the static approach and to use
Cascade only if the static method doesn’t do well enough.
<b>Handlingnewfeatures</b>
Another situation where bridging might be needed is when the input provider adds
extra information to the input feed. For example, in our taxi fare example, we may
start receiving data on whether the taxi’s wipers are on or whether the vehicle is mov‐
ing. From this data, we can craft a feature on whether it was raining at the time the
taxi trip started, the fraction of the trip time that the taxi is idle, and so on.
If we have new input features we want to start using immediately, we should bridge
the older data (where this new feature will be missing) by imputing a value for the
new feature. Recommended choices for the imputation value are:
• The mean value of the feature if the feature is numeric and normally distributed
• The median value of the feature if the feature is numeric and skewed or has lots
of outliers
• The median value of the feature if the feature is categorical and sortable
• The mode of the feature if the feature is categorical and not sortable
• The frequency of the feature being true if it is boolean
If the feature is whether or not it was raining, it is boolean, and so the imputed value
would be something like 0.02 if it rains 2% of the time in the training dataset. If the
feature is the proportion of idle minutes, we could use the median value. The Cascade
pattern approach remains viable for all these cases, but a static imputation is simpler
and often sufficient."|Bridged Schema design pattern; Cascade design pattern; imputation
"<i>Figure</i> <i>3-9.</i> <i>Understanding</i> <i>the</i> <i>Multilabel</i> <i>pattern</i> <i>by</i> <i>breaking</i> <i>down</i> <i>the</i> <i>problem</i> <i>into</i>
<i>smaller</i> <i>binary</i> <i>classification</i> <i>tasks.</i>
<b>Parsingsigmoidresults</b>
To extract the predicted label for a model with softmax output, we can simply take
the argmax (highest value index) of the output array to get the predicted class. Pars‐
ing sigmoid outputs is less straightforward. Instead of taking the class with the high‐
est predicted probability, we need to evaluate the probability of each class in our
output layer and consider the probability <i>threshold</i> for our use case. Both of these
choices are largely dependent on the end user application of our model.
By threshold, we’re referring to the probability we’re comfortable
with for confirming an input belongs to a particular class. For
example, if we’re building a model to classify different types of ani‐
mals in images, we might be comfortable saying an image has a cat
even if the model is only 80% confident the image contains a cat.
Alternatively, if we’re building a model that’s making healthcare
predictions, we’ll likely want the model to be closer to 99% confi‐
dent before confirming a specific medical condition is present or
not. While thresholding is something we’ll need to consider for any
type of classification model, it’s especially relevant to the Multilabel
design pattern since we’ll need to determine thresholds for each
class and they may be different.
To look at a specific example, let’s take the Stack Overflow dataset in BigQuery and
use it to build a model that predicts the tags associated with a Stack Overflow ques‐
tion given its title. We’ll limit our dataset to questions that contain only five tags to
keep things simple:
SELECT
title,
REPLACE(tags, ""|"", "","") <b>as</b> tags
FROM
`bigquery-public-data.stackoverflow.posts_questions`
WHERE"|BigQuery; binary classification; multilabel classification; Multilabel design pattern; sigmoid activation; Stack Overflow; threshold
"This dataset contains information on taxi rides in New York City with features such
as the timestamp of pickup, the pickup and drop-off latitude and longitude, and
number of passengers. The label here is fare_amount , the cost of the taxi ride. Which
feature crosses might be relevant for this dataset?
pickup_datetime
There could be many. Let’s consider the . From this feature, we can
use information about the ride’s hour and day of the week. Each of these is a categori‐
cal variable, and certainly both contain predictive power in determining the price of a
day_of_week
taxi ride. For this dataset, it makes sense to consider a feature cross of
and hour_of_day since it’s reasonable to assume that taxi rides at 5pm on Monday
should be treated differently than taxi rides at 5 p.m. on Friday (see Table 2-9).
<i>Table</i> <i>2-9.</i> <i>A</i> <i>preview</i> <i>of</i> <i>the</i> <i>data</i> <i>we’re</i> <i>using</i> <i>to</i> <i>create</i> <i>a</i> <i>feature</i> <i>cross:</i> <i>the</i> <i>day</i> <i>of</i> <i>week</i> <i>and</i>
<i>hour</i> <i>of</i> <i>day</i> <i>columns</i>
<b>day_of_week</b> <b>hour_of_day</b>
Sunday 00
Sunday 01
... ...
Saturday 23
A feature cross of these two features would be a 168-dimensional one-hot encoded
vector (24 hours × 7 days = 168) with the example “Monday at 5 p.m.” occupying a
day_of_week hour_of_day
single index denoting ( is Monday concatenated with
is 17).
While the two features are important on their own, allowing for a feature cross of
hour_of_day and day_of_week makes it easier for a taxi fare prediction model to rec‐
ognize that end-of-the-week rush hour influences the taxi ride duration and thus the
taxi fare in its own way.
<b>FeaturecrossinBigQueryML</b>
ML.FEATURE_CROSS
To create the feature cross in BigQuery, we can use the function
and pass in a STRUCT of the features day_of_week and hour_of_day :
ML.FEATURE_CROSS(STRUCT(day_of_week,hour_of_week)) AS day_X_hour
The STRUCT clause creates an ordered pair of the two features. If our software frame‐
work doesn’t support a feature cross function, we can get the same effect using string
concatenation:
CONCAT(CAST(day_of_week AS STRING),
CAST(hour_of_week AS STRING)) AS day_X_hour"|BigQuery; BigQuery ML; Feature Cross design pattern
"<b>Handlingprecisionincreases</b>
When the input provider increases the precision of their data stream, follow the
bridging approach to create a training dataset that consists of the higher-resolution
data, augmented with some of the older data.
For floating-point values, it is not necessary to explicitly bridge the older data to
match the newer data’s precision. To see why, consider the case where some data was
originally provided to one decimal place (e.g., 3.5 or 4.2) but is now being provided to
two decimal places (e.g., 3.48 or 4.23). If we assume that 3.5 in the older data consists
distributed1
of values that would be uniformly in [3.45, 3.55] in the newer data, the
statically imputed value would be 3.5, which is precisely the value that is stored in the
older data.
For categorical values—for example, if the older data stored the location as a state or
provincial code and the newer data provided the county or district code—use the fre‐
quency distribution of counties within states as described in the main solution to
carry out static imputation.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>24:</b></largefont> <largefont><b>Windowed</b></largefont> <largefont><b>Inference</b></largefont></header>
The Windowed Inference design pattern handles models that require an ongoing
sequence of instances in order to run inference. This pattern works by externalizing
the model state and invoking the model from a stream analytics pipeline. This pat‐
tern is also useful when a machine learning model requires features that need to be
computed from aggregates over time windows. By externalizing the state to a stream
pipeline, the Windowed Inference design pattern ensures that features calculated in a
dynamic, time-dependent way can be correctly repeated between training and
serving. It is a way of avoiding training–serving skew in the case of temporal aggre‐
gate features.
<header><largefont><b>Problem</b></largefont></header>
Take a look at the arrival delays at Dallas Fort Worth (DFW) airport depicted for a
couple of days in May 2010 in Figure 6-5 (the full notebook is on GitHub).
1 Notethattheoverallprobabilitydistributionfunctiondoesn’tneedtobeuniform—allthatwerequireisthat
theoriginalbinsarenarrowenoughforustobeabletoapproximatetheprobabilitydistributionfunctionbya
staircasefunction.Wherethisassumptionfailsiswhenwehaveahighlyskeweddistributionthatwasinade‐
quatelysampledintheolderdata.Insuchcases,itispossiblethat3.46ismorelikelythan3.54,andthiswould
needtobereflectedinthebridgeddataset."|Bridged Schema design pattern; imputation; Windowed Inference design pattern
"In this chapter, we’ll look at design patterns that address different aspects of reprodu‐
cibility. The <i>Transform</i> design pattern captures data preparation dependencies from
the model training pipeline to reproduce them during serving. <i>Repeatable</i> <i>Splitting</i>
captures the way data is split among training, validation, and test datasets to ensure
that a training example that is used in training is never used for evaluation or testing
even as the dataset grows. The <i>Bridged</i> <i>Schema</i> design pattern looks at how to ensure
reproducibility when the training dataset is a hybrid of data conforming to different
schema. The <i>Workflow</i> <i>Pipeline</i> design pattern captures all the steps in the machine
learning process to ensure that as the model is retrained, parts of the pipeline can be
reused. The <i>Feature</i> <i>Store</i> design pattern addresses reproducibility and reusability of
features across different machine learning jobs. The <i>Windowed</i> <i>Inference</i> design pat‐
tern ensures that features that are calculated in a dynamic, time-dependent way can
be correctly repeated between training and serving. <i>Versioning</i> of data and models is
a prerequisite to handle many of the design patterns in this chapter.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>21:</b></largefont> <largefont><b>Transform</b></largefont></header>
The Transform design pattern makes moving an ML model to production much eas‐
ier by keeping inputs, features, and transforms carefully separate.
<header><largefont><b>Problem</b></largefont></header>
The problem is that the <i>inputs</i> to a machine learning model are not the <i>features</i> that
the machine learning model uses in its computations. In a text classification model,
for example, the inputs are the raw text documents and the features are the numerical
embedding representations of this text. When we train a machine learning model, we
train it with features that are extracted from the raw inputs. Take this model that is
trained to predict the duration of bicycle rides in London using BigQuery ML:
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL ch09eu.bicycle_model
<b>OPTIONS(input_label_cols=['duration'],</b>
model_type='linear_reg')
<b>AS</b>
<b>SELECT</b>
duration
, start_station_name
, <b>CAST(EXTRACT(dayofweek</b> <b>from</b> start_date) <b>AS</b> STRING)
<b>as</b> dayofweek
, <b>CAST(EXTRACT(hour</b> <b>from</b> start_date) <b>AS</b> STRING)
<b>as</b> hourofday
<b>FROM</b>
`bigquery-public-data.london_bicycles.cycle_hire`
(start_station_name, dayofweek, hourofday)
This model has three features and
computed from two inputs, start_station_name and start_date , as shown in
Figure 6-1."|BigQuery ML; Bridged Schema design pattern; Feature Store design pattern; Model Versioning design pattern; model; Repeatable Splitting design pattern; Transform design pattern; Windowed Inference design pattern; Workflow Pipeline design pattern
"continually update your training dataset, retrain your model, and modify the weight
your model assigns to particular groups of input data.
To see a less-obvious example of drift, look at the NOAA dataset of severe storms in
BigQuery. If we were training a model to predict the likelihood of a storm in a given
area, we would need to take into account the way weather reporting has changed over
time. We can see in Figure 1-3 that the total number of severe storms recorded has
been steadily increasing since 1950.
<i>Figure</i> <i>1-3.</i> <i>Number</i> <i>of</i> <i>severe</i> <i>storms</i> <i>reported</i> <i>in</i> <i>a</i> <i>year,</i> <i>as</i> <i>recorded</i> <i>by</i> <i>NOAA</i> <i>from</i>
<i>1950</i> <i>to</i> <i>2011.</i>
From this trend, we can see that training a model on data before 2000 to generate
predictions on storms today would lead to inaccurate predictions. In addition to the
total number of reported storms increasing, it’s also important to consider other fac‐
tors that may have influenced the data in Figure 1-3. For example, the technology for
observing storms has improved over time, most dramatically with the introduction of
weather radars in the 1990s. In the context of features, this may mean that newer data
contains more information about each storm, and that a feature available in today’s
data may not have been observed in 1950. Exploratory data analysis can help identify
this type of drift and can inform the correct window of data to use for training. Sec‐
tion , “Design Pattern 23: Bridged Schema” on page 266 provides a way to handle data‐
sets in which the availability of features improves over time."|data drift
"classification problems, this is a natural extension of the labeling phase because the
labels for eye disease are created through human labeling.
It is sometimes advantageous to use human experts even if we have actual ground
truth. For example, when building a model to predict the cost of auto repair after an
accident, we can look at historical data and find the actual cost of the repair. We will
not typically use human experts for this problem because the ground truth is directly
available from the historical dataset. However, for the purposes of communicating
the benchmark, it can be helpful to have insurance agents assess the cars for a damage
estimate, and compare our model’s estimates to those of the agents.
Using human experts need not be limited to unstructured data as with eye disease or
damage cost estimation. For example, if we are building a model to predict whether
or not a loan will get refinanced within a year, the data will be tabular and the ground
truth will be available in the historical data. However, even in this case, we might ask
human experts to identify loans that will get refinanced for the purposes of commu‐
nicating how often loan agents in the field would get it right.
<b>Utilityvalue</b>
Even if we have an operational model or excellent heuristic to compare against, we
will still have to explain the impact of the improvement that our model offers. Com‐
municating that the MAE is 30 seconds lower or that the MAP is 1% higher might
not be enough. The next question might very well be, “Is a 1% improvement good? Is
it worth the hassle of putting an ML model into production rather than the simple
heuristic rule?”
If you can, it is important to translate the improvement in model performance into
the model’s utility value. This value could be monetary, but it could also correspond
with other measures of utility, like better search results, earlier disease detection, or
less waste resulting from improved manufacturing efficiency. This utility value is use‐
ful in deciding whether or not to deploy this model, since deploying or changing a
production model always carries a certain cost in terms of reliability and error budg‐
ets. For example, if the image classification model is used to pre-fill an order form, we
can calculate that a 1% improvement will translate to 20 fewer abandoned orders per
day, and is therefore worth a certain amount of money. If this is more than the thres‐
hold set by our Site Reliability Engineering team, we’d deploy the model.
In our bicycle rental problem, it might be possible to measure the impact on the busi‐
ness by using this model. For example, we might be able to calculate the increased
availability of bicycles or the increased profits based on using the model in a dynamic
pricing solution."|Heuristic Benchmark design pattern
"keras.layers.Dense(128, activation='relu'),
keras.layers.Dense(3, activation='sigmoid')
])
The main difference in output between the sigmoid model here and the softmax
example in the Problem section is that the softmax array is guaranteed to contain
three values that sum to 1, whereas the sigmoid output will contain three values, each
between 0 and 1.
<header><largefont><b>Sigmoid</b></largefont> <largefont><b>Versus</b></largefont> <largefont><b>Softmax</b></largefont> <largefont><b>Activation</b></largefont></header>
Sigmoid is a nonlinear, continuous, and differentiable activation function that takes
the outputs of each neuron in the previous layer in the ML model and squashes the
value of those outputs between 0 and 1. Figure 3-7 shows what the sigmoid function
looks like.
<i>Figure</i> <i>3-7.</i> <i>A</i> <i>sigmoid</i> <i>function.</i>
While sigmoid takes a single value as input and provides a single value as output,
softmax takes an array of values as input and transforms it into an array of probabili‐
ties that sum to 1. The input to the softmax function could be the output of <i>N</i>
sigmoids.
In a multiclass classification problem where each example can only have one label,
use softmax as the last layer to get a probability distribution. In the Multilabel
pattern, it’s acceptable for the output array to not sum to 1 since we’re evaluating the
probability of each individual label.
Following are sample sigmoid and softmax output arrays:"|Multilabel design pattern; sigmoid activation; softmax activation
"WINDOW depart_time_window <b>AS</b>
(PARTITION <b>BY</b> departure_airport <b>ORDER</b> <b>BY</b>
UNIX_SECONDS(TIMESTAMP(scheduled_depart_time))
RANGE <b>BETWEEN</b> 7200 PRECEDING <b>AND</b> 1 PRECEDING)
The training dataset now includes the average delay as just another feature:
<b>Row</b> <b>arrival_delay</b> <b>departure_delay</b> <b>departure_airport</b> <b>hour_of_day</b> <b>avg_depart_delay</b>
1 -3.0 -7.0 LFT 8 <b>-4.0</b>
2 56.0 50.0 LFT 8 <b>41.0</b>
3 -14.0 -9.0 LFT 8 <b>5.0</b>
4 -3.0 0.0 LFT 8 <b>-2.0</b>
During inference, though, we will need a streaming pipeline to compute this average
departure delay so that we can supply it to the model. To limit training–serving skew,
it is preferable to use the same SQL in a tumbling window function in a streaming
pipeline, rather than trying to translate the SQL into Scala, Python, or Java.
<b>Batchingpredictionrequests</b>
Another scenario where we might want to use Windowed Inference even if the model
is stateless is when the model is deployed on the cloud, but the client is embedded
into a device or on-premises. In such cases, the network latency of sending inference
requests one by one to a cloud-deployed model might be overwhelming. In this situa‐
tion, “Design Pattern 19: Two-Phase Predictions” on page 232 from Chapter 5 can be
used where the first phase uses a pipeline to collect a number of requests and the sec‐
ond phase sends it to the service in one batch.
This is suitable only for latency-tolerant use cases. If we are collecting input instances
over five minutes, then the client will have to be tolerant of up to five minutes delay
in getting back the predictions.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>25:</b></largefont> <largefont><b>Workflow</b></largefont> <largefont><b>Pipeline</b></largefont></header>
In the Workflow Pipeline design pattern, we address the problem of creating an end-
to-end reproducible pipeline by containerizing and orchestrating the steps in our
machine learning process. The containerization might be done explicitly, or using a
framework that simplifies the process.
<header><largefont><b>Problem</b></largefont></header>
An individual data scientist may be able to run data preprocessing, training, and
model deployment steps from end to end (depicted in Figure 6-6) within a single
script or notebook. However, as each step in an ML process becomes more complex,"|containers; data scientists; Two-Phase Predictions design pattern; Windowed Inference design pattern; Workflow Pipeline design pattern
"<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
So far, we haven’t discussed methods of modifying the weights of our original model
when implementing transfer learning. Here, we’ll examine two approaches for this:
feature extraction and fine-tuning. We’ll also discuss why transfer learning is primar‐
ily focused on image and text models and look at the relationship between text sen‐
tence embeddings and transfer learning.
<b>Fine-tuningversusfeatureextraction</b>
<i>Feature</i> <i>extraction</i> describes an approach to transfer learning where you freeze the
weights of all layers before the bottleneck layer and train the following layers on your
own data and labels. Another option is instead <i>fine-tuning</i> the weights of the pre-
trained model’s layers. With fine-tuning, you can either update the weights of each
layer in the pre-trained model, or just a few of the layers right before the bottleneck.
Training a transfer learning model using fine-tuning typically takes longer than fea‐
ture extraction. You’ll notice in our text classification example above, we set
trainable=True when initializing our TF Hub layer. This is an example of fine-
tuning.
When fine-tuning, it’s common to leave the weights of the model’s initial layers fro‐
zen since these layers have been trained to recognize basic features that are often
common across many types of images. To fine-tune a MobileNet model, for example,
we’d set trainable=False only for a subset of layers in the model, rather than mak‐
ing every layer non-trainable. For example, to fine-tune after the 100th layer, we
could run:
base_model = tf.keras.applications.MobileNetV2(input_shape=(160,160,3),
include_top=False,
weights='imagenet')
<b>for</b> layer <b>in</b> base_model.layers[:100]:
layer.trainable = False
One recommended approach to determining how many layers to freeze is known as
<i>progressive</i> <i>fine-tuning,</i> and it involves iteratively unfreezing layers after every train‐
ing run to find the ideal number of layers to fine-tune. This works best and is most
efficient if you keep your learning rate low (0.001 is common) and the number of
training iterations relatively small. To implement progressive fine-tuning, start by
unfreezing only the last layer of your transferred model (the layer closest to the out‐
put) and calculate your model’s loss after training. Then, one by one, unfreeze more
layers until you reach the Input layer or until the loss starts to plateau. Use this to
inform the number of layers to fine-tune.
How should you determine whether to fine-tune or freeze all layers of your pre-
trained model? Typically, when you’ve got a small dataset, it’s best to use the"|feature extraction; fine-tuning; progressive fine-tuning; Transfer Learning design pattern
"<i>Figure</i> <i>8-4.</i> <i>Manual</i> <i>development</i> <i>of</i> <i>AI</i> <i>models.</i> <i>Figure</i> <i>adapted</i> <i>from</i> <i>Google</i> <i>Cloud</i> <i>doc‐</i>
<i>umentation.</i>
<b>Strategicphase:Utilizingpipelines</b>
Organizations in the strategic phase have aligned AI efforts with business objectives
and priorities, and ML is seen as a pivotal accelerator for the business. As such, there
is often senior executive sponsorship and dedicated budget for ML projects that are
executed by skilled teams and strategic partners. There is infrastructure in place for
these teams to easily share assets and develop ML systems that leverage both ready-
to-use and custom models. There is a clear distinction between development and
production environments.
Teams typically already have skills in data wrangling with expertise in descriptive and
predictive analytics. Data is stored in an enterprise data warehouse, and there is a
unified model for centralized data and ML asset management. The development of
ML models occurs as an orchestrated experiment. The ML assets and source code for
these pipelines is stored in a centralized source repository and easily shared among
members of the organization.
The data pipelines for developing ML models are automated utilizing a fully man‐
aged, serverless data service for ingestion and processing and are either scheduled or
event driven. Additionally, the ML workflow for training, evaluation, and batch pre‐
diction is managed by an automated pipeline so that the stages of the ML life cycle,
from data validation and preparation to model training and validation (see
Figure 8-5), are executed by a performance monitoring trigger. These models are
stored in a centralized trained models registry and able to be deployed automatically
based on predetermined model validation metrics.
There may be several ML systems deployed and maintained in production with log‐
ging, performance monitoring, and notifications in place. The ML systems leverage a"|AI readiness
"cannot know whether the model complexity is too high for a particular dataset until
you actually train that model on that dataset. Therefore, evaluation needs to be done
within the training loop, and <i>error</i> <i>metrics</i> on a withheld split of the training data,
called the <i>validation</i> <i>dataset,</i> have to be monitored as well. Because the training and
validation datasets have been used in the training loop, it is necessary to withhold yet
another split of the training dataset, called the <i>testing</i> <i>dataset,</i> to report the actual
error metrics that could be expected on new and unseen data. This evaluation is car‐
ried out at the end.
<header><largefont><b>Keras</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Loop</b></largefont></header>
The typical training loop in Keras looks like this:
model = keras.Model(...)
model.compile(optimizer=keras.optimizers.Adam(),
loss=keras.losses.categorical_crossentropy(),
metrics=['accuracy'])
history = model.fit(x_train, y_train,
batch_size=64,
epochs=3,
validation_data=(x_val, y_val))
results = model.evaluate(x_test, y_test, batch_size=128))
model.save(...)
Here, the model uses the Adam optimizer to carry out SGD on the cross entropy over
the training dataset and reports out the final accuracy obtained on the testing dataset.
The model fitting loops over the training dataset three times (each traversal over the
training dataset is termed an <i>epoch)</i> with the model seeing batches consisting of 64
training examples at a time. At the end of every epoch, the error metrics are calcula‐
ted on the validation dataset and added to the history. At the end of the fitting loop,
the model is evaluated on the testing dataset, saved, and potentially deployed for
serving, as shown in Figure 4-1.
<i>Figure</i> <i>4-1.</i> <i>A</i> <i>typical</i> <i>training</i> <i>loop</i> <i>consisting</i> <i>of</i> <i>three</i> <i>epochs.</i> <i>Each</i> <i>epoch</i> <i>is</i> <i>processed</i> <i>in</i>
<i>chunks</i> <i>of</i> <i>batch_size</i> <i>examples.</i> <i>At</i> <i>the</i> <i>end</i> <i>of</i> <i>the</i> <i>third</i> <i>epoch,</i> <i>the</i> <i>model</i> <i>is</i> <i>evaluated</i> <i>on</i>
<i>the</i> <i>testing</i> <i>dataset,</i> <i>and</i> <i>saved</i> <i>for</i> <i>potential</i> <i>deployment</i> <i>as</i> <i>a</i> <i>web</i> <i>service.</i>
Instead of using the prebuilt fit() function, we could also write a custom training
loop that iterates over the batches explicitly, but we will not need to do this for any of
the design patterns discussed in this chapter."|epochs; Keras Training Loop; SGD; test data; training loop; validation data
"<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
While we recommend the Model Versioning design pattern over maintaining a single
model version, there are a few implementation alternatives to the solution outlined
above. Here, we’ll look at other serverless and open source tooling for this pattern
and the approach of creating multiple serving functions. We’ll also discuss when to
create an entirely new model resource instead of a version.
<b>Otherserverlessversioningtools</b>
We used a managed service specifically designed for versioning ML models, but we
could achieve similar results with other serverless offerings. Under the hood, each
model version is a stateless function with a specified input and output format,
deployed behind a REST endpoint. We could therefore use a service like Cloud Run,
for example, to build and deploy each version in a separate container. Each container
has a unique URL and can be invoked by an API request. This approach gives us
more flexibility in how to configure the deployed model environment, letting us add
functionality like server-side preprocessing for model inputs. In our flight example
above, we may not want to require clients to one-hot encode categorical values.
Instead, we could let clients pass the categorical values as strings, and handle prepro‐
cessing in our container.
Why would we use a managed ML service like AI Platform Prediction instead of a
more generalized serverless tool? Since AI Platform was built specifically for ML
model deployment, it has built-in support for deploying models with GPUs opti‐
mized for ML. It also handles dependency management. When we deployed our
XGBoost model above, we didn’t need to worry about installing the correct XGBoost
version or other library dependencies.
<b>TensorFlowServing</b>
Instead of using Cloud AI Platform or another cloud-based serverless offering for
model versioning, we could use an open source tool like TensorFlow Serving. The
recommended approach for implementing TensorFlow Serving is to use a Docker
tensorflow/serving
container via the latest Docker image. With Docker, we could
then serve the model using whichever hardware we’d like, including GPUs. The
TensorFlow Serving API has built-in support for model versioning, following a simi‐
lar approach to the one discussed in the Solution section. In addition to TensorFlow
Serving, there are also other open source model serving options, including Seldon
and MLFlow.
<b>Multipleservingfunctions</b>
Another alternative to deploying multiple versions is to define multiple serving func‐
tions for a single version of an exported model. “Design Pattern 16: Stateless Serving"|AI Platform Prediction; Cloud Run; Docker container; Model Versioning design pattern; serverless; Stateless Serving Function design pattern; TensorFlow Serving; XGBoost
"conjunction with the Stateless Serving Function pattern to carry out prediction jobs
at scale which, in turn, relies on the Transform pattern under the hood to maintain
consistency between training and serving.
<header><largefont><b>Patterns</b></largefont> <largefont><b>Within</b></largefont> <largefont><b>ML</b></largefont> <largefont><b>Projects</b></largefont></header>
Machine learning systems enable teams within an organization to build, deploy, and
maintain machine learning solutions at scale. They provide a platform for automat‐
ing and accelerating all stages of the ML life cycle, from managing data, to training
models, evaluating performance, deploying models, serving predictions, and moni‐
toring performance. The patterns we have discussed in this book show up throughout
any machine learning project. In this section, we’ll describe the stages of the ML life
cycle and where many of these patterns are likely to arise.
<header><largefont><b>ML</b></largefont> <largefont><b>Life</b></largefont> <largefont><b>Cycle</b></largefont></header>
Building a machine learning solution is a cyclical process that begins with a clear
understanding of the business goals and ultimately leads to having a machine learn‐
ing model in production that benefits that goal. This high-level overview of the ML
life cycle (see Figure 8-2) provides a useful roadmap designed to enable ML to bring
value to businesses. Each of the stages is equally important, and failure to complete
any one of these steps increases the risk in later stages of producing misleading
insights or models of no value.
<i>Figure</i> <i>8-2.</i> <i>The</i> <i>ML</i> <i>life</i> <i>cycle</i> <i>begins</i> <i>with</i> <i>defining</i> <i>the</i> <i>business</i> <i>use</i> <i>case</i> <i>and</i> <i>ultimately</i>
<i>leads</i> <i>to</i> <i>having</i> <i>a</i> <i>machine</i> <i>learning</i> <i>model</i> <i>in</i> <i>production</i> <i>that</i> <i>benefits</i> <i>that</i> <i>goal.</i>"|ML life cycle
"While it’s tempting to assign significant meaning to the learned
weights in linear regression or decision tree models, we must be
extremely cautious when doing so. The conclusions we drew earlier
are still correct (i.e., inverse relationship between number of cylin‐
ders and fuel efficiency), but we cannot conclude from the magni‐
tude of coefficients, for example, that the categorical origin feature
or the number of cylinders are more important to our model than
horsepower or weight. First, each of these features is represented in
a different unit. One cylinder bears no equivalence to one pound—
the cars in this dataset have a maximum of 8 cylinders, but weigh
over 3,000 pounds. Additionally, origin is a categorical feature rep‐
resented with dummy values, so each origin value can only be 0 or
1. The coefficients also don’t tell us anything about the relationship
<i>between</i> features in our model. More cylinders are often correlated
with more horsepower, but we can’t conclude this from the learned
weights.4
When models are more complex, we use <i>post</i> <i>hoc</i> explainability methods to approxi‐
mate the relationships between a model’s features and its output. Typically, post hoc
methods perform this analysis without relying on model internals like learned
weights. This is an area of ongoing research, and there are a variety of proposed
explanation methods, along with tooling for adding these methods to your ML work‐
flow. The type of explanation methods we’ll discuss are known as <i>feature</i> <i>attributions.</i>
These methods aim to attribute a model’s output—whether it be an image, classifica‐
tion, or numerical value—to its features, by assigning attribution values to each fea‐
ture indicating how much that feature contributed to the output. There are two types
of feature attributions:
<i>Instance-level</i>
Feature attributions that explain a model’s output for an individual prediction.
For example, in a model predicting whether someone should be approved for a
line of credit, an instance-level feature attribution would provide insight into
why a specific person’s application was denied. In an image model, an instance-
level attribution might highlight the pixels in an image that caused it to predict it
contained a cat.
<i>Global</i>
Global feature attributions analyze the model’s behavior across an aggregate to
draw conclusions about how the model is behaving as a whole. Typically this is
done by averaging instance-level feature attributions from a test dataset. In a
model predicting whether a flight will be delayed, global attributions might tell
4 Thescikit-learndocumentationgoesintomoredetailonhowtocorrectlyinterpretthelearnedweightsin
linearmodels."|explainability; Explainable Predictions design pattern; feature attributions; post hoc explainability method
"<b>as</b> hourofday –- feature2
)
<b>AS</b>
<b>SELECT</b>
duration, start_station_name, start_date <i>--</i> <i>inputs</i>
<b>FROM</b>
`bigquery-public-data.london_bicycles.cycle_hire`
SELECT
Notice how we have clearly separated out the inputs (in the clause) from the
features (in the TRANSFORM clause). Now, prediction is much easier. We can simply
send to the model the station name and a timestamp (the inputs):
<b>SELECT</b> * <b>FROM</b> ML.PREDICT(MODEL ch09eu.bicycle_model,(
'Kings Cross' <b>AS</b> start_station_name
, <b>CURRENT_TIMESTAMP()</b> <b>as</b> start_date
))
The model will then take care of carrying out the appropriate transformations to cre‐
ate the necessary features. It does so by capturing both the transformation logic and
artifacts (such as scaling constants, embedding coefficients, lookup tables, and so on)
to carry out the transformation.
As long as we carefully use only the raw inputs in the SELECT statement and put all
subsequent processing of the input in the TRANSFORM clause, BigQuery ML will auto‐
matically apply these transformations during prediction.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
The solution described above works because BigQuery ML keeps track of the trans‐
formation logic and artifacts for us, saves them in the model graph, and automatically
applies the transformations during prediction.
If we are using a framework where support for the Transform design pattern is not
built in, we should design our model architecture in such a way that the transforma‐
tions carried out during training are easy to reproduce during serving. We can do
this by making sure to save the transformations in the model graph or by creating a
repository of transformed features (“Design Pattern 26: Feature Store” on page 295).
<b>TransformationsinTensorFlowandKeras</b>
Assume that we are training an ML model to estimate taxi fare in New York and have
six inputs (pickup latitude, pickup longitude, dropoff latitude, dropoff longitude, pas‐
senger count, and pickup time). TensorFlow supports the concept of feature
columns, which are saved in the model graph. However, the API is designed assum‐
ing that the raw inputs are the same as the features.
Let’s say that we want to scale the latitudes and longitudes (see “Simple Data Repre‐
sentations” on page 22 in Chapter 2 for details), create a transformed feature that is"|BigQuery ML; feature columns; Keras; TensorFlow; Transform design pattern
"(is_male, plurality) hash_bucket_size
there are 18 possible pairs. If we set to
1,000, we can be 85% sure there are no collisions.
Finally, to use a crossed column in a DNN model, we need to wrap it either in an
indicator_column embedding_column
or an depending on whether we want to one-
hot encode it or represent it in a lower dimension (see the “Design Pattern 2: Embed‐
dings” on page 39 in this chapter):
gender_x_plurality = fc.crossed_column([""is_male"", ""plurality""],
hash_bucket_size=1000)
crossed_feature = fc.embedding_column(gender_x_plurality, dimension=2)
or
gender_x_plurality = fc.crossed_column([""is_male"", ""plurality""],
hash_bucket_size=1000)
crossed_feature = fc.indicator_column(gender_x_plurality)
<header><largefont><b>Why</b></largefont> <largefont><b>It</b></largefont> <largefont><b>Works</b></largefont></header>
Feature crosses provide a valuable means of feature engineering. They provide more
complexity, more expressivity, and more capacity to simple models. Think again
about the crossed feature of is_male and plurality in the natality dataset. This Fea‐
ture Cross pattern allows the model to treat twin males separately from female twins
and separately from triplet males and separately from single females and so on. When
indicator_column,
we use an the model is able to treat each of the resulting crosses
as an independent variable, essentially adding 18 additional binary categorical fea‐
tures to the model (see Figure 2-16 ).
Feature crosses scale well to massive data. While adding extra layers to a deep neural
is_male,
network could potentially provide enough nonlinearity to learn how pairs (
plurality ) behave, this drastically increases the training time. On the natality data‐
set, we observed that a linear model with a feature cross trained in BigQuery ML per‐
forms comparably with a DNN trained without a feature cross. However, the linear
model trains substantially faster."|BigQuery ML; DNN model; Feature Cross design pattern
"<i>Figure</i> <i>2-14.</i> <i>This</i> <i>dataset</i> <i>is</i> <i>not</i> <i>linearly</i> <i>separable</i> <i>using</i> <i>only</i> <i>x_1</i> <i>and</i> <i>x_2</i> <i>as</i> <i>inputs.</i>
<header><largefont><b>Solution</b></largefont></header>
In machine learning, feature engineering is the process of using domain knowledge
to create new features that aid the machine learning process and increase the predic‐
tive power of our model. One commonly used feature engineering technique is creat‐
ing a feature cross.
A feature cross is a synthetic feature formed by concatenating two or more categori‐
cal features in order to capture the interaction between them. By joining two features
in this way, it is possible to encode nonlinearity into the model, which can allow for
predictive abilities beyond what each of the features would have been able to provide
individually. Feature crosses provide a way to have the ML model learn relationships
between the features faster. While more complex models like neural networks and
trees can learn feature crosses on their own, using feature crosses explicitly can allow
us to get away with training just a linear model. Consequently, feature crosses can
speed up model training (less expensive) and reduce model complexity (less training
data is needed).
To create a feature column for the dataset above, we can bucketize x_1 and x_2 each
into two buckets, depending on their sign. This converts x_1 and x_2 into categorical
features. Let A denote the bucket where x_1 >= 0 and B the bucket where x_1 < 0. Let
C denote the bucket where x_2 >= 0 and D the bucket where x_2 < 0 (Figure 2-15)."|Feature Cross design pattern
"epochs when training large distributed jobs instead of epochs; see “Design Pattern 12:
Checkpoints” on page 149 for a discussion of virtual epochs.
<i>Figure</i> <i>4-16.</i> <i>In</i> <i>asynchronous</i> <i>training,</i> <i>each</i> <i>worker</i> <i>performs</i> <i>a</i> <i>gradient</i> <i>descent</i> <i>step</i>
<i>with</i> <i>a</i> <i>split</i> <i>of</i> <i>the</i> <i>mini-batch.</i> <i>No</i> <i>one</i> <i>worker</i> <i>waits</i> <i>for</i> <i>updates</i> <i>to</i> <i>the</i> <i>model</i> <i>from</i> <i>any</i>
<i>of</i> <i>the</i> <i>other</i> <i>workers.</i>
In addition, since there is no synchronization between the weight updates, it is possi‐
ble that one worker updates the model weights based on stale model state. However,
in practice, this doesn’t seem to be a problem. Typically, large neural networks are
trained for multiple epochs, and these small discrepancies become negligible in the
end.
ParameterServerStrategy
In Keras, implements asynchronous parameter server
training on multiple machines. When using this distribution, some machines are des‐
ignated as workers and some are held as parameter servers. The parameter servers
hold each variable of the model, and computation is performed on the workers, typi‐
cally GPUs.
The implementation is similar to that of other distribution strategies in Keras. For
example, in your code, you would just replace MirroredStrategy() with
ParameterServerStrategy()
.
Another distribution strategy supported in Keras worth mention‐
OneDeviceStrategy
ing is . This strategy will place any variables
created in its scope on the specified device. This strategy is particu‐
larly useful as a way to test your code before switching to other
strategies that actually distribute to multiple devices/machines."|asynchronous training; Distribution Strategy design pattern; epochs; OneDeviceStrategy; ParameterServerStrategy; synchronous training; training
"Bayesian optimization is the hyperparameter tuning service provided by Google
Cloud AI Platform. This service is based on Vizier, the black-box optimization tool
used internally at Google.
The underlying concepts of the Cloud service work similarly to keras-tuner : you
specify each hyperparameter’s name, type, range, and scale, and these values are ref‐
erenced in your model training code. We’ll show you how to run hyperparameter
tuning in AI Platform using a PyTorch model trained on the BigQuery natality data‐
set to predict a baby’s birth weight.
The first step is to create a <i>config.yaml</i> file specifying the hyperparameters you want
the job to optimize, along with some other metadata on your job. One benefit of
using the Cloud service is that you can scale your tuning job by running it on GPUs
or TPUs and spreading it across multiple parameter servers. In this config file, you
also specify the total number of hyperparameter trials you want to run and how many
of these trials you want to run in parallel. The more you run in parallel, the faster
your job will run. However, the benefit of running fewer trials in parallel is that the
service will be able to learn from the results of each completed trial to optimize the
next ones.
For our model, a sample config file that makes use of GPUs might look like the fol‐
lowing. In this example, we’ll tune three hyperparameters—our model’s learning rate,
the optimizer’s momentum value, and the number of neurons in our model’s hidden
layer. We also specify our optimization metric. In this example, our goal will be to
<i>minimize</i> our model’s loss on our validation set:
trainingInput:
scaleTier: BASIC_GPU
parameterServerType: large_model
workerCount: 9
parameterServerCount: 3
hyperparameters:
goal: MINIMIZE
maxTrials: 10
maxParallelTrials: 5
hyperparameterMetricTag: val_error
enableTrialEarlyStopping: TRUE
params:
- parameterName: lr
type: DOUBLE
minValue: 0.0001
maxValue: 0.1
scaleType: UNIT_LINEAR_SCALE
- parameterName: momentum
type: DOUBLE
minValue: 0.0
maxValue: 1.0
scaleType: UNIT_LINEAR_SCALE
- parameterName: hidden-layer-size"|Hyperparameter Tuning design pattern
"processes, such as text classification (email filtering), entity extraction, question
answering, speech recognition, text summarization, and sentiment analysis.
• Embeddings
• Hashed Feature
• Neutral Class
• Multimodal Input
• Transfer Learning
• Two-Phase Predictions
• Cascade
• Windowed Inference
<header><largefont><b>Computer</b></largefont> <largefont><b>Vision</b></largefont></header>
Computer vision is the broad parent name for AI that trains machines to understand
visual input, such as images, videos, icons, and anything where pixels might be
involved. Computer vision models aim to automate any task that might rely on
human vision, from using an MRI to detect lung cancer to self-driving cars. Some
classical applications of computer vision are image classification, video motion analy‐
sis, image segmentation, and image denoising.
• Reframing
• Neutral Class
• Multimodal Input
• Transfer Learning
• Embeddings
• Multilabel
• Cascade
• Two-Phase Predictions
<header><largefont><b>Predictive</b></largefont> <largefont><b>Analytics</b></largefont></header>
Predictive modeling uses historical data to find patterns and determine the likelihood
of a certain event occurring in the future. Predictive models can be found across
many different industry domains. For example, businesses might use predictive mod‐
els to forecast revenue more accurately or anticipate future demand for products. In
medicine, predictive models might be used to assess the risk of a patient developing a
chronic disease or predict when a patient might not show up for a scheduled"|computer vision; predictive modeling
"<b>Nonlinearoptimization</b>
The hyperparameters that need to be tuned fall into two groups: those related to
model <i>architecture</i> and those related to model <i>training.</i> Model architecture hyper‐
parameters, like the number of layers in your model or the number of neurons per
layer, control the mathematical function that underlies the machine learning model.
Parameters related to model training, like the number of epochs, learning rate, and
batch size, control the training loop and often have to do with the way that the gradi‐
ent descent optimizer works. Taking both these types of parameters into considera‐
tion, it is clear that the overall model function with respect to these hyperparameters
is, in general, not differentiable.
The inner training loop is differentiable, and the search for optimal parameters can
be carried out through stochastic gradient descent. A single step of a machine learn‐
ing model trained through stochastic gradient might take only a few milliseconds. On
the other hand, a single trial in the hyperparameter tuning problem involves training
a complete model on the training dataset and might take several hours. Moreover, the
optimization problem for the hyperparameters will have to be solved through nonlin‐
ear optimization methods that apply to nondifferentiable problems.
Once we decide that we are going to use nonlinear optimization methods, our choice
of metric becomes wider. This metric will be evaluated on the validation dataset and
does not have to be the same as the training loss. For a classification model, your
optimization metric might be accuracy, and you’d therefore want to find the combi‐
nation of hyperparameters that leads to the highest model accuracy even if the loss is
binary cross entropy. For a regression model, you might want to optimize median
absolute error even if the loss is squared error. In that case, you’d want to find the
hyperparameters that yield the <i>lowest</i> mean squared error. This metric can even be
chosen based on business goals. For example, we might choose to maximize expected
revenue or minimize losses due to fraud.
<b>Bayesianoptimization</b>
Bayesian optimization is a technique for optimizing black-box functions, originally
developed in the 1970s by Jonas Mockus. The technique has been applied to many
domains and was first applied to hyperparameter tuning in 2012. Here, we’ll focus on
Bayesian optimization as it relates to hyperparameter tuning. In this context, a
machine learning model is our <i>black-box</i> <i>function,</i> since ML models produce a set of
outputs from inputs we provide without requiring us to know the internal details of
the model itself. The process of training our ML model is referred to as calling the
<i>objective</i> <i>function.</i>
The goal of Bayesian optimization is to directly train our model as few times as possi‐
ble since doing so is costly. Remember that each time we try a new combination of
hyperparameters on our model, we need to run through our model’s entire training"|Bayesian optimization; Hyperparameter Tuning design pattern; Mockus; objective function
"<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>14:</b></largefont> <largefont><b>Distribution</b></largefont> <largefont><b>Strategy</b></largefont></header>
In Distribution Strategy, the training loop is carried out at scale over multiple work‐
ers, often with caching, hardware acceleration, and parallelization.
<header><largefont><b>Problem</b></largefont></header>
These days, it’s common for large neural networks to have millions of parameters
and be trained on massive amounts of data. In fact, it’s been shown that increasing
the scale of deep learning, with respect to the number of training examples, the num‐
ber of model parameters, or both, drastically improves model performance. However,
as the size of models and data increases, the computation and memory demands
increase proportionally, making the time it takes to train these models one of the big‐
gest problems of deep learning.
GPUs provide a substantial computational boost and bring the training time of mod‐
estly sized deep neural networks within reach. However, for very large models trained
on massive amounts of data, individual GPUs aren’t enough to make the training
time tractible. For example, at the time of writing, training ResNet-50 on the bench‐
mark ImageNet dataset for 90 epochs on a single NVIDIA M40 GPU requires 10 18
single precision operations and takes 14 days. As AI is being used more and more to
solve problems within complex domains, and open source libraries like Tensorflow
and PyTorch make building deep learning models more accessible, large neural net‐
works comparable to ResNet-50 have become the norm.
This is a problem. If it takes two weeks to train your neural network, then you have to
wait two weeks before you can iterate on new ideas or experiment with tweaking the
settings. Furthermore, for some complex problems like medical imaging, autono‐
mous driving, or language translation, it’s not always feasible to break the problem
down into smaller components or work with only a subset of the data. It’s only with
the full scale of the data that you can assess whether things work or not.
Training time translates quite literally to money. In the world of serverless machine
learning, rather than buying your own expensive GPU, it is possible to submit train‐
ing jobs via a cloud service where you are charged for training time. The cost of train‐
ing a model, whether it is to pay for a GPU or to pay for a serverless training service,
quickly adds up.
Is there a way to speed up the training of these large neural networks?
<header><largefont><b>Solution</b></largefont></header>
One way to accelerate training is through distribution strategies in the training loop.
There are different distribution techniques, but the common idea is to split the effort
of training the model across multiple machines. There are two ways this can be done:"|data parallelism; Distribution Strategy design pattern; GPU; model parallelism; PyTorch; TensorFlow
"<b>Needforregularization</b>
When crossing two categorical features both with large cardinality, we produce a
cross feature with multiplicative cardinality. Naturally, given more categories for an
individual feature, the number of categories in a feature cross can increase dramati‐
cally. If this gets to the point where individual buckets have too few items, it will hin‐
der the model’s ability to generalize. Think of the latitude and longitude example. If
we were to take very fine buckets for latitude and longitude, then a feature cross
would be so precise it would allow the model to memorize every point on the map.
However, if that memorization was based on just a handful of examples, the memori‐
zation would actually be an overfit.
To illustrate, take the example of predicting the taxi fare in New York given the
pickup and dropoff locations and the time of pickup:6
<b>CREATE</b> <b>OR</b> <b>REPLACE</b> MODEL mlpatterns.taxi_l2reg
<b>TRANSFORM(</b>
fare_amount
, ML.FEATURE_CROSS(STRUCT(CAST(EXTRACT(DAYOFWEEK <b>FROM</b> pickup_datetime)
<b>AS</b> STRING) <b>AS</b> dayofweek,
<b>CAST(EXTRACT(HOUR</b> <b>FROM</b> pickup_datetime)
<b>AS</b> STRING) <b>AS</b> hourofday), 2) <b>AS</b> day_hr
, CONCAT(
ML.BUCKETIZE(pickuplon, GENERATE_ARRAY(-78, -70, <b>0.01)),</b>
ML.BUCKETIZE(pickuplat, GENERATE_ARRAY(37, 45, 0.01)),
ML.BUCKETIZE(dropofflon, GENERATE_ARRAY(-78, -70, 0.01)),
ML.BUCKETIZE(dropofflat, GENERATE_ARRAY(37, 45, 0.01))
) <b>AS</b> pickup_and_dropoff
)
<b>OPTIONS(input_label_cols=['fare_amount'],</b>
model_type='linear_reg', <b>l2_reg=0.1)</b>
<b>AS</b>
<b>SELECT</b> * <b>FROM</b> mlpatterns.taxi_data
There are two feature crosses here: one in time (of day of week and hour of day) and
the other in space (of the pickup and dropoff locations). The location, in particular, is
very high cardinality, and it is likely that some of the buckets will have very few
examples.
For this reason, it is advisable to pair feature crosses with L1 regularization, which
encourages sparsity of features, or L2 regularization, which limits overfitting. This
allows our model to ignore the extraneous noise generated by the many synthetic fea‐
tures and combat overfitting. Indeed, on this dataset, the regularization improves the
RMSE slightly, by 0.3%.
6 Fullcodeisin02_data_representation/feature_cross.ipynbinthecoderepositoryofthisbook."|Feature Cross design pattern; feature cross
"learned by Word2Vec are the same regardless of the sentence where the word
appears. However, the BERT word embeddings are contextual, meaning the embed‐
ding vector is different depending on the context of how the word is used.
A pre-trained text embedding, like Word2Vec, NNLM, GLoVE, or BERT, can be
added to a machine learning model to process text features in conjunction with struc‐
tured inputs and other learned embeddings from our customer and video dataset
(Figure 2-13).
Ultimately, embeddings learn to preserve information relevant to the prescribed
training task. In the case of image captioning, the task is to learn how the context of
the elements of an image relates to text. In the autoencoder architecture, the label is
the same as the feature, so the dimension reduction of the bottleneck attempts to
learn everything with no specific context of what is important.
<i>Figure</i> <i>2-13.</i> <i>A</i> <i>pre-trained</i> <i>text</i> <i>embedding</i> <i>can</i> <i>be</i> <i>added</i> <i>to</i> <i>a</i> <i>model</i> <i>to</i> <i>process</i> <i>text</i>
<i>features.</i>
<b>Embeddingsinadatawarehouse</b>
Machine learning on structured data is best carried out directly in SQL on a data
warehouse. This avoids the need to export data out of the warehouse and mitigates
problems with data privacy and security.
Many problems, however, require a mix of structured data and natural language text
or image data. In data warehouses, natural language text (such as reviews) is stored
directly as columns, and images are typically stored as URLs to files in a cloud storage"|autoencoders; BERT; context language models; data warehouses; Embedding design pattern; GLoVE; NNLM; Word2Vec
"results when the app is able to send the user’s queries to a cloud-hosted model. With
this solution, the user still gets some functionality when they aren’t connected. When
they come back online, they can then benefit from a full-featured app and robust ML
model.
<b>Handlingmanypredictionsinnearrealtime</b>
In other cases, end users of your ML model may have reliable connectivity but might
need to make hundreds or even thousands of predictions to your model at once. If
you only have a cloud-hosted model and each prediction requires an API call to a
hosted service, getting prediction responses on thousands of examples at once will
take too much time.
To understand this, let’s say we have embedded devices deployed in various areas
throughout a user’s house. These devices are capturing data on temperature, air pres‐
sure, and air quality. We have a model deployed in the cloud for detecting anomalies
from this sensor data. Because the sensors are continuously collecting new data, it
would be inefficient and expensive to send every incoming data point to our cloud
model. Instead, we can have a model deployed directly on the sensors to identify pos‐
sible anomaly candidates from incoming data. We can then send only the potential
anomalies to our cloud model for consolidated verification, taking sensor readings
from all the locations into account. This is a variation of the Two-Phase Predictions
pattern described earlier, the main difference being that both the offline and cloud
models perform the same prediction task but with different inputs. In this case, mod‐
els also end up throttling the number of prediction requests sent to the cloud model
at one time.
<b>Continuousevaluationforofflinemodels</b>
How can we ensure our on-device models stay up to date and don’t suffer from data
drift? There are a few options for performing continuous evaluation on models that
do not have network connectivity. First, we could save a subset of predictions that are
received on-device. We could then periodically evaluate our model’s performance on
these examples and determine if the model needs retraining. In the case of our two-
phase model, it’s important we do this evaluation regularly since it’s likely that many
calls to our on-device model will not go onto the second-phase cloud model. Another
option is to create a replica of our on-device model to run <i>online,</i> only for continuous
evaluation purposes. This solution is preferred if our offline and cloud models are
running similar prediction tasks, like in the translation case mentioned previously.
<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>20:</b></largefont> <largefont><b>Keyed</b></largefont> <largefont><b>Predictions</b></largefont></header>
Normally, you train your model on the same set of input features that the model will
be supplied in real time when it is deployed. In many situations, however, it can be"|anomaly detection; data drift; Keyed Predictions design pattern; keys; Two-Phase Predictions design pattern
"<b>CASE(ABS(MOD(FARM_FINGERPRINT(date),</b> 10)))
<b>WHEN</b> 9 <b>THEN</b> 'test'
<b>WHEN</b> 8 <b>THEN</b> 'validation'
<b>ELSE</b> 'training' <b>END</b> <b>AS</b> split_col
<b>FROM</b>
`bigquery-samples`.airline_ontime_data.flights
We can then use the split_col column to decide which of three datasets any partic‐
ular row falls in. Using a single query decreases computational time but requires cre‐
split_col
ating a new table or modifying the source table to add the extra column.
<b>Randomsplit</b>
What if the rows are not correlated? In that case, we want a random, repeatable split
but do not have a natural column to split by. We can hash the entire row of data by
converting it to a string and hashing that string:
<b>SELECT</b>
airline,
departure_airport,
departure_schedule,
arrival_airport,
arrival_delay
<b>FROM</b>
`bigquery-samples`.airline_ontime_data.flights f
<b>WHERE</b>
<b>ABS(MOD(FARM_FINGERPRINT(TO_JSON_STRING(f),</b> 10)) < 8
Note that if we have duplicate rows, they will always end up in the same split. This
might be exactly what we desire. If not, then we will have to add a unique ID column
to the SELECT query.
<b>Splitonmultiplecolumns</b>
We have talked about a single column that captures the correlation between rows.
What if it is a combination of columns that capture when two rows are correlated? In
such cases, simply concatenate the fields (this is a feature cross) before computing the
hash. For example, suppose we only wish to ensure that flights from the same airport
on the same day do not show up in different splits. In that case, we’d do the
following:
<b>SELECT</b>
airline,
departure_airport,
departure_schedule,
arrival_airport,
arrival_delay
<b>FROM</b>
`bigquery-samples`.airline_ontime_data.flights"|Repeatable Splitting design pattern
"The name of the method reflects the fact that the scaled value has zero mean and
is normalized by the standard deviation so that it has unit variance over the
training dataset. The scaled value is unbounded, but does lie between [–1, 1] the
majority of the time (67%, if the underlying distribution is normal). Values out‐
side this range get rarer the larger their absolute value gets, but are still present.
<i>Winsorizing</i>
Uses the empirical distribution in the training dataset to clip the dataset to
bounds given by the 10th and 90th percentile of the data values (or 5th and 95th
percentile, and so forth). The winsorized value is min-max scaled.
All the methods discussed so far scale the data linearly (in the case of clipping and
winsorizing, linear within the typical range). Min-max and clipping tend to work best
for uniformly distributed data, and Z-score tends to work best for normally dis‐
mother_age
tributed data. The impact of different scaling functions on the column in
the baby weight prediction example is shown in Figure 2-3 (see the full code).
<header><largefont><b>Don’t</b></largefont> <largefont><b>Throw</b></largefont> <largefont><b>Away</b></largefont> <largefont><b>“Outliers”</b></largefont></header>
Note that we defined clipping as taking scaled values less than –1 and treating them
as –1, and scaled values greater than 1 and treating them as 1. We don’t simply dis‐
card such “outliers” because we expect that the machine learning model will
encounter outliers like this in production. Take, for example, babies born to 50-year-
old mothers. Because we don’t have enough older mothers in our dataset, clipping
ends up treating all mothers older than 45 (for example) as 45. This same treatment
will be applied in production, and therefore, our model will be able to handle older
mothers. The model would not learn to reflect outliers if we had simply thrown away
all the training examples of babies born to mothers aged 50+!
Another way to think about this is that while it is acceptable to throw away <i>invalid</i>
<i>input,</i> it is not acceptable to throw away <i>valid</i> <i>data.</i> Thus, we would be justified in
throwing away rows where mother_age is negative because it’s probably a data entry
error. In production, validation of the input form will ensure that the admitting clerk
has to reenter the mother’s age. However, we are not justified in throwing away rows
where mother_age is 50 because 50 is a perfectly valid input and we expect to
encounter 50-year-old mothers once the model is deployed in production.
In Figure 2-3, note that minmax_scaled gets the x values into the desired range of
[–1, 1] but continues to retain values at the extreme ends of the distribution where
there are not enough examples. Clipping rolls up many of the problematic values, but
requires getting the clipping thresholds exactly correct—here, the slow decline in the
number of babies with mothers’ ages above 40 poses problems in setting a hard
threshold. Winsorizing, similar to clipping, requires getting the percentile thresholds
exactly correct. Z-score normalization improves the range (but does not constrain"|clipping; min-max scaling; outliers; winsorizing; z-score normalization
"<b>Scenario</b> <b>Heuristicbenchmark</b> <b>Exampletask</b> <b>Implementationforexampletask</b>
Regressionproblemthat Persistenceorlinear Predictweeklysales Predictthatnextweek’ssales=s
0
involvespredictingthe trend.Takeseasonality volume wheres isthesalesthisweek.
0
futurevalueofatimeseries. intoaccount.Forannual (or)
data,compareagainstthe Nextweek’ssales=s +(s -s )
0 0 -1
sameday/week/quarter wheres islastweek’ssales.
-1
ofpreviousyear. (or)
Nextweek’ssales=s wheres is
-1y -1y
thesalesofthecorrespondingweeklast
year.
Avoidthetemptationtocombinethe
threeoptionssincethevalueofthe
relativeweightsisnotintuitive.
Classificationproblem Performanceofhuman Detectingeyedisease Havethreeormorephysiciansexamine
currentlybeingsolvedby experts. fromretinalscans. eachimage.Treatthedecisionofa
humanexperts. majorityofphysiciansasbeingcorrect,
Thisiscommonforimage, andlookatthepercentilerankingof
video,andtexttasksand theMLmodelamonghumanexperts.
includesscenarioswhereit
iscost-prohibitiveto
routinelysolvetheproblem
withhumanexperts.
Preventiveorpredictive Performmaintenanceon Preventive Bringcarsinformaintenanceonce
maintenance. afixedschedule. maintenanceofacar. everythreemonths.
Thethreemonthsisthemediantimeto
failureofcarsfromthelastservicedate.
Anomalydetection. 99thpercentilevalue Identifyadenialof Findthe99thpercentileofthenumber
estimatedfromthe service(DoS)attack ofrequestsperminuteinthehistorical
trainingdataset. fromnetworktraffic. data.Ifoveranyone-minuteperiod,the
numberofrequestsexceedsthis
number,flagitasaDoSattack.
Recommendationmodel. Recommendthemost Recommendmoviesto Ifauserjustsaw(andliked)Inception
populariteminthe users. (asci-fimovie),recommendIcarusto
categoryofthe them(themostpopularsci-fimovie
customer’slastpurchase. theyhaven’tyetwatched).
Many of the scenarios in Table 7-1 refer to “important features.” These are important
features in the sense that they are widely accepted within the business as having a
well-understood impact on the prediction problem. In particular, these are not fea‐
tures ascertained using feature importance methods on your training dataset. As an
example, it’s well accepted within the taxicab industry that the most important deter‐
minant of a taxi fare is distance, and that longer trips cost more. That’s what makes
distance an important feature, not the outcome of a feature importance study."|heuristic benchmark; Heuristic Benchmark design pattern
"The histology dataset comes with images as (150,150,3) dimensional arrays. This
150×150×3 representation is the <i>highest</i> dimensionality. To use the VGG model with
our image data, we can load it with the following:
vgg_model = tf.keras.applications.VGG19(
include_top=False,
weights='imagenet',
input_shape=((150,150,3))
)
vgg_model.trainable = False
By setting include_top=False , we’re specifying that the last layer of VGG we want to
input_shape
load is the bottleneck layer. The we passed in matches the input shape
of our histology images. A summary of the last few layers of this updated VGG model
looks like the following:
block5_conv3 (Conv2D) (None, 9, 9, 512) 2359808
_________________________________________________________________
block5_conv4 (Conv2D) (None, 9, 9, 512) 2359808
_________________________________________________________________
block5_pool (MaxPooling2D) (None, 4, 4, 512) 0
=================================================================
Total params: 20,024,384
Trainable params: 0
Non-trainable params: 20,024,384
_________________________________________________________________
The last layer is now our bottleneck layer. You may notice that the size of
block5_pool
is (4,4,512), whereas before, it was (7,7,512). This is because we instanti‐
ated VGG with an input_shape parameter to account for the size of the images in
our dataset. It’s also worth noting that setting include_top=False is hardcoded to
block5_pool
use as the bottleneck layer, but if you want to customize this, you can
load the full model and delete any additional layers you don’t want to use.
Before this model is ready to be trained, we’ll need to add a few layers on top, specific
to our data and classification task. It’s also important to note that because we’ve set
trainable=False
, there are 0 trainable parameters in the current model.
As a general rule of thumb, the bottleneck layer is typically the last,
lowest-dimensionality, flattened layer before a flattening operation."|bottleneck layer; Transfer Learning design pattern; VGG
"Transform
of data drift or potential training–serving skew.3 The component also
takes output from SchemaGen and is where we perform feature engineering to trans‐
form our data input into the right format for our model. This could include convert‐
ing free-form text inputs into embeddings, normalizing numeric inputs, and more.
Once our data is ready to be fed into a model, we can pass it to the Trainer compo‐
nent. When we set up our Trainer component, we point to a function that defines
our model code, and we can specify where we’d like to train the model. Here, we’ll
show how to use Cloud AI Platform Training from this component. Finally, the
Pusher component handles model deployment. There are many other pre-built com‐
ponents provided by TFX—we’ve only included a few here that we’ll use in our sam‐
ple pipeline.
For this example, we’ll use the NOAA hurricane dataset in BigQuery to build a model
that infers the SSHS code4 for a hurricane. We’ll keep the features, components, and
model code relatively short in order to focus on the pipeline tooling. The steps of our
pipeline are outlined below, and roughly follow the workflow outlined in Figure 6-6:
1. Data collection: run a query to get the hurricane data from BigQuery.
ExampleValidator
2. Data validation: use the component to identify anomalies and
check for data drift.
3. Data analysis and preprocessing: generate some statistics on the data and define
the schema.
4. Model training: train a tf.keras model on AI Platform.
5. Model deployment: deploy the trained model to AI Platform Prediction.5
When our pipeline is complete, we’ll be able to invoke the entire process outlined
above with a single API call. Let’s start by discussing the scaffolding for a typical TFX
pipeline and the process for running it on AI Platform.
<b>BuildingtheTFXpipeline</b>
We’ll use the tfx command-line tools to create and invoke our pipeline. New invoca‐
tions of a pipeline are known as <i>runs,</i> which are distinct from updates we make to the
3 Formoreondatavalidation,see“DesignPattern30:FairnessLens”onpage343inChapter7,ResponsibleAI.
4 SSHSstandsforSaffir–SimpsonHurricaneScale,andisascalefrom1to5usedtomeasurethestrengthand
severityofahurricane.NotethattheMLmodeldoesnotforecasttheseverityofthehurricaneatalatertime.
Instead,itsimplylearnsthewindspeedthresholdsusedintheSaffir–Simpsonscale.
5 Whiledeploymentisthelaststepinourexamplepipeline,productionpipelinesoftenincludemoresteps,
suchasstoringthemodelinasharedrepositoryorexecutingaseparateservingpipelinethatdoesCI/CDand
testing."|BigQuery; Cloud AI Platform Training; Pusher component; runs; TFX; Trainer component; Workflow Pipeline design pattern
"avg_val_mse = (val_mse / num_batches)
hpt.report_hyperparameter_tuning_metric(
hyperparameter_metric_tag='val_mse',
metric_value=avg_val_mse,
global_step=epochs
)
Once we’ve submitted our training job to AI Platform, we can monitor logs in the
Cloud console. After each trial completes, you’ll be able to see the values chosen for
each hyperparameter and the resulting value of your optimization metric, as seen in
Figure 4-25.
<i>Figure</i> <i>4-25.</i> <i>A</i> <i>sample</i> <i>of</i> <i>the</i> <i>HyperTune</i> <i>summary</i> <i>in</i> <i>the</i> <i>AI</i> <i>Platform</i> <i>console.</i> <i>This</i> <i>is</i>
<i>for</i> <i>a</i> <i>PyTorch</i> <i>model</i> <i>optimizing</i> <i>three</i> <i>model</i> <i>parameters,</i> <i>with</i> <i>the</i> <i>goal</i> <i>of</i> <i>minimizing</i>
<i>mean</i> <i>squared</i> <i>error</i> <i>on</i> <i>the</i> <i>validation</i> <i>dataset.</i>
By default, AI Platform Training will use Bayesian optimization for your tuning job,
but you can also specify if you’d like to use grid or random search algorithms instead.
The Cloud service also optimizes your hyperparameter search <i>across</i> training jobs. If
we run another training job similar to the one above, but with a few tweaks to our
hyperparameters and search space, it’ll use the results of our last job to efficiently
choose values for the next set of trials.
We’ve shown a PyTorch example here, but you can use AI Platform Training for
hyperparameter tuning in any machine learning framework by packaging your train‐
ing code and providing a <i>setup.py</i> file that installs any library dependencies.
<b>Geneticalgorithms</b>
We’ve explored various algorithms for hyperparameter optimization: manual search,
grid search, random search, and Bayesian optimization. Another less-common alter‐
native is a genetic algorithm, which is roughly based on Charles Darwin’s"|AI Platform Training; Darwin; genetic algorithms; Hyperparameter Tuning design pattern; survival of the fittest theory
"FeatureSet
directly from the pandas dataframe. We simply specify the column name
that represents the entity. The schema and data types for the features of the Feature
Set
are then inferred from the dataframe:
<i>#</i> <i>Infer</i> <i>the</i> <i>features</i> <i>of</i> <i>the</i> <i>feature</i> <i>set</i> <i>from</i> <i>the</i> <i>pandas</i> <i>DataFrame</i>
taxi_fs.infer_fields_from_df(taxi_df,
entities=[Entity(name='taxi_id', dtype=ValueType.INT64)],
replace_existing_features=True)
<b>RegisteringtheFeatureSet.</b> Once the FeatureSet is created, we can register it with Feast
client.apply(taxi_fs)
using . To confirm that the feature set was correctly regis‐
tered or to explore the contents of another feature set, we can retrieve it
using .get_feature_set(...):
<b>print(client.get_feature_set(""taxi_rides""))</b>
This returns a JSON object containing the data schema for the taxi_rides feature
set:
{
<b>""spec"":</b> {
<b>""name"":</b> ""taxi_rides"",
<b>""entities"":</b> [
{
<b>""name"":</b> ""key"",
<b>""valueType"":</b> ""INT64""
}
],
<b>""features"":</b> [
{
<b>""name"":</b> ""dropoff_lon"",
<b>""valueType"":</b> ""DOUBLE""
},
{
<b>""name"":</b> ""pickup_lon"",
<b>""valueType"":</b> ""DOUBLE""
},
...
...
],
}
}
<b>IngestingfeaturedataintotheFeatureSet.</b>
Once we are happy with our schema, we can
.ingest(...)
ingest the dataframe feature data into Feast using . We’ll specify the
FeatureSet , called taxi_fs , and the dataframe from which to populate the feature
taxi_df
data, called .
<i>#</i> <i>Load</i> <i>feature</i> <i>data</i> <i>into</i> <i>Feast</i> <i>for</i> <i>this</i> <i>specific</i> <i>feature</i> <i>set</i>
client.ingest(taxi_fs, taxi_df)"|Feast; Feature Store design pattern; FeatureSet
"From these Performance & Fairness charts, we can see:
• Our model’s accuracy on loans supervised by HUD is significantly higher—94%
compared to 85%.
• According to the confusion matrix, non-HUD loans are approved at a higher
rate—72% compared to 55%. This is likely due to the data representation bias
identified in the previous section (we purposely left the dataset this way to show
how models can amplify data bias).
There are a few ways to act on these insights, as shown in the “Optimization strategy”
box in Figure 7-14. These optimization methods involve changing our model’s <i>classi‐</i>
<i>fication</i> <i>threshold—the</i> threshold at which a model will output a positive classifica‐
tion. In the context of this model, what confidence threshold are we OK with to mark
an application as “approved”? If our model is more than 60% confident an applica‐
tion should be approved, should we approve it? Or are we only OK approving appli‐
cations when our model is more than 98% confident? This decision is largely
dependent on a model’s context and prediction task. If we’re predicting whether or
not an image contains a cat, we may be OK returning the label “cat” even when our
model is only 60% confident. However, if we have a model that predicts whether or
not a medical image contains a disease, we’d likely want our threshold to be much
higher.
The What-If Tool helps us choose a threshold based on various optimizations. Opti‐
mizing for “Demographic parity,” for example, would ensure that our model appro‐
ves the same percentage of applications for both HUD and non-HUD loans.11
Alternatively, using an equality of opportunity 12 fairness metric will ensure that data‐
points from both the HUD and non-HUD slice with a ground truth value of
“approved” in the test dataset are given an equal chance of being predicted
“approved” by the model.
Note that changing a model’s prediction threshold is only one way to act on fairness
evaluation metrics. There are many other approaches, including rebalancing training
data, retraining a model to optimize for a different metric, and more.
11 ThisarticleprovidesmoredetailontheWhat-IfTool’soptionsforfairnessoptimizationstrategies.
12 Moredetailsonequalityofopportunityasafairnessmetriccanbefoundhere."|bias; classification threshold; Fairness Lens design pattern; What-If Tool
"<header><largefont><b>Design</b></largefont> <largefont><b>Pattern</b></largefont> <largefont><b>23:</b></largefont> <largefont><b>Bridged</b></largefont> <largefont><b>Schema</b></largefont></header>
The Bridged Schema design pattern provides ways to adapt the data used to train a
model from its older, original data schema to newer, better data. This pattern is use‐
ful because when an input provider makes improvements to their data feed, it often
takes time for enough data of the improved schema to be collected for us to ade‐
quately train a replacement model. The Bridged Schema pattern allows us to use as
much of the newer data as is available, but augment it with some of the older data to
improve model accuracy.
<header><largefont><b>Problem</b></largefont></header>
Consider a point-of-sale application that suggests how much to tip a delivery person.
The application might use a machine learning model that predicts the tip amount,
taking into account the order amount, delivery time, delivery distance, and so on.
Such a model would be trained on the actual tips added by customers.
Assume that one of the inputs to the model is the payment type. In the historical
data, this has been recorded as “cash” or “card.” However, let’s say the payment sys‐
tem has been upgraded and it now provides more detail on the type of card (gift card,
debit card, credit card) that was used. This is extremely useful information because
the tipping behavior varies between the three types of cards.
At prediction time, the newer information will always be available since we are
always predicting tip amounts on transactions conducted after the payment system
upgrade. Because the new information is extremely valuable, and it is already avail‐
able in production to the prediction system, we would like to use it in the model as
soon as possible.
We cannot train a new model exclusively on the newer data because the quantity of
new data will be quite small, limited as it is to transactions after the payment system
upgrade. Because the quality of an ML model is highly dependent on the amount of
data used to train it, it is likely that a model trained with only the new data is going to
fare poorly.
<header><largefont><b>Solution</b></largefont></header>
The solution is to bridge the schema of the old data to match the new data. Then, we
train an ML model using as much of the new data as is available and augment it with
the older data. There are two questions to answer. First, how will we square the fact
that the older data has only two categories for payment type, whereas the new data
has four categories? Second, how will the augmentation be done to create datasets for
training, validation, and testing?"|Bridged Schema design pattern
"ple where multiple serving functions could reduce server-side processing time. It’s
also worth noting that we can have multiple serving functions <i>with</i> multiple model
versions, though there is a risk that this could create too much complexity.
<b>Newmodelsversusnewmodelversions</b>
Sometimes it can be difficult to decide whether to create another model version or an
entirely new model resource. We recommend creating a new model when a model’s
prediction task changes. A new prediction task typically results in a different model
output format, and changing this could result in breaking existing clients. If we’re
unsure about whether to use a new version or model, we can think about whether we
want existing clients to upgrade. If the answer is yes, chances are we have improved
the model without changing the prediction task, and creating a new version will suf‐
fice. If we’ve changed the model in a way that would require users to decide whether
they want the update, we’ll likely want to create a new model resource.
To see this in practice, let’s return to our flight prediction model to see an example.
The current model has defined what it considers a delay (30+ minutes late), but our
end users may have different opinions on this. Some users think just 15 minutes late
counts as delayed, whereas others think a flight is only delayed if it’s over an hour
late. Let’s imagine that we’d now like our users to be able to incorporate their own
definition of delayed rather than use ours. In this case we’d use “Design Pattern 5:
Reframing ” on page 80 (discussed in Chapter 3) to change this to a regression model.
The input format to this model is the same, but the output is now a numerical value
representing the delay prediction.
The way our model users parse this response will obviously be different than the first
version. With our latest regression model, app developers might choose to display the
predicted delay when users search for flights, replacing something like “This flight is
usually delayed more than 30 minutes” from the first version. In this scenario, the
flight_model_regres
best solution is to create a new model <i>resource,</i> perhaps called
sion , to reflect the changes. This way, app developers can choose which to use, and
we can continue to make performance updates to each model by deploying new
versions.
<header><largefont><b>Summary</b></largefont></header>
This chapter focused on design patterns that address different aspects of reproduci‐
bility. Starting with the <i>Transform</i> design, we saw how this pattern is used to ensure
reproducibility of the data preparation dependencies between the model training
pipeline and the model serving pipeline. This is achieved by explicitly capturing the
transformations applied to convert the model inputs into the model features. The
<i>Repeatable</i> <i>Splitting</i> design pattern captures the way data is split among training, vali‐"|Model Versioning design pattern
"In such cases, it can be helpful to look at the validation error at the end of every
epoch and stop the training process when the validation error is more than that of the
previous epoch. In Figure 4-9, this will be at the end of the fourth epoch, shown by
the thick dashed line. This is called <i>early</i> <i>stopping.</i>
Had we been checkpointing at the end of every batch, we might
have been able to capture the true minimum, which might have
been a bit before or after the epoch boundary. See the discussion
on virtual epochs in this section for a more frequent way to
checkpoint.
If we are checkpointing much more frequently, it can be helpful if
early stopping isn’t overly sensitive to small perturbations in the
validation error. Instead, we can apply early stopping only after the
validation error doesn’t improve for more than <i>N</i> checkpoints.
<b>Checkpointselection.</b> While early stopping can be implemented by stopping the train‐
ing as soon as the validation error starts to increase, we recommend training longer
and choosing the optimal run as a postprocessing step. The reason we suggest train‐
ing well into phase 3 (see the preceding “Why It Works” section for an explanation of
the three phases of the training loop) is that it is not uncommon for the validation
error to increase for a bit and then start to drop again. This is usually because the
training initially focuses on more common scenarios (phase 1), then starts to home in
on the rarer situations (phase 2). Because rare situations may be imperfectly sampled
between the training and validation datasets, occasional increases in the validation
error during the training run are to be expected in phase 2. In addition, there are sit‐
uations endemic to big models where deep double descent is expected, and so it is
essential to train a bit longer just in case.
In our example, instead of exporting the model at the end of the training run, we will
load up the fourth checkpoint and export our final model from there instead. This is
called <i>checkpoint</i> <i>selection,</i> and in TensorFlow, it can be achieved using BestExporter.
<b>Regularization.</b>
Instead of using early stopping or checkpoint selection, it can be help‐
ful to try to add L2 regularization to your model so that the validation error does not
increase and the model never gets into phase 3. Instead, both the training loss and the
validation error should plateau, as shown in Figure 4-10. We term such a training
loop (where both training and validation metrics reach a plateau) a <i>well-behaved</i>
training loop."|checkpoint selection; checkpoints; Checkpoints design pattern; early stopping; epochs; regularization; TensorFlow; training loop; well-behaved training loop
"Here, the distance between this example and each centroid is quite large. We could
then use these high-distance values to conclude that this data point might be an
anomaly. This unsupervised clustering approach is especially useful if we don’t know
the labels for our data in advance. Once we’ve generated cluster predictions on
enough examples, we could then build a supervised learning model using the predic‐
ted clusters as labels.
<b>Numberofminorityclassexamplesavailable</b>
While the minority class in our first fraud detection example only made up 0.1% of
the data, the dataset was large enough that we still had 8,000 fraudulent data points to
work with. For datasets with even fewer examples of the minority class, downsam‐
pling may make the resulting dataset too small for a model to learn from. There isn’t
a hard-and-fast rule for determining how many examples is too few to use downsam‐
pling, since it largely depends on our problem and model architecture. A general rule
of thumb is that if you only have hundreds of examples of the minority class, you
might want to consider a solution other than downsampling for handling dataset
imbalance.
It’s also worth noting that the natural effect of removing a subset of our majority class
is losing some information stored in those examples. This might slightly decrease our
model’s ability to identify the majority class, but often the benefits of downsampling
still outweigh this.
<b>Combiningdifferenttechniques</b>
The downsampling and class weight techniques described above can be combined for
optimal results. To do this, we start by downsampling our data until we find a bal‐
ance that works for our use case. Then, based on the label ratios for the rebalanced
dataset, use the method described in the weighted classes section to pass new weights
to our model. Combining these approaches can be especially useful when we have an
anomaly detection problem and care most about predictions for our minority class.
For example, if we’re building a fraud detection model, we’re likely much more con‐
cerned about the transactions our model flags as “fraud” rather than the ones it flags
as “nonfraud.” Additionally, as mentioned by SMOTE, the approach of generating
synthetic examples from the minority class is often combined with removing a ran‐
dom sample of examples from the minority class.
Downsampling is also often combined with the Ensemble design pattern. Using this
approach, instead of entirely removing a random sample of our majority class, we use
different subsets of it to train multiple models and then ensemble those models. To
illustrate this, let’s say we have a dataset with 100 minority class examples and 1,000
majority examples. Rather than removing 900 examples from our majority class to
perfectly balance the dataset, we’d randomly split the majority examples into 10
groups with 100 examples each. We’d then train 10 classifiers, each with the same 100"|anomaly detection; centroid; downsampling; Ensemble design pattern; fraud detection; Rebalancing design pattern; SMOTE
"experimentation (where there is no validation dataset) and plays the part of the vali‐
dation dataset in production (where there is no test dataset).
The larger your training dataset, the more complex a model you can use, and the
more accurate a model you can get. Using regularization rather than early stopping
or checkpoint selection allows you to use a larger training dataset. In the experimen‐
tation phase (when you are exploring different model architectures, training techni‐
ques, and hyperparameters), we recommend that you turn off early stopping and
train with larger models (see also “Design Pattern 11: Useful Overfitting” on page
141). This is to ensure that the model has enough capacity to learn the predictive pat‐
terns. During this process, monitor error convergence on the training split. At the
end of experimentation, you can use the evaluation dataset to diagnose how well your
model does on data it has not encountered during training.
When training the model to deploy in production, you will need to prepare to be able
to do continuous evaluation and model retraining. Turn on early stopping or check‐
point selection and monitor the error metric on the evaluation dataset. Choose
between early stopping and checkpoint selection depending on whether you need to
control cost (in which case, you would choose early stopping) or want to prioritize
model accuracy (in which case, you would choose checkpoint selection).
<b>Fine-tuning</b>
In a well-behaved training loop, gradient descent behaves such that you get to the
neighborhood of the optimal error quickly on the basis of the majority of your data,
then slowly converge toward the lowest error by optimizing on the corner cases.
Now, imagine that you need to periodically retrain the model on fresh data. You typi‐
cally want to emphasize the fresh data, not the corner cases from last month. You are
often better off resuming your training, not from the last checkpoint, but the check‐
point marked by the blue line in Figure 4-11. This corresponds to the start of phase 2
in our discussion of the phases of model training described earlier in “Why It Works”
on page 169. This helps ensure that you have a general method that you are able to then
fine-tune for a few epochs on just the fresh data.
When you resume from the checkpoint marked by the thick dashed vertical line, you
will be on the fourth epoch, and so the learning rate will be quite low. Therefore, the
fresh data will not dramatically change the model. However, the model will behave
optimally (in the context of the larger model) on the fresh data because you will have
sharpened it on this smaller dataset. This is called <i>fine-tuning.</i> Fine-tuning is also dis‐
cussed in “Design Pattern 13: Transfer Learning” on page 161."|capacity; checkpoint selection; Checkpoints design pattern; fine-tuning; regularization; well-behaved training loop
"The implementation in scikit-learn is also straightforward:
<b>from</b> <b>sklearn.ensemble</b> <b>import</b> GradientBoostingRegressor
<i>#</i> <i>Create</i> <i>the</i> <i>Gradient</i> <i>Boosting</i> <i>regressor</i>
GB_model = GradientBoostingRegressor(n_estimators=1,
max_depth=1,
learning_rate=1,
criterion='mse')
<i>#</i> <i>Fit</i> <i>on</i> <i>training</i> <i>data</i>
GB_model.fit(X_train, Y_train)
<b>Stacking</b>
Stacking is an ensemble method that combines the outputs of a collection of models
to make a prediction. The initial models, which are typically of different model types,
are trained to completion on the full training dataset. Then, a secondary meta-model
is trained using the initial model outputs as features. This second meta-model learns
how to best combine the outcomes of the initial models to decrease the training error
and can be any type of machine learning model.
To implement a stacking ensemble, we first train all the members of the ensemble on
the training dataset. The following code calls a function, fit_model , that takes as
X_train Y_train
arguments a model and the training dataset inputs and label . This
way <i>members</i> is a list containing all the trained models in our ensemble. The full code
for this example can be found in the code repository for this book:
members = [model_1, model_2, model_3]
<i>#</i> <i>fit</i> <i>and</i> <i>save</i> <i>models</i>
n_members = len(members)
<b>for</b> i <b>in</b> range(n_members):
<i>#</i> <i>fit</i> <i>model</i>
model = fit_model(members[i])
<i>#</i> <i>save</i> <i>model</i>
filename = 'models/model_' + str(i + 1) + '.h5'
model.save(filename, save_format='tf')
<b>print('Saved</b> {}\n'.format(filename))
These submodels are incorporated into a larger stacking ensemble model as individ‐
ual inputs. Since these input models are trained alongside the secondary ensemble
model, we fix the weights of these input models. This can be done by setting
layer.trainable False
to for the ensemble member models:
<b>for</b> i <b>in</b> range(n_members):
model = members[i]
<b>for</b> layer <b>in</b> model.layers:
<i>#</i> <i>make</i> <i>not</i> <i>trainable</i>
layer.trainable = False"|Ensemble design pattern; stacking
"For example, if our model is classifying images as cats, dogs, or rabbits, the softmax
output might look like this for a given image: [ .89 , .02 , .09 ]. This means our model
is predicting an 89% chance the image is a cat, 2% chance it’s a dog, and 9% chance
it’s a rabbit. Because each image can have <i>only</i> <i>one</i> <i>possible</i> <i>label</i> in this scenario, we
can take the argmax (index of the highest probability) to determine our model’s pre‐
dicted class. The less-common scenario is when each training example can be
assigned <i>more</i> <i>than</i> <i>one</i> label, which is what this pattern addresses.
The Multilabel design pattern exists for models trained on all data modalities. For
image classification, in the earlier cat, dog, rabbit example, we could instead use
training images that each depicted <i>multiple</i> animals, and could therefore have multi‐
ple labels. For text models, we can imagine a few scenarios where text can be labeled
with multiple tags. Using the dataset of Stack Overflow questions on BigQuery as an
example, we could build a model to predict the tags associated with a particular ques‐
tion. As an example, the question “How do I plot a pandas DataFrame?” could be tag‐
ged as “Python,” “pandas,” and “visualization.” Another multilabel text classification
example is a model that identifies toxic comments. For this model, we might want to
flag comments with multiple toxicity labels. A comment could therefore be labeled
both “hateful” and “obscene.”
This design pattern can also apply to tabular datasets. Imagine a healthcare dataset
with various physical characteristics for each patient, like height, weight, age, blood
pressure, and more. This data could be used to predict the presence of multiple con‐
ditions. For example, a patient could show risk of both heart disease and diabetes.
<header><largefont><b>Solution</b></largefont></header>
The solution for building models that can assign <i>more</i> <i>than</i> <i>one</i> <i>label</i> to a given train‐
ing example is to use the <i>sigmoid</i> activation function in our final output layer. Rather
than generating an array where all values sum to 1 (as in softmax), each <i>individual</i>
value in a sigmoid array is a float between 0 and 1. That is to say, when implementing
the Multilabel design pattern, our label needs to be multi-hot encoded. The length of
the multi-hot array corresponds with the number of classes in our model, and each
output in this label array will be a sigmoid value.
Building on the image example above, let’s say our training dataset included images
with more than one animal. The sigmoid output for an image that contained a cat
and a dog but not a rabbit might look like the following: [ .92 , .85 , .11 ]. This output
means the model is 92% confident the image contains a cat, 85% confident it contains
a dog, and 11% confident it contains a rabbit.
A version of this model for 28×28-pixel images with sigmoid output might look like
this, using the Keras Sequential API:
model = keras.Sequential([
keras.layers.Flatten(input_shape=(28, 28)),"|BigQuery; Keras Sequential API; Multilabel design pattern; sigmoid activation; Stack Overflow; tabular data
"more in-depth overview of the What-If Tool in the Fairness Lens design pattern—
here, we’ll focus specifically on its counterfactual analysis functionality. When visual‐
izing data points from our test set in the What-If Tool, we have the option to show
the nearest counterfactual data point to the one we’re selecting. Doing this will let us
compare feature values and model predictions for these two data points, which can
help us better understand the features our model is relying on most. In Figure 7-7, we
see a counterfactual comparison for two data points from a mortgage application
dataset. In bold, we see the features where these two data points are different, and at
the bottom, we can see the model output for each.
Example-based explanations compare new examples and their corresponding predic‐
tions to similar examples from our training dataset. This type of explanation is espe‐
cially useful for understanding how our training dataset affects model behavior.
Example-based explanations work best on image or text data, and can be more intu‐
itive than feature attributions or counterfactual analysis since they map a model’s
prediction directly to the data used for training."|counterfactual analysis; example-based explanation; Explainable Predictions design pattern
"cycle which customers have discontinued their service. Or, for a financial forecasting
model, the true revenue isn’t known until after that quarter’s close and earnings
report. In either of these cases, evaluation cannot take place until ground truth data is
available.
To see how continuous evaluation works, we’ll deploy a text classification model
trained on the HackerNews dataset to Google Cloud AI Platform. The full code for
this example can be found in the continuous evaluation notebook in the repository
accompanying this book.
<b>Deployingthemodel</b>
The input for our training dataset is an article title and its associated label is the news
nytimes techcrunch github
source where the article originated, either , , or . As news
trends evolve over time, the words associated with a <i>New</i> <i>York</i> <i>Times</i> headline will
change. Similarly, releases of new technology products will affect the words to be
found in TechCrunch. Continuous evaluation allows us to monitor model predic‐
tions to track how those trends affect our model performance and kick off retraining
if necessary.
Suppose that the model is exported with a custom serving input function as described
in “Design Pattern 16: Stateless Serving Function” on page 201:
@tf.function(input_signature=[tf.TensorSpec([None], dtype=tf.string)])
<b>def</b> source_name(text):
labels = tf.constant(['github', 'nytimes', 'techcrunch'],dtype=tf.string)
probs = txtcls_model(text, training=False)
indices = tf.argmax(probs, axis=1)
pred_source = tf.gather(params=labels, indices=indices)
pred_confidence = tf.reduce_max(probs, axis=1)
<b>return</b> {'source': pred_source,
'confidence': pred_confidence}
After deploying this model, when we make an online prediction, the model will
return the predicted news source as a string value and a numeric score of that predic‐
tion label related to how confident the model is. For example, we can create an online
prediction by writing an input JSON example to a file called <i>input.json</i> to send for
prediction:
%%writefile input.json
{""text"":
""YouTube introduces Video Chapters to make it easier to navigate longer videos""}
This returns the following prediction output:
CONFIDENCE SOURCE
0.918685 techcrunch"|Continued Model Evaluation design pattern
"these metrics, but that is OK since precision, recall, and F-score are a better indica‐
tion of model performance in this case.
Note that, when evaluating models trained on imbalanced datasets, we need to use
<i>unsampled</i> <i>data</i> when calculating success metrics. This means that no matter how we
modify our dataset for training per the solutions we’ll outline below, we should leave
our test set as is so that it provides an accurate representation of the original dataset.
In other words, our test set should have roughly the same class balance as the original
dataset. For the example above, that would be 5% fraud/95% nonfraud.
If we are looking for a metric that captures the performance of the model across all
thresholds, average precision-recall is a more informative metric than area under the
ROC curve (AUC) for model evaluation. This is because average precision-recall
places more emphasis on how many predictions the model got right out of the <i>total</i>
number it assigned to the positive class. This gives more weight to the positive class,
which is important for imbalanced datasets. The AUC, on the other hand, treats both
classes equally and is less sensitive to model improvements, which isn’t optimal in sit‐
uations with imbalanced data.
<b>Downsampling</b>
Downsampling is a solution for handling imbalanced datasets by changing the
underlying dataset, rather than the model. With downsampling, we decrease the
number of examples from the majority class used during model training. To see how
this works, let’s take a look at the synthetic fraud detection dataset on Kaggle. 4 Each
example in the dataset contains various information about the transaction, including
the transaction type, the amount of the transaction, and the account balance both
before and after the transaction took place. The dataset contains 6.3 million exam‐
ples, only 8,000 of which are fraudulent transactions. That’s a mere 0.1% of the entire
dataset.
While a large dataset can often improve a model’s ability to identify patterns, it’s less
helpful when the data is significantly imbalanced. If we train a model on this entire
dataset (6.3M rows) without any modifications, chances are we’ll see a misleading
accuracy of 99.9% as a result of the model randomly guessing the nonfraudulent class
each time. We can solve for this by removing a large chunk of the majority class from
the dataset.
We’ll take all 8,000 of the fraudulent examples and set them aside to use when train‐
ing the model. Then, we’ll take a small, random sample of the nonfraudulent transac‐
4 ThedatasetwasgeneratedbasedonthePaySimresearchproposedinthispaper:EdgarLopez-Rojas,Ahmad
Elmir,andStefanAxelsson,“PaySim:Afinancialmobilemoneysimulatorforfrauddetection,”28thEuro‐
<i>peanModelingandSimulationSymposium,EMSS,Larnaca,Cyprus(2016):249–255.</i>"|downsampling; fraud detection; Rebalancing design pattern; unsampled data
"purchases based on items in your shopping cart. Recommendation systems are popu‐
lar throughout many businesses, particularly for product recommendation, personal‐
ized and dynamic marketing, and streaming video or music platforms.
• Embeddings
• Ensemble
• Multilabel
• Transfer Learning
• Feature Store
• Hashed Feature
• Reframing
• Transform
• Windowed Inference
• Two-Phase Predictions
• Neutral Class
• Multimodal Input
• Batch Serving
<header><largefont><b>Fraud</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Anomaly</b></largefont> <largefont><b>Detection</b></largefont></header>
Many financial institutions use machine learning for fraud detection to keep their
consumers’ accounts safe. These machine learning models are trained to flag transac‐
tions that appear fraudulent based on certain characteristics or patterns that have
been learned in the data.
More broadly, anomaly detection is a technique used to find abnormal behavior or
outlier elements in a dataset. Anomalies can arise as spikes or dips that deviate from
the normal patterns, or they can be longer-term abnormal trends. Anomaly detection
shows up through many different use cases in machine learning and might even be
used in conjunction with a separate use case. For example, consider a machine learn‐
ing model that identifies anomalous train tracks based on images.
• Rebalancing
• Feature Cross
• Embeddings
• Ensemble
• Two-Phase Predictions
• Transform"|anomaly detection
"linear_model.LinearRegression().fit(scaled, diabetes_y)
raw_time = timeit.timeit(train_raw, number=1000)
scaled_time = timeit.timeit(train_scaled, number=1000)
When we ran this, we got a nearly 9% improvement on this model which uses just
one input feature. Considering the number of features in a typical machine learning
model, the savings can add up.
Another important reason for scaling is that some machine learning algorithms and
techniques are very sensitive to the relative magnitudes of the different features. For
example, a k-means clustering algorithm that uses the Euclidean distance as its prox‐
imity measure will end up relying heavily on features with larger magnitudes. Lack of
scaling also affects the efficacy of L1 or L2 regularization since the magnitude of
weights for a feature depends on the magnitude of values of that feature, and so dif‐
ferent features will be affected differently by regularization. By scaling all features to
lie between [–1, 1], we ensure that there is not much of a difference in the relative
magnitudes of different features.
<b>Linearscaling</b>
Four forms of scaling are commonly employed:
<i>Min-max</i> <i>scaling</i>
The numeric value is linearly scaled so that the minimum value that the input
can take is scaled to –1 and the maximum possible value to 1:
x1_scaled = (2*x1 - max_x1 - min_x1)/(max_x1 - min_x1)
The problem with min-max scaling is that the maximum and minimum value
(max_x1 min_x1)
and have to be estimated from the training dataset, and they are
often outlier values. The real data often gets shrunk to a very narrow range in the
[–1, 1] band.
<i>Clipping</i> <i>(in</i> <i>conjunction</i> <i>with</i> <i>min-max</i> <i>scaling)</i>
Helps address the problem of outliers by using “reasonable” values instead of
estimating the minimum and maximum from the training dataset. The numeric
value is linearly scaled between these two reasonable bounds, then clipped to lie
in the range [–1, 1]. This has the effect of treating outliers as –1 or 1.
<i>Z-score</i> <i>normalization</i>
Addresses the problem of outliers without requiring prior knowledge of what the
reasonable range is by linearly scaling the input using the mean and standard
deviation estimated over the training dataset:
x1_scaled = (x1 - mean_x1)/stddev_x1"|clipping; min-max scaling; scaling; z-score normalization
"cycle. This might seem trivial with a small model like the scikit-learn one we trained
above, but for many production models, the training process requires significant
infrastructure and time.
Instead of training our model each time we try a new combination of hyperparame‐
ters, Bayesian optimization defines a new function that emulates our model but is
much cheaper to run. This is referred to as the <i>surrogate</i> <i>function—the</i> inputs to this
function are your hyperparameter values and the output is your optimization metric.
The surrogate function is called much more frequently than the objective function,
with the goal of finding an optimal combination of hyperparameters <i>before</i> complet‐
ing a training run on your model. With this approach, more compute time is spent
choosing the hyperparameters for each trial as compared with grid search. However,
because this is significantly cheaper than running our objective function each time we
try different hyperparameters, the Bayesian approach of using a surrogate function is
preferred. Common approaches to generate the surrogate function include a Gaus‐
sian process or a tree-structured Parzen estimator.
So far, we’ve touched on the different pieces of Bayesian optimization, but how do
they work together? First, we must choose the hyperparameters we want to optimize
and define a range of values for each hyperparameter. This part of the process is
manual and will define the space in which our algorithm will search for optimal val‐
ues. We’ll also need to define our objective function, which is the code that calls our
model training process. From there, Bayesian optimization develops a surrogate
function to simulate our model training process and uses that function to determine
the best combination of hyperparameters to run on our model. It is only once this
surrogate arrives at what it thinks is a good combination of hyperparameters that we
do a full training run (trial) on our model. The results of this are then fed back to the
surrogate function and the process is repeated for the number of trials we’ve
specified.
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Genetic algorithms are an alternative to Bayesian methods for hyperparameter tun‐
ing, but they tend to require many more model training runs than Bayesian methods.
We’ll also show you how to use a managed service for hyperparameter tuning optimi‐
zation on models built with a variety of ML frameworks.
<b>Fullymanagedhyperparametertuning</b>
keras-tuner
The approach may not scale to large machine learning problems
because we’d like the trials to happen in parallel, and the likelihood of machine error
and other failure increases as the time for model training stretches into the hours.
Hence, a fully managed, resilient approach that provides black-box optimization is
useful for hyperparameter tuning. An example of a managed service that implements"|Bayesian optimization; Gaussian process; genetic algorithms; Hyperparameter Tuning design pattern; Parzen estimator; surrogate function
"into chunks of sliding windows allows our model to recognize more granular details
in an image like edges and shapes.
<b>Combiningdifferentimagerepresentations.</b>
In addition, as with the bag of words and
text embedding, it may be useful to represent the same image data in multiple ways.
Again, we can accomplish this with the Keras functional API.
Here’s how we’d combine our pixel values with the sliding window representation
using the Keras Concatenate layer:
<i>#</i> <i>Define</i> <i>image</i> <i>input</i> <i>layer</i> <i>(same</i> <i>shape</i> <i>for</i> <i>both</i> <i>pixel</i> <i>and</i> <i>tiled</i>
<i>#</i> <i>representation)</i>
image_input = Input(shape=(28,28,3))
<i>#</i> <i>Define</i> <i>pixel</i> <i>representation</i>
pixel_layer = Flatten()(image_input)
<i>#</i> <i>Define</i> <i>tiled</i> <i>representation</i>
tiled_layer = Conv2D(filters=16, kernel_size=3,
activation='relu')(image_input)
tiled_layer = MaxPooling2D()(tiled_layer)
tiled_layer = tf.keras.layers.Flatten()(tiled_layer)
<i>#</i> <i>Concatenate</i> <i>into</i> <i>a</i> <i>single</i> <i>layer</i>
merged_image_layers = keras.layers.concatenate([pixel_layer, tiled_layer])
To define a model that accepts that multimodal input representation, we can then
feed our concatenated layer into our output layer:
merged_dense = Dense(16, activation='relu')(merged_image_layers)
merged_output = Dense(1)(merged_dense)
model = Model(inputs=image_input, outputs=merged_output)
Choosing which image representation to use or whether to use multimodal represen‐
tations depends largely on the type of image data we’re working with. In general, the
more detailed our images, the more likely it is that we’ll want to represent them as
tiles or sliding windows of tiles. For the MNIST dataset, representing images as pixel
values alone may suffice. With complex medical images, on the other hand, we may
see increased accuracy by combining multiple representations. Why combine multi‐
ple image representations? Representing images as pixel values allows the model to
identify higher-level focus points in an image like dominant, high-contrast objects.
Tiled representations, on the other hand, help models identify more granular, lower-
contrast edges and shapes."|MNIST dataset; Multimodal Input design pattern
"based on predictions. And predictions have errors. So, the second and third models
will be required to make predictions on data that they might have never seen during
training.
As an extreme example, assume that the address that resellers provide is always in an
industrial area of the city, whereas retail buyers can live anywhere. If the first (classifi‐
cation) model makes a mistake and a retail buyer is wrongly identified as a reseller,
the cancellation prediction model that is invoked will not have the neighborhood
where the customer lives in its vocabulary.
How do we train a cascade of models where the output of one model is an input to
the following model or determines the selection of subsequent models?
<header><largefont><b>Solution</b></largefont></header>
Any machine learning problem where the output of the one model is an input to the
following model or determines the selection of subsequent models is called a <i>cascade.</i>
Special care has to be taken when training a cascade of ML models.
For example, a machine learning problem that sometimes involves unusual circum‐
stances can be solved by treating it as a cascade of four machine learning problems:
1. A classification model to identify the circumstance
2. One model trained on unusual circumstances
3. A separate model trained on typical circumstances
4. A model to combine the output of the two separate models, because the output is
a probabilistic combination of the two outputs
This appears, at first glance, to be a specific case of the Ensemble design pattern, but
is considered separately because of the special experiment design required when
doing a cascade.
As an example, assume that, in order to estimate the cost of stocking bicycles at sta‐
tions, we wish to predict the distance between rental and return stations for bicycles
in San Francisco. The goal of the model, in other words, is to predict the distance we
need to transport the bicycle back to the rental location given features such as the
time of day the rental starts, where the bicycle is being rented from, whether the
renter is a subscriber or not, etc. The problem is that rentals that are longer than four
hours involve extremely different renter behavior than shorter rentals, and the stock‐
ing algorithm requires both outputs (the probability that the rental is longer than
four hours and the likely distance the bicycle needs to be transported). However, only
a very small fraction of rentals involve such abnormal trips."|cascade; Cascade design pattern; Ensemble design pattern
"becomes important to be able to update the model parameters with each element.
This can be done by changing the ModelFn as follows (full code is on GitHub):
<b>class</b> <b>OnlineModelFn(beam.CombineFn):</b>
...
<b>def</b> add_input(self, inmem_state, input_dict):
(sum, sumsq, count) = inmem_state
input = input_dict['delay']
<b>return</b> (sum + input, sumsq + input*input, count + 1)
<b>def</b> extract_output(self, inmem_state):
(sum, sumsq, count) = inmem_state
...
mean = sum / count
variance = (sumsq / count) - mean*mean
stddev = np.sqrt(variance) <b>if</b> variance > 0 <b>else</b> 0
<b>return</b> {
'prediction': mean,
'acceptable_deviation': 4 * stddev
}
...
The key difference is that the only thing held in memory are three floating point
sum sum2 count
numbers ( , , ) required to extract the output model state, not the entire
dataframe of received instances. Updating the model parameters one instance at a
time is called an <i>online</i> <i>update</i> and is something that can be done only if the model
training doesn’t require iteration over the entire dataset. Therefore, in the above
x2
implementation, the variance is computed by maintaining a sum of so that we
don’t need a second pass through the data after computing the mean.
<b>StreamingSQL</b>
If our infrastructure consists of a high-performance SQL database that is capable of
processing streaming data, it is possible to implement the Windowed Inference pat‐
tern in an alternative way by using an aggregation window (full code is on GitHub).
We pull out the flight data from BigQuery:
<b>WITH</b> <b>data</b> <b>AS</b> (
<b>SELECT</b>
PARSE_DATETIME('%Y-%m-%d-%H%M',
CONCAT(CAST(date <b>AS</b> STRING),
'-', FORMAT('%04d', arrival_schedule))
) <b>AS</b> scheduled_arrival_time,
arrival_delay
<b>FROM</b> `bigquery-samples.airline_ontime_data.flights`
<b>WHERE</b> arrival_airport = 'DFW' <b>AND</b> SUBSTR(date, 0, 7) = '2010-05'
),
model_state
Then, we create the by computing the model parameters over a time
window specified as two hours preceding to one second preceding:"|online update; Windowed Inference design pattern
"explainability is <i>one</i> approach for diagnosing the presence of bias. For example,
applying explainability to a sentiment analysis model might reveal that the model is
relying on identity terms to make its prediction when it should instead be using
words like “worst,” “amazing,” or “not.”
Explainability can also be used outside the context of fairness to reveal things like
why a model is flagging particular fraudulent transactions, or the pixels that caused a
model to predict “diseased” in a medical image. Explainability, therefore, is a method
for improving model transparency. Sometimes transparency can reveal areas where a
model is treating certain groups unfairly, but it can also provide higher-level insight
into a model’s decision-making process.
<header><largefont><b>Summary</b></largefont></header>
While Peter Parker may not have been referring to machine learning when he said,
“With great power comes great responsibility,” the quote certainly applies here. ML
has the power to disrupt industries, improve productivity, and generate new insights
from data. With this potential, it’s especially important that we understand how our
models will impact different groups of stakeholders. Model stakeholders could
include varying demographic slices of model users, regulatory groups, a data science
team, or business teams within an organization.
The Responsible AI patterns outlined in this chapter are an essential part of every ML
workflow—they can help us better understand the predictions generated by our mod‐
els and catch potential adverse behavior before models go to production. Starting
with the <i>Heuristic</i> <i>Benchmark</i> pattern, we looked at how to identify an initial metric
for model evaluation. This metric is useful as a comparison point for understanding
subsequent model versions and summarizing model behavior for business decision
makers. In the <i>Explainable</i> <i>Predictions</i> pattern, we demonstrated how to use feature
attributions to see which features were most important in signaling a model’s predic‐
tion. Feature attributions are one type of explainability method and can be used for
both evaluating the prediction on a single example or over a group of test inputs.
Finally, the <i>Fairness</i> <i>Lens</i> design pattern presented tools and metrics for ensuring a
model’s predictions treat all groups of users in a way that is fair, equitable, and
unbiased."|bias; explainability; Fairness Lens design pattern
"tendency to be underfit. By iteratively focusing on the hard-to-predict examples,
boosting effectively decreases the bias of the resulting model.
<b>Stacking</b>
Stacking can be thought of as an extension of simple model averaging where we train
<i>k</i> models to completion on the training dataset, then average the results to determine
a prediction. Simple model averaging is similar to bagging, but the models in the
ensemble could be of different types, while for bagging, the models are of the same
type. More generally, we could modify the averaging step to take a weighted average,
for example, to give more weight to one model in our ensemble over the others, as
shown in Figure 3-14.
<i>Figure</i> <i>3-14.</i> <i>The</i> <i>simplest</i> <i>form</i> <i>of</i> <i>model</i> <i>averaging</i> <i>averages</i> <i>the</i> <i>outputs</i> <i>of</i> <i>two</i> <i>or</i> <i>more</i>
<i>different</i> <i>machine</i> <i>learning</i> <i>models.</i> <i>Alternatively,</i> <i>the</i> <i>average</i> <i>could</i> <i>be</i> <i>replaced</i> <i>with</i> <i>a</i>
<i>weighted</i> <i>average</i> <i>where</i> <i>the</i> <i>weight</i> <i>might</i> <i>be</i> <i>based</i> <i>on</i> <i>the</i> <i>relative</i> <i>accuracy</i> <i>of</i> <i>the</i>
<i>models.</i>
You can think of stacking as a more advanced version of model averaging, where
instead of taking an average or weighted average, we train a second machine learning
model on the outputs to learn how best to combine the results to the models in our
ensemble to produce a prediction as shown in Figure 3-15. This provides all the bene‐
fits of decreasing variance as with bagging techniques but also controls for high bias.
<i>Figure</i> <i>3-15.</i> <i>Stacking</i> <i>is</i> <i>an</i> <i>ensemble</i> <i>learning</i> <i>technique</i> <i>that</i> <i>combines</i> <i>the</i> <i>outputs</i> <i>of</i>
<i>several</i> <i>different</i> <i>ML</i> <i>models</i> <i>as</i> <i>the</i> <i>input</i> <i>to</i> <i>a</i> <i>secondary</i> <i>ML</i> <i>model</i> <i>that</i> <i>makes</i>
<i>predictions.</i>
<header><largefont><b>Trade-Offs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Alternatives</b></largefont></header>
Ensemble methods have become quite popular in modern machine learning and have
played a large part in winning well-known challenges, perhaps most notably the"|Ensemble design pattern; Netflix Prize; stacking
