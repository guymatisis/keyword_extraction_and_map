artists and venues, the loan problem on personal income, and the taxi duration on
urban traffic patterns. For these reasons, there are inherent challenges in transferring
the learnings from one tabular model to another.
Although transfer learning is not yet as common on tabular data as it is for image and
text domains, a new model architecture called TabNet presents novel research in this
area. Most tabular models require significant feature engineering when compared
with image and text models. TabNet employs a technique that first uses unsupervised
learning to learn representations for tabular features, and then fine-tunes these
learned representations to produce predictions. In this way, TabNet automates fea‐
ture engineering for tabular models.
<b>Embeddingsofwordsversussentences</b>
In our discussion of text embeddings so far, we’ve referred mostly to <i>word</i> embed‐
dings. Another type of text embedding is <i>sentence</i> embeddings. Where word embed‐
dings represent individual words in a vector space, sentence embeddings represent
entire sentences. Consequently, word embeddings are context agnostic. Let’s see how
this plays out with the following sentence:
<i>“I’ve</i> <i>left</i> <i>you</i> <i>fresh</i> <i>baked</i> <i>cookies</i> <i>on</i> <i>the</i> <i>left</i> <i>side</i> <i>of</i> <i>the</i> <i>kitchen</i> <i>counter.”</i>
Notice that the word <i>left</i> appears twice in that sentence, first as a verb and then as an
adjective. If we were to generate word embeddings for this sentence, we’d get a sepa‐
rate array for each word. With word embeddings, the array for both instances of the
word <i>left</i> would be the same. Using sentence-level embeddings, however, we’d get a
single vector to represent the entire sentence. There are several approaches for gener‐
ating sentence embeddings—from averaging a sentence’s word embeddings to train‐
ing a supervised learning model on a large corpus of text to generate the embeddings.
How does this relate to transfer learning? The latter method—training a supervised
learning model to generate sentence-level embeddings—is actually a form of transfer
learning. This is the approach used by Google’s Universal Sentence Encoder (avail‐
able in TF Hub) and BERT. These methods differ from word embeddings in that they
go beyond simply providing a weight lookup for individual words. Instead, they have
been built by training a model on a large dataset of varied text to understand the
meaning conveyed by <i>sequences</i> of words. In this way, they are designed to be trans‐
ferred to different natural language tasks and can thus be used to build models that
implement transfer learning.