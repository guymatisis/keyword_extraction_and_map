explainability is <i>one</i> approach for diagnosing the presence of bias. For example,
applying explainability to a sentiment analysis model might reveal that the model is
relying on identity terms to make its prediction when it should instead be using
words like “worst,” “amazing,” or “not.”
Explainability can also be used outside the context of fairness to reveal things like
why a model is flagging particular fraudulent transactions, or the pixels that caused a
model to predict “diseased” in a medical image. Explainability, therefore, is a method
for improving model transparency. Sometimes transparency can reveal areas where a
model is treating certain groups unfairly, but it can also provide higher-level insight
into a model’s decision-making process.
<header><largefont><b>Summary</b></largefont></header>
While Peter Parker may not have been referring to machine learning when he said,
“With great power comes great responsibility,” the quote certainly applies here. ML
has the power to disrupt industries, improve productivity, and generate new insights
from data. With this potential, it’s especially important that we understand how our
models will impact different groups of stakeholders. Model stakeholders could
include varying demographic slices of model users, regulatory groups, a data science
team, or business teams within an organization.
The Responsible AI patterns outlined in this chapter are an essential part of every ML
workflow—they can help us better understand the predictions generated by our mod‐
els and catch potential adverse behavior before models go to production. Starting
with the <i>Heuristic</i> <i>Benchmark</i> pattern, we looked at how to identify an initial metric
for model evaluation. This metric is useful as a comparison point for understanding
subsequent model versions and summarizing model behavior for business decision
makers. In the <i>Explainable</i> <i>Predictions</i> pattern, we demonstrated how to use feature
attributions to see which features were most important in signaling a model’s predic‐
tion. Feature attributions are one type of explainability method and can be used for
both evaluating the prediction on a single example or over a group of test inputs.
Finally, the <i>Fairness</i> <i>Lens</i> design pattern presented tools and metrics for ensuring a
model’s predictions treat all groups of users in a way that is fair, equitable, and
unbiased.