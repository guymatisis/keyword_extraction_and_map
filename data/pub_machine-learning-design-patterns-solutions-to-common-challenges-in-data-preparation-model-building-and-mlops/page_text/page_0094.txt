The first case (binary classification) is unique in that it is the only type of single-label
classification problem where we would consider using sigmoid as our activation
function. For nearly any other multiclass classification problem (for example, classi‐
fying text into one of five possible categories), we would use softmax. However, when
we only have two classes, softmax is redundant. Take for example a model that pre‐
dicts whether or not a specific transaction is fraudulent. Had we used a softmax out‐
put in this example, here’s what a fraudulent model prediction might look like:
[.02, .98]
In this example, the first index corresponds with “not fraudulent” and the second
index corresponds with “fraudulent.” This is redundant because we could also repre‐
sent this with a single scalar value, and thus use a sigmoid output. The same predic‐
tion could be represented as simply .98. Because each input can only be assigned a
.98
single class, we can infer from this output of that the model has predicted a 98%
chance of fraud and a 2% chance of nonfraud.
Therefore, for binary classification models, it is optimal to use an output shape of 1
with a sigmoid activation function. Models with a single output node are also more
efficient, since they will have fewer trainable parameters and will likely train faster.
Here is what the output layer of a binary classification model would look like:
keras.layers.Dense(1, activation='sigmoid')
For the second case where a training example could belong to <i>both</i> <i>possible</i> <i>classes</i>
and fits into the Multilabel design pattern, we’ll also want to use sigmoid, this time
with a two-element output:
keras.layers.Dense(2, activation='sigmoid')
<b>Whichlossfunctionshouldweuse?</b>
Now that we know when to use sigmoid as an activation function in our model, how
should we choose which loss function to use with it? For the binary classification case
where our model has a one-element output, use binary cross-entropy loss. In Keras,
we provide a loss function when we compile our model:
model.compile(loss='binary_crossentropy', optimizer='adam',
metrics=['accuracy'])
Interestingly, we also use binary cross-entropy loss for multilabel models with sig‐
moid output. This is because, as shown in Figure 3-9, a multilabel problem with three
classes is essentially three smaller binary classification problems.