<header><largefont><b>Solution</b></largefont></header>
The solution consists of the following steps:
1. Export the model into a format that captures the mathematical core of the model
and is programming language agnostic.
2. In the production system, the formula consisting of the “forward” calculations of
the model is restored as a stateless function.
3. The stateless function is deployed into a framework that provides a REST end‐
point.
<b>Modelexport</b>
The first step of the solution is to export the model into a format (TensorFlow uses
SavedModel, but ONNX is another choice) that captures the mathematical core of
the model. The entire model state (learning rate, dropout, short-circuit, etc.) doesn’t
need to be saved—just the mathematical formula required to compute the output
from the inputs. Typically, the trained weight values are constants in the mathemati‐
cal formula.
In Keras, this is accomplished by:
model.save('export/mymodel')
The SavedModel format relies on protocol buffers for a platform-neutral, efficient
restoration mechanism. In other words, the model.save() method writes the model
.pb
as a protocol buffer (with the extension ) and externalizes the trained weights,
vocabularies, and so on into other files in a standard directory structure:
<i>export/.../variables/variables.data-00000-of-00001</i>
<i>export/.../assets/tokens.txt</i>
<i>export/.../saved_model.pb</i>
<b>InferenceinPython</b>
In a production system, the model’s formula is restored from the protocol buffer and
other associated files as a stateless function that conforms to a specific model signa‐
ture with input and output variable names and data types.
We can use the TensorFlow saved_model_cli tool to examine the exported files to
determine the signature of the stateless function that we can use in serving:
saved_model_cli show --dir ${export_path} <b>\</b>
--tag_set serve --signature_def serving_default
This outputs: