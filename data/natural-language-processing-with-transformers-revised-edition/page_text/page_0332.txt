datasets.utils.logging.set_verbosity_error()
transformers.utils.logging.set_verbosity_error()
<b>return</b> logger, tb_writer, run_name
Each worker gets a unique accelerator.process_index , which we use with the File
Handler to write the logs of each worker to an individual file. We also use the
accelerator.is_main_process true
attribute, which is only for the main worker.
We make sure we don’t initialize the TensorBoard and Weights & Biases loggers sev‐
eral times, and we decrease the logging levels for the other workers. We return the
wandb.run.name
autogenerated, unique , which we use later to name our experiment
branch on the Hub.
We’ll also define a function to log the metrics with TensorBoard and Weights & Bia‐
ses. We again use the accelerator.is_main_process here to ensure that we only log
the metrics once and not for each worker:
<b>def</b> log_metrics(step, metrics):
logger.info(f"Step {step}: {metrics}")
<b>if</b> accelerator.is_main_process:
wandb.log(metrics)
[tb_writer.add_scalar(k, v, step) <b>for</b> k, v <b>in</b> metrics.items()]
Next, let’s write a function that creates the dataloaders for the training and validation
sets with our brand new ConstantLengthDataset class:
<b>from</b> <b>torch.utils.data.dataloader</b> <b>import</b> DataLoader
<b>def</b> create_dataloaders(dataset_name):
train_data = load_dataset(dataset_name+'-train', split="train",
streaming=True)
train_data = train_data.shuffle(buffer_size=args.shuffle_buffer,
seed=args.seed)
valid_data = load_dataset(dataset_name+'-valid', split="validation",
streaming=True)
train_dataset = ConstantLengthDataset(tokenizer, train_data,
seq_length=args.seq_length)
valid_dataset = ConstantLengthDataset(tokenizer, valid_data,
seq_length=args.seq_length)
train_dataloader=DataLoader(train_dataset, batch_size=args.train_batch_size)
eval_dataloader=DataLoader(valid_dataset, batch_size=args.valid_batch_size)
<b>return</b> train_dataloader, eval_dataloader
DataLoader
At the end we wrap the dataset in a , which also handles the batching.
Accelerate will take care of distributing the batches to each worker.
Another aspect we need to implement is optimization. We will set up the optimizer
and learning rate schedule in the main loop, but we define a helper function here to
differentiate the parameters that should receive weight decay. In general, biases and
LayerNorm weights are not subject to weight decay: