ourselves to see what goes on under the hood. To warm up, we’ll take the same itera‐
tive approach shown in Figure 5-3: we’ll use “Transformers are the” as the input
prompt and run the decoding for eight timesteps. At each timestep, we pick out the
model’s logits for the last token in the prompt and wrap them with a softmax to get a
probability distribution. We then pick the next token with the highest probability, add
it to the input sequence, and run the process again. The following code does the job,
and also stores the five most probable tokens at each timestep so we can visualize the
alternatives:
<b>import</b> <b>pandas</b> <b>as</b> <b>pd</b>
input_txt = "Transformers are the"
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
iterations = []
n_steps = 8
choices_per_step = 5
<b>with</b> torch.no_grad():
<b>for</b> _ <b>in</b> range(n_steps):
iteration = dict()
iteration["Input"] = tokenizer.decode(input_ids[0])
output = model(input_ids=input_ids)
<i>#</i> <i>Select</i> <i>logits</i> <i>of</i> <i>the</i> <i>first</i> <i>batch</i> <i>and</i> <i>the</i> <i>last</i> <i>token</i> <i>and</i> <i>apply</i> <i>softmax</i>
next_token_logits = output.logits[0, -1, :]
next_token_probs = torch.softmax(next_token_logits, dim=-1)
sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
<i>#</i> <i>Store</i> <i>tokens</i> <i>with</i> <i>highest</i> <i>probabilities</i>
<b>for</b> choice_idx <b>in</b> range(choices_per_step):
token_id = sorted_ids[choice_idx]
token_prob = next_token_probs[token_id].cpu().numpy()
token_choice = (
f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"
)
iteration[f"Choice {choice_idx+1}"] = token_choice
<i>#</i> <i>Append</i> <i>predicted</i> <i>next</i> <i>token</i> <i>to</i> <i>input</i>
input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)
iterations.append(iteration)
pd.DataFrame(iterations)
<b>Input</b> <b>Choice1</b> <b>Choice2</b> <b>Choice3</b> <b>Choice4</b> <b>Choice5</b>
<b>0</b> Transformersarethe most(8.53%) only(4.96%) best(4.65%) Transformers ultimate
(4.37%) (2.16%)
<b>1</b> Transformersarethemost popular powerful common(4.96%) famous(3.72%) successful
(16.78%) (5.37%) (3.20%)
<b>2</b> Transformersarethemost toy(10.63%) toys(7.23%) Transformers of(5.46%) and(3.76%)
popular (6.60%)
<b>3</b> Transformersarethemost line(34.38%) in(18.20%) of(11.71%) brand(6.10%) line(2.69%)
populartoy