<i>Figure</i> <i>1-2.</i> <i>Unrolling</i> <i>an</i> <i>RNN</i> <i>in</i> <i>time</i>
These architectures were (and continue to be) widely used for NLP tasks, speech pro‐
cessing, and time series. You can find a wonderful exposition of their capabilities in
Andrej Karpathy’s blog post, “The Unreasonable Effectiveness of Recurrent Neural
Networks”.
One area where RNNs played an important role was in the development of machine
translation systems, where the objective is to map a sequence of words in one lan‐
guage to another. This kind of task is usually tackled with an <i>encoder-decoder</i> or
<i>sequence-to-sequence</i> architecture,5 which is well suited for situations where the input
and output are both sequences of arbitrary length. The job of the encoder is to
encode the information from the input sequence into a numerical representation that
is often called the <i>last</i> <i>hidden</i> <i>state.</i> This state is then passed to the decoder, which
generates the output sequence.
In general, the encoder and decoder components can be any kind of neural network
architecture that can model sequences. This is illustrated for a pair of RNNs in
Figure 1-3, where the English sentence “Transformers are great!” is encoded as a hid‐
den state vector that is then decoded to produce the German translation “Trans‐
former sind grossartig!” The input words are fed sequentially through the encoder
and the output words are generated one at a time, from top to bottom.
5 I.Sutskever,O.Vinyals,andQ.V.Le,“SequencetoSequenceLearningwithNeuralNetworks”,(2014).