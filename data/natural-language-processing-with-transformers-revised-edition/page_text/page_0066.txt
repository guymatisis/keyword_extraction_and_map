create the query, key, and value vectors and calculate the attention scores using the
dot product as the similarity function:
<b>import</b> <b>torch</b>
<b>from</b> <b>math</b> <b>import</b> sqrt
query = key = value = inputs_embeds
dim_k = key.size(-1)
scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)
scores.size()
torch.Size([1, 5, 5])
This has created a 5 × 5 matrix of attention scores per sample in the batch. We’ll see
later that the query, key, and value vectors are generated by applying independent
weight matrices <i>W</i> to the embeddings, but for now we’ve kept them equal for
<i>Q,K,V</i>
simplicity. In scaled dot-product attention, the dot products are scaled by the size of
the embedding vectors so that we don’t get too many large numbers during training
that can cause the softmax we will apply next to saturate.
The torch.bmm() function performs a <i>batch</i> <i>matrix-matrix</i> <i>product</i>
that simplifies the computation of the attention scores where the
query and key vectors have the shape [batch_size, seq_len,
hidden_dim] . If we ignored the batch dimension we could calculate
the dot product between each query and key vector by simply
[hidden_dim,
transposing the key tensor to have the shape
seq_len]
and then using the matrix product to collect all the dot
[seq_len, seq_len]
products in a matrix. Since we want to do
this for all sequences in the batch independently, we use
torch.bmm()
, which takes two batches of matrices and multiplies
each matrix from the first batch with the corresponding matrix in
the second batch.
Let’s apply the softmax now:
<b>import</b> <b>torch.nn.functional</b> <b>as</b> <b>F</b>
weights = F.softmax(scores, dim=-1)
weights.sum(dim=-1)
tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)
The final step is to multiply the attention weights by the values:
attn_outputs = torch.bmm(weights, value)
attn_outputs.shape
torch.Size([1, 5, 768])