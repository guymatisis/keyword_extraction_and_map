samples_per_step = accelerator.state.num_processes * args.train_batch_size
<i>#</i> <i>Logging</i>
logger, tb_writer, run_name = setup_logging(project_name.split("/")[1])
logger.info(accelerator.state)
<i>#</i> <i>Load</i> <i>model</i> <i>and</i> <i>tokenizer</i>
<b>if</b> accelerator.is_main_process:
hf_repo = Repository("./", clone_from=project_name, revision=run_name)
model = AutoModelForCausalLM.from_pretrained("./", gradient_checkpointing=True)
tokenizer = AutoTokenizer.from_pretrained("./")
<i>#</i> <i>Load</i> <i>dataset</i> <i>and</i> <i>dataloader</i>
train_dataloader, eval_dataloader = create_dataloaders(dataset_name)
<i>#</i> <i>Prepare</i> <i>the</i> <i>optimizer</i> <i>and</i> <i>learning</i> <i>rate</i> <i>scheduler</i>
optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)
lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,
num_warmup_steps=args.num_warmup_steps,
num_training_steps=args.max_train_steps,)
<b>def</b> get_lr():
<b>return</b> optimizer.param_groups[0]['lr']
<i>#</i> <i>Prepare</i> <i>everything</i> <i>with</i> <i>our</i> <i>`accelerator`</i> <i>(order</i> <i>of</i> <i>args</i> <i>is</i> <i>not</i> <i>important)</i>
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
model, optimizer, train_dataloader, eval_dataloader)
<i>#</i> <i>Train</i> <i>model</i>
model.train()
completed_steps = 0
<b>for</b> step, batch <b>in</b> enumerate(train_dataloader, start=1):
loss = model(batch, labels=batch).loss
log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,
'steps': completed_steps, 'loss/train': loss.item()})
loss = loss / args.gradient_accumulation_steps
accelerator.backward(loss)
<b>if</b> step % args.gradient_accumulation_steps == 0:
optimizer.step()
lr_scheduler.step()
optimizer.zero_grad()
completed_steps += 1
<b>if</b> step % args.save_checkpoint_steps == 0:
logger.info('Evaluating and saving model checkpoint')
eval_loss, perplexity = evaluate()
log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})
accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
<b>if</b> accelerator.is_main_process:
unwrapped_model.save_pretrained("./")
hf_repo.push_to_hub(commit_message=f'step {step}')
model.train()
<b>if</b> completed_steps >= args.max_train_steps: