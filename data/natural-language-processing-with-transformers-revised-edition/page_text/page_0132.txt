<b>import</b> <b>numpy</b> <b>as</b> <b>np</b>
sum([np.log(0.5)] * 1024)
-709.7827128933695
This is a number we can easily deal with, and this approach still works for much
smaller numbers. Since we only want to compare relative probabilities, we can do this
directly with log probabilities.
Let’s calculate and compare the log probabilities of the texts generated by greedy and
beam search to see if beam search can improve the overall probability. Since Trans‐
formers models return the unnormalized logits for the next token given the input
tokens, we first need to normalize the logits to create a probability distribution over
the whole vocabulary for each token in the sequence. We then need to select only the
token probabilities that were present in the sequence. The following function imple‐
ments these steps:
<b>import</b> <b>torch.nn.functional</b> <b>as</b> <b>F</b>
<b>def</b> log_probs_from_logits(logits, labels):
logp = F.log_softmax(logits, dim=-1)
logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)
<b>return</b> logp_label
This gives us the log probability for a single token, so to get the total log probability
of a sequence we just need to sum the log probabilities for each token:
<b>def</b> sequence_logprob(model, labels, input_len=0):
<b>with</b> torch.no_grad():
output = model(labels)
log_probs = log_probs_from_logits(
output.logits[:, :-1, :], labels[:, 1:])
seq_log_prob = torch.sum(log_probs[:, input_len:])
<b>return</b> seq_log_prob.cpu().numpy()
Note that we ignore the log probabilities of the input sequence because they are not
generated by the model. We can also see that it is important to align the logits and the
labels; since the model predicts the next token, we do not get a logit for the first label,
and we don’t need the last logit because we don’t have a ground truth token for it.
Let’s use these functions to first calculate the sequence log probability of the greedy
decoder on the OpenAI prompt:
logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))
<b>print(tokenizer.decode(output_greedy[0]))</b>
<b>print(f"\nlog-prob:</b> {logp:.2f}")
In a shocking finding, scientist discovered a herd of unicorns living in a
remote, previously unexplored valley, in the Andes Mountains. Even more
surprising to the researchers was the fact that the unicorns spoke perfect
English.