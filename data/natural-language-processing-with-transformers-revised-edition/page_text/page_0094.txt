<b>XLM-R</b> <s> ▁Jack ▁Spar row ▁love s ▁New ▁York ! </s>
Here we see that instead of the [CLS] and [SEP] tokens that BERT uses for sentence
<s> <\s>
classification tasks, XLM-R uses and to denote the start and end of a
sequence. These tokens are added in the final stage of tokenization, as we’ll see next.
<header><largefont><b>The</b></largefont> <largefont><b>Tokenizer</b></largefont> <largefont><b>Pipeline</b></largefont></header>
So far we have treated tokenization as a single operation that transforms strings to
integers we can pass through the model. This is not entirely accurate, and if we take a
closer look we can see that it is actually a full processing pipeline that usually consists
of four steps, as shown in Figure 4-1.
<i>Figure</i> <i>4-1.</i> <i>The</i> <i>steps</i> <i>in</i> <i>the</i> <i>tokenization</i> <i>pipeline</i>
Let’s take a closer look at each processing step and illustrate their effect with the
example sentence “Jack Sparrow loves New York!”:
<i>Normalization</i>
This step corresponds to the set of operations you apply to a raw string to make it
“cleaner.” Common operations include stripping whitespace and removing accen‐
ted characters. Unicode normalization is another common normalization opera‐
tion applied by many tokenizers to deal with the fact that there often exist various
ways to write the same character. This can make two versions of the “same” string
(i.e., with the same sequence of abstract characters) appear different; Unicode
normalization schemes like NFC, NFD, NFKC, and NFKD replace the various
ways to write the same character with standard forms. Another example of nor‐
malization is lowercasing. If the model is expected to only accept and use lower‐
case characters, this technique can be used to reduce the size of the vocabulary it
requires. After normalization, our example string would look like “jack sparrow
loves new york!”.
<i>Pretokenization</i>
This step splits a text into smaller objects that give an upper bound to what your
tokens will be at the end of training. A good way to think of this is that the preto‐
kenizer will split your text into “words,” and your final tokens will be parts of
those words. For the languages that allow this (English, German, and many Indo-
European languages), strings can typically be split into words on whitespace and
punctuation. For example, this step might transform our ["jack", "sparrow",
"loves", "new", "york", "!"] . These words are then simpler to split into