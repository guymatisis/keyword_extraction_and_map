those values, such that the more epochs we do, the more extreme our activations
become.
With Mixup, we no longer have that problem, because our labels will be exactly 1 or 0
only if we happen to “mix” with another image of the same class. The rest of the time,
our labels will be a linear combination, such as the 0.7 and 0.3 we got in the church
and gas station example earlier.
One issue with this, however, is that Mixup is “accidentally” making the labels bigger
than 0 or smaller than 1. That is to say, we’re not <i>explicitly</i> telling our model that we
want to change the labels in this way. So, if we want to change to make the labels
closer to or further away from 0 and 1, we have to change the amount of Mixup—
which also changes the amount of data augmentation, which might not be what we
want. There is, however, a way to handle this more directly, which is to use <i>label</i>
<i>smoothing.</i>
<header><largefont><b>Label</b></largefont> <largefont><b>Smoothing</b></largefont></header>
In the theoretical expression of loss, in classification problems, our targets are one-
hot encoded (in practice, we tend to avoid doing this to save memory, but what we
compute is the same loss as if we had used one-hot encoding). That means the model
is trained to return 0 for all categories but one, for which it is trained to return 1.
Even 0.999 is not “good enough”; the model will get gradients and learn to predict
activations with even higher confidence. This encourages overfitting and gives you at
inference time a model that is not going to give meaningful probabilities: it will
always say 1 for the predicted category even if it’s not too sure, just because it was
trained this way.
This can become very harmful if your data is not perfectly labeled. In the bear classi‐
fier we studied in Chapter 2, we saw that some of the images were mislabeled, or con‐
tained two different kinds of bears. In general, your data will never be perfect. Even if
the labels were manually produced by humans, they could make mistakes, or have
differences of opinions on images that are harder to label.
Instead, we could replace all our 1s with a number a bit less than 1, and our 0s with a
number a bit more than 0, and then train. This is called <i>label</i> <i>smoothing.</i> By encourag‐
ing your model to be less confident, label smoothing will make your training more
robust, even if there is mislabeled data. The result will be a model that generalizes
better at inference.
This is how label smoothing works in practice: we start with one-hot-encoded labels,

then replace all 0s with (that’s the Greek letter <i>epsilon,</i> which is what was used in
<i>N</i>
the paper that introduced label smoothing and is used in the fastai code), where <i>N</i> is
the number of classes and  is a parameter (usually 0.1, which would mean we are
10% unsure of our labels). Since we want the labels to add up to 1, we also replace the