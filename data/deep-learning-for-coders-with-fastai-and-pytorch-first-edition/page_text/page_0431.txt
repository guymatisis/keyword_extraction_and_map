• By training with higher learning rates, we overfit less because we skip over the
sharp local minima to end up in a smoother (and therefore more generalizable)
part of the loss.
The second point is an interesting and subtle one; it is based on the observation that a
model that generalizes well is one whose loss would not change very much if you
changed the input by a small amount. If a model trains at a large learning rate for
quite a while, and can find a good loss when doing so, it must have found an area that
also generalizes well, because it is jumping around a lot from batch to batch (that is
basically the definition of a high learning rate). The problem is that, as we have dis‐
cussed, just jumping to a high learning rate is more likely to result in diverging losses,
rather than seeing your losses improve. So we don’t jump straight to a high learning
rate. Instead, we start at a low learning rate, where our losses do not diverge, and we
allow the optimizer to gradually find smoother and smoother areas of our parameters
by gradually going to higher and higher learning rates.
Then, once we have found a nice smooth area for our parameters, we want to find the
very best part of that area, which means we have to bring our learning rates down
again. This is why 1cycle training has a gradual learning rate warmup, and a gradual
learning rate cooldown. Many researchers have found that in practice this approach
leads to more accurate models and trains more quickly. That is why it is the approach
fine_tune
that is used by default for in fastai.
In Chapter 16, we’ll learn all about <i>momentum</i> in SGD. Briefly, momentum is a tech‐
nique whereby the optimizer takes a step not only in the direction of the gradients,
but also that continues in the direction of previous steps. Leslie Smith introduced the
idea of <i>cyclical</i> <i>momentum</i> in “A Disciplined Approach to Neural Network Hyper-
Parameters: Part 1”. It suggests that the momentum varies in the opposite direction of
the learning rate: when we are at high learning rates, we use less momentum, and we
use more again in the annealing phase.
We can use 1cycle training in fastai by calling fit_one_cycle:
<b>def</b> fit(epochs=1, lr=0.06):
learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,
metrics=accuracy, cbs=ActivationStats(with_hist=True))
learn.fit_one_cycle(epochs, lr)
<b>return</b> learn
learn = fit()
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.210838 0.084827 0.974300 00:08
We’re finally making some progress! It’s giving us a reasonable accuracy now.