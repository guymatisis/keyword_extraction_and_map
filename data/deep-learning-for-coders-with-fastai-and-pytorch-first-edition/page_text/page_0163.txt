We begin by comparing the outputs the model gives us with our targets (we have
labeled data, so we know what result the model should give) using a <i>loss</i> <i>function,</i>
which returns a number that we want to make as low as possible by improving our
weights. To do this, we take a few data items (such as images) from the training set
and feed them to our model. We compare the corresponding targets using our loss
function, and the score we get tells us how wrong our predictions were. We then
change the weights a little bit to make it slightly better.
To find how to change the weights to make the loss a bit better, we use calculus to
calculate the <i>gradients.</i> (Actually, we let PyTorch do it for us!) Let’s consider an anal‐
ogy. Imagine you are lost in the mountains with your car parked at the lowest point.
To find your way back to it, you might wander in a random direction, but that proba‐
bly wouldn’t help much. Since you know your vehicle is at the lowest point, you
would be better off going downhill. By always taking a step in the direction of the
steepest downward slope, you should eventually arrive at your destination. We use the
magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to
take; specifically, we multiply the gradient by a number we choose called the <i>learning</i>
<i>rate</i> to decide on the step size. We then <i>iterate</i> until we have reached the lowest point,
which will be our parking lot; then we can <i>stop.</i>
All of what we just saw can be transposed directly to the MNIST dataset, except for
the loss function. Let’s now see how we can define a good training objective.
<header><largefont><b>The</b></largefont> <largefont><b>MNIST</b></largefont> <largefont><b>Loss</b></largefont> <largefont><b>Function</b></largefont></header>
We already have our xs—that is, our independent variables, the images themselves.
We’ll concatenate them all into a single tensor, and also change them from a list of
matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). We can do this using
view , which is a PyTorch method that changes the shape of a tensor without changing
its contents. -1 is a special parameter to view that means “make this axis as big as
necessary to fit all the data”:
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)
We need a label for each image. We’ll use 1 for 3s and 0 for 7s:
train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)
train_x.shape,train_y.shape
(torch.Size([12396, 784]), torch.Size([12396, 1]))