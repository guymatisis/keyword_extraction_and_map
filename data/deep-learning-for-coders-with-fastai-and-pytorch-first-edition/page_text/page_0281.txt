<i>Figure</i> <i>9-4.</i> <i>Date</i> <i>embeddings</i> <i>(courtesy</i> <i>of</i> <i>Cheng</i> <i>Guo</i> <i>and</i> <i>Felix</i> <i>Berkhahn)</i>
In addition, it is valuable in its own right that embeddings are continuous, because
models are better at understanding continuous variables. This is unsurprising consid‐
ering models are built of many continuous parameter weights and continuous activa‐
tion values, which are updated via gradient descent (a learning algorithm for finding
the minimums of continuous functions).
Another benefit is that we can combine our continuous embedding values with truly
continuous input data in a straightforward manner: we just concatenate the variables
and feed the concatenation into our first dense layer. In other words, the raw catego‐
rical data is transformed by an embedding layer before it interacts with the raw con‐
tinuous input data. This is how fastai and Guo and Berkhahn handle tabular models
containing continuous and categorical variables.
An example using this concatenation approach is how Google does its recommenda‐
tions on Google Play, as explained in the paper “Wide & Deep Learning for Recom‐
mender Systems”. Figure 9-5 illustrates this.
Interestingly, the Google team combined both approaches we saw in the previous
chapter: the dot product (which they call <i>cross</i> <i>product)</i> and neural network
approaches.