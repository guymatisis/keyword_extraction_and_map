<header><largefont><b>SGD</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Mini-Batches</b></largefont></header>
Now that we have a loss function suitable for driving SGD, we can consider some of
the details involved in the next phase of the learning process, which is to change or
update the weights based on the gradients. This is called an <i>optimization</i> <i>step.</i>
To take an optimization step, we need to calculate the loss over one or more data
items. How many should we use? We could calculate it for the whole dataset and take
the average, or we could calculate it for a single data item. But neither of these is ideal.
Calculating it for the whole dataset would take a long time. Calculating it for a single
item would not use much information, so it would result in an imprecise and unsta‐
ble gradient. You’d be going to the trouble of updating the weights, but taking into
account only how that would improve the model’s performance on that single item.
So instead we compromise: we calculate the average loss for a few data items at a
time. This is called a <i>mini-batch.</i> The number of data items in the mini-batch is called
the <i>batch</i> <i>size.</i> A larger batch size means that you will get a more accurate and stable
estimate of your dataset’s gradients from the loss function, but it will take longer, and
you will process fewer mini-batches per epoch. Choosing a good batch size is one of
the decisions you need to make as a deep learning practitioner to train your model
quickly and accurately. We will talk about how to make this choice throughout this
book.
Another good reason for using mini-batches rather than calculating the gradient on
individual data items is that, in practice, we nearly always do our training on an accel‐
erator such as a GPU. These accelerators perform well only if they have lots of work
to do at a time, so it’s helpful if we can give them lots of data items to work on. Using
mini-batches is one of the best ways to do this. However, if you give them too much
data to work on at once, they run out of memory—making GPUs happy is also tricky!
As you saw in our discussion of data augmentation in Chapter 2, we get better gener‐
alization if we can vary things during training. One simple and effective thing we can
vary is what data items we put in each mini-batch. Rather than simply enumerating
our dataset in order for every epoch, instead what we normally do is randomly shuffle
it on every epoch, before we create mini-batches. PyTorch and fastai provide a class
that will do the shuffling and mini-batch collation for you, called DataLoader.
A DataLoader can take any Python collection and turn it into an iterator over many
batches, like so:
coll = range(15)
dl = DataLoader(coll, batch_size=5, shuffle=True)
list(dl)
[tensor([ 3, 12, 8, 10, 2]),
tensor([ 9, 4, 7, 14, 5]),
tensor([ 1, 13, 0, 6, 11])]