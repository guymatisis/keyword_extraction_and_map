ful recognition of handwritten digit sequences. This was one of the most important
breakthroughs in the history of AI.
<header><largefont><b>Tenacity</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont></header>
The story of deep learning is one of tenacity and grit by a handful of dedicated
researchers. After early hopes (and hype!), neural networks went out of favor in the
1990s and 2000s, and just a handful of researchers kept trying to make them work
well. Three of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded
the highest honor in computer science, the Turing Award (generally considered the
“Nobel Prize of computer science”), in 2018 after triumphing despite the deep skepti‐
cism and disinterest of the wider machine learning and statistics community.
Hinton has told of how academic papers showing dramatically better results than
anything previously published would be rejected by top journals and conferences, just
because they used a neural network. Lecun’s work on convolutional neural networks,
which we will study in the next section, showed that these models could read hand‐
written text—something that had never been achieved before. However, his break‐
through was ignored by most researchers, even as it was used commercially to read
10% of the checks in the US!
In addition to these three Turing Award winners, many other researchers have battled
to get us to where we are today. For instance, Jurgen Schmidhuber (who many believe
should have shared in the Turing Award) pioneered many important ideas, including
working with his student Sepp Hochreiter on the long short-term memory (LSTM)
architecture (widely used for speech recognition and other text modeling tasks, and
used in the IMDb example in Chapter 1). Perhaps most important of all, Paul Werbos
in 1974 invented backpropagation for neural networks, the technique shown in this
chapter and used universally for training neural networks (Werbos 1994). His devel‐
opment was almost entirely ignored for decades, but today it is considered the most
important foundation of modern AI.
There is a lesson here for all of us! On your deep learning journey, you will face many
obstacles, both technical and (even more difficult) posed by people around you who
don’t believe you’ll be successful. There’s one <i>guaranteed</i> way to fail, and that’s to stop
trying. We’ve seen that the only consistent trait among every fast.ai student who’s
gone on to be a world-class practitioner is that they are all very tenacious.
For this initial tutorial, we are just going to try to create a model that can classify any
image as a 3 or a 7. So let’s download a sample of MNIST that contains images of just
these digits:
path = untar_data(URLs.MNIST_SAMPLE)