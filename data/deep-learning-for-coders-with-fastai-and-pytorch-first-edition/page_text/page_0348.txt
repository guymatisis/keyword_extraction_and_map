to create a mini-batch containing the first 10 documents. First we’ll numericalize
them:
nums_samp = toks200[:10].map(num)
Let’s now look at how many tokens each of these 10 movie reviews has:
nums_samp.map(len)
(#10) [228,238,121,290,196,194,533,124,581,155]
Remember, PyTorch DataLoaders need to collate all the items in a batch into a single
tensor, and a single tensor has a fixed shape (i.e., it has a particular length on every
axis, and all items must be consistent). This should sound familiar: we had the same
issue with images. In that case, we used cropping, padding, and/or squishing to make
all the inputs the same size. Cropping might not be a good idea for documents,
because it seems likely we’d remove some key information (having said that, the same
issue is true for images, and we use cropping there; data augmentation hasn’t been
well explored for NLP yet, so perhaps there are actually opportunities to use cropping
in NLP too!). You can’t really “squish” a document. So that leaves padding!
We will expand the shortest texts to make them all the same size. To do this, we use a
special padding token that will be ignored by our model. Additionally, to avoid mem‐
ory issues and improve performance, we will batch together texts that are roughly the
same lengths (with some shuffling for the training set). We do this by (approximately,
for the training set) sorting the documents by length prior to each epoch. The result
is that the documents collated into a single batch will tend of be of similar lengths. We
won’t pad every batch to the same size, but will instead use the size of the largest
document in each batch as the target size.
<b>DynamicallyResizeImages</b>
It is possible to do something similar with images, which is espe‐
cially useful for irregularly sized rectangular images, but at the time
of writing no library provides good support for this yet, and there
aren’t any papers covering it. It’s something we’re planning to add
to fastai soon, however, so keep an eye on the book’s website; we’ll
add information about this as soon as we have it working well.
The sorting and padding are automatically done by the data block API for us when
using a TextBlock with is_lm=False. (We don’t have this same issue for language
model data, since we concatenate all the documents together first and then split them
into equally sized sections.)
We can now create a model to classify our texts:
learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,
metrics=accuracy).to_fp16()