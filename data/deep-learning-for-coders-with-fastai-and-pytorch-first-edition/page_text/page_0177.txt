w1 = init_params((28*28,30))
b1 = init_params(30)
w2 = init_params((30,1))
b2 = init_params(1)
The key point is that w1 has 30 output activations (which means that w2 must have 30
input activations, so they match). That means that the first layer can construct 30 dif‐
ferent features, each representing a different mix of pixels. You can change that 30 to
anything you like, to make the model more or less complex.
That little function res.max(tensor(0.0)) is called a <i>rectified</i> <i>linear</i> <i>unit,</i> also known
as <i>ReLU.</i> We think we can all agree that <i>rectified</i> <i>linear</i> <i>unit</i> sounds pretty fancy and
complicated…But actually, there’s nothing more to it than res.max(tensor(0.0))—
in other words, replace every negative number with a zero. This tiny function is also
available in PyTorch as F.relu :
plot_function(F.relu)
<b>JeremySays</b>
There is an enormous amount of jargon in deep learning, including
terms like <i>rectified</i> <i>linear</i> <i>unit.</i> The vast majority of this jargon is no
more complicated than can be implemented in a short line of code,
as we saw in this example. The reality is that for academics to get
their papers published, they need to make them sound as impres‐
sive and sophisticated as possible. One way that they do that is to
introduce jargon. Unfortunately, this results in the field becoming
far more intimidating and difficult to get into than it should be.
You do have to learn the jargon, because otherwise papers and
tutorials are not going to mean much to you. But that doesn’t mean
you have to find the jargon intimidating. Just remember, when you
come across a word or phrase that you haven’t seen before, it will
almost certainly turn out to be referring to a very simple concept.