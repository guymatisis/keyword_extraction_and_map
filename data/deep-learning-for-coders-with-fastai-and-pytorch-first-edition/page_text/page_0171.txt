For training a model, we don’t just want any Python collection, but a collection con‐
taining independent and dependent variables (the inputs and targets of the model). A
collection that contains tuples of independent and dependent variables is known in
PyTorch as a Dataset . Here’s an example of an extremely simple Dataset :
ds = L(enumerate(string.ascii_lowercase))
ds
(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7,
> 'h'),(8, 'i'),(9, 'j')...]
When we pass a Dataset to a DataLoader we will get back many batches that are
themselves tuples of tensors representing batches of independent and dependent
variables:
dl = DataLoader(ds, batch_size=6, shuffle=True)
list(dl)
[(tensor([17, 18, 10, 22, 8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),
(tensor([20, 15, 9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),
(tensor([ 7, 25, 6, 5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),
(tensor([ 1, 3, 0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),
(tensor([2, 4]), ('c', 'e'))]
We are now ready to write our first training loop for a model using SGD!
<header><largefont><b>Putting</b></largefont> <largefont><b>It</b></largefont> <largefont><b>All</b></largefont> <largefont><b>Together</b></largefont></header>
It’s time to implement the process we saw in Figure 4-1. In code, our process will be
implemented something like this for each epoch:
<b>for</b> x,y <b>in</b> dl:
pred = model(x)
loss = loss_func(pred, y)
loss.backward()
parameters -= parameters.grad * lr
First, let’s reinitialize our parameters:
weights = init_params((28*28,1))
bias = init_params(1)
A DataLoader can be created from a Dataset:
dl = DataLoader(dset, batch_size=256)
xb,yb = first(dl)
xb.shape,yb.shape
(torch.Size([256, 784]), torch.Size([256, 1]))
We’ll do the same for the validation set:
valid_dl = DataLoader(valid_dset, batch_size=256)