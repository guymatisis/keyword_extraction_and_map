academic baseline datasets, 47
accountability for ethics violations, 101, 122
accuracy metric
classification models, 145
deeper models, 180
improving while validation loss worse, 212
Mixup augmentation improving, 246
multi-label classifier, 229
validation set, 28, 30
validation set size, 49
actionable outcomes via Drivetrain Approach, 64
softmax as activation function, 195, 207
nonlinear layer, 178, 181
activation regularization, 397
histogram of activations, 433
activations, 181
binary problems, 196
forward pass, 155
hidden state, 380
models returning, 174, 227
plotting during training, 428
transforming into predictions, 195
ActivationStats, 428
Adam, 479
AdaptiveAvgPool2d, 443
aggregation bias, 113
algorithm buggy, ethics of, 95, 101, 115
Ali, Muhammad, 112
Amazon
	batching production operations, 83
	facial recognition bias, 100
	recommendation systems, 62
Ameisin, Emmanuel, 87
Angwin, Julia, 126
annealing learning rate, 430
	annealing callbacks, 540
	cosine annealing, 432
Apple APIs for apps under iOS, 86
applications, 355
	(see also web applications)
architecture of model
	AWD-LSTM architecture, 343
	AWD-LSTM for NLP RNNs, 394
	computer vision, 459
	cnn_learner, 459
	Siamese network, 463-465
	unet_learner, 461
	deeper architectures, 213
	definition, 24, 40
	exporting models, 78
	long short-term memory, 134
	natural language processing, 465
	picking not so important, 30
	ResNet, 30, 213, 441
	(see also ResNet architecture)
	tabular models, 466
argument binding with partial function, 229
Arkansas healthcare buggy algorithm (ethics), 95, 101, 115
arrays
	about, 143
	APIs, 144
	arrays within arrays, 143
	column selected, 144
	creating an array, 144
	image section, 135
	operators, 145
	row selected, 144
	slicing row or column, 145
arrest record Google bias, 95
artificial intelligence (see machine learning)
Artificial Intelligence: A Frontier of Automa‐
	tion article, 20
autocompletion in notebooks, 68
autogenerated text (see text generation)
autonomous vehicles, 42, 64
AWD-LSTM architecture
	activation regularization, 397
	dropout, 395
	NLP RNNs, 343, 394
	temporal activation regularization, 397
	training weight-tied regularized LSTM, 398-399
axis of tensor or matrix, 181
Azure Cognitive Services (Microsoft), 66

backpropagation
	backward hook for custom behavior, 519
	training neural networks, 134
backpropagation through time (BPTT), 382
	text classification, 466
	truncated BPTT, 382
backward hook, 519
backward pass, 155, 182, 503
	gradients and, 508-510
bagging, 298-323
Barocas, Solon, 122
batch normalization, 435
batch operations
	batch size, 170
	out-of-memory error, 214
	data augmentation, 74
	GPU serving production model, 83
	mini-batch, 71, 182
	PyTorch single item or batch same code, 228
	resizing images, 28
	SGD and mini-batches, 170
	show_batch method, 192
	texts into batches for language model, 339-342
BCELoss, 228
BCEWithLogitsLoss, 228, 237
bear classifier (see image classifier models)
beginning
	actionable outcomes via Drivetrain
	Approach, 63
	begin in known areas, 59
	begin with simple baseline model, 137, 193
	book website, 128
	deep learning applicability to problem, 60
	experiments lead to projects, 58
	first model, 13-18
	code for, 26-33
	error rate, 19
	tested, 19
	first notebook, 15-18
	GPU servers, 14
	(see also GPU deep learning servers)
	Jupyter Notebook, 15
	(see also Jupyter Notebook)
	pretrained model accuracy, 31
	process (see process end-to-end)
	steps toward starting, 129
Bengio, Yoshua, 134, 505
Berkhahn, Felix, 278, 324
bias
	about, 105, 107, 115
	aggregation bias, 113
	Bing Image Search example, 68
	facial recognition, 100, 108
	feedback loops, 26, 89
	arrest rates on racial grounds, 89
	recommendation system, 95
	gender and, 121
	(see also gender)
	Google advertising, 95
	historical bias, 106-112
	measurement bias, 112
	mitigation, 115
	racial bias, 106
	(see also racial bias)
	representation bias, 113, 271
	socioeconomic bias, 115
binary cross entropy loss function, 226-231
binary database format as data type, 186
Binder free app hosting, 84
Bing Image Search for gathering data, 65
	API, 66
	biases in data gathering, 68
Bittman, Ladislav, 116
Black, Edwin, 98
blogging about deep learning journey
	about, 90
	browser-based interface, 549
	creating posts, 553
	GitHub account, 550
	GitHub Pages host, 549
	home page setup, 551
	Jupyter for blogging, 556
	synchronizing GitHub and computer, 555
body of a model, 460
	cutting model, 459
The Book of Why (Pearl and Mackenzie), 310
book updates on website, 128
boosting, 323
bootstrapping problem of new users, 270
BPTT (see backpropagation through time)
Breiman, Leo, 298
broadcasting, 147, 147, 497
	broadcasting with a scalar, 498
	rules of, 501
	vector to matrix, 498-501
Brostow, Gabriel J., 42
buggy algorithm ethics, 95, 101, 115
Buolamwini, Joy, 109
button click event handler, 81

C programming language, 144
calculus and SymPy, 510
California gang database (ethics), 101
callbacks
	annealing, 540
	building Learner class from scratch, 539
	creating, 483
	exceptions, 487
	HookCallback, 520
	language model, 383
	Learner, 248, 428
	mid-level API, 356
	training process, 480
Callison-Burch, Chris, 48
CAM (see class activation map)
capacity of a model, 213
car safety for ethics inspiration, 125
cardinality
	decision tree ensembles and, 283
	definition, 283
	entity embedding and, 278
casting in PyTorch, 139
categorical outcome cross-entropy loss, 194
categorical variables
	cardinality and decision tree ensembles, 283
	definition, 277
	embedding, 259-265, 297
	continuous variables from, 278
	decision trees, 287
	tabular data, 277-281
	tabular dataset, 284-287
	tabular dataset prep, 289-292
	predicting sales from stores (see tabular
	data)
	recommendation system model (see collab‐
	orative filtering)
	time series dataset splitting, 290
	(see also tabular data)
CategoryBlock
	image classifier, 70, 188
	MultiCategoryBlock, 224
cats and dogs first model, 13-18
	dataset, 17, 28
Ceglowski, Maciej, 124
cells in notebooks
	copying, 17
	execution order, 44
	first cell CLICK ME, 17, 44
	image output by, 18
	Markdown, 16
	table output by, 17
	text ouput by, 18
	types of, 16
census data weaponization, 119
center of person’s face in image (see key point
	model)
Chaslot, Guillaume, 103
Chomsky, Noam, 188
Chou, Tracy, 121
CIFAR10 dataset, 240
Cipolla, Roberto, 42
class activation map (CAM)
	about, 519
	gradient CAM, 522
	hooks, 519-522
class methods, 342
classes in object-oriented programming, 261
	dunder init, 261
classification models definition, 28
click event handler, 82
CNN (see convolutional neural network)
cnn_learner
	architecture, 459
	first model, 30
	image classifier model, 75
	loss function parameter, 236
	multi-label classifier, 226
	normalization of data, 242
collaborative filtering
	about, 253
	bootstrapping problem, 270
	building from scratch, 262-265
	collab_learner, 269
	DataLoaders, 257
	dataset, 254
	deep learning model, 272
	embedding, 259-265
	built from scratch, 265-274
	embedding distance, 270, 280
	fitting model, 262
	interpretting embeddings and biases, 267
	items rather than products, 253
	latent factors, 253
	embedding, 259-265
	layers via printing model, 269
	learning latent factors, 256
	embedding, 259-265
	probabilistic matrix factorization, 271
	skew from small number of users, 271
	structuring model, 254
	tables as matrices, 259
	look-up index, 259
collab_learner, 269
color image as rank-3 tensor, 423
color_dim, 433
community support, 546
COMPAS algorithm, 107, 126
computer vision models
	architecture, 459
	cnn_learner, 459
	Siamese network, 463-465
	unet_learner, 461
	autonomous vehicles localizing objects, 42
	convolutional neural networks for, 30
	current state of, 60
	dataset image representation rule, 39
	datasets for, 239
	labels, 28
	examples of, 4
	(see also image classifier models)
	fastai vision library in first model, 26
	finding edges via convolution, 404
	image basics, 133-136
	image classifier (see image classifier models)
	labels in datasets, 28
	non-image tasks, 36-40, 41
	object detection, 60, 110
	pixels as foundation, 133-136
	pretrained model weight values, 31
	Python Imaging Library, 135
	ResNets for, 445-451
	self-supervised learning for, 329
concatenating categorical and continuous vari‐
	ables, 281
	Google Play, 281
confusion matrix with image classifiers, 76, 203
conspiracy theory feedback loop, 95, 102, 105
context manager, 522
continuous variables
	definition, 277
	embedded categorical combined with, 281
	embedding transforming categorical into, 278
convolutional neural network (CNN)
	1cycle training, 430
	about, 207, 414
	batch size increased, 429
	building a CNN, 415-438
	batch normalization, 435
	channels, 416
	color images, 423-426
	convolution arithmetic, 418
	dataset, 426
	features, 416
	receptive fields, 419
	building Learner class from scratch, 534
	building ResNet CNN, 445-451
	computer vision models, 30
	convolution as matrix multiplication, 414
	convolution described, 404, 408
	definition, 40
	equations, 412
	first model, 30
	fully convolutional networks, 443, 444
	head, 460
	kernel, 404-406
	convolution described, 408
	mapping, 407-408
	learning rate for, 445
	nested list of comprehensions, 407
	padding, 411
	pretrained parameter, 31
	last layer and, 31
	PyTorch convolutions, 408
	refactoring, 416
	stem, 452
	top 5 accuracy, 451
	training, 418
	all digits, 427-438
	more stable, 429-438
	visualizing learning, 33
	Yann Lecun’s work, 134
cosine annealing, 432
CPU servers, 84, 86
crash test dummies and gender, 125
credit report system errors (ethics), 101
cross-entropy loss
	about, 194, 202, 237
	gradient, 203
	image classifier, 194-203
	image data and categorical outcome, 194
CSV data for models
	multi-label classification, 220-222
	as tabular data, 45
	(see also tabular data)
CT scan stroke analysis, 62
cutting network, 459
cyclical momentum, 431

data augmentation
	applied to coordinates, 233
	definition, 60, 74
	image classifier model, 74
	Mixup, 246
	presizing, 189
	progressive resizing as, 243
	test time augmentation, 245
	text data complications, 394
data leakage of illegitimate information, 311
	missing values as, 311
data project checklist
	about, 559
	analytics, 563
	constraints, 565
	data, 562
	data scientists, 560
	implementation, 564
	maintenance, 564
	strategy, 561
database data for models, 45
	binary database format as data type, 186
DataBlock
	checking, 191-193
	DataFrame to DataLoaders, 222-226
	debugging, 192, 192, 223, 226, 342
	image classifier model, 70
	image regression example, 232-237
	language model using, 342
	mid-level API foundation, 355
	(see also mid-level API)
	movie review classifier, 346
	presizing, 189
DataFrame
	color-code image values, 136
	DataLoaders object from, 222-226
	multi-label CSV file, 220
	TabularPandas class, 290
DataLoader iterator, 222
	building Learner class from scratch, 530
DataLoaders
	customization, 70
	DataFrame converted to, 222-226
	definition, 223
	export method, 79
	image classifier model, 70-72
	movie review classifier, 346
	text classifier, 43, 44, 355
Dataset collection
	building from scratch, 529
	definition, 222
datasets
	academic baselines, 47
	best models for majority of, 282
	bias (see bias)
	Bing Image Search for gathering data, 65
	Blue Book for Bulldozers Kaggle competi‐
	tion, 284
	bootstrapping problem of new users, 270
	CIFAR10 dataset, 240
	cleaning
	before versus after training, 77, 78
	fastai GUI for, 77
	time required for, 78
	computer vision, 239
	labels, 28
cut-down versions of popular, 48
data augmentation definition, 60, 74
data availability, 58
data leakage of illegitimate information, 311
data product design integrated with ML, 99
date handling, 289
definition, 15
demographics, 46
dependent variable definition, 25
domain shift, 87
download first model, 15, 27
ethics, 93
	errors in dataset, 101
	examples of, 94
	use of data, 119
examining data importance, 286, 379, 427
facial recognition across races, 109
feature engineering, 403
filename extraction, 187-188
	regular expressions, 187
freely available, 15
gathering data, 65-68
handwritten digits, 133, 240, 426, 442
Human Numbers, 373
image representation rule of thumb, 39
ImageNet dataset, 239, 442
	Imagenette subset, 239, 441, 527
	top 5 accuracy, 451
IMDb Large Movie Review, 44
independent variable definition, 24
Kaggle as source, 284
Kinect Head Pose, 232
label importance, 25
	(see also labels)
missing values as data leakage, 311
MNIST handwritten digits dataset, 133, 240, 426, 442
MovieLens, 46, 254
normalization of data, 241
	statistics distributed with model, 242
other data types, 63
out-of-domain data, 60, 87, 316
PASCAL multi-label dataset, 220
path to dataset, 27
pet images, 17, 28, 186, 364
pretrained model weight values, 31
racial balance of, 109
****************************************************************************
save method, 292
	structure of, 27
	tabular data for models
	about, 45, 277
	architecture, 466
	current state of, 62
	as data type, 186
	deep dive into (see tabular data)
	multi-label classification, 220
	recommendation systems as, 62
	test set, 49
	training set, 28, 30, 40
	training, validation, test, 50-54
	types of data, 186
	validation set, 28, 30, 40, 49
	validation set defined, 71
Datasets iterator, 223
	Transforms, 362
date handling in tabular data, 289
De-Arteaga, Maria, 113
debugging
	DataBlock, 192, 223, 226, 342
	help with errors, 68
	out-of-memory error, 214
	summary method for, 192, 226, 342
decision trees
	about, 282, 287
	displaying tree, 292-295
	libraries for, 283
	overfitting, 295
	random forests, 298-323
	creating a random forest, 299
	data leakage, 311
	ensembling, 322
	ensembling, boosting, 323
	extrapolation problem, 314
	feature importances, 303
	hyperparameter insensitivity, 299
	model interpretation, 302-314
	out-of-bag error, 301
	out-of-domain data, 316
	partial dependence, 308
	removing low-importance variables, 305
	removing redundant features, 306
	tree interpreter, 312
	tree variance for prediction confidence, 302
	training, 288-296
	creating decision tree, 292
	data preparation, 289-292
	displaying tree, 292-295
decode method, 356
deep learning
	about, xxi, 3-5
	about the importance of parameters, 32
	architecture not so important, 30
	beyond deep learning, 282
	blogging about journey (see blogging)
	capabilities and constraints, 57
	community support, 421, 546
	current state of, 60
	dataset image representation rule, 39
	history, 134
	how to learn, 9-11
	image recognition (see image classifier
	models)
	as machine learning, 20, 40
	machine learning visualized, 33
	manual process in parallel, 88
	model and human interaction, 61, 88
	neural networks beyond understanding, 88
	neural networks used, 3, 20
	(see also neural networks)
	non-image tasks, 36-40, 41
	overview, 40
	predicting sales from stores, 278
	(see also tabular data)
	process of creating application (see process
	end-to-end)
	risk mitigation, 88
	scikit-learn library instead, 283
	server requirements, 14
	tabular data needing more, 282
	terminology, 24, 40, 181
	Twitter for help, 421
deeper models having more layers, 180
delegates, 274
demographics dataset, 46
dependent variable
	definition, 25, 71
	distracted driver model, 53
	model defined by, 231
	prediction variable importance, 287
	viewing in mini-batch, 195
	as y, 71
deployment
	app from notebook, 82
	Binder free app hosting, 84
	CPU-based server, 86
	exporting model, 78
	export.pkl file, 79
	mobile devices, 85
	prediction inference, 79
	Raspberry Pi, 85
	risk mitigation, 88
	unforeseen challenges, 89
	web application, 78-86
	deployment, 83-86
	disaster avoidance, 86
	web resource discussing, 87
derivative of a function, 153
	backpropagation, 155
DeVries, Terrance, 110
diabetes data aggregation bias, 113
digital signature, 117
dimension multiple meanings, 139
DiResta, Renee, 105
disaster avoidance with web applications, 86
discriminative learning rates, 210
disinformation, 61, 116, 350
	identity generation, 351
diversity against ethical risks, 121
doc for method documentation, 45, 68
dogs and cats first model, 13-18
	dataset, 17, 28
domain shift, 87
dot product of vectors, 256, 281
download_images, 67
Drivetrain Approach for actionable outcomes, 63
dropout, 395
Dumoulin, Vincent, 408
dunder init, 261
Durbin, Meredith, 123

early stopping, 213
Einstein summation, 502
electronic health record measurement bias, 112
elementwise arithmetic, 496
embedding, 259-265
	built from scratch, 265-274
	categorical variables transformed into con‐
	tinuous, 278
	combining with other methods, 324
	continuous values, continuous input, 281
	delegates, 274
	embedding distance, 270, 280
	dates on calendar and, 280
	geographic distance matching, 280
	embedding layer, 278
	embedding matrix, 260
	entity embedding, 278
	kwargs, 273
	tabular data with categorical columns, 319
encoder, 345
end-to-end process (see process end-to-end)
Enlitic company malignant tumor identifica‐
	tion, 4, 7
ensembles of decision trees (see decision trees)
ensembling random forests, 322
	boosting, 323
entity embedding, 278
epochs
	definition, 32, 33, 40
	early stopping, 213
	fitting models, 194
	image cropping, 73
	test time augmentation, 245
	number of, 212
	overfitting and, 30
error debugging, 68
	out-of-memory error, 214
error rate, 19, 31
errors in data (ethics), 101
escape key for command/edit mode, 17
Estola, Evan, 105
ethics
	accountability, 101
	addressing ethical issues, 118
	early stages of, 126
	ethical lenses, 119
	fairness, accountability, transparency, 122
	policy’s role, 123-125
	power of diversity, 121
	processes to implement, 119
	bias
	about, 105, 115
	aggregation bias, 113
	facial recognition, 100, 108
	geo-diversity, 110
	historical bias, 106-112
	measurement bias, 112
	mitigation, 115
	natural language processing, 112
	racial bias (see racial bias)
	representation bias, 113
	socioeconomic bias, 115
	buggy algorithm, 95, 101, 115
	car safety inspiration, 125
	consideration of project as whole, 99
	data ethics, 93
	examples of, 94
	description of, 93
	disinformation, 61, 116, 351
	healthcare benefits buggy algorithm, 95, 101
	IBM and Nazi Germany, 97, 119
	identifying ethical issues, 118
	importance of, 96
	medicine and text generation, 61
	product design integrated with ML, 99
	recourse, 101
	Volkswagen emission test cheating, 99
	YouTube recommendation feedback loops, 95, 102
Etzioni, Oren, 117
evaluating models (see testing models)
exponential function (exp), 197
export method, 78
	export.pkl file, 79

F (torch.nn.functional), 142, 408
face center in image (see key point model)
Facebook
	bias in advertisements, 112
	feedback loop, 105
	genocide and, 124
	hate speech law compliance, 124
facial recognition bias, 100, 108
factory methods versus customization, 70
Fairness and Machine Learning online book
	(Barocas, Hardt, and Narayanan), 122
fairness, accountability, and transparency, 122
fast.ai ML courses
	about, xxii, 7, 8
	free online course, 547
	image recognition applications, 65
	website, xviii
fastai software library
	about, 12
	accuracy with validation set, 29
	data augmentation, 233
	data cleaning GUI, 77
	documentation for methods, 45
	forums for community support, 546
	import efficiency in notebook, 27
	L class returning collections, 187
	labeling methods, 28
	layered API, 355
	loss function selected by, 194, 228, 235, 237
	metrics, 31
	Tabular classes, 322
	Transforms, 28
	validation set, 28, 30
	version 2 in book, 12
Fauqueur, Julien, 42
feature engineering, 403
feedback loops
	arrest rates on racial grounds, 89
	conspiracy theories fed by, 95, 102, 105
	description, 26, 89
	Facebook and conspiracy theories, 105
	metrics driving algorithms, 103
	recommendation system ethics, 95, 102
	skew from small number of users, 271
Fergus, Rob, 33, 208
Feynman, Richard, 11
file upload to web widget, 80
files as data type, 186
fine-tuning models
	definition, 32, 40
	fine-tune method, 33
	first model, 17, 32
	image classifier model, 75
	natural language models, 329
	classifier, 349
	language model, 343-345
	non-pretrained, 47
	pretrained models, 32, 208
	language model, 343-345
	transfer learning, 207
first model, 13-18
	code for, 26-33
	convolutional neural network, 30
	as deep learning, 20
	error rate, 19
	fine-tuning, 17, 32
	GPU servers, 14
	machine learning visualized, 33
	as neural net, 24
	process of creating application (see process
	end-to-end)
	tested, 19
	Transforms, 28
fisheries monitoring model competition, 53
fitting models
	collaborative filtering system, 262
	definition, 40
	fit method, 32
	pretrained models, 32
	(see also fine-tuning models)
	table of results of each epoch, 194
fix_html, 335
floating point numbers
	casting in PyTorch, 139
	half-precision floating point (fp16), 214
forgery via AI, 117
forward hook, 519
forward method, 261
forward pass, 155, 182, 503
	defining and initializing a layer, 503-508
fraud detection, 38
freezing pretrained models, 207
fully convolutional networks, 443, 444

Gebru, Timnit, 125
gender
	bias in Facebook advertising, 112
	crash test dummies, 125
	facial recognition accuracy, 108
	harassment online, 121
	Meetup recommendation algorithm, 105
	occupations and, 112, 113
	representation bias, 113
	tech industry, 121
generalization by models, 41, 314
genocide and Facebook, 124
geo-diveristy of datasets, 110
Géron, Aurélien, 104
get_dummies for categorical variables, 297
get_preds function, 195
Giomo, Stefano, 433
GitHub Pages hosting blog, 549
	account creation, 550
	synchronizing GitHub and computer, 555
Glorot, Xavier, 505
Google
	bias
	advertising bias, 95
	Photos label, 107
	Translate, 112
	Play concatenation approach, 281
	YouTube recommendation feedback loops, 95, 102
GPU deep learning servers
	about, 14
	complexity of running, 84
	CPU servers cheaper for production, 84
	GPU acceleration, 62
	inference complexity, 83
	production model and, 83
	PyTorch tensors optimized for, 143
	recommended options, 14
	tensor core support, 214
gradient boosted decision trees (GBDTs), 324
gradient boosting machines (GBMs), 324
gradient descent, 162, 182
gradients
	backward pass and, 508-510
	calculating, 154-155
	cross-entropy loss, 203
	definition, 182
	definition as rise/run, 153, 166
	gradient class activation map, 522
Gramian Angular Difference Field (GADF), 37
graphics processing unit (GPU), 14
Greek letters, 247
Guo, Cheng, 278, 324
Guttag, John, 105

H for help, 17
half-precision floating point (fp16), 214
handwritten digits dataset, 133, 240, 426, 442
	downloading, 134
handwritten text read by models, 134
	(see also numerical digit classifier)
Hardt, Mortiz, 122
He, Kaiming, 441, 506
He, Tong, 451
head of model
	cutting model, 459
	definition, 31, 33, 460
	pretrained models and, 31
	Siamese model with custom head, 463-465
head pose dataset, 232
	format of pose text file, 233
healthcare benefits buggy algorithm (ethics), 95, 101, 115
help by pressing H, 17
hidden state, 380
Hinton, Geoffrey, 134, 395, 477
historical bias, 106-112
history
	deep learning, 134
	machine learning, 20
	neural networks, 5, 207
Hitler, Adolf, 97, 119
Hochreiter, Sepp, 134
HookCallback, 520
hooks in PyTorch, 519-522
	Hook class as context manager, 522
	memory leak, 522
horizontal scaling, 85
Human Numbers dataset, 373
Hutson, Jevan, 123
hyperparameters, 49
	random forest insensitivity, 299
	validation set picking threshold, 231

IBM and Nazi Germany, 97, 119
IBM and the Holocaust book (Black), 98
identity function, 446
identity generation by ML, 351
identity mapping, 446
Image class, 135
image classifier model training
	activations, 194
	activations into predictions, 195
	baseline simple model, 193
	baseline training run, 241
	cross-entropy loss, 194-203
	discriminative learning rates, 210
	epochs, number of, 212, 212
	freezing pretrained layers, 207
	Imagenette dataset, 239
	images sized progressively, 243
	transfer learning performance hurt, 244
	improving, 205
	label smoothing, 249
	reasoning behind, 250
	learning rate finder, 205
	logarithms for loss, 200
	metrics and validation loss, 213
	Mixup, 246
	script for training with and without, 248
	normalization of data, 241
	cnn_learner handles, 242
	pretrained models, 242
	statistics distributed with model, 242
	predictions, 195
	process end-to-end, 75
	softmax activation function, 195
	test time augmentation, 245
	testing with confusion matrix, 203
image classifier models
	accuracy as metric, 145
	architecture, 213
	autonomous vehicles localizing objects, 42
	capabilities and constraints, 57
	convolutional neural networks for, 30
	CT scan stroke analysis, 62
	current state of, 60
	data augmentation, 74
	data availability, 58
	data gathering, 65-68
	DataLoaders, 70-72
	customization, 70
	dataset, 186, 239
	checking, 191-193
	debugging, 192
	examination of, 186-187
	filename extraction, 187-188
	image representation rule, 39
	labeling, 188
	labels, 28
	presizing, 189
	regular expressions, 187
	types of data, 186
	distracted driver model, 53
	download_images, 67
	facial recognition bias, 100, 108
	first model, 13-18
	code for, 26-33
	as deep learning, 20
	error rate, 19
	machine learning visualized, 33
	as neural net, 24
	tested, 19
	Google Photos label racial bias, 107
	image basics, 133-136
	image size, 28, 71, 73, 189
	labels in datasets, 28
	(see also labels)
	machine learning explained, 20
	manual process in parallel, 88
	multi-label classification, 219
	non-image tasks, 36-40, 41
	numerical digit (see numerical digit classi‐
	fier)
	performance of model via loss, 76
	prediction inference, 79
	presizing, 189
	pretrained model weight values, 31
	production complexity, 87
	Python Imaging Library, 135
	Siamese model image comparison, 364-367
	softmax, 198
	testing
	complexity of, 87
	confusion matrix for, 76
	training deep dive (see image classifier
	model training)
	verify_images, 67
	web application from model, 78-86
image regression
	about, 232
	dataset, 232
	extracting head center point, 233
	splitting, 233
	key point model description, 232
ImageBlock
	image classifier model, 70, 188
	key point model, 233
	multi-label classifier, 224
ImageClassifierCleaner, 77
	IPython widgets code, 80
ImageNet dataset, 239, 442
	Imagenette subset, 239, 441, 527
	top 5 accuracy, 451
images combined with text, 61
IMDb Large Movie Review dataset, 44
	language model using DataBlock, 342
	pretraining NLP on, 329
	word tokenization, 333
independent variable
	definition, 24
	distracted driver model, 53
	model defined by, 231
	predictions, 24, 71
	viewing in mini-batch, 195
	as x, 71
inference
	definition, 79
	GPU inference complexity, 83
	image classifier models, 79
	predictions models, 79
	test time augmentation, 245
inheritance in object-oriented programming, 261
init (dunder init), 261
inputs
	image classification explanation, 24
	label importance, 25
	(see also labels)
interpretation via class activation map, 519-522
Ioffe, Sergey, 435
IPython widgets, 80
	applications via Voilà, 80
	image cleaner written in, 80
Isaac, William, 89
item transforms, 71
iterate development end to end, 58
	iteration speed, 240

jagged arrays, 143
jargon, 177
	(see also terminology)
jobs and gender, 112, 113
Jupyter Notebook, 13, 15, 18
	(see also notebooks)

Kaggle machine learning community
	about, 7
	Blue Book for Bulldozers competition, 284
	datasets and other resources, 284
	distracted driver model competition, 53
	fisheries monitoring model competition, 53
	predicting sales from stores competition, 278
	leaderboard beater, 323
	predictive modeling competitions, 51
	time series analysis model competition, 53
Kalash, Mahmoud, 39
Kao, Jeff, 350
kernel in notebooks
	restarting, 44
kernel of convolution, 404-406
	apply_kernel, 407-408
	convolution described, 408
Keskar, Nitish Shirish, 394
key point model of image regression
	about, 232
	dataset, 232
	extracting head center point, 233
	splitting, 233
Keyes, Os, 123
The KGB and Soviet Disinformation book
	(Bittman), 116
Khan Academy math tutorials online, xviii, 141
	derivatives, 153
Kinect Head Pose dataset, 232
	format of pose text file, 233
Kohavi, Ron, 46
König, Inke, 297
kwargs, 273

L class returning collections, 187
L1 norm (mean absolute difference), 141, 142
L2 norm (root mean squared error), 141
L2 regularization, 264
label smoothing, 249
	reasoning behind, 250
labels
	bias in, 107
	challenge of object detection, 60
	checking, 191-193
	definition, 40
	dependent variable definition, 25
	extraction from dataset
	first model, 28, 28
	pet breeds dataset, 187-188
	regular expressions, 187
	incorrect affecting loss, 77
	independent variable definition, 24
	multi-label classification, 219-231
	0s and 1s threshold, 229, 230
	DataFrame to DataLoaders, 222-226
	dataset, 220-222
	loss function, 226-228
	metric, 229
	need for, 25
lambda functions, 224
language model
	building from scratch
	building model, 375-380
	building model in PyTorch, 376
	callback, 383
	data, 373-374
	hidden state activations, 380
	LSTM model, 390-394
	LSTM model, regularizing, 394-399
	LSTM training, 398-399
	metric, 379
	multilayer RNNs, 386-390
	recurrent neural network, first, 379
	recurrent neural network, improved, 381-386
	training, 378
	weight tying, 398
	DataBlock, 342
	definition, 329
	NLP (see natural language processing)
language translation (see translation of lan‐
	guages)
latent factors, 253, 271
law enforcement
	arrest rates bias, 89
	database error ethics, 101
	environmental regulation working, 124
	errors in credit report system, 102
	regulating ethics, 124
	sentencing and bail algorithm bias, 107
layered API, 355
layers
	backpropagation for derivative, 155
	deeper models having more layers, 180, 213
	encoding of, 208, 210
	final layer matrix, 207
	forward pass for activations, 155
	last layer and pretrained models, 31
	more linear layers, more computations, 178, 180
	nonlinear function between linears, 178, 180, 207
	optimization and, 180
	out-of-memory error, 214
	prediction viewing, 195
	printing model to see, 269
	ResNet architecture, 30
	training, overfitting, and, 30
	visualizing convolutional networks, 33
Learner
	about, 175, 226
	building Learner class from scratch
	callbacks, 539
	DataLoader, 530
	Dataset, 529
	images, 527
	Learner class, 537
	learning rate scheduling, 540
	loss function, 536
	Module, 531-534
	Parameter, 531-534
	simple CNN, 534
	stochastic gradient descent, 537
	untar_data, 527
	callbacks for custom behavior, 248, 428
	cnn_learner
	architecture, 459
	first model, 30
	image classifier model, 75
	loss function parameter, 236
	multi-label classifier, 226
	normalization of data, 242
	collaborative filtering system, 262
	collab_learner, 269
	fully convolutional network, 444
	lambda functions and exporting, 224
	learn.load, 345
	learn.model, 273
	layers printed, 269
	learn.recorder, 432
	learn.save, 345
	numerical digit classifier, 175
	show_results, 236
learning rate (LR)
	about, 156-157
	building Learner class from scratch, 540
	changing during training, 430
	convolutional neural networks, 445
	definition, 182
	discriminative learning rates, 210
	learning rate finder, 205, 236
	building Learner class from scratch, 540
Lecun, Yann, 133, 134
Li, Hao, 451
Liang, James, 99
linear and nonlinear layers, 176, 178, 181, 207
LinkedIn ML-generated profile, 352
list comprehensions, 138
load method, 345
load_learner, 79
Lockhart, Paul, 9
logarithmic scale
	about, 201
	learning rate finder plot, 207
	loss in pet breed image classifier, 200
	slide rules using, 201
long short-term memory (LSTM), 134
look-up index as one-hot-encoded vector, 259
loss
	BCELoss, 228
	BCEWithLogitsLoss, 228, 237
	bear image classifier, 76
	binary cross entropy, 226-231
	building Learner class from scratch, 536
	categorical outcome cross-entropy loss, 194
	class versus plain functional form, 202
	cross-entropy, 194
	(see also cross-entropy loss)
	definition, 24, 40, 182
	fastai selecting function, 194, 228, 235, 237
	label incorrect, not model, 77
	logarithms for, 200
	metrics versus, 31
	MNIST loss function, 163-169, 198-200
	model defined by, 231
	MSELoss, 236
	multi-label classifier loss function, 226-228
	numerical digit image classifier, 163-169
	sigmoid function, 168
	softmax function, 198-200
	passing to learner, 236
	pet breeds image classifier, 194-203
	negative log likelihood, 202
	probability as confidence level, 76
	PyTorch functions for comparisons, 142
	reinforcement learning, 102
	selecting loss function for problem, 237
	validation loss improvement slowing, 212
lowercase rule, 336
ls method in Path class, 79, 135
	dataset examination, 186
LSTM language model
	about, 390
	building from scratch, 390-394
	regularizing, 394-399
	activation regularization, 397
	dropout, 395
	temporal activation regularization, 397
	training a regularized LSTM, 398-399
	training a language model using, 393
Lum, Kristian, 89

Maas, Andrew, 44
machine learning (ML)
	bagging, 298-323
	bias, 105
	(see also bias)
	capabilities and constraints, 57
	classification model definition, 28
	concepts of, 20-23
	current state of, 60
	defined, 23
	explained, 20-23, 40
	fairness and, 122
	(see also ethics)
	feature engineering, 403
	first model as neural net, 24
	(see also first model)
	history of development, 20, 134
	key techniques, 282
	key to ML via derivatives, 154
	limitations inherent to, 25-26
	manual process in parallel, 88
	mobile landscape, 86
	neural networks beyond understanding, 88
	product design integrated with, 99
	regression model definition, 28
	risk mitigation, 88
	scikit-learn library, 283
	Twitter for help, 421
	visualizing, 33
	weights, 21-22
	via neural networks, 23
Mackenzie, Dana, 310
Making Learning Whole book (Perkins), 9
malware classification, 39
manual process in parallel, 88
Mark I Perceptron, 6
Markdown in notebook cells, 16
math tutorials online, xviii, 141
	derivatives, 153
matrix multiplication, 164
	function from scratch, 495
McClelland, James, 6
McCulloch, Warren, 5
McKinney, Wes, 222, 284
mean absolute difference (L1 norm), 141, 142
mean average percent error metric, 325
mean squared error (MSE), 142
measurement bias, 112
medicine
	aggregation bias, 113
	correct responses not ensured, 61
	measurement bias, 112
	pretrained model availability, 32
	stroke analysis, 62, 112
	tumor identification, 4, 7
Meetup recommendation algorithm, 105
memory usage
	batch operations out-of-memory error, 214
	entity embedding reducing, 278
	hooks might leak, 522
Merity, Stephen, 394
methods
	class methods, 342
	doc for documentation, 45
	Python method double underscores, 261
	source code display, 67, 335
	tab for autocomplete and documentation, 68
metrics
	about, 287
	definition, 31, 40, 145
	fastai library, 31
	feedback loops driven by, 103
	first model declaration, 30, 31
	loss versus, 31
	mean average percent error, 325
	numerical digit classifier, 145-148
	pet breeds image classifier, 213
	root mean squared log error, 287, 295, 301
	top 5 accuracy, 451
Microsoft
	Azure Cognitive Services, 66
	batching production operations, 83
	Fairness, Accountability, Transparency, and
	Ethics, 122
mid-level API
	about, 355
	callbacks, 356
	Datasets, 362
	fastai layered API, 355
	Pipeline class, 359
	Siamese model image comparison, 364-367
	TfmdLists, 359-362
	Transforms
	collections, 359
	Transform class, 356
	writing your own, 358, 361
mini-batch, 71, 182
	DataLoader variables, 222
	dependent and independent variables, 195
	manually grabbing and passing into a
	model, 227
	models returning activations, 227
Minsky, Marvin, 6
missing values as data leakage, 311
mixed-precision training, 214
Mixup augmentation technique, 246
	loss improvement, 248
ML (see machine learning)
MNIST handwritten digits dataset, 133, 240, 426, 442
	binary to multiple categories, 198-200
	downloading, 134
	read by models, 134
	(see also numerical digit classifier)
	validation set, 145
mobile device deployment of apps, 85
models
	accuracy (see accuracy)
	actionable outcomes via Drivetrain
	Approach, 63
	autonomous vehicles localizing objects, 42
	begin simply, 137, 193
	(see also beginning)
	best methods for majority of datasets, 282
	capacity, 213
	classification model definition, 28
	data seen changing over time, 87, 95
	defined by variables and loss function, 231
	definition, 40
	encoder, 345
	exporting, 78
	first model (see first model)
	GPUs and production models, 83
	head and pretrained models, 31
	load method, 345
	model and human interaction, 61, 88
	modeling competitions, 51
	more parameters, more accuracy, 213
	overfitting importance, 468
	(see also overfitting)
	parameter importance, 32
	(see also parameters)
	printing to see layers, 269
	process of creating application (see process
	end-to-end)
	programs constrasted, 21, 23
	regression model definition, 28
	results versus performance, 22
	save method, 345, 345
	system behavior changed by, 89
	tabular data for, 45
	(see also tabular data)
	advice for modeling, 325
	training, 22-23
	(see also training)
	web application from, 78-86
Module class
	activations returned, 227
	building Learner class from scratch, 531-534
	calling module calls forward method, 261
	inheritance, 261
	Parameter class, 266
modules, 174
momentum in SGD, 474-477
	cyclical momentum, 431
Monroe, Fred, 126
mouse movements for fraud detection, 38
movie recommendation system
	collaborative filtering
	about, 253
	biases, 263
	bootstrapping problem, 270
	collab_learner, 269
	DataLoaders, 257
	dataset, 254
	deep learning model, 272
	embedding, 259-265
	embedding distance, 270
	embedding from scratch, 265-274
	fitting model, 262
	interpretting embeddings and biases, 267
	items rather than products, 253
	latent factors, 253
	layers via printing model, 269
	Learner from scratch, 262
	learning latent factors, 256
	look-up index as one-hot-encoded vec‐
	tor, 259
	overfitting, 264
	probabilistic matrix factorization, 271
	structuring model, 254
	tables as matrices, 259
	weight decay, 264
	MovieLens sample model, 46
	skew from small number of users, 271
movie review sentiment model, 43, 329
	(see also natural language processing)
MovieLens dataset, 46, 254
MSELoss, 236, 237
Mueller report, 104, 116
Mullainathan, Sendhil, 112
multi-label classification, 219-231
	0s and 1s threshold, 229, 230
	DataFrame to DataLoaders, 222-226
	dataset, 220-222
	loss function, 226-228
	metric, 229
MultiCategoryBlock, 224
multilayer RNNs, 386-390
multilayered neural networks learned with
	SGD, 282

Nader, Ralph, 125
Narayanan, Arvind, 122
National Institute of Standards and Technol‐
	ogy, 133
natural language processing (NLP)
	architecture, 465
	backpropagation through time for, 466
	bias in data, 112
	Chomsky’s syntax book, 188
	correct response not ensured, 61
	current state of, 61
	data augmentation of text data, 394
	disinformation, 61, 117, 350
	fine-tuning
	classifier, 349
	language model before classification
	model, 329
	pretrained language model, 343-345
	language model from scratch
	building model, 375-380
	building model in PyTorch, 376
	callback, 383
	data, 373-374
	hidden state activations, 380
	LSTM model, 390-394
	LSTM model, regularizing, 394-399
	LSTM training, 398-399
	metric, 379
	multilayer RNNs, 386-390
	recurrent neural network, first, 379
	recurrent neural network, improved, 381-386
	training, 378
	weight tying, 398
	Mixup data augmentation, 248
	pretrained English language model, 330
	protein chains as, 63
	recurrent neural network, 332, 343
	(see also recurrent neural networks)
	about process, 331
	accuracy, 350
	classifier DataLoaders, 346
	fine-tuning classifier, 349
	fine-tuning pretrained language model, 343-345
	language model using DataBlock, 342
	numericalization, 332, 338
	pretraining, 329
	text generation, 346
	texts into batches for language model, 339-342
	training text classifier, 342
	unfreezing classifier, 349
	sentiment of movie review, 43, 329
	style of target corpus, 330
	text generation, 61
	tokenization
	approaches to, 332
	definition, 332
	fastai tokenization, 333
	rule explanations, 335
	showing rules used, 335
	special tokens, 334
	subword tokenization, 336
	word tokenization, 333, 336
	unfreezing classifiers, 349
	Wikipedia for pretraining, 329
Nazi Germany and IBM, 97, 119
negative log likelihood loss (nll_loss), 202
nested list comprehensions, 407
net neutrality disinformation, 350
neural networks
	beyond understanding, 88
	building layer from scratch, 493-503
	backward pass, 503
	broadcasting, 497
	broadcasting rules, 501
	broadcasting vector to matrix, 498-501
	broadcasting with a scalar, 498
	defining and initializing a layer, 503-508
	Einstein summation, 502
	elementwise arithmetic, 496
	forward pass, 503
	gradients and backward pass, 508-510
	matrix multiplication, 495
	modeling a neuron, 493
	PyTorch, 512-515
	refactoring the model, 511
	Coursera class, 477
	deep learning using, 3, 20, 40
	explained, 23-24, 176
	first model as, 24
	(see also first model)
	fundamental weights and bias equation, 165
	GPU running, 14
	(see also GPU)
	history, 5-7, 207
	layers (see layers)
	multilayered neural networks learned with
	SGD, 282
	natural language processing, 331
	(see also natural language processing)
	refactoring, 416
	risk mitigation, 88
	RNN definition, 380
	(see also recurrent neural networks)
	tabular data, 318
	categorical columns, 319
	combining with other methods, 324
	testing, complexity of, 87
	training via backpropagation, 134
	training with large learning rates, 430
	visualizing learning, 33
new user bootstrapping problem, 270
NLP (see natural language processing)
nonlinear and linear layers, 176, 178, 181, 207
normalization of data, 241
	cnn_learner handles, 242
	pretrained models, 242
	statistics distributed with model, 242
	Transform class, 357
notebooks
	about, 13, 15
	app from notebook, 82
	Binder free app hosting, 84
	blogging with, 556
	book written in, 18
	cell execution order, 44
	cells, 16
	code from book, xviii, 18, 44
	command mode, 17
	edit mode, 17
	escape key for command/edit mode, 17
	features for efficiency, 68
	first cell CLICK ME, 17, 44
	first notebook, 15-18
	code for, 26-33
	error rate, 19
	tested, 19
	full versus stripped, 16
	GPU server setup, 14
	H for help, 17
	kernel
	restarting, 44
	library efficiency, 27
	Markdown formatting, 16
	opening, 16
	out-of-memory error, 214
	process of creating application (see process
	end-to-end)
	showing source code, 335
	utils class, 66
	web application deployment, 78-86
number precision and training, 214
number-related datasets
	handwritten digits dataset, 133, 240, 426, 442
	downloading, 134
	Human Numbers dataset, 373
numerical digit classifier
	accuracy metric, 145-148
	activations, 196
	color-code array or tensor, 136
	comparing with ideal digit, 141
	convolutional neural network
	1cycle training, 430
	batch normalization, 435
	batch size increased, 429
	building a CNN, 415-438
	color images, 423
	convolution arithmetic, 418
	convolution described, 404
	dataset, 426
	equations, 412
	kernel, 404-406
	kernel mapping, 407-408
	nested list of comprehensions, 407
	padding, 411
	PyTorch convolutions, 408
	receptive fields, 419
	training, 418
	training more stable, 429-438
	training on all digits, 427-438
	dataset download, 134
	feature engineering, 403
	fully convolutional networks and, 444
	ideal digit creation, 137-140
	image as array or tensor, 135
	Learner creation, 175
	MNIST loss function, 163-169, 198-200
	optimization step, 170-180
	pixel similarity, 137-142
	stochastic gradient descent, 148-163
	calculating gradients, 153-155
	example end-to-end, 157-162
	stepping with learning rate, 156-157
	summarizing, 162
	terminology, 181
	validation set, 145
	viewing dataset images, 135
numericalization
	defaults, 338
	definition, 332, 338
	Transform class, 356
	word-tokenized text, 338
NumPy
	arrays
	about, 143
	arrays within arrays, 143
	image section, 135
	sklearn and Pandas rely on, 283
NVIDIA GPU deep learning server, 14
	(see also GPU)
	tensor core support, 214

Obermeyer, Ziad, 112
object detection
	current state of, 60
	labeling challenge, 60
object recognition
	current state of, 60
	dataset provenance, 110
object-oriented programming, 260
	classes, 261
	dunder init, 261
	inheritance, 261
	superclass, 261
objectives via Drivetrain Approach, 64
occupations and gender, 112, 113
OCR (see numerical digit classifier)
one-hot encoding
	definition, 225
	embedding categorical variables, 259-265, 297
	multiple columns for variable levels, 297
	entity embedding contrasted, 278
	label smoothing, 249
	look-up index as one-hot-encoded vector, 259
	multi-label classifier, 225, 227
online advertisement bias, 112
online applications (see web applications)
online resources (see web resources)
optical character recognition (see numerical
	digit classifier)
optimization
	Adam as default, 479
	creating an optimizer, 174-180
	generic optimizer, 473
	gradient descent, 162, 182
	layers and, 180
	module parameters, 265
	nonlinearity added, 176
	numerical digit classifier, 170-180
	pet breeds image classifier, 194-203
	stochastic gradient descent, 170-180
ordinal columns in tabular data, 286
out-of-domain data, 60
	image classifier in production, 87
out-of-memory error, 214
outputs
	cells containing executable code, 16, 44
	forward hook for custom behavior, 519
	image, 18
	results of last execution, 44
	table, 17
	text, 18
	web display Output widget, 81
overfitting
	avoid only when occurring, 30
	definition, 40
	importance of, 30, 468
	layers and, 30
	learning rate finder, 205
	model memorizing training set, 29
	reducing, 468
	regularizing RNNs against, 394
	retrain from scratch, 213
	training versus validation loss, 212
	validation set, 49
	hyperparameter picked by, 231
	weight decay against, 264
O’Neill, Cathy, 115

padding a convolution, 411
Pandas library
	DataFrame
	color-code image values, 136
	DataLoaders object from, 222-226
	multi-label CSV file, 220
	dataset viewing, 286
	fastai TabularPandas class, 290, 318
	get_dummies for categorical variables, 297
	NumPy needed, 283
	tabular data processing, 283, 318
	tutorial, 222
papers (see research papers)
Papert, Seymour, 6
Parallel Distributed Processing (PDP) book
	(Rumelhart, McClelland, and PDP Research
	Group), 6
parameters
	architecture requiring many, 32
	calling module calls forward method, 261
	deeper models and, 180
	definition, 40, 181
	derivative of a function, 153
	exporting models, 78
	hyperparameters, 49
	random forest insensitivity, 299
	validation set picking threshold, 231
	importance of, 32
	loss function selected by fastai, 194
	machine learning concepts, 21, 24
	more accuracy from more parameters, 213
	neural networks beyond understanding, 88
	Parameter class, 266
	building Learner class from scratch, 531-534
Parr, Terence, 294
partial function to bind arguments, 229
PASCAL multi-label dataset, 220
path to dataset
	ls method, 79, 135, 186
	Path object returned, 27
PDP Research Group, 6
Pearl, Judea, 310
pedophiles and YouTube, 103
Perceptrons book (Minsky and Papert), 6
performance of model as loss, 24, 40
Perkins, David, 9
person’s face center in image (see key point
	model)
pet breeds image classifier (see image classifier
	models)
pet images dataset, 17, 28, 186, 364
pickle system for save method, 292
PIL images, 135
Pipeline class, 359
Pitts, Walter, 5
pixels
	image basics, 133-136
	pixel count
	image sizes same, 71, 189
	pretrained models, 28
	size tradeoffs, 28
	sizing difficulties, 73
	tensor shape, 139
	pixel similarity, 137-142
plain text data approach, 283
PointBlock, 233
policy’s role in ethics, 123-125
	rights and policy, 124
positive feedback loop, 26
precision of numbers and training, 214
predictions
	activations transformed into, 195
	bagging, 298-323
	button for web application, 82
	definition, 24
	dependent variable for, 287
	hypothetical world of, 310
	independent variable, 24, 71
	inference instead of training, 79
	inference with image classifier, 79
	as machine learning limitation, 25
	metric measuring quality, 31
	model changing system behavior, 89
	model overconfidence, 212
	movie recommendation system, 46
	overfitting and, 30
	predictive modeling competitions, 51
	predictive policing algorithm, 89
	random forest confidence, 302
	sales from stores, 278
	(see also tabular data)
	softmax sum of 1 requirement, 227
	stroke prediction, 62, 112
	viewing, 195
prerequisite for book, xviii
presizing, 189
pretrained models
	accuracy from, 31
	convolutional neural network parameter, 31
	definition, 31, 40
	discriminative learning rates, 210
	fine-tuning first model, 17
	first model, 17
	freezing, 207
	last layer and, 31, 207
	NLP English language, 330
	normalization of data, 242
	statistics distributed with model, 242
	pixel count required, 28
	recommendation system rarity, 47
	self-supervised learning for, 329
	tabular model rarity, 46
	transfer learning, 32, 162
	freezing, 207
	Wikipedia for pretraining NLP, 329
privacy
	deployed apps, 85
	regulation needed, 124
	rights and policy, 124
probabilistic matrix factorization, 271
process end-to-end
	actionable outcomes via Drivetrain
	Approach, 63
	applicability of deep learning to problem, 60
	begin in known areas, 59, 129
	(see also beginning)
	capabilities and contraints of deep learning, 57
	data availability, 58
	data biases, 68
	data cleaning, 77
	data gathering, 65-68
	DataLoaders, 70-72
	customization, 70
	deployment
	app from notebook, 82
	Binder free app hosting, 84
	deployment file, 79
	exporting model, 78
	mobile devices, 85
	prediction inference, 79
	risk mitigation, 88
	unforeseen challenges, 89
	web application, 78-86
	web application deployment, 83-86
	web application disaster avoidance, 86
	web resource discussing issues, 87
	experiments lead to projects, 58
	image size, 71, 73, 189
	iterate end to end, 58
	model and human interaction, 61, 88
	performance of model via loss, 76
	prototyping, 59
	risk mitigation, 88
	testing with confusion matrix, 76
	training the model, 75
	web application disaster avoidance, 86
	web application from model, 78-86
production
	CPU servers cheaper than GPU, 84
	data seen changing over time, 87, 95
	GPU for model in production, 83
	manual process in parallel, 88
	out-of-domain data, 60, 87
	product design integrated with ML, 99
	testing, complexity of, 87
	web application from model, 78-86
profile identity generated by ML, 352
programs versus models, 21, 23
progressive resizing, 243
	transfer learning performance hurt, 244
protein chains as natural language, 63
prototyping
	datasets cut down, 48
	project buy-in, 59
	web application from model, 78-86
publishing app on Binder, 84
Python
	array APIs, 144
	class methods, 342
	context manager, 522
	error debugging, 68
	fastai library efficiency, 27
	IPython widgets, 80
	applications via Voilà, 80
	Jupyter for, 13
	lambda functions, 224
	list comprehensions, 138
	list type as fastai L class, 187
	loop inefficiency, 144, 164
	method double underscores, 261
	nested list comprehensions, 407
	Pandas library, 222
	partial function to bind arguments, 229
	Path class, 27, 79
	tensor APIs, 144
	web browser functionality, 80
Python for Data Analysis book (McKinney), 222, 284
Python Imaging Library (PIL), 135
PyTorch
	about, xxi, 12
	about fastai software library, 12
	building NLP model, 376
	casting, 139
	convolutions, 408
	decision trees don’t use, 283
	fastai torch.nn.functional import, 142, 408
	hooks, 519-522
	loss functions for comparisons, 142
	most important technique, 147
	names ending in underscore, 173
	object-oriented programming, 260
	optimizer creation, 174-180
	SGD class, 471-474
	single item or batch same code, 228
	tensors
	about, 143
	broadcasting, 147, 147
	image section, 136

racial bias
	arrest rates, 89
	datasets for training models, 109
	Facebook advertising, 112
	facial recognition, 100, 108
	Google advertising, 95
	Google Photos label, 107
	historical, 106
	power of diversity, 121
	sentencing and bail algorithm, 107
radiologist-model interaction, 62
Raji, Deb, 69
random forests, 298-323
	creating a random forest, 299
	ensembling, 322
	boosting, 323
	extrapolation problem, 314
	out-of-domain data, 316
	hyperparameter insensitivity, 299
	model interpretation, 302
	data leakage, 311
	feature importances, 303
	partial dependence, 308
	removing low-importance variables, 305
	removing redundant features, 306
	tree interpreter, 312
	tree variance for prediction confidence, 302
	out-of-bag error, 301
random seed for validation set selection, 28, 71
RandomResizedCrop
	image classifier model, 73
	test time augmentation instead, 245
rank correlation, 306
rank of tensor
	definition, 139, 181
	scalar versus vector versus matrix, 181
recommendation systems
	about, 26
	actionable outcomes via Drivetrain
	Approach, 65
	Amazon, 62
	collaborative filtering (see collaborative fil‐
	tering)
	conspiracy theory feedback loops, 95, 102, 105
	current state of, 62
	feedback loop ethics, 95, 102
	Google Play concatenation approach, 281
	Meetup and gender, 105
	movies based on viewing habits, 46
	pretrained model rarity, 47
	skew from small number of users, 271
	as tabular data, 62
	YouTube feedback loop ethics, 95, 102
recourse for ethics violations, 101
rectified linear unit (ReLU), 177, 182
recurrent neural networks (RNNs)
	backpropagation through time, 382
	creating more signal, 384
	definition, 380
	language model from scratch
	first RNN, 379
	improved RNN, 381-386
	multilayer RNNs, 386-390
	LSTM language model, 390-394
	regularizing, 394-399
	maintaining state of, 381
	multilayer RNNs, 386-390
	natural language processing using, 332, 343
	AWD-LSTM architecture, 343, 394
	training, 394
refactoring parts of neural networks, 416, 511
regression models definition, 28
regular expressions (regex), 187
regulating ethics, 124
reinforcement learning, 102
replace_all_caps, 336
replace_maj, 336
replace_rep, 335
replace_wrep, 335
representation bias, 113, 271
research papers
	about, 247
	advertising bias, 95
	bagging predictors, 298
	batch normalization, 435
	bias in machine learning, 105
	class activation map, 519
	convolution arithmetic, 408
	cyclical momentum, 431
	data leakage, 311
	deep residual learning, 441
	demographics dataset, 46
	ethical lens versus ethical intuitions, 123
	geo-diversity of datasets, 110
	gradient class activation map, 522
	label smoothing, 250
	malware classification, 39
	measurement bias, 112
	Mixup, 246
	model bias, 69
	object recognition, 110
	predicting sales from stores, 278
	predictive policing, 89
	rectifier deep dive, 506
	regularizing LSTM language models, 394
	representation bias, 113
	ResNet improved, 451
	sentiment analysis, 44
	skip connections smoothing loss, 451
	training a segmentation model, 42
	training deep feedforward neural networks, 505
	training with large learning rates, 430
	visualizing neural network weights, 33, 208
Resize, 72
	image classifier model, 72
	presizing, 191
ResNet architecture
	about, 441, 447, 450
	building ResNet CNN, 445-451
	building state-of-the-art ResNet, 451-456
	bottleneck layers, 454-456
	top 5 accuracy, 451
	ease of learning, 448
	first model, 30
	fully convolutional networks, 443
	first, 443
	Learner, 444
	image classifier, 213
	Imagenette dataset, 441
	model approach, 442
	layer quantity variants, 213
	ResNet-18, -34, -50 versions, 213, 454
	skip connections, 445-451
	about, 445
results (see predictions)
rights and policy, 124
RMSProp, 477
rm_useless_spaces, 336
RNN (see recurrent neural networks)
root mean squared error (RMSE or L2 norm), 141
root mean squared log error as metric, 287, 295, 301
Rosenblatt, Frank, 5
Rumelhart, David, 6
Russia and 2016 election, 116
Russia Today and Mueller report, 104

Samuel, Arthur, 20
save method, 292, 345
	encoder, 345
Schmidhuber, Jurgen, 134
scikit-learn library, 283
search_images_bing, 66
seed for validation set selection, 71
segmentation, 60
	autonomous vehicle training, 42
self-driving cars, 42, 64
self-supervised learning
	about, 329
	definition, 329
	language model, 329
	vision applications, 329
Sequential class, 178, 534
server for running code, 14
setup
	first model, 13
	(see also first model)
	Jupyter Notebook, 13, 15, 18
	(see also beginning)
	NVIDIA GPU deep learning server, 14
SGD (see stochastic gradient descent)
SGD class, 175, 471-474
	(see also stochastic gradient descent)
Shankar, Shreya, 110
show_batch method, 192
show_image function, 138
Siamese model image comparison, 364-367
	pretrained architecture, custom head, 463-465
sigmoid function
	binary decision, 168, 195
	one-hot-encoded targets, 228
	softmax for more than two columns, 197
	two-activation version, 197
sigmoid_range, 235, 262
signature of function
	creating, 158
	delegates, 274
	displaying, 67, 68, 335
skip connections, 445-451
sklearn
	creating decision tree, 292
	default leaf node splitting, 296
	docs online, 300
	max_features choices, 299
	NumPy needed, 283
	TabularPandas class, 290
Smith, Leslie, 205, 207, 430, 431
Socher, Richard, 394
socioeconomic bias, 115
softmax activation function, 195, 207
	image classifier, 198, 198-200
	sum of 1 for predictions, 227
sound analyzed as spectrogram, 36, 60, 63
source code of function displayed, 335
special tokens, 334
spec_add_spaces, 335
Splunk.com fraud detection, 38
spreadsheet data for models, 45
starting (see beginning)
stem in convolutional neural network, 452, 460
stochastic gradient descent (SGD)
	about, 23, 148-153, 471
	backward, 155
	building Learner class from scratch, 537
	calculating gradients, 153-155
	cyclical momentum, 431
	example end-to-end, 157-162
	mini-batches, 170
	momentum, 474-477
	multilayered neural networks learned with, 282
	optimization of numerical digit classifier, 170-180
	SGD class, 175, 471-474
	stepping with learning rate, 156-157
	summarizing, 162
store sales predictions
	embedding distance and store distance, 280
stride-1 convolutions, 412
stride-2 convolutions, 411
	increasing number of features, 419
stroke prediction, 62, 112
subword tokenization, 336
summary method
	debugging image dataset, 192
	debugging tabular dataset, 226
	debugging text dataset, 342
Suresh, Harini, 105
Sweeney, Latanya, 95
symbolic computation library, 510
SymPy library and calculus, 510
Syntactic Structures book (Chomsky), 188
Szegedy, Christian, 250, 435

Tabular classes, 322
tabular data for models
	about, 45, 277
	advice for modeling, 325
	architecture, 466
	categorical embeddings, 277
	current state of, 62
	as data type, 186
	dataset for deep dive, 284
	data leakage, 310
	date handling, 289
	examining data, 285
	neural network model, 318
	ordinal columns, 286
	overfitting, 295
	TabularPandas class, 290, 318
	decision trees as first approach, 283
	about, 282, 287
	bagging, 298-323
	displaying tree, 292-295
	libraries for, 283
	metric, 287, 295, 301
	training, 288-296
	deep learning not best starting point, 282
	entity embedding, 278
	model interpretation, 302
	data leakage, 310
	feature importances, 303
	partial dependence, 308
	removing low-importance variables, 305
	removing redundant features, 306
	tree interpreter, 312
	tree variance for prediction confidence, 302
	multi-label classification, 220-222
	neural network model, 318
	ordinal columns, 286
	predicting sales from stores, 278
	pretrained model rarity, 46
	recommendation systems as, 62
TabularPandas class, 290
TabularProc, 290
tech industry and gender, 121
temporal activation regularization, 397
tensor core support by GPUs, 214
tensors
	about, 143
	all images in directory, 137
	APIs, 144
	broadcasting, 147, 147
	color image as rank-3 tensor, 423
	column selected, 144
	creating a tensor, 144
	definition, 181
	displaying as images, 138
	elementwise arithmetic, 496
	image section, 136
	image sizes same, 71, 189
	matrix multiplication, 164
	function from scratch, 495
	operators, 145
	rank, 139
	row selected, 144
	shape, 139
	length for rank, 139
	slicing row or column, 145
	type, 145
terminology for deep learning, 24, 40, 181
test time augmentation (TTA), 245
testing models
	build it, test it, 531
	complexity of production model testing, 87
	confusion matrix, 76
	first model, 19
	test set, 49
	building, 50-54
text combined with images, 61
text data approach, 283
	(see also natural language processing)
text generation
	correct responses not ensured, 61
	current state of, 61
	disinformation, 61, 117, 350
	NLP, 346
	(see also natural language processing)
TextBlock, 342
TextDataLoaders.from_folder, 355
TfmdLists, 359-362
Thomas, Rachel, 90, 118
time series analysis
	converting to image, 37
	current state of, 62
	sales from stores, 278
	(see also tabular data)
	TabularPandas splitting data, 290
	training and validation sets, 51, 301
tokenization
	approaches to, 332
	definition, 332
	fastai interface, 333
	most common token prediction, 379
	numericalization, 338
	showing rules used, 335
	special tokens, 334
	subword tokenization, 336
	texts into batches for language model, 339-342
	token definition, 333
	Transform class, 356
	unknown word token, 338
	word tokenization, 333, 336
top 5 accuracy, 451
torch.nn.functional, 142, 408
training
	1cycle training, 430
	backpropagation for neural networks, 134
	bagging, 298-323
	baseline, 137, 193, 471-473
	biases, 68
	black-and-white or hand-drawn images, 60
	cyclical momentum, 431
	data cleanup before versus after, 77, 78
	decision trees, 288-296
	deeper models, 180, 214
	definition, 40
	early stopping, 213
	epochs, number of, 32
	ethics importance, 96
	(see also ethics)
	experiments lead to projects, 58
	fine-tuning definition, 32
	(see also fine-tuning)
	first model, 15
	head of model, 31
	image classifier models (see image classifier
	model training)
	image differences during, 73
	labels for examples, 25
	layers and, 30, 207
	learning rate, 156-157
	changing during training, 430
	definition, 182
	learning rate finder, 205
	machine learning concepts, 22-23
	mixed-precision training, 214
	model memorizing data, 29, 50
	neural networks and learning rate, 430
	numerical digit classifier (see numerical
	digit classifier)
	out-of-domain data, 60
	overfitting, 29
	importance of, 30, 468
	reducing, 468
	retrain from scratch, 213
	weight decay against, 264
	prediction model inference, 79
	pretrained models (see pretrained models)
	process
	about, 471
	Adam, 479
	baseline established, 471-473
	callbacks, 480
	callbacks, creating, 483
	callbacks, exceptions, 487
	decoupled weight decay, 480
	momentum, 474-477
	optimizer generic, 473
	RMSProp, 477
	SGD class, 471-474
	random variations, 18
	recurrent neural networks, 394
	self-supervised learning, 329
	(see also self-supervised learning)
	stochastic gradient descent, 148-153
	calculating gradients, 153-155
	example end-to-end, 157-162
	momentum, 474-477
	stepping with learning rate, 156-157
	summarizing, 162
	tensor core support for speed, 214
	text classifier, 342
	fine-tuning language model, 343-345
	language model using DataBlock, 342
	time spent, 18
	trained model is program, 23
	training set, 28, 30, 40
	building, 50-54
	classes for representing, accessing, 222
	cleaning GUI, 77
	DataLoaders, 70-72
	DataLoaders customization, 70
	presizing, 189
	production complexity and, 87
	racial balance of, 109
	time series, 290
transfer learning
	about, 207
	cutting network, 459
	definition, 32
	final layer, 207
	fine-tuning as, 32, 207
	image classifier, 207
	natural language processing, 329
	progressive resizing hurting performance, 244
	self-supervised learning, 329
	weights, 162
Transforms
	collections, 359
	Datasets, 362
	definition, 28
	image cropping, 73
	test time augmentation, 245
	image size, 28, 71, 189
	item transforms, 71
	Pipeline class, 359
	presizing, 189
	Siamese model image comparison, 364-367
	TabularProc, 290
	TfmdLists, 359-362
	Transform class, 356
	writing your own, 358, 361
translation of languages
	bias in Google Translate, 112
	current state of, 61
	French/English parallel text data, 48
tumor identification, 4, 7
Turing Award, 134
tutorials
	book chapters, 27
	math tutorials online, xviii, 141
	derivatives, 153
	Pandas library, 222
Twitter for deep learning help, 421

unet_learner architecture, 461
unfreezing
	gradual unfreezing NLP classifier, 349
	image classifier, 207
universal approximation theorem, 23, 178, 448
Universal Language Model Fine-tuning (ULM‐
	FiT) approach, 330
untar_data, 527

validation set
	building, 50-54
	numeric digit classifier, 145
	classes for representing and accessing, 222
	cleaning GUI, 77
	DataLoaders, 70-72
	definition, 40, 49
	error rate, 31
	export method, 79
	first model, 28
	hyperparameter picked by, 231
	NLP most common token, 379
	numeric digit classifier, 145
	out-of-domain data, 316
	overfitting, 49
	random seed, 28
	size of, 49
	splitting from training set, 71
	test time augmentation, 245
	testing with confusion matrix, 76
	time series, 290
variables
	categorical variables, 277
	embedding and, 278
	continuous variables, 277
	error debugging, 68
	viewing as mini-batch, 195
vector dot product, 256, 281
verify_images, 67
Visin, Francesco, 408
vocabulary (see terminology)
Voilà, 80
Volkswagen emission test cheating (ethics), 99

warmup learning rate, 430
Watson, Thomas, 97
Weapons of Math Destruction book (O’Neill), 115
web applications
	Binder free app hosting, 84
	deployment file, 79
	disaster avoidance, 86
	file upload widget, 80
	model into, 78-86
	recommended hosts, 85
	web display Output widget, 81
web resources
	actionable outcomes via Drivetrain
	Approach, 64
	bias in machine learning, 105
	Binder free app hosting, 84
	blogging article, 90
	book updates, 128
	code from book, xviii, 18, 44
	datasets and other Kaggle resources, 284
	decision tree viewer, 294
	deployment issue discussion, 87
	documentation for methods, 45
	ethics description, 94
	ethics toolkits, 119
	Fairness and Machine Learning book, 122
	fast.ai free online course, 547
	fast.ai website, xviii
	fastai forums, 546
	fraud detection at Splunk.com, 38
	GitHub Pages hosting blog, 549
	GPU servers, 14
	Jupyter, 13
	Kaggle machine learning community, 7
	malware classification, 39
	math tutorials, xviii, 141
	derivatives, 153
	mathematical symbols, 247
	predicting sales from stores paper, 278
	predictive policing paper, 89
	Python debugger, 68
	recommended web app hosts, 85
	regular expression tutorials, 188
	segmentation training, 42
	sklearn docs, 300
	sound analyzed as spectrogram, 36
	SymPy library, 510
	tutorials for each book chapter, 27
	visualizing convolutional networks, 33
weights
	machine learning, 21-22
	neural networks, 23
	as parameters, 22, 24
	pretrained parameter, 31
	random in training from scratch, 162
	stochastic gradient descent, 148-153
	calculating gradients, 153-155
	example end-to-end, 157-162
	stepping with learning rate, 156-157
	summarizing, 162
	transfer learning
	freezing pretrained layers, 207
	pretrained models, 162
	visualizing learning, 33
	weight decay, 264
	decoupled, 480
	weight tying, 398
Werbos, Paul, 134
Wikipedia for pretraining NLP, 329
word tokenization, 333, 336
Wright, Marvin, 297

XGBoost library, 324

YouTube
	recommendation feedback loops, 95, 102
	Russia Today possibly gaming, 104
y_range
	coordinate range, 235
	recommendation system ratings, 47

Zeiler, Matt, 33, 208
Zhang, Hongyi, 246
Zhou, Bolei, 519
Zuckerberg, Mark, 124