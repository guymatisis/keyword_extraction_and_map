corrects = (preds>0.0).float() == train_y
corrects
tensor([[ True],
[ True],
[ True],
...,
[False],
[False],
[False]])
corrects.float().mean().item()
0.4912068545818329
Now let’s see what the change in accuracy is for a small change in one of the weights:
weights[0] *= 1.0001
preds = linear1(train_x)
((preds>0.0).float() == train_y).float().mean().item()
0.4912068545818329
As we’ve seen, we need gradients in order to improve our model using SGD, and in
order to calculate gradients we need a <i>loss</i> <i>function</i> that represents how good our
model is. That is because the gradients are a measure of how that loss function
changes with small tweaks to the weights.
So, we need to choose a loss function. The obvious approach would be to use accu‐
racy, which is our metric, as our loss function as well. In this case, we would calculate
our prediction for each image, collect these values to calculate an overall accuracy,
and then calculate the gradients of each weight with respect to that overall accuracy.
Unfortunately, we have a significant technical problem here. The gradient of a func‐
tion is its <i>slope,</i> or its steepness, which can be defined as <i>rise</i> <i>over</i> <i>run—that</i> is, how
much the value of the function goes up or down, divided by how much we changed
the input. We can write this mathematically as:
(y_new – y_old) / (x_new – x_old)
This gives a good approximation of the gradient when x_new is very similar to x_old,
meaning that their difference is very small. But accuracy changes at all only when a
prediction changes from a 3 to a 7, or vice versa. The problem is that a small change
in weights from x_old to x_new isn’t likely to cause any prediction to change, so
(y_new – y_old) will almost always be 0. In other words, the gradient is 0 almost
everywhere.
A very small change in the value of a weight will often not change the accuracy at all.
This means it is not useful to use accuracy as a loss function—if we do, most of the
time our gradients will be 0, and the model will not be able to learn from that
number.