We reached 94.3% accuracy, which was state-of-the-art performance just three years
ago. By training another model on all the texts read backward and averaging the pre‐
dictions of those two models, we can even get to 95.1% accuracy, which was the state
of the art introduced by the ULMFiT paper. It was beaten only a few months ago, by
fine-tuning a much bigger model and using expensive data augmentation techniques
(translating sentences in another language and back, using another model for
translation).
Using a pretrained model let us build a fine-tuned language model that is pretty pow‐
erful, to either generate fake reviews or help classify them. This is exciting stuff, but
it’s good to remember that this technology can also be used for malign purposes.
<header><largefont><b>Disinformation</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Models</b></largefont></header>
Even simple algorithms based on rules, before the days of widely available deep learn‐
ing language models, could be used to create fraudulent accounts and try to influence
policymakers. Jeff Kao, now a computational journalist at ProPublica, analyzed the
comments that were sent to the US Federal Communications Commission (FCC)
regarding a 2017 proposal to repeal net neutrality. In his article “More than a Million
Pro-Repeal Net Neutrality Comments Were Likely Faked”, he reports how he discov‐
ered a large cluster of comments opposing net neutrality that seemed to have been
generated by some sort of Mad Libs–style mail merge. In Figure 10-2, the fake com‐
ments have been helpfully color-coded by Kao to highlight their formulaic nature.
<i>Figure</i> <i>10-2.</i> <i>Comments</i> <i>received</i> <i>by</i> <i>the</i> <i>FCC</i> <i>during</i> <i>the</i> <i>net</i> <i>neutrality</i> <i>debate</i>