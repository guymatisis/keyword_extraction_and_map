<header><largefont><b>Stepping</b></largefont> <largefont><b>with</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Rate</b></largefont></header>
Deciding how to change our parameters based on the values of the gradients is an
important part of the deep learning process. Nearly all approaches start with the basic
idea of multiplying the gradient by some small number, called the <i>learning</i> <i>rate</i> (LR).
The learning rate is often a number between 0.001 and 0.1, although it could be any‐
thing. Often people select a learning rate just by trying a few, and finding which
results in the best model after training (we’ll show you a better approach later in this
book, called the <i>learning</i> <i>rate</i> <i>finder).</i> Once you’ve picked a learning rate, you can
adjust your parameters using this simple function:
w -= w.grad * lr
This is known as <i>stepping</i> your parameters, using an <i>optimization</i> <i>step.</i>
If you pick a learning rate that’s too low, it can mean having to do a lot of steps.
Figure 4-2 illustrates that.
<i>Figure</i> <i>4-2.</i> <i>Gradient</i> <i>descent</i> <i>with</i> <i>low</i> <i>LR</i>