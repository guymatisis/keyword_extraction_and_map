<header><largefont><b>CHAPTER</b></largefont> <largefont><b>10</b></largefont></header>
<header><largefont><b>NLP</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Dive:</b></largefont> <largefont><b>RNNs</b></largefont></header>
In Chapter 1, we saw that deep learning can be used to get great results with natural
language datasets. Our example relied on using a pretrained language model and
fine-tuning it to classify reviews. That example highlighted a difference between
transfer learning in NLP and computer vision: in general, in NLP the pretrained
model is trained on a different task.
What we call a <i>language</i> <i>model</i> is a model that has been trained to guess the next word
in a text (having read the ones before). This kind of task is called <i>self-supervised</i> <i>learn‐</i>
<i>ing:</i> we do not need to give labels to our model, just feed it lots and lots of texts. It has
a process to automatically get labels from the data, and this task isn’t trivial: to prop‐
erly guess the next word in a sentence, the model will have to develop an understand‐
ing of the English (or other) language. Self-supervised learning can also be used in
other domains; for instance, see “Self-Supervised Learning and Computer Vision” for
an introduction to vision applications. Self-supervised learning is not usually used for
the model that is trained directly, but instead is used for pretraining a model used for
transfer learning.
<b>Jargon:Self-SupervisedLearning</b>
Training a model using labels that are embedded in the independ‐
ent variable, rather than requiring external labels. For instance,
training a model to predict the next word in a text.
The language model we used in Chapter 1 to classify IMDb reviews was pretrained
on Wikipedia. We got great results by directly fine-tuning this language model to a
movie review classifier, but with one extra step, we can do even better. The Wikipedia
English is slightly different from the IMDb English, so instead of jumping directly to