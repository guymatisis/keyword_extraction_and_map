3. Why didn’t we have to care about normalization when using a pretrained model?
4. What is progressive resizing?
5. Implement progressive resizing in your own project. Did it help?
6. What is test time augmentation? How do you use it in fastai?
7. Is using TTA at inference slower or faster than regular inference? Why?
8. What is Mixup? How do you use it in fastai?
9. Why does Mixup prevent the model from being too confident?
10. Why does training with Mixup for five epochs end up worse than training
without Mixup?
11. What is the idea behind label smoothing?
12. What problems in your data can label smoothing help with?
13. When using label smoothing with five categories, what is the target associated
with the index 1?
14. What is the first step to take when you want to prototype quick experiments on a
new dataset?
<header><largefont><b>Further</b></largefont> <largefont><b>Research</b></largefont></header>
1. Use the fastai documentation to build a function that crops an image to a square
in each of the four corners; then implement a TTA method that averages the pre‐
dictions on a center crop and those four crops. Did it help? Is it better than the
TTA method of fastai?
2. Find the Mixup paper on arXiv and read it. Pick one or two more recent articles
introducing variants of Mixup and read them; then try to implement them on
your problem.
3. Find the script training Imagenette using Mixup and use it as an example to
build a script for a long training on your own project. Execute it and see if it
helps.
4. Read the sidebar “Label Smoothing, the Paper” on page 250; then look at the rele‐
vant section of the original paper and see if you can follow it. Don’t be afraid to
ask for help!