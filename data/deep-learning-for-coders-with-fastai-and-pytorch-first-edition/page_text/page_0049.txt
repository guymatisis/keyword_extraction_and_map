To avoid this, our first step was to split our dataset into two sets: the <i>training</i> <i>set</i>
(which our model sees in training) and the <i>validation</i> <i>set,</i> also known as the <i>develop‐</i>
<i>ment</i> <i>set</i> (which is used only for evaluation). This lets us test that the model learns
lessons from the training data that generalize to new data, the validation data.
One way to understand this situation is that, in a sense, we don’t want our model to
get good results by “cheating.” If it makes an accurate prediction for a data item, that
should be because it has learned characteristics of that kind of item, and not because
the model has been shaped by <i>actually</i> <i>having</i> <i>seen</i> <i>that</i> <i>particular</i> <i>item.</i>
Splitting off our validation data means our model never sees it in training and so is
completely untainted by it, and is not cheating in any way. Right?
In fact, not necessarily. The situation is more subtle. This is because in realistic sce‐
narios we rarely build a model just by training its parameters once. Instead, we are
likely to explore many versions of a model through various modeling choices regard‐
ing network architecture, learning rates, data augmentation strategies, and other fac‐
tors we will discuss in upcoming chapters. Many of these choices can be described as
choices of <i>hyperparameters.</i> The word reflects that they are parameters about parame‐
ters, since they are the higher-level choices that govern the meaning of the weight
parameters.
The problem is that even though the ordinary training process is looking at only pre‐
dictions on the training data when it learns values for the weight parameters, the
same is not true of us. We, as modelers, are evaluating the model by looking at pre‐
dictions on the validation data when we decide to explore new hyperparameter val‐
ues! So subsequent versions of the model are, indirectly, shaped by us having seen the
validation data. Just as the automatic training process is in danger of overfitting the
training data, we are in danger of overfitting the validation data through human trial
and error and exploration.
The solution to this conundrum is to introduce another level of even more highly
reserved data: the <i>test</i> <i>set.</i> Just as we hold back the validation data from the training
process, we must hold back the test set data even from ourselves. It cannot be used to
improve the model; it can be used only to evaluate the model at the very end of our
efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want
to hide it from training and modeling processes: training data is fully exposed, the
validation data is less exposed, and test data is totally hidden. This hierarchy parallels
the different kinds of modeling and evaluation processes themselves—the automatic
training process with backpropagation, the more manual process of trying different
hyperparameters between training sessions, and the assessment of our final result.
The test and validation sets should have enough data to ensure that you get a good
estimate of your accuracy. If you’re creating a cat detector, for instance, you generally
want at least 30 cats in your validation set. That means that if you have a dataset with