We now have 90 tokens, separated by spaces. Let’s say we want a batch size of 6. We
need to break this text into 6 contiguous parts of length 15:
xxbos xxmaj in this chapter , we will go back over the example of classifying
movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj
first we will look at the processing steps necessary to convert text into numbers and
how to customize it . xxmaj by doing this , we ‘ll have another example
of the preprocessor used in the data block xxup api . \n xxmaj then we
will study how we build a language model and train it for a while .
In a perfect world, we could then give this one batch to our model. But that approach
doesn’t scale, because outside this toy example, it’s unlikely that a single batch con‐
taining all the tokens would fit in our GPU memory (here we have 90 tokens, but all
the IMDb reviews together give several million).
So, we need to divide this array more finely into subarrays of a fixed sequence length.
It is important to maintain order within and across these subarrays, because we will
use a model that maintains a state so that it remembers what it read previously when
predicting what comes next.
Going back to our previous example with 6 batches of length 15, if we chose a
sequence length of 5, that would mean we first feed the following array:
xxbos xxmaj in this chapter
movie reviews we studied in
first we will look at
how to customize it .
of the preprocessor used in
will study how we build
Then, this one:
, we will go back
chapter 1 and dig deeper
the processing steps necessary to
xxmaj by doing this ,
the data block xxup api
a language model and train