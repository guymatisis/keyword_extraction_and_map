Let’s see if the loss has improved:
preds = f(time,params)
mse(preds, speed)
tensor(5435.5366, grad_fn=<MeanBackward0>)
And take a look at the plot:
show_preds(preds)
We need to repeat this a few times, so we’ll create a function to apply one step:
<b>def</b> apply_step(params, prn=True):
preds = f(time, params)
loss = mse(preds, speed)
loss.backward()
params.data -= lr * params.grad.data
params.grad = None
<b>if</b> prn: <b>print(loss.item())</b>
<b>return</b> preds
<b>Step6:Repeattheprocess</b>
Now we iterate. By looping and performing many improvements, we hope to reach a
good result:
<b>for</b> i <b>in</b> range(10): apply_step(params)
5435.53662109375
1577.4495849609375
847.3780517578125
709.22265625
683.0757446289062
678.12451171875
677.1839599609375
677.0025024414062
676.96435546875
676.9537353515625