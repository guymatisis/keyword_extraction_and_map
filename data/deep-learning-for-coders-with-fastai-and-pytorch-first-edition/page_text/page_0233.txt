Let’s take a look at our first image:
im = PILImage.create(img_files[0])
im.shape
(480, 640)
im.to_thumb(160)
The Biwi dataset website used to explain the format of the pose text file associated
with each image, which shows the location of the center of the head. The details of
this aren’t important for our purposes, so we’ll just show the function we use to
extract the head center point:
cal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)
<b>def</b> get_ctr(f):
ctr = np.genfromtxt(img2pose(f), skip_header=3)
c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]
c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]
<b>return</b> tensor([c1,c2])
This function returns the coordinates as a tensor of two items:
get_ctr(img_files[0])
tensor([384.6370, 259.4787])
We can pass this function to DataBlock as get_y, since it is responsible for labeling
each item. We’ll resize the images to half their input size, to speed up training a bit.
One important point to note is that we should not just use a random splitter. The
same people appear in multiple images in this dataset, but we want to ensure that our
model can generalize to people that it hasn’t seen yet. Each folder in the dataset con‐
tains the images for one person. Therefore, we can create a splitter function that
returns True for just one person, resulting in a validation set containing just that per‐
son’s images.
The only other difference from the previous data block examples is that the second
block is a PointBlock. This is necessary so that fastai knows that the labels represent
coordinates; that way, it knows that when doing data augmentation, it should do the
same augmentation to these coordinates as it does to the images: