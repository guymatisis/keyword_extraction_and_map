And finally:
over the example of classifying
under the surface . xxmaj
convert text into numbers and
we ‘ll have another example
. \n xxmaj then we
it for a while .
Going back to our movie reviews dataset, the first step is to transform the individual
texts into a stream by concatenating them together. As with images, it’s best to ran‐
domize the order of the inputs, so at the beginning of each epoch we will shuffle the
entries to make a new stream (we shuffle the order of the documents, not the order of
the words inside them, or the texts would not make sense anymore!).
We then cut this stream into a certain number of batches (which is our <i>batch</i> <i>size).</i>
For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will
give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the
order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to
10,000…), because we want the model to read continuous rows of text (as in the pre‐
ceding example). An xxbos token is added at the start of each text during preprocess‐
ing, so that the model knows when it reads the stream when a new entry is beginning.
So to recap, at every epoch we shuffle our collection of documents and concatenate
them into a stream of tokens. We then cut that stream into a batch of fixed-size con‐
secutive mini-streams. Our model will then read the mini-streams in order, and
thanks to an inner state, it will produce the same activation, whatever sequence
length we picked.
This is all done behind the scenes by the fastai library when we create an
LMDataLoader. We do this by first applying our Numericalize object to the tokenized
texts
nums200 = toks200.map(num)
and then passing that to LMDataLoader:
dl = LMDataLoader(nums200)
Let’s confirm that this gives the expected results, by grabbing the first batch
x,y = first(dl)
x.shape,y.shape
(torch.Size([64, 72]), torch.Size([64, 72]))