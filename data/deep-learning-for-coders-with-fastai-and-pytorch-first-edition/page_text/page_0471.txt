<header><largefont><b>CHAPTER</b></largefont> <largefont><b>16</b></largefont></header>
<header><largefont><b>The</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Process</b></largefont></header>
You now know how to create state-of-the-art architectures for computer vision, natu‐
ral image processing, tabular analysis, and collaborative filtering, and you know how
to train them quickly. So we’re done, right? Not quite yet. We still have to explore a
little bit more of the training process.
We explained in Chapter 4 the basis of stochastic gradient descent: pass a mini-batch
to the model, compare it to our target with the loss function, then compute the gradi‐
ents of this loss function with regard to each weight before updating the weights with
the formula:
new_weight = weight - lr * weight.grad
We implemented this from scratch in a training loop, and saw that PyTorch provides
nn.SGD
a simple class that does this calculation for each parameter for us. In this
chapter, we will build some faster optimizers, using a flexible foundation. But that’s
not all we might want to change in the training process. For any tweak of the training
loop, we will need a way to add some code to the basis of SGD. The fastai library has a
system of callbacks to do this, and we will teach you all about it.
Let’s start with standard SGD to get a baseline; then we will introduce the most com‐
monly used optimizers.
<header><largefont><b>Establishing</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Baseline</b></largefont></header>
First we’ll create a baseline using plain SGD and compare it to fastai’s default opti‐
mizer. We’ll start by grabbing Imagenette with the same get_data we used in
Chapter 14:
dls = get_data(URLs.IMAGENETTE_160, 160, 128)