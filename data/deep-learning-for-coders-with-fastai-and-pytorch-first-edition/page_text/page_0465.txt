Then we can define our Learner by passing the data, model, loss function, splitter,
and any metric we want. Since we are not using a convenience function from fastai
for transfer learning (like cnn_learner ), we have to call learn.freeze manually. This
will make sure only the last parameter group (in this case, the head) is trained:
learn = Learner(dls, model, loss_func=loss_func,
splitter=siamese_splitter, metrics=accuracy)
learn.freeze()
Then we can directly train our model with the usual method:
learn.fit_one_cycle(4, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.367015 0.281242 0.885656 00:26
1 0.307688 0.214721 0.915426 00:26
2 0.275221 0.170615 0.936401 00:26
3 0.223771 0.159633 0.943843 00:26
Now we unfreeze and fine-tune the whole model a bit more with discriminative
learning rates (that is, a lower learning rate for the body and a higher one for the
head):
learn.unfreeze()
learn.fit_one_cycle(4, slice(1e-6,1e-4))
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.212744 0.159033 0.944520 00:35
1 0.201893 0.159615 0.942490 00:35
2 0.204606 0.152338 0.945196 00:36
3 0.213203 0.148346 0.947903 00:36
94.8% is very good when we remember that a classifier trained the same way (with no
data augmentation) had an error rate of 7%.
Now that we’ve seen how to create complete state-of-the-art computer vision models,
let’s move on to NLP.
<header><largefont><b>Natural</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Processing</b></largefont></header>
Converting an AWD-LSTM language model into a transfer learning classifier, as we
did in Chapter 10, follows a very similar process to what we did with cnn_learner in
the first section of this chapter. We do not need a “meta” dictionary in this case,
because we do not have such a variety of architectures to support in the body. All we
need to do is select the stacked RNN for the encoder in the language model, which is