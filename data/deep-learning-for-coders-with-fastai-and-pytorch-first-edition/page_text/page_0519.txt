<header><largefont><b>CHAPTER</b></largefont> <largefont><b>18</b></largefont></header>
<header><largefont><b>CNN</b></largefont> <largefont><b>Interpretation</b></largefont> <largefont><b>with</b></largefont> <largefont><b>CAM</b></largefont></header>
Now that we know how to build up pretty much anything from scratch, let’s use that
knowledge to create entirely new (and very useful!) functionality: the <i>class</i> <i>activation</i>
<i>map.</i> It gives a us some insight into why a CNN made the predictions it did.
In the process, we’ll learn about one handy feature of PyTorch we haven’t seen before,
the <i>hook,</i> and we’ll apply many of the concepts introduced in the rest of the book. If
you want to really test out your understanding of the material in this book, after
you’ve finished this chapter, try putting it aside and re-creating the ideas here yourself
from scratch (no peeking!).
<header><largefont><b>CAM</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Hooks</b></largefont></header>
The <i>class</i> <i>activation</i> <i>map</i> (CAM) was introduced by Bolei Zhou et al. in “Learning
Deep Features for Discriminative Localization”. It uses the output of the last convolu‐
tional layer (just before the average pooling layer) together with the predictions to
give us a heatmap visualization of why the model made its decision. This is a useful
tool for interpretation.
More precisely, at each position of our final convolutional layer, we have as many fil‐
ters as in the last linear layer. We can therefore compute the dot product of those acti‐
vations with the final weights to get, for each location on our feature map, the score of
the feature that was used to make a decision.
We’re going to need a way to get access to the activations inside the model while it’s
training. In PyTorch, this can be done with a <i>hook.</i> Hooks are PyTorch’s equivalent of
fastai’s callbacks. However, rather than allowing you to inject code into the training
loop like a fastai Learner callback, hooks allow you to inject code into the forward
and backward calculations themselves. We can attach a hook to any layer of the
model, and it will be executed when we compute the outputs (forward hook) or