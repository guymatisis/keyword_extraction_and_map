doing everything from scratch ourselves (except for calculating the gradients), you
know that there is no special magic hiding behind the scenes.
<header><largefont><b>Going</b></largefont> <largefont><b>Deeper</b></largefont></header>
There is no need to stop at just two linear layers. We can add as many as we want, as
long as we add a nonlinearity between each pair of linear layers. As you will learn,
however, the deeper the model gets, the harder it is to optimize the parameters in
practice. Later in this book, you will learn about some simple but brilliantly effective
techniques for training deeper models.
We already know that a single nonlinearity with two linear layers is enough to
approximate any function. So why would we use deeper models? The reason is per‐
formance. With a deeper model (one with more layers), we do not need to use as
many parameters; it turns out that we can use smaller matrices, with more layers, and
get better results than we would get with larger matrices and few layers.
That means that we can train the model more quickly, and it will take up less mem‐
ory. In the 1990s, researchers were so focused on the universal approximation theo‐
rem that few were experimenting with more than one nonlinearity. This theoretical
but not practical foundation held back the field for years. Some researchers, however,
did experiment with deep models, and eventually were able to show that these models
could perform much better in practice. Eventually, theoretical results were developed
that showed why this happens. Today, it is extremely unusual to find anybody using a
neural network with just one nonlinearity.
Here is what happens when we train an 18-layer model using the same approach we
saw in Chapter 1:
dls = ImageDataLoaders.from_folder(path)
learn = cnn_learner(dls, resnet18, pretrained=False,
loss_func=F.cross_entropy, metrics=accuracy)
learn.fit_one_cycle(1, 0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.082089 0.009578 0.997056 00:11
Nearly 100% accuracy! That’s a big difference compared to our simple neural net. But
as you’ll learn in the remainder of this book, there are just a few little tricks you need
to use to get such great results from scratch yourself. You already know the key foun‐
dational pieces. (Of course, even when you know all the tricks, you’ll nearly always
want to work with the prebuilt classes provided by PyTorch and fastai, because they
save you from having to think about all the little details yourself.)