16. Why do we need a custom loss function for LMModel4 ?
17. Why is the training of LMModel4 unstable?
18. In the unrolled representation, we can see that a recurrent neural network has
many layers. So why do we need to stack RNNs to get better results?
19. Draw a representation of a stacked (multilayer) RNN.
20. Why should we get better results in an RNN if we call detach less often? Why
might this not happen in practice with a simple RNN?
21. Why can a deep network result in very large or very small activations? Why does
this matter?
22. In a computer’s floating-point representation of numbers, which numbers are the
most precise?
23. Why do vanishing gradients prevent training?
24. Why does it help to have two hidden states in the LSTM architecture? What is
the purpose of each one?
25. What are these two states called in an LSTM?
26. What is tanh, and how is it related to sigmoid?
27. What is the purpose of this code in LSTMCell:
h = torch.stack([h, input], dim=1)
28. What does chunk do in PyTorch?
29. Study the refactored version of LSTMCell carefully to ensure you understand how
and why it does the same thing as the nonrefactored version.
30. Why can we use a higher learning rate for LMModel6?
31. What are the three regularization techniques used in an AWD-LSTM model?
32. What is dropout?
33. Why do we scale the weights with dropout? Is this applied during training, infer‐
ence, or both?
34. What is the purpose of this line from Dropout :
<b>if</b> <b>not</b> self.training: <b>return</b> x
35. Experiment with bernoulli_ to understand how it works.
36. How do you set your model in training mode in PyTorch? In evaluation mode?
37. Write the equation for activation regularization (in math or code, as you prefer).
How is it different from weight decay?
38. Write the equation for temporal activation regularization (in math or code, as
you prefer). Why wouldn’t we use this for computer vision problems?