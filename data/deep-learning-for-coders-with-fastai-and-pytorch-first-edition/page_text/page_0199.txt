Just as we moved from sigmoid to softmax, we need to extend the loss function to
work with more than just binary classification—it needs to be able to classify any
number of categories (in this case, we have 37 categories). Our activations, after soft‐
max, are between 0 and 1, and sum to 1 for each row in the batch of predictions. Our
targets are integers between 0 and 36.
In the binary case, we used torch.where to select between inputs and 1-inputs.
When we treat a binary classification as a general classification problem with two cat‐
egories, it becomes even easier, because (as we saw in the previous section) we now
have two columns containing the equivalent of inputs and 1-inputs. So, all we need
to do is select from the appropriate column. Let’s try to implement this in PyTorch.
For our synthetic 3s and 7s example, let’s say these are our labels:
targ = tensor([0,1,0,1,1,0])
And these are the softmax activations:
sm_acts
tensor([[0.6025, 0.3975],
[0.5021, 0.4979],
[0.1332, 0.8668],
[0.9966, 0.0034],
[0.5959, 0.4041],
[0.3661, 0.6339]])
Then for each item of targ , we can use that to select the appropriate column of
sm_acts using tensor indexing, like so:
idx = range(6)
sm_acts[idx, targ]
tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])
To see exactly what’s happening here, let’s put all the columns together in a table.
Here, the first two columns are our activations, then we have the targets, the row
index, and finally the result shown in the preceding code:
<b>3</b> <b>7</b> <b>targ</b> <b>idx</b> <b>loss</b>
0.602469 0.397531 0 0 0.602469
0.502065 0.497935 1 1 0.497935
0.133188 0.866811 0 2 0.133188
0.99664 0.00336017 1 3 0.00336017
0.595949 0.404051 1 4 0.404051
0.366118 0.633882 0 5 0.366118
Looking at this table, you can see that the final column can be calculated by taking the
targ and idx columns as indices into the two-column matrix containing the 3 and 7
columns. That’s what sm_acts[idx, targ] is doing.