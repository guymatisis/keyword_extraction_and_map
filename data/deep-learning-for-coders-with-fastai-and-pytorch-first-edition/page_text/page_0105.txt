There are positive examples of people and organizations attempting to combat these
problems. Evan Estola, lead machine learning engineer at Meetup, discussed the
example of men expressing more interest than women in tech meetups. Taking gen‐
der into account could therefore cause Meetup’s algorithm to recommend fewer tech
meetups to women, and as a result, fewer women would find out about and attend
tech meetups, which could cause the algorithm to suggest even fewer tech meetups to
women, and so on in a self-reinforcing feedback loop. So, Evan and his team made
the ethical decision for their recommendation algorithm to not create such a feed‐
back loop, by explicitly not using gender for that part of their model. It is encouraging
to see a company not just unthinkingly optimize a metric, but consider its impact.
According to Evan, “You need to decide which feature not to use in your algorithm…
the most optimal algorithm is perhaps not the best one to launch into production.”
While Meetup chose to avoid such an outcome, Facebook provides an example of
allowing a runaway feedback loop to run wild. Like YouTube, it tends to radicalize
users interested in one conspiracy theory by introducing them to more. As Renee
DiResta, a researcher on proliferation of disinformation, writes:
Once people join a single conspiracy-minded [Facebook] group, they are algorithmi‐
cally routed to a plethora of others. Join an anti-vaccine group, and your suggestions
will include anti-GMO, chemtrail watch, flat Earther (yes, really), and “curing cancer
naturally” groups. Rather than pulling a user out of the rabbit hole, the recommenda‐
tion engine pushes them further in.
It is extremely important to keep in mind that this kind of behavior can happen, and
to either anticipate a feedback loop or take positive action to break it when you see
the first signs of it in your own projects. Another thing to keep in mind is <i>bias,</i> which,
as we discussed briefly in the previous chapter, can interact with feedback loops in
very troublesome ways.
<header><largefont><b>Bias</b></largefont></header>
Discussions of bias online tend to get pretty confusing pretty fast. The word “bias”
means so many different things. Statisticians often think when data ethicists are talk‐
ing about bias that they’re talking about the statistical definition of the term bias—but
they’re not. And they’re certainly not talking about the biases that appear in the
weights and biases that are the parameters of your model!
What they’re talking about is the social science concept of bias. In “A Framework for
Understanding Unintended Consequences of Machine Learning” MIT’s Harini
Suresh and John Guttag describe six types of bias in machine learning, summarized
in Figure 3-6.