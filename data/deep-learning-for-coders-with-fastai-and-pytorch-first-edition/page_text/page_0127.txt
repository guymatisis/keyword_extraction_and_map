2. How can working with people of different backgrounds help when considering
ethical questions?
3. What was the role of IBM in Nazi Germany? Why did the company participate as
it did? Why did the workers participate?
4. What was the role of the first person jailed in the Volkswagen diesel scandal?
5. What was the problem with a database of suspected gang members maintained
by California law enforcement officials?
6. Why did YouTube’s recommendation algorithm recommend videos of partially
clothed children to pedophiles, even though no employee at Google had pro‐
grammed this feature?
7. What are the problems with the centrality of metrics?
8. Why did Meetup.com not include gender in its recommendation system for tech
meetups?
9. What are the six types of bias in machine learning, according to Suresh and
Guttag?
10. Give two examples of historical race bias in the US.
11. Where are most images in ImageNet from?
12. In the paper “Does Machine Learning Automate Moral Hazard and Error?” why
is sinusitis found to be predictive of a stroke?
13. What is representation bias?
14. How are machines and people different, in terms of their use for making
decisions?
15. Is disinformation the same as “fake news”?
16. Why is disinformation through autogenerated text a particularly significant
issue?
17. What are the five ethical lenses described by the Markkula Center?
18. Where is policy an appropriate tool for addressing data ethics issues?
<header><largefont><b>Further</b></largefont> <largefont><b>Research</b></largefont></header>
1. Read the article “What Happens When an Algorithm Cuts Your Healthcare”.
How could problems like this be avoided in the future?
2. Research to find out more about YouTube’s recommendation system and its soci‐
etal impacts. Do you think recommendation systems must always have feedback
loops with negative results? What approaches could Google take to avoid them?
What about the government?