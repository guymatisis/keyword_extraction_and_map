Note that the input of the model is a tensor of shape batch_size x 2 , where the first
column (x[:, 0]) contains the user IDs, and the second column (x[:, 1]) contains
the movie IDs. As explained before, we use the <i>embedding</i> layers to represent our
matrices of user and movie latent factors:
x,y = dls.one_batch()
x.shape
torch.Size([64, 2])
Now that we have defined our architecture and created our parameter matrices, we
need to create a Learner to optimize our model. In the past, we have used special
functions, such as cnn_learner , which set up everything for us for a particular appli‐
cation. Since we are doing things from scratch here, we will use the plain Learner
class:
model = DotProduct(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
We are now ready to fit our model:
learn.fit_one_cycle(5, 5e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 1.326261 1.295701 00:12
1 1.091352 1.091475 00:11
2 0.961574 0.977690 00:11
3 0.829995 0.893122 00:11
4 0.781661 0.876511 00:12
The first thing we can do to make this model a little bit better is to force those predic‐
tions to be between 0 and 5. For this, we just need to use sigmoid_range, as in Chap‐
ter 6. One thing we discovered empirically is that it’s better to have the range go a
little bit over 5, so we use (0, 5.5):
<b>class</b> <b>DotProduct(Module):</b>
<b>def</b> <b>__init__(self,</b> n_users, n_movies, n_factors, y_range=(0,5.5)):
self.user_factors = Embedding(n_users, n_factors)
self.movie_factors = Embedding(n_movies, n_factors)
self.y_range = y_range
<b>def</b> forward(self, x):
users = self.user_factors(x[:,0])
movies = self.movie_factors(x[:,1])
<b>return</b> sigmoid_range((users * movies).sum(dim=1), *self.y_range)
model = DotProduct(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3)