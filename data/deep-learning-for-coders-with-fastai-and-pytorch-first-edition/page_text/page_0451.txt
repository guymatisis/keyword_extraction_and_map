experiments, or even just to see an unexpected result, say, “Hmmm, that’s interesting,”
and then, most importantly, set about figuring out what on earth is going on, with
great tenacity, is at the heart of many scientific discoveries. Deep learning is not like
pure mathematics. It is a heavily experimental field, so it’s important to be a strong
practitioner, not just a theoretician.
Since the ResNet was introduced, it’s been widely studied and applied to many
domains. One of the most interesting papers, published in 2018, is “Visualizing the
Loss Landscape of Neural Nets” by Hao Li et al. It shows that using skip connections
helps smooth the loss function, which makes training easier as it avoids falling into a
very sharp area. Figure 14-3 shows a stunning picture from the paper, illustrating the
difference between the bumpy terrain that SGD has to navigate to optimize a regular
CNN (left) versus the smooth surface of a ResNet (right).
<i>Figure</i> <i>14-3.</i> <i>Impact</i> <i>of</i> <i>ResNet</i> <i>on</i> <i>loss</i> <i>landscape</i> <i>(courtesy</i> <i>of</i> <i>Hao</i> <i>Li</i> <i>et</i> <i>al.)</i>
Our first model is already good, but further research has discovered more tricks we
can apply to make it better. We’ll look at those next.
<header><largefont><b>A</b></largefont> <largefont><b>State-of-the-Art</b></largefont> <largefont><b>ResNet</b></largefont></header>
In “Bag of Tricks for Image Classification with Convolutional Neural Networks”,
Tong He et al. study variations of the ResNet architecture that come at almost no
additional cost in terms of number of parameters or computation. By using a tweaked
ResNet-50 architecture and Mixup, they achieved 94.6% top-5 accuracy on ImageNet,
in comparison to 92.2% with a regular ResNet-50 without Mixup. This result is better
than that achieved by regular ResNet models that are twice as deep (and twice as slow,
and much more likely to overfit).