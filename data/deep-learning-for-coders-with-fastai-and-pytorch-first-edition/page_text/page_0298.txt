<header><largefont><b>Random</b></largefont> <largefont><b>Forests</b></largefont></header>
In 1994, Berkeley professor Leo Breiman, one year after his retirement, published a
small technical report called “Bagging Predictors”, which turned out to be one of the
most influential ideas in modern machine learning. The report began:
Bagging predictors is a method for generating multiple versions of a predictor and
using these to get an aggregated predictor. The aggregation averages over the ver‐
sions…The multiple versions are formed by making bootstrap replicates of the learn‐
ing set and using these as new learning sets. Tests…show that bagging can give
substantial gains in accuracy. The vital element is the instability of the prediction
method. If perturbing the learning set can cause significant changes in the predictor
constructed, then bagging can improve accuracy.
Here is the procedure that Breiman is proposing:
1. Randomly choose a subset of the rows of your data (i.e., “bootstrap replicates of
your learning set”).
2. Train a model using this subset.
3. Save that model, and then return to step 1 a few times.
4. This will give you multiple trained models. To make a prediction, predict using
all of the models, and then take the average of each of those model’s predictions.
This procedure is known as <i>bagging.</i> It is based on a deep and important insight:
although each of the models trained on a subset of data will make more errors than a
model trained on the full dataset, those errors will not be correlated with each other.
Different models will make different errors. The average of those errors, therefore, is
zero! So if we take the average of all of the models’ predictions, we should end up
with a prediction that gets closer and closer to the correct answer, the more models
we have. This is an extraordinary result—it means that we can improve the accuracy
of nearly any kind of machine learning algorithm by training it multiple times, each
time on a different random subset of the data, and averaging its predictions.
In 2001, Breiman went on to demonstrate that this approach to building models,
when applied to decision tree building algorithms, was particularly powerful. He
went even further than just randomly choosing rows for each model’s training, but
also randomly selected from a subset of columns when choosing each split in each
decision tree. He called this method the <i>random</i> <i>forest.</i> Today it is, perhaps, the most
widely used and practically important machine learning method.
In essence, a random forest is a model that averages the predictions of a large number
of decision trees, which are generated by randomly varying various parameters that
specify what data is used to train the tree and other tree parameters. Bagging is a
particular approach to <i>ensembling,</i> or combining the results of multiple models