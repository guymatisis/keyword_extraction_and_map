To tell Module that we want to treat a tensor as a parameter, we have to wrap it in the
nn.Parameter class. This class doesn’t add any functionality (other than automatically
calling requires_grad_ for us). It’s used only as a “marker” to show what to include
in parameters :
<b>class</b> <b>T(Module):</b>
<b>def</b> <b>__init__(self):</b> self.a = nn.Parameter(torch.ones(3))
L(T().parameters())
(#1) [Parameter containing:
tensor([1., 1., 1.], requires_grad=True)]
All PyTorch modules use nn.Parameter for any trainable parameters, which is why
we haven’t needed to explicitly use this wrapper until now:
<b>class</b> <b>T(Module):</b>
<b>def</b> <b>__init__(self):</b> self.a = nn.Linear(1, 3, bias=False)
t = T()
L(t.parameters())
(#1) [Parameter containing:
tensor([[-0.9595],
[-0.8490],
[ 0.8159]], requires_grad=True)]
type(t.a.weight)
torch.nn.parameter.Parameter
We can create a tensor as a parameter, with random initialization, like so:
<b>def</b> create_params(size):
<b>return</b> nn.Parameter(torch.zeros(*size).normal_(0, 0.01))
Let’s use this to create DotProductBias again, but without Embedding:
<b>class</b> <b>DotProductBias</b> (Module):
<b>def</b> <b>__init__(self,</b> n_users, n_movies, n_factors, y_range=(0,5.5)):
self.user_factors = create_params([n_users, n_factors])
self.user_bias = create_params([n_users])
self.movie_factors = create_params([n_movies, n_factors])
self.movie_bias = create_params([n_movies])
self.y_range = y_range
<b>def</b> forward(self, x):
users = self.user_factors[x[:,0]]
movies = self.movie_factors[x[:,1]]
res = (users*movies).sum(dim=1)
res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]
<b>return</b> sigmoid_range(res, *self.y_range)
Then let’s train it again to check we get around the same results we saw in the previ‐
ous section: