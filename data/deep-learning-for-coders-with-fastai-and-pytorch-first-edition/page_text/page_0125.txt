right, yet there are societal impacts to widespread surveillance (which would still be
the case even if it was possible for a few individuals to opt out).
Many of the issues we are seeing in tech are human rights issues, such as when a
biased algorithm recommends that Black defendants have longer prison sentences,
when particular job ads are shown only to young people, or when police use facial
recognition to identify protesters. The appropriate venue to address human rights
issues is typically through the law.
We need both regulatory and legal changes, <i>and</i> the ethical behavior of individuals.
Individual behavior change can’t address misaligned profit incentives, externalities
(where corporations reap large profits while offloading their costs and harms to the
broader society), or systemic failures. However, the law will never cover all edge cases,
and it is important that individual software developers and data scientists are equip‐
ped to make ethical decisions in practice.
<header><largefont><b>Cars:</b></largefont> <largefont><b>A</b></largefont> <largefont><b>Historical</b></largefont> <largefont><b>Precedent</b></largefont></header>
The problems we are facing are complex, and there are no simple solutions. This can
be discouraging, but we find hope in considering other large challenges that people
have tackled throughout history. One example is the movement to increase car safety,
covered as a case study in “Datasheets for Datasets” by Timnit Gebru et al. and in the
design podcast 99% Invisible. Early cars had no seatbelts, metal knobs on the dash‐
board that could lodge in people’s skulls during a crash, regular plate glass windows
that shattered in dangerous ways, and noncollapsible steering columns that impaled
drivers. However, car companies were resistant to even discussing safety as something
they could help address, and the widespread belief was that cars are just the way they
are, and that it was the people using them who caused problems.
It took consumer safety activists and advocates decades of work to change the
national conversation to consider that perhaps car companies had some responsibility
that should be addressed through regulation. When the collapsible steering column
was invented, it was not implemented for several years as there was no financial
incentive to do so. Major car company General Motors hired private detectives to try
to dig up dirt on consumer safety advocate Ralph Nader. The requirement of seat‐
belts, crash test dummies, and collapsible steering columns were major victories. It
was only in 2011 that car companies were required to start using crash test dummies
that would represent the average woman, and not just average men’s bodies; prior to
this, women were 40% more likely to be injured in a car crash of the same impact
compared to a man. This is a vivid example of the ways that bias, policy, and technol‐
ogy have important consequences.