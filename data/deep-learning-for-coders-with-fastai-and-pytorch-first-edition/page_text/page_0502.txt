In our earlier examples with a 3×3 matrix and a vector of size 3, broadcasting was
done on the rows:
Matrix (2d tensor): 3 x 3
Vector (1d tensor): (1) 3
Result (2d tensor): 3 x 3
As an exercise, try to determine what dimensions to add (and where) when you need
to normalize a batch of images of size 64 x 3 x 256 x 256 with vectors of three
elements (one for the mean and one for the standard deviation).
Another useful way of simplifying tensor manipulations is the use of Einstein sum‐
mation convention.
<header><largefont><b>Einstein</b></largefont> <largefont><b>Summation</b></largefont></header>
Before using the PyTorch operation @ or torch.matmul, there is one last way we can
(einsum).
implement matrix multiplication: <i>Einstein</i> <i>summation</i> This is a compact
representation for combining products and sums in a general way. We write an equa‐
tion like this:
ik,kj -> ij
The lefthand side represents the operands dimensions, separated by commas. Here
we have two tensors that each have two dimensions (i,k and k,j). The righthand
side represents the result dimensions, so here we have a tensor with two dimensions
i,j.
The rules of Einstein summation notation are as follows:
1. Repeated indices are implicitly summed over.
2. Each index can appear at most twice in any term.
3. Each term must contain identical nonrepeated indices.
So in our example, since k is repeated, we sum over that index. In the end, the for‐
(i,j)
mula represents the matrix obtained when we put in the sum of all the coeffi‐
cients (i,k) in the first tensor multiplied by the coefficients (k,j) in the second
tensor… which is the matrix product!
Here is how we can code this in PyTorch:
<b>def</b> matmul(a,b): <b>return</b> torch.einsum('ik,kj->ij', a, b)
Einstein summation is a very practical way of expressing operations involving index‐
ing and sum of products. Note that you can have one member on the lefthand side.
For instance,
torch.einsum('ij->ji', a)