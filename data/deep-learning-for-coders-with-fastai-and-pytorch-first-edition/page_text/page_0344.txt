pretrained model are merged with random embeddings added for words that weren’t
in the pretraining vocabulary. This is handled automatically inside
language_model_learner:
learn = language_model_learner(
dls_lm, AWD_LSTM, drop_mult=0.3,
metrics=[accuracy, Perplexity()]).to_fp16()
The loss function used by default is cross-entropy loss, since we essentially have a
classification problem (the different categories being the words in our vocab). The
<i>perplexity</i> metric used here is often used in NLP for language models: it is the expo‐
nential of the loss (i.e., torch.exp(cross_entropy)). We also include the accuracy
metric to see how many times our model is right when trying to predict the next
word, since cross entropy (as we’ve seen) is both hard to interpret and tells us more
about the model’s confidence than its accuracy.
Let’s go back to the process diagram from the beginning of this chapter. The first
arrow has been completed for us and made available as a pretrained model in fastai,
and we’ve just built the DataLoaders and Learner for the second stage. Now we’re
ready to fine-tune our language model!
It takes quite a while to train each epoch, so we’ll be saving the intermediate model
results during the training process. Since fine_tune doesn’t do that for us, we’ll use
fit_one_cycle. Just like cnn_learner, language_model_learner automatically calls
freeze
when using a pretrained model (which is the default), so this will train only
the embeddings (the only part of the model that contains randomly initialized
weights—i.e., embeddings for words that are in our IMDb vocab, but aren’t in the
pretrained model vocab):
learn.fit_one_cycle(1, 2e-2)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>perplexity</b> <b>time</b>
0 4.120048 3.912788 0.299565 50.038246 11:39
This model takes a while to train, so it’s a good opportunity to talk about saving inter‐
mediary results.