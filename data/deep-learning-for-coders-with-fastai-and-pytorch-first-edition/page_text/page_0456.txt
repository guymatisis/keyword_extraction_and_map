<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
10 0.593401 1.180786 0.676433 00:32
11 0.536634 0.879937 0.713885 00:32
12 0.479208 0.798356 0.741656 00:32
13 0.440071 0.600644 0.806879 00:32
14 0.402952 0.450296 0.858599 00:32
15 0.359117 0.486126 0.846369 00:32
16 0.313642 0.442215 0.861911 00:32
17 0.294050 0.485967 0.853503 00:32
18 0.270583 0.408566 0.875924 00:32
19 0.266003 0.411752 0.872611 00:33
We’re getting a great result now! Try adding Mixup, and then training this for a hun‐
dred epochs while you go get lunch. You’ll have yourself a very accurate image classi‐
fier, trained from scratch.
The bottleneck design we’ve shown here is typically used in only ResNet-50, -101, and
-152 models. ResNet-18 and -34 models usually use the non-bottleneck design seen
in the previous section. However, we’ve noticed that the bottleneck layer generally
works better even for the shallower networks. This just goes to show that the little
details in papers tend to stick around for years, even if they’re not quite the best
design! Questioning assumptions and “stuff everyone knows” is always a good idea,
because this is still a new field, and lots of details aren’t always done well.
<header><largefont><b>Conclusion</b></largefont></header>
You have now seen how the models we have been using for computer vision since the
first chapter are built, using skip connections to allow deeper models to be trained.
Even though there has been a lot of research into better architectures, they all use one
version or another of this trick to make a direct path from the input to the end of the
network. When using transfer learning, the ResNet is the pretrained model. In the
next chapter, we will look at the final details of how the models we used were built
from it.
<header><largefont><b>Questionnaire</b></largefont></header>
1. How did we get to a single vector of activations in the CNNs used for MNIST in
previous chapters? Why isn’t that suitable for Imagenette?
2. What do we do for Imagenette instead?
3. What is adaptive pooling?
4. What is average pooling?