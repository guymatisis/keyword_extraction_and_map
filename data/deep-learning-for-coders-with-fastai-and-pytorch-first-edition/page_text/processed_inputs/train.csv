text|keyphrases
"This output shows that the image with the highest loss is one that has been predicted
as “grizzly” with high confidence. However, it’s labeled (based on our Bing image
search) as “black.” We’re not bear experts, but it sure looks to us like this label is
incorrect! We should probably change its label to “grizzly.”
The intuitive approach to doing data cleaning is to do it <i>before</i> you train a model. But
as you’ve seen in this case, a model can help you find data issues more quickly and
easily. So, we normally prefer to train a quick and simple model first, and then use it
to help us with data cleaning.
ImageClassifierCleaner
fastai includes a handy GUI for data cleaning called that
allows you to choose a category and the training versus validation set and view the
highest-loss images (in order), along with menus to allow images to be selected for
removal or relabeling:
cleaner = ImageClassifierCleaner(learn)
cleaner
We can see that among our “black bears” is an image that contains two bears: one
grizzly, one black. So, we should choose <Delete> in the menu under this image.
ImageClassifierCleaner doesn’t do the deleting or changing of labels for you; it just
returns the indices of items to change. So, for instance, to delete (unlink) all images
selected for deletion, we would run this:
<b>for</b> idx <b>in</b> cleaner.delete(): cleaner.fns[idx].unlink()"|datasets; before versus after training; data cleaning GUI; process end-to-end; ImageClassifierCleaner; incorrect affecting loss; loss; data cleanup before versus after; cleaning GUI
"Does “logarithm” ring a bell? The logarithm function has this identity:
y = b**a
a = log(y,b)
In this case, we’re assuming that log(y,b) returns <i>log</i> <i>y</i> <i>base</i> <i>b.</i> However, PyTorch
doesn’t define log this way: log in Python uses the special number e (2.718…) as the
base.
Perhaps a logarithm is something that you have not thought about for the last 20
years or so. But it’s a mathematical idea that is going to be really critical for many
things in deep learning, so now would be a great time to refresh your memory. The
key thing to know about logarithms is this relationship:
log(a*b) = log(a)+log(b)
When we see it in that format, it looks a bit boring; but think about what this really
means. It means that logarithms increase linearly when the underlying signal increa‐
ses exponentially or multiplicatively. This is used, for instance, in the Richter scale of
earthquake severity and the dB scale of noise levels. It’s also often used on financial
charts, where we want to show compound growth rates more clearly. Computer sci‐
entists love using logarithms, because it means that modification, which can create
really, really large and really, really small numbers, can be replaced by addition, which
is much less likely to result in scales that are difficult for our computers to handle.
<b>SylvainSays</b>
It’s not just computer scientists who love logs! Until computers
came along, engineers and scientists used a special ruler called a
<i>slide</i> <i>rule</i> that did multiplication by adding logarithms. Logarithms
are widely used in physics, for multiplying very big or very small
numbers, and many other fields."|cross-entropy loss; logarithmic scale; slide rules using; pet breeds image classifier
"<b>def</b> average_grad(p, mom, grad_avg=None, **kwargs):
<b>if</b> grad_avg <b>is</b> None: grad_avg = torch.zeros_like(p.grad.data)
<b>return</b> {'grad_avg': grad_avg*mom + p.grad.data}
To use it, we just have to replace p.grad.data with grad_avg in our step function:
<b>def</b> momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg)
opt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9)
Learner will automatically schedule mom and lr, so fit_one_cycle will even work
with our custom Optimizer:
learn = get_learner(opt_func=opt_func)
learn.fit_one_cycle(3, 0.03)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 2.856000 2.493429 0.246115 00:10
1 2.504205 2.463813 0.348280 00:10
2 2.187387 1.755670 0.418853 00:10
learn.recorder.plot_sched()
We’re still not getting great results, so let’s see what else we can do.
<header><largefont><b>RMSProp</b></largefont></header>
RMSProp is another variant of SGD introduced by Geoffrey Hinton in Lecture 6e of
his Coursera class “Neural Networks for Machine Learning”. The main difference
from SGD is that it uses an adaptive learning rate: instead of using the same learning
rate for every parameter, each parameter gets its own specific learning rate controlled
by a global learning rate. That way, we can speed up training by giving a higher learn‐
ing rate to the weights that need to change a lot, while the ones that are good enough
get a lower learning rate."|Hinton; momentum in SGD; neural networks; RMSProp; stochastic gradient descent (SGD); training
"for every number of repetitions of every punctuation mark. Similarly, a capitalized
word will be replaced with a special capitalization token, followed by the lowercase
version of the word. This way, the embedding matrix needs only the lowercase ver‐
sions of the words, saving compute and memory resources, but can still learn the
concept of capitalization.
Here are some of the main special tokens you’ll see:
xxbos
Indicates the beginning of a text (here, a review)
xxmaj
Indicates the next word begins with a capital (since we lowercased everything)
xxunk
Indicates the next word is unknown
To see the rules that were used, you can check the default rules:
defaults.text_proc_rules
[<function fastai.text.core.fix_html(x)>,
<function fastai.text.core.replace_rep(t)>,
<function fastai.text.core.replace_wrep(t)>,
<function fastai.text.core.spec_add_spaces(t)>,
<function fastai.text.core.rm_useless_spaces(t)>,
<function fastai.text.core.replace_all_caps(t)>,
<function fastai.text.core.replace_maj(t)>,
<function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]
As always, you can look at the source code for each of them in a notebook by typing
the following:
??replace_rep
Here is a brief summary of what each does:
fix_html
Replaces special HTML characters with a readable version (IMDb reviews have
quite a few of these)
replace_rep
Replaces any character repeated three times or more with a special token for rep‐
etition (xxrep), the number of times it’s repeated, then the character
replace_wrep
Replaces any word repeated three times or more with a special token for word
repetition (xxwrep), the number of times it’s repeated, then the word
spec_add_spaces
Adds spaces around / and #"|fix_html; source code display; natural language processing (NLP); showing rules used; showing source code; replace_rep; replace_wrep; signature of function; source code of function displayed; spec_add_spaces
"learn = fit(5, lr=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.191731 0.121738 0.960900 00:11
1 0.083739 0.055808 0.981800 00:10
2 0.053161 0.044485 0.987100 00:10
3 0.034433 0.030233 0.990200 00:10
4 0.017646 0.025407 0.991200 00:10
learn = fit(5, lr=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.183244 0.084025 0.975800 00:13
1 0.080774 0.067060 0.978800 00:12
2 0.050215 0.062595 0.981300 00:12
3 0.030020 0.030315 0.990700 00:12
4 0.015131 0.025148 0.992100 00:12
At this point, I think it’s fair to say we know how to recognize digits! It’s time to move
on to something harder…
<header><largefont><b>Conclusion</b></largefont></header>
We’ve seen that convolutions are just a type of matrix multiplication, with two con‐
straints on the weight matrix: some elements are always zero, and some elements are
tied (forced to always have the same value). In Chapter 1, we saw the eight require‐
ments from the 1986 book <i>Parallel</i> <i>Distributed</i> <i>Processing;</i> one of them was “A pattern
of connectivity among units.” That’s exactly what these constraints do: they enforce a
certain pattern of connectivity.
These constraints allow us to use far fewer parameters in our model, without sacrific‐
ing the ability to represent complex visual features. That means we can train deeper
models faster, with less overfitting. Although the universal approximation theorem
shows that it should be <i>possible</i> to represent anything in a fully connected network in
one hidden layer, we’ve seen now that in <i>practice</i> we can train much better models by
being thoughtful about network architecture.
Convolutions are by far the most common pattern of connectivity we see in neural
nets (along with regular linear layers, which we refer to as <i>fully</i> <i>connected),</i> but it’s
likely that many more will be discovered.
We’ve also seen how to interpret the activations of layers in the network to see
whether training is going well or not, and how batchnorm helps regularize the"|convolutional neural network (CNN); building a CNN; training more stable; training on all digits
"<i>Figure</i> <i>1-15.</i> <i>Converting</i> <i>a</i> <i>time</i> <i>series</i> <i>into</i> <i>an</i> <i>image</i>
Another interesting fast.ai student project example comes from Gleb Esman. He was
working on fraud detection at Splunk, using a dataset of users’ mouse movements
and mouse clicks. He turned these into pictures by drawing an image displaying the
position, speed, and acceleration of the mouse pointer by using colored lines, and the
clicks were displayed using small colored circles, as shown in Figure 1-16. He fed this
into an image recognition model just like the one we’ve used in this chapter, and it
worked so well that it led to a patent for this approach to fraud analytics!
<i>Figure</i> <i>1-16.</i> <i>Converting</i> <i>computer</i> <i>mouse</i> <i>behavior</i> <i>to</i> <i>an</i> <i>image</i>"|non-image tasks; fraud detection; mouse movements for fraud detection; Splunk.com fraud detection; fraud detection at Splunk.com
"hyperparameter tuning. They have also been popular for quite a lot longer than deep
learning, so there is a more mature ecosystem of tooling and documentation around
them.
Most importantly, the critical step of interpreting a model of tabular data is signifi‐
cantly easier for decision tree ensembles. There are tools and methods for answering
the pertinent questions, like these: Which columns in the dataset were the most
important for your predictions? How are they related to the dependent variable? How
do they interact with each other? And which particular features were most important
for some particular observation?
Therefore, ensembles of decision trees are our first approach for analyzing a new tab‐
ular dataset.
The exception to this guideline is when the dataset meets one of these conditions:
• There are some high-cardinality categorical variables that are very important
(“cardinality” refers to the number of discrete levels representing categories, so a
high-cardinality categorical variable is something like a zip code, which can take
on thousands of possible levels).
• There are some columns that contain data that would be best understood with a
neural network, such as plain text data.
In practice, when we deal with datasets that meet these exceptional conditions, we
always try both decision tree ensembles and deep learning to see which works best.
Deep learning will likely be a useful approach in our example of collaborative filter‐
ing, as we have at least two high-cardinality categorical variables: the users and the
movies. But in practice, things tend to be less cut-and-dried, and there will often be a
mixture of high- and low-cardinality categorical variables and continuous variables.
Either way, it’s clear that we are going to need to add decision tree ensembles to our
modeling toolbox!
Up to now, we’ve used PyTorch and fastai for pretty much all of our heavy lifting. But
these libraries are mainly designed for algorithms that do lots of matrix multiplica‐
tion and derivatives (that is, stuff like deep learning!). Decision trees don’t depend on
these operations at all, so PyTorch isn’t much use.
Instead, we will be largely relying on a library called <i>scikit-learn</i> (also known as
<i>sklearn).</i> Scikit-learn is a popular library for creating machine learning models, using
approaches that are not covered by deep learning. In addition, we’ll need to do some
tabular data processing and querying, so we’ll want to use the Pandas library. Finally,
we’ll also need NumPy, since that’s the main numeric programming library that both
sklearn and Pandas rely on."|cardinality; decision tree ensembles and; cardinality and decision tree ensembles; decision trees; scikit-learn library instead; scikit-learn library; sklearn and Pandas rely on; NumPy needed; tabular data processing; plain text data approach; decision trees don’t use; tabular data for models; decision trees as first approach; text data approach
"In our earlier examples with a 3×3 matrix and a vector of size 3, broadcasting was
done on the rows:
Matrix (2d tensor): 3 x 3
Vector (1d tensor): (1) 3
Result (2d tensor): 3 x 3
As an exercise, try to determine what dimensions to add (and where) when you need
to normalize a batch of images of size 64 x 3 x 256 x 256 with vectors of three
elements (one for the mean and one for the standard deviation).
Another useful way of simplifying tensor manipulations is the use of Einstein sum‐
mation convention.
<header><largefont><b>Einstein</b></largefont> <largefont><b>Summation</b></largefont></header>
Before using the PyTorch operation @ or torch.matmul, there is one last way we can
(einsum).
implement matrix multiplication: <i>Einstein</i> <i>summation</i> This is a compact
representation for combining products and sums in a general way. We write an equa‐
tion like this:
ik,kj -> ij
The lefthand side represents the operands dimensions, separated by commas. Here
we have two tensors that each have two dimensions (i,k and k,j). The righthand
side represents the result dimensions, so here we have a tensor with two dimensions
i,j.
The rules of Einstein summation notation are as follows:
1. Repeated indices are implicitly summed over.
2. Each index can appear at most twice in any term.
3. Each term must contain identical nonrepeated indices.
So in our example, since k is repeated, we sum over that index. In the end, the for‐
(i,j)
mula represents the matrix obtained when we put in the sum of all the coeffi‐
cients (i,k) in the first tensor multiplied by the coefficients (k,j) in the second
tensor… which is the matrix product!
Here is how we can code this in PyTorch:
<b>def</b> matmul(a,b): <b>return</b> torch.einsum('ik,kj->ij', a, b)
Einstein summation is a very practical way of expressing operations involving index‐
ing and sum of products. Note that you can have one member on the lefthand side.
For instance,
torch.einsum('ij->ji', a)"|Einstein summation; neural networks; building layer from scratch
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 3.000821 2.663942 0.438314 00:02
1 2.139642 2.184780 0.240479 00:02
2 1.607275 1.812682 0.439779 00:02
3 1.347711 1.830982 0.497477 00:02
4 1.123113 1.937766 0.594401 00:02
5 0.852042 2.012127 0.631592 00:02
6 0.565494 1.312742 0.725749 00:02
7 0.347445 1.297934 0.711263 00:02
8 0.208191 1.441269 0.731201 00:02
9 0.126335 1.569952 0.737305 00:02
10 0.079761 1.427187 0.754150 00:02
11 0.052990 1.494990 0.745117 00:02
12 0.039008 1.393731 0.757894 00:02
13 0.031502 1.373210 0.758464 00:02
14 0.028068 1.368083 0.758464 00:02
Now that’s better than a multilayer RNN! We can still see there is a bit of overfitting,
however, which is a sign that a bit of regularization might help.
<header><largefont><b>Regularizing</b></largefont> <largefont><b>an</b></largefont> <largefont><b>LSTM</b></largefont></header>
Recurrent neural networks, in general, are hard to train, because of the problem of
vanishing activations and gradients we saw before. Using LSTM (or GRU) cells makes
training easier than with vanilla RNNs, but they are still very prone to overfitting.
Data augmentation, while a possibility, is less often used for text data than for images
because in most cases it requires another model to generate random augmentations
(e.g., by translating the text into another language and then back into the original lan‐
guage). Overall, data augmentation for text data is currently not a well-explored
space.
However, we can use other regularization techniques instead to reduce overfitting,
which were thoroughly studied for use with LSTMs in the paper “Regularizing and
Optimizing LSTM Language Models” by Stephen Merity et al. This paper showed
how effective use of dropout, activation regularization, and temporal activation regu‐
larization could allow an LSTM to beat state-of-the-art results that previously
required much more complicated models. The authors called an LSTM using these
techniques an <i>AWD-LSTM.</i> We’ll look at each of these techniques in turn."|architecture of model; AWD-LSTM architecture; NLP RNNs; building from scratch; text data complications; Keskar; LSTM model; LSTM language model; Merity; natural language processing (NLP); regularizing RNNs against; recurrent neural networks (RNNs); regularizing LSTM language models; Socher; recurrent neural networks
"For the identifier columns, a partial dependence plot showed that when the informa‐
tion was missing, the application was almost always rejected. It turned out that in
practice, the university filled out much of this information only <i>after</i> a grant applica‐
tion was accepted. Often, for applications that were not accepted, it was just left
blank. Therefore, this information was not something that was available at the time
that the application was received, and it would not be available for a predictive model
—it was data leakage.
In the same way, the final processing of successful applications was often done auto‐
matically as a batch at the end of the week, or the end of the year. It was this final
processing date that ended up in the data, so again, this information, while predictive,
was not actually available at the time that the application was received.
This example showcases the most practical and simple approaches to identifying data
leakage, which are to build a model and then do the following:
• Check whether the accuracy of the model is <i>too</i> <i>good</i> <i>to</i> <i>be</i> <i>true.</i>
• Look for important predictors that don’t make sense in practice.
• Look for partial dependence plot results that don’t make sense in practice.
Thinking back to our bear detector, this mirrors the advice that we provided in Chap‐
ter 2—it is often a good idea to build a model first and then do your data cleaning,
rather than vice versa. The model can help you identify potentially problematic data
issues.
It can also help you identify which factors influence specific predictions, with tree
interpreters.
<header><largefont><b>Tree</b></largefont> <largefont><b>Interpreter</b></largefont></header>
At the start of this section, we said that we wanted to be able to answer five questions:
• How confident are we in our predictions using a particular row of data?
• For predicting with a particular row of data, what were the most important fac‐
tors, and how did they influence that prediction?
• Which columns are the strongest predictors?
• Which columns are effectively redundant with each other, for purposes of
prediction?
• How do predictions vary as we vary these columns?"|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"<header><largefont><b>Getting</b></largefont> <largefont><b>a</b></largefont> <largefont><b>GPU</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Server</b></largefont></header>
To do nearly everything in this book, you’ll need access to a computer with an
NVIDIA GPU (unfortunately, other brands of GPU are not fully supported by the
main deep learning libraries). However, we don’t recommend you buy one; in fact,
even if you already have one, we don’t suggest you use it just yet! Setting up a com‐
puter takes time and energy, and you want all your energy to focus on deep learning
right now. Therefore, we instead suggest you rent access to a computer that already
has everything you need preinstalled and ready to go. Costs can be as little as $0.25
per hour while you’re using it, and some options are even free.
<b>Jargon:GraphicsProcessingUnit(GPU)</b>
Also known as a <i>graphics</i> <i>card.</i> A special kind of processor in your
computer that can handle thousands of single tasks at the same
time, especially designed for displaying 3D environments on a
computer for playing games. These same basic tasks are very simi‐
lar to what neural networks do, such that GPUs can run neural net‐
works hundreds of times faster than regular CPUs. All modern
computers contain a GPU, but few contain the right kind of GPU
necessary for deep learning.
The best choice of GPU servers to use with this book will change over time, as com‐
panies come and go and prices change. We maintain a list of our recommended
options on the book’s website, so go there now and follow the instructions to get con‐
nected to a GPU deep learning server. Don’t worry; it takes only about two minutes to
get set up on most platforms, and many don’t even require any payment or even a
credit card to get started.
<b>AlexisSays</b>
My two cents: heed this advice! If you like computers, you will be
tempted to set up your own box. Beware! It is feasible but surpris‐
ingly involved and distracting. There is a good reason this book is
not titled <i>Everything</i> <i>You</i> <i>Ever</i> <i>Wanted</i> <i>to</i> <i>Know</i> <i>About</i> <i>Ubuntu</i> <i>Sys‐</i>
<i>tem</i> <i>Administration,</i> <i>NVIDIA</i> <i>Driver</i> <i>Installation,</i> <i>apt-get,</i> <i>conda,</i> <i>pip,</i>
<i>and</i> <i>Jupyter</i> <i>Notebook</i> <i>Configuration.</i> That would be a book of its
own. Having designed and deployed our production machine
learning infrastructure at work, I can testify it has its satisfactions,
but it is as unrelated to modeling as maintaining an airplane is to
flying one.
Each option shown on the website includes a tutorial; after completing the tutorial,
you will end up with a screen looking like Figure 1-2."|beginning; GPU servers; cats and dogs first model; deep learning; dogs and cats first model; first model; GPU deep learning servers; graphics processing unit (GPU); GPU running; GPU server setup; NVIDIA GPU deep learning server; server for running code
"onward. The reason for this will become clear in just a moment. Since different archi‐
tectures might use different types of pooling layers, or even completely different kinds
of <i>heads,</i> we don’t just search for the adaptive pooling layer to decide where to cut the
pretrained model. Instead, we have a dictionary of information that is used for each
model to determine where its body ends and its head starts. We call this model_meta
—here it is for resnet50 :
model_meta[resnet50]
{'cut': -2,
'split': <function fastai.vision.learner._resnet_split(m)>,
'stats': ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])}
<b>Jargon:BodyandHead</b>
The head of a neural net is the part that is specialized for a particu‐
lar task. For a CNN, it’s generally the part after the adaptive average
pooling layer. The body is everything else, and includes the stem
(which we learned about in Chapter 14).
If we take all of the layers prior to the cut point of -2, we get the part of the model
that fastai will keep for transfer learning. Now, we put on our new head. This is cre‐
create_head:
ated using the function
create_head(20,2)
Sequential(
(0): AdaptiveConcatPool2d(
(ap): AdaptiveAvgPool2d(output_size=1)
(mp): AdaptiveMaxPool2d(output_size=1)
)
(1): Flatten()
(2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True)
(3): Dropout(p=0.25, inplace=False)
(4): Linear(in_features=20, out_features=512, bias=False)
(5): ReLU(inplace=True)
(6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True)
(7): Dropout(p=0.5, inplace=False)
(8): Linear(in_features=512, out_features=2, bias=False)
)
With this function, you can choose how many additional linear layers are added to
the end, how much dropout to use after each one, and what kind of pooling to use. By
default, fastai will apply both average pooling and max pooling, and will concatenate
AdaptiveConcatPool2d
the two together (this is the layer). This is not a particularly
common approach, but it was developed independently at fastai and other research
labs in recent years and tends to provide a small improvement over using just average
pooling."|body of a model; convolutional neural network (CNN); head of model; stem in convolutional neural network
"learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1)
<i>Table</i> <i>1-2.</i> <i>Results</i> <i>from</i> <i>the</i> <i>first</i> <i>training</i>
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.169390 0.021388 0.005413 00:14
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.058748 0.009240 0.002706 00:19
You will probably not see exactly the same results shown here. A lot of sources of
small random variation are involved in training models. We generally see an error
rate of well less than 0.02 in this example, however.
<b>TrainingTime</b>
Depending on your network speed, it might take a few minutes to
download the pretrained model and dataset. Running fine_tune
might take a minute or so. Often models in this book take a few
minutes to train, as will your own models, so it’s a good idea to
come up with good techniques to make the most of this time. For
instance, keep reading the next section while your model trains, or
open up another notebook and use it for some coding experiments.
<header><largefont><b>This</b></largefont> <largefont><b>Book</b></largefont> <largefont><b>Was</b></largefont> <largefont><b>Written</b></largefont> <largefont><b>in</b></largefont> <largefont><b>Jupyter</b></largefont> <largefont><b>Notebooks</b></largefont></header>
We wrote this book using Jupyter notebooks, so for nearly every chart, table, and cal‐
culation in this book, we’ll be showing you the exact code required to replicate it
yourself. That’s why very often in this book, you will see some code immediately fol‐
lowed by a table, a picture, or just some text. If you go on the book’s website, you will
find all the code, and you can try running and modifying every example yourself.
You just saw how a cell that outputs a table looks in the book. Here is an example of a
cell that outputs text:
1+1
2
Jupyter will always print or show the result of the last line (if there is one). For
instance, here is an example of a cell that outputs an image:
img = PILImage.create('images/chapter1_cat_example.jpg')
img.to_thumb(192)"|beginning; Jupyter Notebook; cats and dogs first model; image output by; text ouput by; dogs and cats first model; first model; notebooks; code from book; outputs; training
"We know 0 (for False ) is “dog,” because the classes are automatically sorted in fastai,
but we can still double-check by looking at dls.vocab:
dls.vocab
(#2) [False,True]
So, our model is very confident this was a picture of a cat.
To do the dot product of our weight matrix (2 by number of activations) with the
activations (batch size by activations by rows by cols), we use a custom einsum :
x.shape
torch.Size([1, 3, 224, 224])
cam_map = torch.einsum('ck,kij->cij', learn.model[1][-1].weight, act)
cam_map.shape
torch.Size([2, 7, 7])
For each image in our batch, and for each class, we get a 7×7 feature map that tells us
where the activations were higher and where they were lower. This will let us see
which areas of the pictures influenced the model’s decision.
For instance, we can find out which areas made the model decide this animal was a
cat (note that we need to decode the input x since it’s been normalized by the Data
Loader , and we need to cast to TensorImage since at the time this book is written,
PyTorch does not maintain types when indexing—this may be fixed by the time you
are reading this):
x_dec = TensorImage(dls.train.decode((x,))[0][0])
_,ax = plt.subplots()
x_dec.show(ctx=ax)
ax.imshow(cam_map[1].detach().cpu(), alpha=0.6, extent=(0,224,224,0),
interpolation='bilinear', cmap='magma');"|class activation map (CAM); hooks in PyTorch; interpretation via class activation map; PyTorch
"<b>In-PlaceOperations</b>
Methods in PyTorch whose names end in an underscore modify
their objects <i>in</i> <i>place.</i> For instance, bias.zero_ sets all elements of
the tensor bias to 0.
Our only remaining step is to update the weights and biases based on the gradient
and learning rate. When we do so, we have to tell PyTorch not to take the gradient of
this step too—otherwise, things will get confusing when we try to compute the deriv‐
ative at the next batch! If we assign to the data attribute of a tensor, PyTorch will not
take the gradient of that step. Here’s our basic training loop for an epoch:
<b>def</b> train_epoch(model, lr, params):
<b>for</b> xb,yb <b>in</b> dl:
calc_grad(xb, yb, model)
<b>for</b> <b>in</b>
p params:
p.data -= p.grad*lr
p.grad.zero_()
We also want to check how we’re doing, by looking at the accuracy of the validation
set. To decide if an output represents a 3 or a 7, we can just check whether it’s greater
than 0. So our accuracy for each item can be calculated (using broadcasting, so no
loops!) as follows:
(preds>0.0).float() == train_y[:4]
tensor([[False],
[ True],
[ True],
[False]])
That gives us this function to calculate our validation accuracy:
<b>def</b> batch_accuracy(xb, yb):
preds = xb.sigmoid()
correct = (preds>0.5) == yb
<b>return</b> correct.float().mean()
We can check it works:
batch_accuracy(linear1(batch), train_y[:4])
tensor(0.5000)
And then put the batches together:
<b>def</b>
validate_epoch(model):
accs = [batch_accuracy(model(xb), yb) <b>for</b> xb,yb <b>in</b> valid_dl]
<b>return</b> round(torch.stack(accs).mean().item(), 4)
validate_epoch(linear1)
0.5219"|numerical digit classifier; PyTorch; stochastic gradient descent (SGD)
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>1</b></largefont></header>
<header><largefont><b>Your</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Journey</b></largefont></header>
Hello, and thank you for letting us join you on your deep learning journey, however
far along that you may be! In this chapter, we will tell you a little bit more about what
to expect in this book, introduce the key concepts behind deep learning, and train our
first models on different tasks. It doesn’t matter if you don’t come from a technical or
a mathematical background (though it’s OK if you do too!); we wrote this book to
make deep learning accessible to as many people as possible.
<header><largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Is</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Everyone</b></largefont></header>
A lot of people assume that you need all kinds of hard-to-find stuff to get great results
with deep learning, but as you’ll see in this book, those people are wrong. Table 1-1
lists a few things you <i>absolutely</i> <i>don’t</i> <i>need</i> for world-class deep learning.
<i>Table</i> <i>1-1.</i> <i>What</i> <i>you</i> <i>don’t</i> <i>need</i> <i>for</i> <i>deep</i> <i>learning</i>
<b>Myth(don’tneed)</b> <b>Truth</b>
Lotsofmath Highschoolmathissufficient.
Lotsofdata We’veseenrecord-breakingresultswith<50itemsofdata.
Lotsofexpensivecomputers Youcangetwhatyouneedforstate-of-the-artworkforfree.
<i>Deep</i> <i>learning</i> is a computer technique to extract and transform data—with use cases
ranging from human speech recognition to animal imagery classification—by using
multiple layers of neural networks. Each of these layers takes its inputs from previous
layers and progressively refines them. The layers are trained by algorithms that mini‐
mize their errors and improve their accuracy. In this way, the network learns to per‐
form a specified task. We will discuss training algorithms in detail in the next section."|deep learning; neural networks used; deep learning using
"(By the way, what Samuel called “weights” are most generally referred to as model
<i>parameters</i> these days, in case you have encountered that term. The term <i>weights</i> is
reserved for a particular type of model parameter.)
Next, Samuel said we need an <i>automatic</i> <i>means</i> <i>of</i> <i>testing</i> <i>the</i> <i>effectiveness</i> <i>of</i> <i>any</i> <i>cur‐</i>
<i>rent</i> <i>weight</i> <i>assignment</i> <i>in</i> <i>terms</i> <i>of</i> <i>actual</i> <i>performance.</i> In the case of his checkers pro‐
gram, the “actual performance” of a model would be how well it plays. And you could
automatically test the performance of two models by setting them to play against each
other, and seeing which one usually wins.
Finally, he says we need <i>a</i> <i>mechanism</i> <i>for</i> <i>altering</i> <i>the</i> <i>weight</i> <i>assignment</i> <i>so</i> <i>as</i> <i>to</i> <i>maxi‐</i>
<i>mize</i> <i>the</i> <i>performance.</i> For instance, we could look at the difference in weights
between the winning model and the losing model, and adjust the weights a little fur‐
ther in the winning direction.
We can now see why he said that such a procedure <i>could</i> <i>be</i> <i>made</i> <i>entirely</i> <i>automatic</i>
<i>and…a</i> <i>machine</i> <i>so</i> <i>programmed</i> <i>would</i> <i>“learn”</i> <i>from</i> <i>its</i> <i>experience.</i> Learning would
become entirely automatic when the adjustment of the weights was also automatic—
when instead of us improving a model by adjusting its weights manually, we relied on
an automated mechanism that produced adjustments based on performance.
Figure 1-6 shows the full picture of Samuel’s idea of training a machine learning
model.
<i>Figure</i> <i>1-6.</i> <i>Training</i> <i>a</i> <i>machine</i> <i>learning</i> <i>model</i>
Notice the distinction between the model’s <i>results</i> (e.g., the moves in a checkers game)
and its <i>performance</i> (e.g., whether it wins the game, or how quickly it wins).
Also note that once the model is trained—that is, once we’ve chosen our final, best,
favorite weight assignment—then we can think of the weights as being <i>part</i> <i>of</i> <i>the</i>
<i>model,</i> since we’re not varying them anymore.
Therefore, actually <i>using</i> a model after it’s trained looks like Figure 1-7.
<i>Figure</i> <i>1-7.</i> <i>Using</i> <i>a</i> <i>trained</i> <i>model</i> <i>as</i> <i>a</i> <i>program</i>"|machine learning (ML); models; results versus performance; machine learning concepts; weights
"We’ll now explore how to apply a neural network to this language modeling problem,
using the concepts introduced in the preceding two chapters. But before reading fur‐
ther, pause and think about how <i>you</i> would approach this.
<header><largefont><b>Text</b></largefont> <largefont><b>Preprocessing</b></largefont></header>
It’s not at all obvious how we’re going to use what we’ve learned so far to build a lan‐
guage model. Sentences can be different lengths, and documents can be long. So how
can we predict the next word of a sentence using a neural network? Let’s find out!
We’ve already seen how categorical variables can be used as independent variables for
a neural network. Here’s the approach we took for a single categorical variable:
1. Make a list of all possible levels of that categorical variable (we’ll call this list the
<i>vocab).</i>
2. Replace each level with its index in the vocab.
3. Create an embedding matrix for this containing a row for each level (i.e., for each
item of the vocab).
4. Use this embedding matrix as the first layer of a neural network. (A dedicated
embedding matrix can take as inputs the raw vocab indexes created in step 2; this
is equivalent to, but faster and more efficient than, a matrix that takes as input
one-hot-encoded vectors representing the indexes.)
We can do nearly the same thing with text! What is new is the idea of a sequence.
First we concatenate all of the documents in our dataset into one big long string and
split it into words (or <i>tokens),</i> giving us a very long list of words. Our independent
variable will be the sequence of words starting with the first word in our very long list
and ending with the second to last, and our dependent variable will be the sequence
of words starting with the second word and ending with the last word.
Our vocab will consist of a mix of common words that are already in the vocabulary
of our pretrained model and new words specific to our corpus (cinematographic
terms or actor’s names, for instance). Our embedding matrix will be built accord‐
ingly: for words that are in the vocabulary of our pretrained model, we will take the
corresponding row in the embedding matrix of the pretrained model; but for new
words, we won’t have anything, so we will just initialize the corresponding row with a
random vector.
Each of the steps necessary to create a language model has jargon associated with it
from the world of natural language processing, and fastai and PyTorch classes avail‐
able to help. The steps are as follows:"|natural language processing (NLP)
"And finally:
over the example of classifying
under the surface . xxmaj
convert text into numbers and
we ‘ll have another example
. \n xxmaj then we
it for a while .
Going back to our movie reviews dataset, the first step is to transform the individual
texts into a stream by concatenating them together. As with images, it’s best to ran‐
domize the order of the inputs, so at the beginning of each epoch we will shuffle the
entries to make a new stream (we shuffle the order of the documents, not the order of
the words inside them, or the texts would not make sense anymore!).
We then cut this stream into a certain number of batches (which is our <i>batch</i> <i>size).</i>
For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will
give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the
order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to
10,000…), because we want the model to read continuous rows of text (as in the pre‐
ceding example). An xxbos token is added at the start of each text during preprocess‐
ing, so that the model knows when it reads the stream when a new entry is beginning.
So to recap, at every epoch we shuffle our collection of documents and concatenate
them into a stream of tokens. We then cut that stream into a batch of fixed-size con‐
secutive mini-streams. Our model will then read the mini-streams in order, and
thanks to an inner state, it will produce the same activation, whatever sequence
length we picked.
This is all done behind the scenes by the fastai library when we create an
LMDataLoader. We do this by first applying our Numericalize object to the tokenized
texts
nums200 = toks200.map(num)
and then passing that to LMDataLoader:
dl = LMDataLoader(nums200)
Let’s confirm that this gives the expected results, by grabbing the first batch
x,y = first(dl)
x.shape,y.shape
(torch.Size([64, 72]), torch.Size([64, 72]))"|batch operations; natural language processing (NLP); tokenization
"of combining all of these types of information with additional metadata represented
as tables, such as user information, previous transactions, and so forth.
However, nearly all machine learning approaches have the downside that they tell you
only which products a particular user might like, rather than what recommendations
would be helpful for a user. Many kinds of recommendations for products a user
might like may not be at all helpful—for instance, if the user is already familiar with
the products, or if they are simply different packagings of products they have already
purchased (such as a boxed set of novels, when they already have each of the items in
that set). Jeremy likes reading books by Terry Pratchett, and for a while Amazon was
recommending nothing but Terry Pratchett books to him (see Figure 2-1), which
really wasn’t helpful because he was already aware of these books!
<i>Figure</i> <i>2-1.</i> <i>A</i> <i>not-so-useful</i> <i>recommendation</i>
<b>Otherdatatypes</b>
Often you will find that domain-specific data types fit very nicely into existing cate‐
gories. For instance, protein chains look a lot like natural language documents, in that
they are long sequences of discrete tokens with complex relationships and meaning
throughout the sequence. And indeed, it does turn out that using NLP deep learning
methods is the current state-of-the-art approach for many types of protein analysis.
As another example, sounds can be represented as spectrograms, which can be
treated as images; standard deep learning approaches for images turn out to work
really well on spectrograms.
<header><largefont><b>The</b></largefont> <largefont><b>Drivetrain</b></largefont> <largefont><b>Approach</b></largefont></header>
Many accurate models are of no use to anyone, and many inaccurate models are
highly useful. To ensure that your modeling work is useful in practice, you need to
consider how your work will be used. In 2012, Jeremy, along with Margit Zwemer and
Mike Loukides, introduced a method called <i>the</i> <i>Drivetrain</i> <i>Approach</i> for thinking
about this issue."|beginning; other data types; Drivetrain Approach for actionable outcomes; process end-to-end; models; protein chains as; protein chains as natural language
"The paper caused great excitement as soon as it was released, because it included the
chart in Figure 13-16, which clearly demonstrated that batch normalization could
train a model that was even more accurate than the current state of the art (the <i>Incep‐</i>
<i>tion</i> architecture) and around 5× faster.
<i>Figure</i> <i>13-16.</i> <i>Impact</i> <i>of</i> <i>batch</i> <i>normalization</i> <i>(courtesy</i> <i>of</i> <i>Sergey</i> <i>Ioffe</i> <i>and</i> <i>Christian</i>
<i>Szegedy)</i>
Batch normalization (often called <i>batchnorm)</i> works by taking an average of the mean
and standard deviations of the activations of a layer and using those to normalize the
activations. However, this can cause problems because the network might want some
activations to be really high in order to make accurate predictions. So they also added
two learnable parameters (meaning they will be updated in the SGD step), usually
gamma beta.
called and After normalizing the activations to get some new activation
vector y, a batchnorm layer returns gamma*y + beta.
That’s why our activations can have any mean or variance, independent from the
mean and standard deviation of the results of the previous layer. Those statistics are
learned separately, making training easier on our model. The behavior is different
during training and validation: during training we use the mean and standard devia‐
tion of the batch to normalize the data, while during validation we instead use a run‐
ning mean of the statistics calculated during training."|convolutional neural network (CNN); building a CNN; training more stable; training on all digits
"together. To see how it works in practice, let’s get started on creating our own random
forest!
<header><largefont><b>Creating</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Random</b></largefont> <largefont><b>Forest</b></largefont></header>
We can create a random forest just like we created a decision tree, except now we are
also specifying parameters that indicate how many trees should be in the forest, how
we should subset the data items (the rows), and how we should subset the fields (the
columns).
n_estimators
In the following function definition, defines the number of trees we
want, max_samples defines how many rows to sample for training each tree, and
max_features defines how many columns to sample at each split point (where 0.5
means “take half the total number of columns”). We can also specify when to stop
splitting the tree nodes, effectively limiting the depth of the tree, by including the
same min_samples_leaf parameter we used in the preceding section. Finally, we pass
n_jobs=-1
to tell sklearn to use all our CPUs to build the trees in parallel. By creating
a little function for this, we can more quickly try variations in the rest of this chapter:
<b>def</b> rf(xs, y, n_estimators=40, max_samples=200_000,
max_features=0.5, min_samples_leaf=5, **kwargs):
<b>return</b> RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,
max_samples=max_samples, max_features=max_features,
min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)
m = rf(xs, y);
Our validation RMSE is now much improved over our last result produced by the
DecisionTreeRegressor , which made just one tree using all the available data:
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
(0.170896, 0.233502)
One of the most important properties of random forests is that they aren’t very sensi‐
tive to the hyperparameter choices, such as max_features. You can set n_estimators
to as high a number as you have time to train—the more trees you have, the more
accurate the model will be. max_samples can often be left at its default, unless you
have over 200,000 data points, in which case setting it to 200,000 will make it train
faster with little impact on accuracy. max_features=0.5 and min_samples_leaf=4
both tend to work well, although sklearn’s defaults work well too."|bagging; decision trees; creating a random forest; random forest insensitivity; machine learning (ML); predictions; random forests; sklearn; tabular data for models; training
"We now have 90 tokens, separated by spaces. Let’s say we want a batch size of 6. We
need to break this text into 6 contiguous parts of length 15:
xxbos xxmaj in this chapter , we will go back over the example of classifying
movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj
first we will look at the processing steps necessary to convert text into numbers and
how to customize it . xxmaj by doing this , we ‘ll have another example
of the preprocessor used in the data block xxup api . \n xxmaj then we
will study how we build a language model and train it for a while .
In a perfect world, we could then give this one batch to our model. But that approach
doesn’t scale, because outside this toy example, it’s unlikely that a single batch con‐
taining all the tokens would fit in our GPU memory (here we have 90 tokens, but all
the IMDb reviews together give several million).
So, we need to divide this array more finely into subarrays of a fixed sequence length.
It is important to maintain order within and across these subarrays, because we will
use a model that maintains a state so that it remembers what it read previously when
predicting what comes next.
Going back to our previous example with 6 batches of length 15, if we chose a
sequence length of 5, that would mean we first feed the following array:
xxbos xxmaj in this chapter
movie reviews we studied in
first we will look at
how to customize it .
of the preprocessor used in
will study how we build
Then, this one:
, we will go back
chapter 1 and dig deeper
the processing steps necessary to
xxmaj by doing this ,
the data block xxup api
a language model and train"|batch operations; natural language processing (NLP); tokenization
"those values, such that the more epochs we do, the more extreme our activations
become.
With Mixup, we no longer have that problem, because our labels will be exactly 1 or 0
only if we happen to “mix” with another image of the same class. The rest of the time,
our labels will be a linear combination, such as the 0.7 and 0.3 we got in the church
and gas station example earlier.
One issue with this, however, is that Mixup is “accidentally” making the labels bigger
than 0 or smaller than 1. That is to say, we’re not <i>explicitly</i> telling our model that we
want to change the labels in this way. So, if we want to change to make the labels
closer to or further away from 0 and 1, we have to change the amount of Mixup—
which also changes the amount of data augmentation, which might not be what we
want. There is, however, a way to handle this more directly, which is to use <i>label</i>
<i>smoothing.</i>
<header><largefont><b>Label</b></largefont> <largefont><b>Smoothing</b></largefont></header>
In the theoretical expression of loss, in classification problems, our targets are one-
hot encoded (in practice, we tend to avoid doing this to save memory, but what we
compute is the same loss as if we had used one-hot encoding). That means the model
is trained to return 0 for all categories but one, for which it is trained to return 1.
Even 0.999 is not “good enough”; the model will get gradients and learn to predict
activations with even higher confidence. This encourages overfitting and gives you at
inference time a model that is not going to give meaningful probabilities: it will
always say 1 for the predicted category even if it’s not too sure, just because it was
trained this way.
This can become very harmful if your data is not perfectly labeled. In the bear classi‐
fier we studied in Chapter 2, we saw that some of the images were mislabeled, or con‐
tained two different kinds of bears. In general, your data will never be perfect. Even if
the labels were manually produced by humans, they could make mistakes, or have
differences of opinions on images that are harder to label.
Instead, we could replace all our 1s with a number a bit less than 1, and our 0s with a
number a bit more than 0, and then train. This is called <i>label</i> <i>smoothing.</i> By encourag‐
ing your model to be less confident, label smoothing will make your training more
robust, even if there is mislabeled data. The result will be a model that generalizes
better at inference.
This is how label smoothing works in practice: we start with one-hot-encoded labels,

then replace all 0s with (that’s the Greek letter <i>epsilon,</i> which is what was used in
<i>N</i>
the paper that introduced label smoothing and is used in the fastai code), where <i>N</i> is
the number of classes and  is a parameter (usually 0.1, which would mean we are
10% unsure of our labels). Since we want the labels to add up to 1, we also replace the"|image classifier model training; label smoothing; one-hot encoding
"Then we’ll do a training run that will serve as a baseline:
model = xresnet50()
learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)
learn.fit_one_cycle(5, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.583403 2.064317 0.401792 01:03
1 1.208877 1.260106 0.601568 01:02
2 0.925265 1.036154 0.664302 01:03
3 0.730190 0.700906 0.777819 01:03
4 0.585707 0.541810 0.825243 01:03
That’s a good baseline, since we are not using a pretrained model, but we can do bet‐
ter. When working with models that are being trained from scratch, or fine-tuned to
a very different dataset from the one used for the pretraining, some additional techni‐
ques are really important. In the rest of the chapter, we’ll consider some key
approaches you’ll want to be familiar with. The first one is <i>normalizing</i> your data.
<header><largefont><b>Normalization</b></largefont></header>
When training a model, it helps if your input data is <i>normalized—that</i> is, has a mean
of 0 and a standard deviation of 1. But most images and computer vision libraries use
values between 0 and 255 for pixels, or between 0 and 1; in either case, your data is
not going to have a mean of 0 and a standard deviation of 1.
Let’s grab a batch of our data and look at those values, by averaging over all axes
except for the channel axis, which is axis 1:
x,y = dls.one_batch()
x.mean(dim=[0,2,3]),x.std(dim=[0,2,3])
(TensorImage([0.4842, 0.4711, 0.4511], device='cuda:5'),
TensorImage([0.2873, 0.2893, 0.3110], device='cuda:5'))
As we expected, the mean and standard deviation are not very close to the desired
values. Fortunately, normalizing the data is easy to do in fastai by adding the
Normalize
transform. This acts on a whole mini-batch at once, so you can add it to
the batch_tfms section of your data block. You need to pass to this transform the
mean and standard deviation that you want to use; fastai comes with the standard
ImageNet mean and standard deviation already defined. (If you do not pass any sta‐
tistics to the Normalize transform, fastai will automatically calculate them from a sin‐
gle batch of your data.)
Let’s add this transform (using imagenet_stats, as Imagenette is a subset of Image‐
Net) and take a look at one batch now:"|normalization of data; image classifier model training; baseline training run
"3e-3 is often a good learning rate for CNNs, and that appears to be the case here too,
so let’s try that:
learn.fit_one_cycle(5, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.901582 2.155090 0.325350 00:07
1 1.559855 1.586795 0.507771 00:07
2 1.296350 1.295499 0.571720 00:07
3 1.144139 1.139257 0.639236 00:07
4 1.049770 1.092619 0.659108 00:07
That’s a pretty good start, considering we have to pick the correct one of 10 cate‐
gories, and we’re training from scratch for just 5 epochs! We can do way better than
this using a deeper model, but just stacking new layers won’t really improve our
results (you can try and see for yourself!). To work around this problem, ResNets
introduce the idea of <i>skip</i> <i>connections.</i> We’ll explore those and other aspects of
ResNets in the next section.
<header><largefont><b>Building</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Modern</b></largefont> <largefont><b>CNN:</b></largefont> <largefont><b>ResNet</b></largefont></header>
We now have all the pieces we need to build the models we have been using in our
computer vision tasks since the beginning of this book: ResNets. We’ll introduce the
main idea behind them and show how it improves accuracy on Imagenette compared
to our previous model, before building a version with all the recent tweaks.
<header><largefont><b>Skip</b></largefont> <largefont><b>Connections</b></largefont></header>
In 2015, the authors of the ResNet paper noticed something that they found curious.
Even after using batchnorm, they saw that a network using more layers was doing less
well than a network using fewer layers—and there were no other differences between"|convolutional neural network (CNN); building ResNet CNN; ResNet architecture; convolutional neural networks; skip connections
"The other important piece of information that we have to tell fastai is how to get the
labels from the dataset. Computer vision datasets are normally structured in such a
way that the label for an image is part of the filename or path—most commonly the
parent folder name. fastai comes with a number of standardized labeling methods,
and ways to write your own. Here we’re telling fastai to use the is_cat function we
just defined.
Finally, we define the Transforms that we need. A Transform contains code that is
applied automatically during training; fastai includes many predefined Transforms,
and adding new ones is as simple as creating a Python function. There are two kinds:
item_tfms are applied to each item (in this case, each item is resized to a 224-pixel
square), while batch_tfms are applied to a <i>batch</i> of items at a time using the GPU, so
they’re particularly fast (we’ll see many examples of these throughout this book).
Why 224 pixels? This is the standard size for historical reasons (old pretrained mod‐
els require this size exactly), but you can pass pretty much anything. If you increase
the size, you’ll often get a model with better results (since it will be able to focus on
more details), but at the price of speed and memory consumption; the opposite is
true if you decrease the size.
<b>Jargon:ClassificationandRegression</b>
<i>Classification</i> and <i>regression</i> have very specific meanings in
machine learning. These are the two main types of model that we
will be investigating in this book. A <i>classification</i> <i>model</i> is one that
attempts to predict a class, or category. That is, it’s predicting from
a number of discrete possibilities, such as “dog” or “cat.” A <i>regres‐</i>
<i>sion</i> <i>model</i> is one that attempts to predict one or more numeric
quantities, such as a temperature or a location. Sometimes people
use the word <i>regression</i> to refer to a particular kind of model called
a <i>linear</i> <i>regression</i> <i>model;</i> this is a bad practice, and we won’t be
using that terminology in this book!
The Pet dataset contains 7,390 pictures of dogs and cats, consisting of 37 breeds. Each
image is labeled using its filename: for instance, the file <i>great_pyrenees_173.jpg</i> is the
173rd example of an image of a Great Pyrenees breed dog in the dataset. The file‐
names start with an uppercase letter if the image is a cat, and a lowercase letter other‐
wise. We have to tell fastai how to get labels from the filenames, which we do by
calling from_name_func (which means that filenames can be extracted using a func‐
x[0].isupper(), True
tion applied to the filename) and passing which evaluates to if
the first letter is uppercase (i.e., it’s a cat).
The most important parameter to mention here is valid_pct=0.2. This tells fastai to
hold out 20% of the data and <i>not</i> <i>use</i> <i>it</i> <i>for</i> <i>training</i> <i>the</i> <i>model</i> <i>at</i> <i>all.</i> This 20% of the
data is called the <i>validation</i> <i>set;</i> the remaining 80% is called the <i>training</i> <i>set.</i> The"|validation set; batch operations; beginning; classification models definition; datasets; pet images; fastai software library; labels; machine learning (ML); models; notebooks; pet images dataset; pixels; pixel count required; random seed for validation set selection; regression models definition; training; Transforms
"You can get a DataFrame from a CSV file, a database table, Python dictionaries, and
many other sources. In Jupyter, a DataFrame is output as a formatted table, as shown
here.
You can access rows and columns of a DataFrame with the iloc property, as if it were
a matrix:
df.iloc[:,0]
0 000005.jpg
1 000007.jpg
2 000009.jpg
3 000012.jpg
4 000016.jpg
...
5006 009954.jpg
5007 009955.jpg
5008 009958.jpg
5009 009959.jpg
5010 009961.jpg
Name: fname, Length: 5011, dtype: object
df.iloc[0,:]
<i>#</i> <i>Trailing</i> <i>:s</i> <i>are</i> <i>always</i> <i>optional</i> <i>(in</i> <i>numpy,</i> <i>pytorch,</i> <i>pandas,</i> <i>etc.),</i>
<i>#</i> <i>so</i> <i>this</i> <i>is</i> <i>equivalent:</i>
df.iloc[0]
fname 000005.jpg
labels chair
is_valid True
Name: 0, dtype: object
You can also grab a column by name by indexing into a DataFrame directly:
df['fname']
0 000005.jpg
1 000007.jpg
2 000009.jpg
3 000012.jpg
4 000016.jpg
...
5006 009954.jpg
5007 009955.jpg
5008 009958.jpg
5009 009959.jpg
5010 009961.jpg
Name: fname, Length: 5011, dtype: object
You can create new columns and do calculations using columns:
df1 = pd.DataFrame()
df1['a'] = [1,2,3,4]
df1"|multi-label classification; labels
"We can view the learning rate and momentum throughout training by calling
plot_sched on learn.recorder. learn.recorder (as the name suggests) records
everything that happens during training, including losses, metrics, and hyperparame‐
ters such as learning rate and momentum:
learn.recorder.plot_sched()
Smith’s original 1cycle paper used a linear warmup and linear annealing. As you can
see, we adapted the approach in fastai by combining it with another popular
approach: cosine annealing. fit_one_cycle provides the following parameters you
can adjust:
lr_max
The highest learning rate that will be used (this can also be a list of learning rates
for each layer group, or a Python slice object containing the first and last layer
group learning rates)
div
How much to divide lr_max by to get the starting learning rate
div_final
How much to divide lr_max by to get the ending learning rate
pct_start
What percentage of the batches to use for the warmup
moms
A tuple (mom1,mom2,mom3), where <i>mom1</i> is the initial momentum, <i>mom2</i> is the
minimum momentum, and <i>mom3</i> is the final momentum
Let’s take a look at our layer stats again:
learn.activation_stats.plot_layer_stats(-2)"|convolutional neural network (CNN); building a CNN; cosine annealing; Learner; training more stable; training on all digits
"'L.A. Confidential (1997)',
'Silence of the Lambs, The (1991)']
Another interesting thing we can do with these learned embeddings is to look at
<i>distance.</i>
<header><largefont><b>Embedding</b></largefont> <largefont><b>Distance</b></largefont></header>
On a two-dimensional map, we can calculate the distance between two coordinates by
2 2
using the formula of Pythagoras: <i>x</i> + <i>y</i> (assuming that <i>x</i> and <i>y</i> are the distances
between the coordinates on each axis). For a 50-dimensional embedding, we can do
exactly the same thing, except that we add up the squares of all 50 of the coordinate
distances.
If there were two movies that were nearly identical, their embedding vectors would
also have to be nearly identical, because the users who would like them would be
nearly exactly the same. There is a more general idea here: movie similarity can be
defined by the similarity of users who like those movies. And that directly means that
the distance between two movies’ embedding vectors can define that similarity. We
can use this to find the most similar movie to <i>Silence</i> <i>of</i> <i>the</i> <i>Lambs:</i>
movie_factors = learn.model.i_weight.weight
idx = dls.classes['title'].o2i['Silence of the Lambs, The (1991)']
distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None])
idx = distances.argsort(descending=True)[1]
dls.classes['title'][idx]
'Dial M for Murder (1954)'
Now that we have successfully trained a model, let’s see how to deal with the situation
of having no data for a user. How can we make recommendations to new users?
<header><largefont><b>Bootstrapping</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Collaborative</b></largefont> <largefont><b>Filtering</b></largefont> <largefont><b>Model</b></largefont></header>
The biggest challenge with using collaborative filtering models in practice is the <i>boot‐</i>
<i>strapping</i> <i>problem.</i> The most extreme version of this problem is having no users, and
therefore no history to learn from. What products do you recommend to your very
first user?
But even if you are a well-established company with a long history of user transac‐
tions, you still have the question: what do you do when a new user signs up? And
indeed, what do you do when you add a new product to your portfolio? There is no
magic solution to this problem, and really the solutions that we suggest are just varia‐
tions of <i>use</i> <i>your</i> <i>common</i> <i>sense.</i> You could assign new users the mean of all of the
embedding vectors of your other users, but this has the problem that that particular
combination of latent factors may be not at all common (for instance, the average for
the science-fiction factor may be high, and the average for the action factor may be"|bootstrapping problem of new users; collaborative filtering; built from scratch; datasets; embedding from scratch; new user bootstrapping problem
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
5 0.657402 0.611715 00:01
6 0.633079 0.605733 00:01
7 0.622399 0.602674 00:01
8 0.629075 0.601671 00:00
9 0.619955 0.601550 00:01
This model is predicting movie ratings on a scale of 0.5 to 5.0 to within around 0.6
average error. Since we’re predicting a continuous number, rather than a category, we
y_range
have to tell fastai what range our target has, using the parameter.
Although we’re not actually using a pretrained model (for the same reason that we
didn’t for the tabular model), this example shows that fastai lets us use fine_tune
anyway in this case (you’ll learn how and why this works in Chapter 5). Sometimes
it’s best to experiment with fine_tune versus fit_one_cycle to see which works best
for your dataset.
We can use the same show_results call we saw earlier to view a few examples of user
and movie IDs, actual ratings, and predictions:
learn.show_results()
<b>userId</b> <b>movieId</b> <b>rating</b> <b>rating_pred</b>
<b>0</b>
157 1200 4.0 3.558502
<b>1</b>
23 344 2.0 2.700709
<b>2</b> 19 1221 5.0 4.390801
<b>3</b> 430 592 3.5 3.944848
<b>4</b> 547 858 4.0 4.076881
<b>5</b> 292 39 4.5 3.753513
<b>6</b> 529 1265 4.0 3.349463
<b>7</b>
19 231 3.0 2.881087
<b>8</b>
475 4963 4.0 4.023387
<b>9</b> 130 260 4.5 3.979703
<header><largefont><b>Datasets:</b></largefont> <largefont><b>Food</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Models</b></largefont></header>
You’ve already seen quite a few models in this section, each one trained using a differ‐
ent dataset to do a different task. In machine learning and deep learning, we can’t do
anything without data. So, the people who create datasets for us to train our models
on are the (often underappreciated) heroes. Some of the most useful and important
datasets are those that become important <i>academic</i> <i>baselines—</i> datasets that are"|academic baseline datasets; datasets; non-pretrained; recommendation system rarity; recommendation system ratings
"<header><largefont><b>Writing</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Own</b></largefont> <largefont><b>Transform</b></largefont></header>
If you want to write a custom transform to apply to your data, the easiest way is to
write a function. As you can see in this example, a Transform will be applied only to a
matching type, if a type is provided (otherwise, it will always be applied). In the fol‐
lowing code, the :int in the function signature means that f gets applied only to
ints. That’s why tfm(2.0) returns 2.0, but tfm(2) returns 3 here:
<b>def</b> f(x:int): <b>return</b> x+1
tfm = Transform(f)
tfm(2),tfm(2.0)
(3, 2.0)
Here, f is converted to a Transform with no setup and no decode method.
Python has a special syntax for passing a function (like f ) to another function (or
something that behaves like a function, known as a <i>callable</i> in Python), called a <i>deco‐</i>
<i>rator.</i> A decorator is used by prepending a callable with @ and placing it before a func‐
tion definition (there are lots of good online tutorials about Python decorators, so
take a look at one if this is a new concept for you). The following is identical to the
previous code:
@Transform
<b>def</b> f(x:int): <b>return</b> x+1
f(2),f(2.0)
(3, 2.0)
If you need either setup or decode, you will need to subclass Transform to imple‐
ment the actual encoding behavior in encodes, then (optionally) the setup behavior
in setups and the decoding behavior in decodes :
<b>class</b> <b>NormalizeMean(Transform):</b>
<b>def</b> setups(self, items): self.mean = sum(items)/len(items)
<b>def</b> encodes(self, x): <b>return</b> x-self.mean
<b>def</b> decodes(self, x): <b>return</b> x+self.mean
Here, NormalizeMean will initialize a certain state during the setup (the mean of all
elements passed); then the transformation is to subtract that mean. For decoding pur‐
poses, we implement the reverse of that transformation by adding the mean. Here is
an example of NormalizeMean in action:
tfm = NormalizeMean()
tfm.setup([1,2,3,4,5])
start = 2
y = tfm(start)
z = tfm.decode(y)
tfm.mean,y,z
(3.0, -1.0, 2.0)"|writing your own
"Here’s a mini-batch with two items, for testing our collate :
x,y = collate([1,2], train_ds)
x.shape,y
(torch.Size([2, 64, 64, 3]), tensor([0, 0]))
Now that we have a dataset and a collation function, we’re ready to create
DataLoader. We’ll add two more things here: an optional shuffle for the training set,
and a ProcessPoolExecutor to do our preprocessing in parallel. A parallel data
loader is very important, because opening and decoding a JPEG image is a slow pro‐
cess. One CPU core is not enough to decode images fast enough to keep a modern
GPU busy. Here’s our DataLoader class:
<b>class</b> <b>DataLoader:</b>
<b>def</b> <b>__init__(self,</b> ds, bs=128, shuffle=False, n_workers=1):
self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers
<b>def</b> <b>__len__(self):</b> <b>return</b> (len(self.ds)-1)//self.bs+1
<b>def</b> <b>__iter__(self):</b>
idxs = L.range(self.ds)
<b>if</b> self.shuffle: idxs = idxs.shuffle()
chunks = [idxs[n:n+self.bs] <b>for</b> n <b>in</b> range(0, len(self.ds), self.bs)]
<b>with</b> ProcessPoolExecutor(self.n_workers) <b>as</b> ex:
<b>yield</b> <b>from</b> ex.map(collate, chunks, ds=self.ds)
Let’s try it out with our training and validation datasets:
n_workers = min(16, defaults.cpus)
train_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers)
valid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers)
xb,yb = first(train_dl)
xb.shape,yb.shape,len(train_dl)
(torch.Size([128, 64, 64, 3]), torch.Size([128]), 74)
This data loader is not much slower than PyTorch’s, but it’s far simpler. So if you’re
debugging a complex data loading process, don’t be afraid to try doing things man‐
ually to help you see exactly what’s going on.
For normalization, we’ll need image statistics. Generally, it’s fine to calculate these on
a single training mini-batch, since precision isn’t needed here:
stats = [xb.mean((0,1,2)),xb.std((0,1,2))]
stats
[tensor([0.4544, 0.4453, 0.4141]), tensor([0.2812, 0.2766, 0.2981])]
Our Normalize class just needs to store these stats and apply them (to see why the
to_device is needed, try commenting it out, and see what happens later in this note‐
book):"|DataLoader
"3. Read the paper “Discrimination in Online Ad Delivery”. Do you think Google
should be considered responsible for what happened to Dr. Sweeney? What
would be an appropriate response?
4. How can a cross-disciplinary team help avoid negative consequences?
5. Read the paper “Does Machine Learning Automate Moral Hazard and Error?”
What actions do you think should be taken to deal with the issues identified in
this paper?
6. Read the article “How Will We Prevent AI-Based Forgery?” Do you think
Etzioni’s proposed approach could work? Why?
7. Complete the section “Analyze a Project You Are Working On” on page 118.
8. Consider whether your team could be more diverse. If so, what approaches might
help?
<header><largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>in</b></largefont> <largefont><b>Practice:</b></largefont> <largefont><b>That’s</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Wrap!</b></largefont></header>
Congratulations! You’ve made it to the end of the first section of the book. In this sec‐
tion, we’ve tried to show you what deep learning can do, and how you can use it to
create real applications and products. At this point, you will get a lot more out of the
book if you spend some time trying out what you’ve learned. Perhaps you have
already been doing this as you go along—in which case, great! If not, that’s no prob‐
lem either—now is a great time to start experimenting yourself.
If you haven’t been to the book’s website yet, head over there now. It’s really important
that you get yourself set up to run the notebooks. Becoming an effective deep learn‐
ing practitioner is all about practice, so you need to be training models. So, please go
get the notebooks running now if you haven’t already! And have a look on the website
for any important updates or notices; deep learning changes fast, and we can’t change
the words that are printed in this book, so the website is where you need to look to
ensure you have the most up-to-date information."|beginning; book updates on website; web resources
"have a list of 0s, with a 1 in any position where that category is present. For example,
if there is a 1 in the second and fourth positions, that means vocab items two and four
are present in this image. This is known as <i>one-hot</i> <i>encoding.</i> The reason we can’t
easily just use a list of category indices is that each list would be a different length,
and PyTorch requires tensors, where everything has to be the same length.
<b>Jargon:One-HotEncoding</b>
Using a vector of 0s, with a 1 in each location that is represented in
the data, to encode a list of integers.
Let’s check what the categories represent for this example (we are using the conve‐
nient torch.where function, which tells us all of the indices where our condition is
true or false):
idxs = torch.where(dsets.train[0][1]==1.)[0]
dsets.train.vocab[idxs]
(#1) ['dog']
With NumPy arrays, PyTorch tensors, and fastai’s L class, we can index directly using
a list or vector, which makes a lot of code (such as this example) much clearer and
more concise.
We have ignored the column is_valid up until now, which means that DataBlock
has been using a random split by default. To explicitly choose the elements of our val‐
idation set, we need to write a function and pass it to splitter (or use one of fastai’s
predefined functions or classes). It will take the items (here our whole DataFrame)
and must return two (or more) lists of integers:
<b>def</b> splitter(df):
train = df.index[~df['is_valid']].tolist()
valid = df.index[df['is_valid']].tolist()
<b>return</b> train,valid
dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),
splitter=splitter,
get_x=get_x,
get_y=get_y)
dsets = dblock.datasets(df)
dsets.train[0]
(PILImage mode=RGB size=500x333,
TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,
> 0., 0., 0., 0., 0., 0.]))"|multi-label classifier; DataFrame to DataLoaders; DataLoaders object from; DataFrame converted to; one-hot encoding
"<b>class</b> <b>LSTMCell(Module):</b>
<b>def</b> <b>__init__(self,</b> ni, nh):
self.ih = nn.Linear(ni,4*nh)
self.hh = nn.Linear(nh,4*nh)
<b>def</b> forward(self, input, state):
h,c = state
<i>#</i> <i>One</i> <i>big</i> <i>multiplication</i> <i>for</i> <i>all</i> <i>the</i> <i>gates</i> <i>is</i> <i>better</i> <i>than</i> <i>4</i> <i>smaller</i> <i>ones</i>
gates = (self.ih(input) + self.hh(h)).chunk(4, 1)
ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])
cellgate = gates[3].tanh()
c = (forgetgate*c) + (ingate*cellgate)
h = outgate * c.tanh()
<b>return</b> h, (h,c)
Here we use the PyTorch chunk method to split our tensor into four pieces. It works
like this:
t = torch.arange(0,10); t
tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
t.chunk(2)
(tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9]))
Let’s now use this architecture to train a language model!
<header><largefont><b>Training</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>Using</b></largefont> <largefont><b>LSTMs</b></largefont></header>
Here is the same network as LMModel5, using a two-layer LSTM. We can train it at a
higher learning rate, for a shorter time, and get better accuracy:
<b>class</b> <b>LMModel6(Module):</b>
<b>def</b> <b>__init__(self,</b> vocab_sz, n_hidden, n_layers):
self.i_h = nn.Embedding(vocab_sz, n_hidden)
self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)
self.h_o = nn.Linear(n_hidden, vocab_sz)
self.h = [torch.zeros(n_layers, bs, n_hidden) <b>for</b> _ <b>in</b> range(2)]
<b>def</b> forward(self, x):
res,h = self.rnn(self.i_h(x), self.h)
self.h = [h_.detach() <b>for</b> h_ <b>in</b> h]
<b>return</b> self.h_o(res)
<b>def</b> reset(self):
<b>for</b> h <b>in</b> self.h: h.zero_()
learn = Learner(dls, LMModel6(len(vocab), 64, 2),
loss_func=CrossEntropyLossFlat(),
metrics=accuracy, cbs=ModelResetter)
learn.fit_one_cycle(15, 1e-2)"|building from scratch; LSTM language model; training a language model using
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 3.103298 2.874341 0.212565 00:01
1 2.231964 1.971280 0.462158 00:01
2 1.711358 1.813547 0.461182 00:01
3 1.448516 1.828176 0.483236 00:01
4 1.288630 1.659564 0.520671 00:01
5 1.161470 1.714023 0.554932 00:01
6 1.055568 1.660916 0.575033 00:01
7 0.960765 1.719624 0.591064 00:01
8 0.870153 1.839560 0.614665 00:01
9 0.808545 1.770278 0.624349 00:01
10 0.758084 1.842931 0.610758 00:01
11 0.719320 1.799527 0.646566 00:01
12 0.683439 1.917928 0.649821 00:01
13 0.660283 1.874712 0.628581 00:01
14 0.646154 1.877519 0.640055 00:01
We need to train for longer, since the task has changed a bit and is more complicated
now. But we end up with a good result…at least, sometimes. If you run it a few times,
you’ll see that you can get quite different results on different runs. That’s because
effectively we have a very deep network here, which can result in very large or very
small gradients. We’ll see in the next part of this chapter how to deal with this.
Now, the obvious way to get a better model is to go deeper: we have only one linear
layer between the hidden state and the output activations in our basic RNN, so maybe
we’ll get better results with more.
<header><largefont><b>Multilayer</b></largefont> <largefont><b>RNNs</b></largefont></header>
In a multilayer RNN, we pass the activations from our recurrent neural network into
a second recurrent neural network, as in Figure 12-6."|multilayer RNNs; improved RNN
"• The loss depends not only on the predictions, but also on the correct <i>labels</i> (also
known as <i>targets</i> or the <i>dependent</i> <i>variable);</i> e.g., “dog” or “cat.”
After making these changes, our diagram in Figure 1-6 looks like Figure 1-8.
<i>Figure</i> <i>1-8.</i> <i>Detailed</i> <i>training</i> <i>loop</i>
<header><largefont><b>Limitations</b></largefont> <largefont><b>Inherent</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Machine</b></largefont> <largefont><b>Learning</b></largefont></header>
From this picture, we can now see some fundamental things about training a deep
learning model:
• A model cannot be created without data.
• A model can learn to operate on only the patterns seen in the input data used to
train it.
• This learning approach creates only <i>predictions,</i> not recommended <i>actions.</i>
• It’s not enough to just have examples of input data; we need <i>labels</i> for that data
too (e.g., pictures of dogs and cats aren’t enough to train a model; we need a label
for each one, saying which ones are dogs and which are cats).
Generally speaking, we’ve seen that most organizations that say they don’t have
enough data actually mean they don’t have enough <i>labeled</i> data. If any organization is
interested in doing something in practice with a model, then presumably they have
some inputs they plan to run their model against. And presumably they’ve been
doing that some other way for a while (e.g., manually, or with some heuristic pro‐
gram), so they have data from those processes! For instance, a radiology practice will
almost certainly have an archive of medical scans (since they need to be able to check
how their patients are progressing over time), but those scans may not have struc‐
tured labels containing a list of diagnoses or interventions (since radiologists gener‐
ally create free-text natural language reports, not structured data). We’ll be discussing
labeling approaches a lot in this book, because it’s such an important issue in practice.
Since these kinds of machine learning models can only make <i>predictions</i> (i.e., attempt
to replicate labels), this can result in a significant gap between organizational goals
and model capabilities. For instance, in this book you’ll learn how to create a"|dependent variable definition; label importance; dependent variable; inputs; labels; limitations inherent to; as machine learning limitation; training
"It’s instructive to look at a few example cases. Many of these examples come from pre‐
dictive modeling competitions on the <i>Kaggle</i> platform, which is a good representation
of problems and methods you might see in practice.
One case might be if you are looking at time series data. For a time series, choosing a
random subset of the data will be both too easy (you can look at the data both before
and after the dates you are trying to predict) and not representative of most business
use cases (where you are using historical data to build a model for use in the future).
If your data includes the date and you are building a model to use in the future, you
will want to choose a continuous section with the latest dates as your validation set
(for instance, the last two weeks or last month of available data).
Suppose you want to split the time series data in Figure 1-19 into training and valida‐
tion sets.
<i>Figure</i> <i>1-19.</i> <i>A</i> <i>time</i> <i>series</i>
A random subset is a poor choice (too easy to fill in the gaps, and not indicative of
what you’ll need in production), as we can see in Figure 1-20."|validation set; predictive modeling competitions; models; testing models; training and validation sets; training
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>18</b></largefont></header>
<header><largefont><b>CNN</b></largefont> <largefont><b>Interpretation</b></largefont> <largefont><b>with</b></largefont> <largefont><b>CAM</b></largefont></header>
Now that we know how to build up pretty much anything from scratch, let’s use that
knowledge to create entirely new (and very useful!) functionality: the <i>class</i> <i>activation</i>
<i>map.</i> It gives a us some insight into why a CNN made the predictions it did.
In the process, we’ll learn about one handy feature of PyTorch we haven’t seen before,
the <i>hook,</i> and we’ll apply many of the concepts introduced in the rest of the book. If
you want to really test out your understanding of the material in this book, after
you’ve finished this chapter, try putting it aside and re-creating the ideas here yourself
from scratch (no peeking!).
<header><largefont><b>CAM</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Hooks</b></largefont></header>
The <i>class</i> <i>activation</i> <i>map</i> (CAM) was introduced by Bolei Zhou et al. in “Learning
Deep Features for Discriminative Localization”. It uses the output of the last convolu‐
tional layer (just before the average pooling layer) together with the predictions to
give us a heatmap visualization of why the model made its decision. This is a useful
tool for interpretation.
More precisely, at each position of our final convolutional layer, we have as many fil‐
ters as in the last linear layer. We can therefore compute the dot product of those acti‐
vations with the final weights to get, for each location on our feature map, the score of
the feature that was used to make a decision.
We’re going to need a way to get access to the activations inside the model while it’s
training. In PyTorch, this can be done with a <i>hook.</i> Hooks are PyTorch’s equivalent of
fastai’s callbacks. However, rather than allowing you to inject code into the training
loop like a fastai Learner callback, hooks allow you to inject code into the forward
and backward calculations themselves. We can attach a hook to any layer of the
model, and it will be executed when we compute the outputs (forward hook) or"|backpropagation; backward hook; class activation map (CAM); forward hook; hooks in PyTorch; interpretation via class activation map; PyTorch; outputs; class activation map; Zhou
"And test that it works:
l = Linear(4,2)
r = l(torch.ones(3,4))
r.shape
torch.Size([3, 2])
Let’s also create a testing module to check that if we include multiple parameters as
attributes, they are all correctly registered:
<b>class</b> <b>T(Module):</b>
<b>def</b> <b>__init__(self):</b>
super().__init__()
self.c,self.l = ConvLayer(3,4),Linear(4,2)
Since we have a conv layer and a linear layer, each of which has weights and biases,
we’d expect four parameters in total:
t = T()
len(t.parameters())
4
We should also find that calling cuda on this class puts all these parameters on the
GPU:
t.cuda()
t.l.w.device
device(type='cuda', index=5)
We can now use those pieces to create a CNN.
<header><largefont><b>Simple</b></largefont> <largefont><b>CNN</b></largefont></header>
As we’ve seen, a Sequential class makes many architectures easier to implement, so
let’s make one:
<b>class</b> <b>Sequential(Module):</b>
<b>def</b> <b>__init__(self,</b> *layers):
super().__init__()
self.layers = layers
self.register_modules(*layers)
<b>def</b> forward(self, x):
<b>for</b> l <b>in</b> self.layers: x = l(x)
<b>return</b> x
The forward method here just calls each layer in turn. Note that we have to use the
register_modules method we defined in Module , since otherwise the contents of
layers won’t appear in parameters."|Learner; simple CNN; Sequential class
"We can change our weight by a little in the direction of the slope, calculate our loss
and adjustment again, and repeat this a few times. Eventually, we will get to the lowest
point on our curve:
This basic idea goes all the way back to Isaac Newton, who pointed out that we can
optimize arbitrary functions in this way. Regardless of how complicated our functions
become, this basic approach of gradient descent will not significantly change. The
only minor changes we will see later in this book are some handy ways we can make it
faster, by finding better steps.
<header><largefont><b>Calculating</b></largefont> <largefont><b>Gradients</b></largefont></header>
The one magic step is the bit where we calculate the gradients. As we mentioned, we
use calculus as a performance optimization; it allows us to more quickly calculate
whether our loss will go up or down when we adjust our parameters up or down. In
other words, the gradients will tell us how much we have to change each weight to
make our model better.
You may remember from your high school calculus class that the <i>derivative</i> of a func‐
tion tells you how much a change in its parameters will change its result. If not, don’t
worry; lots of us forget calculus once high school is behind us! But you will need
some intuitive understanding of what a derivative is before you continue, so if this is
all very fuzzy in your head, head over to Khan Academy and complete the lessons on
basic derivatives. You won’t have to know how to calculate them yourself; you just
have to know what a derivative is.
The key point about a derivative is this: for any function, such as the quadratic func‐
tion we saw in the previous section, we can calculate its derivative. The derivative is
another function. It calculates the change, rather than the value. For instance, the
derivative of the quadratic function at the value 3 tells us how rapidly the function
changes at the value 3. More specifically, you may recall that gradient is defined as
<i>rise/run;</i> that is, the change in the value of the function, divided by the change in the"|derivative of a function; gradients; stochastic gradient descent; parameters; stochastic gradient descent (SGD); training; tutorials; web resources; weights
"26% of consumers had at least one mistake in their files, and 5% had errors that could
be devastating.
Yet, the process of getting such errors corrected is incredibly slow and opaque. When
public radio reporter Bobby Allyn discovered that he was erroneously listed as having
a firearms conviction, it took him “more than a dozen phone calls, the handiwork of a
county court clerk and six weeks to solve the problem. And that was only after I con‐
tacted the company’s communications department as a journalist.”
As machine learning practitioners, we do not always think of it as our responsibility
to understand how our algorithms end up being implemented in practice. But we
need to.
<header><largefont><b>Feedback</b></largefont> <largefont><b>Loops</b></largefont></header>
We explained in Chapter 1 how an algorithm can interact with its environment to
create a feedback loop, making predictions that reinforce actions taken in the real
world, which lead to predictions even more pronounced in the same direction. As an
example, let’s again consider YouTube’s recommendation system. A couple of years
ago, the Google team talked about how they had introduced reinforcement learning
(closely related to deep learning, but your loss function represents a result potentially
a long time after an action occurs) to improve YouTube’s recommendation system.
They described how they used an algorithm that made recommendations such that
watch time would be optimized.
However, human beings tend to be drawn to controversial content. This meant that
videos about things like conspiracy theories started to get recommended more and
more by the recommendation system. Furthermore, it turns out that the kinds of
people who are interested in conspiracy theories are also people who watch a lot of
online videos! So, they started to get drawn more and more toward YouTube. The
increasing number of conspiracy theorists watching videos on YouTube resulted in
the algorithm recommending more and more conspiracy theory and other extremist
content, which resulted in more extremists watching videos on YouTube, and more
people watching YouTube developing extremist views, which led to the algorithm rec‐
ommending more extremist content. The system was spiraling out of control."|conspiracy theory feedback loop; YouTube recommendation feedback loops; conspiracy theories fed by; recommendation system ethics; law enforcement; loss; conspiracy theory feedback loops; feedback loop ethics; YouTube feedback loop ethics; reinforcement learning; recommendation feedback loops
"The Drivetrain Approach, illustrated in Figure 2-2, was described in detail in
“Designing Great Data Products”. The basic idea is to start with considering your
objective, then think about what actions you can take to meet that objective and what
data you have (or can acquire) that can help, and then build a model that you can use
to determine the best actions to take to get the best results in terms of your objective.
<i>Figure</i> <i>2-2.</i> <i>The</i> <i>Drivetrain</i> <i>Approach</i>
Consider a model in an autonomous vehicle: you want to help a car drive safely from
point A to point B without human intervention. Great predictive modeling is an
important part of the solution, but it doesn’t stand on its own; as products become
more sophisticated, it disappears into the plumbing. Someone using a self-driving car
is completely unaware of the hundreds (if not thousands) of models and the petabytes
of data that make it work. But as data scientists build increasingly sophisticated prod‐
ucts, they need a systematic design approach.
We use data not just to generate more data (in the form of predictions), but to pro‐
duce <i>actionable</i> <i>outcomes.</i> That is the goal of the Drivetrain Approach. Start by defin‐
ing a clear <i>objective.</i> For instance, Google, when creating its first search engine,
considered “What is the user’s main objective in typing in a search query?” This led
to Google’s objective, which was to “show the most relevant search result.” The next
step is to consider what <i>levers</i> you can pull (i.e., what actions you can take) to better
achieve that objective. In Google’s case, that was the ranking of the search results. The
third step was to consider what new <i>data</i> they would need to produce such a ranking;
they realized that the implicit information regarding which pages linked to which
other pages could be used for this purpose.
Only after these first three steps do we begin thinking about building the predictive
<i>models.</i> Our objective and available levers, what data we already have and what addi‐
tional data we will need to collect, determine the models we can build. The models
will take both the levers and any uncontrollable variables as their inputs; the outputs
from the models can be combined to predict the final state for our objective."|actionable outcomes via Drivetrain Approach; autonomous vehicles; objectives via Drivetrain Approach; self-driving cars; web resources
"As you can see, it takes any input value, positive or negative, and smooshes it into an
output value between 0 and 1. It’s also a smooth curve that only goes up, which makes
it easier for SGD to find meaningful gradients.
mnist_loss sigmoid
Let’s update to first apply to the inputs:
<b>def</b> mnist_loss(predictions, targets):
predictions = predictions.sigmoid()
<b>return</b> torch.where(targets==1, 1-predictions, predictions).mean()
Now we can be confident our loss function will work, even if the predictions are not
between 0 and 1. All that is required is that a higher prediction corresponds to higher
confidence.
Having defined a loss function, now is a good moment to recapitulate why we did
this. After all, we already had a metric, which was overall accuracy. So why did we
define a loss?
The key difference is that the metric is to drive human understanding and the loss is
to drive automated learning. To drive automated learning, the loss must be a function
that has a meaningful derivative. It can’t have big flat sections and large jumps, but
instead must be reasonably smooth. This is why we designed a loss function that
would respond to small changes in confidence level. This requirement means that
sometimes it does not really reflect exactly what we are trying to achieve, but is rather
a compromise between our real goal and a function that can be optimized using its
gradient. The loss function is calculated for each item in our dataset, and then at the
end of an epoch, the loss values are all averaged and the overall mean is reported for
the epoch.
Metrics, on the other hand, are the numbers that we care about. These are the values
that are printed at the end of each epoch that tell us how our model is doing. It is
important that we learn to focus on these metrics, rather than the loss, when judging
the performance of a model."|MNIST loss function; numerical digit image classifier
"This gives us all of the functions and classes we will need to create a wide variety of
computer vision models.
<b>JeremySays</b>
A lot of Python coders recommend avoiding importing a whole
import *
library like this (using the syntax) because in large soft‐
ware projects it can cause problems. However, for interactive work
such as in a Jupyter notebook, it works great. The fastai library is
specially designed to support this kind of interactive use, and it will
import only the necessary pieces into your environment.
The second line downloads a standard dataset from the fast.ai datasets collection (if
not previously downloaded) to your server, extracts it (if not previously extracted),
and returns a Path object with the extracted location:
path = untar_data(URLs.PETS)/'images'
<b>SylvainSays</b>
Throughout my time studying at fast.ai, and even still today, I’ve
learned a lot about productive coding practices. The fastai library
and fast.ai notebooks are full of great little tips that have helped
make me a better programmer. For instance, notice that the fastai
library doesn’t just return a string containing the path to the data‐
set, but a Path object. This is a really useful class from the Python 3
standard library that makes accessing files and directories much
easier. If you haven’t come across it before, be sure to check out its
documentation or a tutorial and try it out. Note that the book’s
website contains links to recommended tutorials for each chapter.
I’ll keep letting you know about little coding tips I’ve found useful
as we come across them.
In the third line, we define a function, is_cat , that labels cats based on a filename
rule provided by the dataset’s creators:
<b>def</b> is_cat(x): <b>return</b> x[0].isupper()
We use that function in the fourth line, which tells fastai what kind of dataset we have
and how it is structured:
dls = ImageDataLoaders.from_name_func(
path, get_image_files(path), valid_pct=0.2, seed=42,
label_func=is_cat, item_tfms=Resize(224))
There are various classes for different kinds of deep learning datasets and problems—
here we’re using ImageDataLoaders. The first part of the class name will generally be
the type of data you have, such as image or text."|beginning; download first model; path to dataset; fastai software library; notebooks; Path object returned; Python; fastai library efficiency; tutorials; web resources
"<header><largefont><b>What</b></largefont> <largefont><b>Is</b></largefont> <largefont><b>Machine</b></largefont> <largefont><b>Learning?</b></largefont></header>
Your classifier is a deep learning model. As was already mentioned, deep learning
models use neural networks, which originally date from the 1950s and have become
powerful very recently thanks to recent advancements.
Another key piece of context is that deep learning is just a modern area in the more
general discipline of <i>machine</i> <i>learning.</i> To understand the essence of what you did
when you trained your own classification model, you don’t need to understand deep
learning. It is enough to see how your model and your training process are examples
of the concepts that apply to machine learning in general.
So in this section, we will describe machine learning. We will explore the key con‐
cepts and see how they can be traced back to the original essay that introduced them.
<i>Machine</i> <i>learning</i> is, like regular programming, a way to get computers to complete a
specific task. But how would we use regular programming to do what we just did in
the preceding section: recognize dogs versus cats in photos? We would have to write
down for the computer the exact steps necessary to complete the task.
Normally, it’s easy enough for us to write down the steps to complete a task when
we’re writing a program. We just think about the steps we’d take if we had to do the
task by hand, and then we translate them into code. For instance, we can write a func‐
tion that sorts a list. In general, we’d write a function that looks something like
Figure 1-4 (where <i>inputs</i> might be an unsorted list, and <i>results</i> a sorted list).
<i>Figure</i> <i>1-4.</i> <i>A</i> <i>traditional</i> <i>program</i>
But for recognizing objects in a photo, that’s a bit tricky; what <i>are</i> the steps we take
when we recognize an object in a picture? We really don’t know, since it all happens in
our brain without us being consciously aware of it!
Right back at the dawn of computing, in 1949, an IBM researcher named Arthur
Samuel started working on a different way to get computers to complete tasks, which
he called <i>machine</i> <i>learning.</i> In his classic 1962 essay “Artificial Intelligence: A Frontier
of Automation,” he wrote:
Programming a computer for such computations is, at best, a difficult task, not primar‐
ily because of any inherent complexity in the computer itself but, rather, because of the
need to spell out every minute step of the process in the most exasperating detail.
Computers, as any programmer will tell you, are giant morons, not giant brains.
His basic idea was this: instead of telling the computer the exact steps required to
solve a problem, show it examples of the problem to solve, and let it figure out how to"|Artificial Intelligence: A Frontier of Automation article; as machine learning; neural networks used; as deep learning; history; machine learning explained; machine learning (ML); deep learning using; Samuel
"bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))
dls = bears.dataloaders(path)
dls.valid.show_batch(max_n=4, nrows=1)
All of these approaches seem somewhat wasteful or problematic. If we squish or
stretch the images, they end up as unrealistic shapes, leading to a model that learns
that things look different from how they actually are, which we would expect to result
in lower accuracy. If we crop the images, we remove some of the features that allow us
to perform recognition. For instance, if we were trying to recognize breeds of dog or
cat, we might end up cropping out a key part of the body or the face necessary to
distinguish between similar breeds. If we pad the images, we have a whole lot of
empty space, which is just wasted computation for our model and results in a lower
effective resolution for the part of the image we actually use.
Instead, what we normally do in practice is to randomly select part of the image and
then crop to just that part. On each epoch (which is one complete pass through all of
our images in the dataset), we randomly select a different part of each image. This
means that our model can learn to focus on, and recognize, different features in our
images. It also reflects how images work in the real world: different photos of the
same thing may be framed in slightly different ways.
In fact, an entirely untrained neural network knows nothing whatsoever about how
images behave. It doesn’t even recognize that when an object is rotated by one degree,
it still is a picture of the same thing! So training the neural network with examples of
images in which the objects are in slightly different places and are slightly different
sizes helps it to understand the basic concept of what an object is, and how it can be
represented in an image.
Here is another example where we replace Resize with RandomResizedCrop , which is
the transform that provides the behavior just described. The most important parame‐
ter to pass in is min_scale , which determines how much of the image to select at
minimum each time:"|epochs; process end-to-end; pixels; image differences during; Transforms
"pretrained model are merged with random embeddings added for words that weren’t
in the pretraining vocabulary. This is handled automatically inside
language_model_learner:
learn = language_model_learner(
dls_lm, AWD_LSTM, drop_mult=0.3,
metrics=[accuracy, Perplexity()]).to_fp16()
The loss function used by default is cross-entropy loss, since we essentially have a
classification problem (the different categories being the words in our vocab). The
<i>perplexity</i> metric used here is often used in NLP for language models: it is the expo‐
nential of the loss (i.e., torch.exp(cross_entropy)). We also include the accuracy
metric to see how many times our model is right when trying to predict the next
word, since cross entropy (as we’ve seen) is both hard to interpret and tells us more
about the model’s confidence than its accuracy.
Let’s go back to the process diagram from the beginning of this chapter. The first
arrow has been completed for us and made available as a pretrained model in fastai,
and we’ve just built the DataLoaders and Learner for the second stage. Now we’re
ready to fine-tune our language model!
It takes quite a while to train each epoch, so we’ll be saving the intermediate model
results during the training process. Since fine_tune doesn’t do that for us, we’ll use
fit_one_cycle. Just like cnn_learner, language_model_learner automatically calls
freeze
when using a pretrained model (which is the default), so this will train only
the embeddings (the only part of the model that contains randomly initialized
weights—i.e., embeddings for words that are in our IMDb vocab, but aren’t in the
pretrained model vocab):
learn.fit_one_cycle(1, 2e-2)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>perplexity</b> <b>time</b>
0 4.120048 3.912788 0.299565 50.038246 11:39
This model takes a while to train, so it’s a good opportunity to talk about saving inter‐
mediary results."|fine-tuning models; pretrained language model; fine-tuning pretrained language model; fine-tuning language model
"If we want to broadcast in the other dimension, we have to change the shape of our
vector to make it a 3×1 matrix. This is done with the unsqueeze method in PyTorch:
c = tensor([10.,20,30])
m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])
c = c.unsqueeze(1)
m.shape,c.shape
(torch.Size([3, 3]), torch.Size([3, 1]))
This time, c is expanded on the column side:
c+m
tensor([[11., 12., 13.],
[24., 25., 26.],
[37., 38., 39.]])
As before, only three scalars are stored in memory:
t = c.expand_as(m)
t.storage()
10.0
20.0
30.0
[torch.FloatStorage of size 3]
And the expanded tensor has the right shape because the column dimension has a
stride of 0:
t.stride(), t.shape
((1, 0), torch.Size([3, 3]))
With broadcasting, if we need to add dimensions, they are added by default at the
beginning. When we were broadcasting before, PyTorch was executing
c.unsqueeze(0) behind the scenes:
c = tensor([10.,20,30])
c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape
(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))
The unsqueeze command can be replaced by None indexing:
c.shape, c[None,:].shape,c[:,None].shape
(torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1]))
You can always omit trailing colons, and ... means all preceding dimensions:
c[None].shape,c[...,None].shape
(torch.Size([1, 3]), torch.Size([3, 1]))"|vector to matrix; building layer from scratch; broadcasting vector to matrix
"As you can see by looking at the righthand side of this picture, the features are now
able to identify and match with higher-level semantic components, such as car
wheels, text, and flower petals. Using these components, layers 4 and 5 can identify
even higher-level concepts, as shown in Figure 1-13.
<i>Figure</i> <i>1-13.</i> <i>Activations</i> <i>of</i> <i>the</i> <i>fourth</i> <i>and</i> <i>fifth</i> <i>layers</i> <i>of</i> <i>a</i> <i>CNN</i> <i>(courtesy</i> <i>of</i> <i>Matthew</i> <i>D.</i>
<i>Zeiler</i> <i>and</i> <i>Rob</i> <i>Fergus)</i>
This article was studying an older model called <i>AlexNet</i> that contained only five lay‐
ers. Networks developed since then can have hundreds of layers—so you can imagine
how rich the features developed by these models can be!
When we fine-tuned our pretrained model earlier, we adapted what those last layers
focus on (flowers, humans, animals) to specialize on the cats versus dogs problem.
More generally, we could specialize such a pretrained model on many different tasks.
Let’s have a look at some examples.
<header><largefont><b>Image</b></largefont> <largefont><b>Recognizers</b></largefont> <largefont><b>Can</b></largefont> <largefont><b>Tackle</b></largefont> <largefont><b>Non-Image</b></largefont> <largefont><b>Tasks</b></largefont></header>
An image recognizer can, as its name suggests, only recognize images. But a lot of
things can be represented as images, which means that an image recognizer can learn
to complete many tasks.
For instance, a sound can be converted to a spectrogram, which is a chart that shows
the amount of each frequency at each time in an audio file. Fast.ai student Ethan
Sutin used this approach to easily beat the published accuracy of a state-of-the-art"|non-image tasks; sound analyzed as spectrogram
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>3</b></largefont></header>
<header><largefont><b>Data</b></largefont> <largefont><b>Ethics</b></largefont></header>
<header><largefont><b>Acknowledgment:</b></largefont> <largefont><b>Dr.</b></largefont> <largefont><b>Rachel</b></largefont> <largefont><b>Thomas</b></largefont></header>
This chapter was coauthored by Dr. Rachel Thomas, the cofounder of fast.ai and
founding director of the Center for Applied Data Ethics at the University of San Fran‐
cisco. It largely follows a subset of the syllabus she developed for the Introduction to
Data Ethics course.
As we discussed in Chapters 1 and 2, sometimes machine learning models can go
wrong. They can have bugs. They can be presented with data that they haven’t seen
before and behave in ways we don’t expect. Or they could work exactly as designed,
but be used for something that we would much prefer they were never, ever used for.
Because deep learning is such a powerful tool and can be used for so many things, it
becomes particularly important that we consider the consequences of our choices.
The philosophical study of <i>ethics</i> is the study of right and wrong, including how we
can define those terms, recognize right and wrong actions, and understand the con‐
nection between actions and consequences. The field of <i>data</i> <i>ethics</i> has been around
for a long time, and many academics are focused on this field. It is being used to help
define policy in many jurisdictions; it is being used in companies big and small to
consider how best to ensure good societal outcomes from product development; and
it is being used by researchers who want to make sure that the work they are doing is
used for good, and not for bad.
As a deep learning practitioner, therefore, you will likely at some point be put in a
situation requiring you to consider data ethics. So what is data ethics? It’s a subfield of
ethics, so let’s start there."|ethics
"But it’s sometimes not flexible enough. For debugging purposes, for instance, we
might need to apply just parts of the transforms that come with this data block. Or we
might want to create a DataLoaders for an application that isn’t directly supported by
fastai. In this section, we’ll dig into the pieces that are used inside fastai to implement
the data block API. Understanding these will enable you to leverage the power and
flexibility of this mid-tier API.
<b>Mid-LevelAPI</b>
The mid-level API does not contain only functionality for creating
DataLoaders.
It also has the <i>callback</i> system, which allows us to
customize the training loop any way we like, and the <i>general</i> <i>opti‐</i>
<i>mizer.</i> Both will be covered in Chapter 16.
<header><largefont><b>Transforms</b></largefont></header>
When we studied tokenization and numericalization in the preceding chapter, we
started by grabbing a bunch of texts:
files = get_text_files(path, folders = ['train', 'test'])
txts = L(o.open().read() <b>for</b> o <b>in</b> files[:2000])
Tokenizer
We then showed how to tokenize them with a
tok = Tokenizer.from_folder(path)
tok.setup(txts)
toks = txts.map(tok)
toks[0]
(#374) ['xxbos','xxmaj','well',',','""','cube','""','(','1997',')'...]
and how to numericalize, including automatically creating the vocab for our corpus:
num = Numericalize()
num.setup(toks)
nums = toks.map(num)
nums[0][:10]
tensor([ 2, 8, 76, 10, 23, 3112, 23, 34, 3113, 33])
The classes also have a decode method. For instance, Numericalize.decode gives us
back the string tokens:
nums_dec = num.decode(nums[0][:10]); nums_dec
(#10) ['xxbos','xxmaj','well',',','""','cube','""','(','1997',')']
Tokenizer.decode
turns this back into a single string (it may not, however, be exactly
the same as the original string; this depends on whether the tokenizer is <i>reversible,</i>
which the default word tokenizer is not at the time we’re writing this book):"|mid-level API; decode method; numericalization; tokenization; Transforms
"Here’s a sample 3:
a_3 = stacked_threes[1]
show_image(a_3);
How can we determine its distance from our ideal 3? We can’t just add up the differ‐
ences between the pixels of this image and the ideal digit. Some differences will be
positive, while others will be negative, and these differences will cancel out, resulting
in a situation where an image that is too dark in some places and too light in others
might be shown as having zero total differences from the ideal. That would be
misleading!
To avoid this, data scientists use two main ways to measure distance in this context:
• Take the mean of the <i>absolute</i> <i>value</i> of differences (absolute value is the function
that replaces negative values with positive values). This is called the <i>mean</i> <i>abso‐</i>
<i>lute</i> <i>difference</i> or <i>L1</i> <i>norm.</i>
• Take the mean of the <i>square</i> of differences (which makes everything positive) and
then take the <i>square</i> <i>root</i> (which undoes the squaring). This is called the <i>root</i>
<i>mean</i> <i>squared</i> <i>error</i> (RMSE) or <i>L2</i> <i>norm.</i>
<b>It’sOKtoHaveForgottenYourMath</b>
In this book, we generally assume that you have completed high
school math, and remember at least some of it—but everybody for‐
gets some things! It all depends on what you happen to have had
reason to practice in the meantime. Perhaps you have forgotten
what a <i>square</i> <i>root</i> is, or exactly how they work. No problem! Any‐
time you come across a math concept that is not explained fully in
this book, don’t just keep moving on; instead, stop and look it up.
Make sure you understand the basic idea, how it works, and why
we might be using it. One of the best places to refresh your under‐
standing is Khan Academy. For instance, Khan Academy has a
great introduction to square roots."|Khan Academy math tutorials online; L1 norm (mean absolute difference); L2 norm (root mean squared error); math tutorials online; mean absolute difference (L1 norm); comparing with ideal digit; pixels; root mean squared error (RMSE or L2 norm); web resources
"Remember, it’s always a good idea to look at your data before you use it:
dls.show_batch(max_n=9, figsize=(4,4))
Now that we have our data ready, we can train a simple model on it.
<header><largefont><b>A</b></largefont> <largefont><b>Simple</b></largefont> <largefont><b>Baseline</b></largefont></header>
Earlier in this chapter, we built a model based on a conv function like this:
<b>def</b> conv(ni, nf, ks=3, act=True):
res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)
<b>if</b> act: res = nn.Sequential(res, nn.ReLU())
<b>return</b> res
Let’s start with a basic CNN as a baseline. We’ll use the same as one as earlier, but with
one tweak: we’ll use more activations. Since we have more numbers to differentiate,
we’ll likely need to learn more filters.
As we discussed, we generally want to double the number of filters each time we have
a stride-2 layer. One way to increase the number of filters throughout our network is
to double the number of activations in the first layer—then every layer after that will
end up twice as big as in the previous version as well.
But this creates a subtle problem. Consider the kernel that is being applied to each
pixel. By default, we use a 3×3-pixel kernel. Therefore, there are a total of 3 × 3 = 9
pixels that the kernel is being applied to at each location. Previously, our first layer
had four output filters. So four values were being computed from nine pixels at each
location. Think about what happens if we double this output to eight filters. Then
when we apply our kernel, we will be using nine pixels to calculate eight numbers.
That means it isn’t really learning much at all: the output size is almost the same as
the input size. Neural networks will create useful features only if they’re forced to do
so—that is, if the number of outputs from an operation is significantly smaller than
the number of inputs."|convolutional neural network (CNN); building a CNN; examining data importance; training on all digits
"For training a model, we don’t just want any Python collection, but a collection con‐
taining independent and dependent variables (the inputs and targets of the model). A
collection that contains tuples of independent and dependent variables is known in
PyTorch as a Dataset . Here’s an example of an extremely simple Dataset :
ds = L(enumerate(string.ascii_lowercase))
ds
(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7,
> 'h'),(8, 'i'),(9, 'j')...]
When we pass a Dataset to a DataLoader we will get back many batches that are
themselves tuples of tensors representing batches of independent and dependent
variables:
dl = DataLoader(ds, batch_size=6, shuffle=True)
list(dl)
[(tensor([17, 18, 10, 22, 8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),
(tensor([20, 15, 9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),
(tensor([ 7, 25, 6, 5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),
(tensor([ 1, 3, 0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),
(tensor([2, 4]), ('c', 'e'))]
We are now ready to write our first training loop for a model using SGD!
<header><largefont><b>Putting</b></largefont> <largefont><b>It</b></largefont> <largefont><b>All</b></largefont> <largefont><b>Together</b></largefont></header>
It’s time to implement the process we saw in Figure 4-1. In code, our process will be
implemented something like this for each epoch:
<b>for</b> x,y <b>in</b> dl:
pred = model(x)
loss = loss_func(pred, y)
loss.backward()
parameters -= parameters.grad * lr
First, let’s reinitialize our parameters:
weights = init_params((28*28,1))
bias = init_params(1)
A DataLoader can be created from a Dataset:
dl = DataLoader(dset, batch_size=256)
xb,yb = first(dl)
xb.shape,yb.shape
(torch.Size([256, 784]), torch.Size([256, 1]))
We’ll do the same for the validation set:
valid_dl = DataLoader(valid_dset, batch_size=256)"|numerical digit classifier; stochastic gradient descent (SGD)
"And this phenomenon was not contained to this particular type of content. In June
2019, the <i>New</i> <i>York</i> <i>Times</i> published an article on YouTube’s recommendation system
titled “On YouTube’s Digital Playground, an Open Gate for Pedophiles”. The article
started with this chilling story:
Christiane C. didn’t think anything of it when her 10-year-old daughter and a friend
uploaded a video of themselves playing in a backyard pool…A few days later…the
video had thousands of views. Before long, it had ticked up to 400,000…“I saw the
video again and I got scared by the number of views,” Christiane said. She had reason
to be. YouTube’s automated recommendation system…had begun showing the video to
users who watched other videos of prepubescent, partially clothed children, a team of
researchers has found.
On its own, each video might be perfectly innocent, a home movie, say, made by a
child. Any revealing frames are fleeting and appear accidental. But, grouped together,
their shared features become unmistakable.
YouTube’s recommendation algorithm had begun curating playlists for pedophiles,
picking out innocent home videos that happened to contain prepubescent, partially
clothed children.
No one at Google planned to create a system that turned family videos into porn for
pedophiles. So what happened?
Part of the problem here is the centrality of metrics in driving a financially important
system. When an algorithm has a metric to optimize, as you have seen, it will do
everything it can to optimize that number. This tends to lead to all kinds of edge
cases, and humans interacting with a system will search for, find, and exploit these
edge cases and feedback loops for their advantage.
There are signs that this is exactly what has happened with YouTube’s recommenda‐
tion system in 2018. <i>The</i> <i>Guardian</i> ran an article called “How an Ex-YouTube Insider
Investigated Its Secret Algorithm” about Guillaume Chaslot, an ex-YouTube engineer
who created a website that tracks these issues. Chaslot published the chart in
Figure 3-5 following the release of Robert Mueller’s “Report on the Investigation Into
Russian Interference in the 2016 Presidential Election.”"|Chaslot; metrics driving algorithms; feedback loops driven by; pedophiles and YouTube
"<i>Figure</i> <i>9-5.</i> <i>The</i> <i>Google</i> <i>Play</i> <i>recommendation</i> <i>system</i>
Let’s pause for a moment. So far, the solution to all of our modeling problems has
been to <i>train</i> <i>a</i> <i>deep</i> <i>learning</i> <i>model.</i> And indeed, that is a pretty good rule of thumb
for complex unstructured data like images, sounds, natural language text, and so
forth. Deep learning also works very well for collaborative filtering. But it is not
always the best starting point for analyzing tabular data.
<header><largefont><b>Beyond</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont></header>
Most machine learning courses will throw dozens of algorithms at you, with a brief
technical description of the math behind them and maybe a toy example. You’re left
confused by the enormous range of techniques shown and have little practical under‐
standing of how to apply them.
The good news is that modern machine learning can be distilled down to a couple of
key techniques that are widely applicable. Recent studies have shown that the vast
majority of datasets can be best modeled with just two methods:
• Ensembles of decision trees (i.e., random forests and gradient boosting
machines), mainly for structured data (such as you might find in a database table
at most companies)
• Multilayered neural networks learned with SGD (i.e., shallow and/or deep learn‐
ing), mainly for unstructured data (such as audio, images, and natural language)
Although deep learning is nearly always clearly superior for unstructured data, these
two approaches tend to give quite similar results for many kinds of structured data.
But ensembles of decision trees tend to train faster, are often easier to interpret, do
not require special GPU hardware for inference at scale, and often require less"|datasets; decision trees; beyond deep learning; tabular data needing more; machine learning (ML); models; multilayered neural networks learned with; SGD; tabular data for models; deep learning not best starting point
"The percentage of nonzero weights is getting much better, although it’s still quite
high. We can see even more about what’s going on in our training by using
color_dim, passing it a layer index:
learn.activation_stats.color_dim(-2)
color_dim was developed by fast.ai in conjunction with a student, Stefano Giomo.
Giomo, who refers to the idea as the <i>colorful</i> <i>dimension,</i> provides an in-depth explan‐
ation of the history and details behind the method. The basic idea is to create a histo‐
gram of the activations of a layer, which we would hope would follow a smooth
pattern such as the normal distribution (Figure 13-14).
<i>Figure</i> <i>13-14.</i> <i>Histogram</i> <i>in</i> <i>colorful</i> <i>dimension</i> <i>(courtesy</i> <i>of</i> <i>Stefano</i> <i>Giomo)</i>"|histogram of activations; color_dim; convolutional neural network (CNN); building a CNN; Giomo; training more stable; training on all digits
"<header><largefont><b>Jargon</b></largefont> <largefont><b>Recap</b></largefont></header>
Congratulations: you now know how to create and train a deep neural network from
scratch! We’ve gone through quite a few steps to get to this point, but you might be
surprised at how simple it really is.
Now that we are at this point, it is a good opportunity to define, and review, some
jargon and key concepts.
A neural network contains a lot of numbers, but they are only of two types: numbers
that are calculated, and the parameters that these numbers are calculated from. This
gives us the two most important pieces of jargon to learn:
<i>Activations</i>
Numbers that are calculated (both by linear and nonlinear layers)
<i>Parameters</i>
Numbers that are randomly initialized, and optimized (that is, the numbers that
define the model)
We will often talk in this book about activations and parameters. Remember that they
have specific meanings. They are numbers. They are not abstract concepts, but they
are actual specific numbers that are in your model. Part of becoming a good deep
learning practitioner is getting used to the idea of looking at your activations and
parameters, and plotting them and testing whether they are behaving correctly.
Our activations and parameters are all contained in <i>tensors.</i> These are simply regu‐
larly shaped arrays—for example, a matrix. Matrices have rows and columns; we call
these the <i>axes</i> or <i>dimensions.</i> The number of dimensions of a tensor is its <i>rank.</i> There
are some special tensors:
• Rank-0: scalar
• Rank-1: vector
• Rank-2: matrix
A neural network contains a number of layers. Each layer is either <i>linear</i> or <i>nonlinear.</i>
We generally alternate between these two kinds of layers in a neural network. Some‐
times people refer to both a linear layer and its subsequent nonlinearity together as a
single layer. Yes, this is confusing. Sometimes a nonlinearity is referred to as an <i>acti‐</i>
<i>vation</i> <i>function.</i>"|nonlinear layer; activations; axis of tensor or matrix; deep learning; linear and nonlinear layers; nonlinear and linear layers; parameters; rank of tensor; scalar versus vector versus matrix; tensors; terminology for deep learning
"model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 0.962146 0.936952 00:14
1 0.858084 0.884951 00:14
2 0.740883 0.838549 00:14
3 0.592497 0.823599 00:14
4 0.473570 0.824263 00:14
Now, let’s take a look at what our model has learned.
<header><largefont><b>Interpreting</b></largefont> <largefont><b>Embeddings</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Biases</b></largefont></header>
Our model is already useful, in that it can provide us with movie recommendations
for our users—but it is also interesting to see what parameters it has discovered. The
easiest to interpret are the biases. Here are the movies with the lowest values in the
bias vector:
movie_bias = learn.model.movie_bias.squeeze()
idxs = movie_bias.argsort()[:5]
[dls.classes['title'][i] <b>for</b> i <b>in</b> idxs]
['Children of the Corn: The Gathering (1996)',
'Lawnmower Man 2: Beyond Cyberspace (1996)',
'Beautician and the Beast, The (1997)',
'Crow: City of Angels, The (1996)',
'Home Alone 3 (1997)']
Think about what this means. What it’s saying is that for each of these movies, even
when a user is very well matched to its latent factors (which, as we will see in a
moment, tend to represent things like level of action, age of movie, and so forth), they
still generally don’t like it. We could have simply sorted the movies directly by their
average rating, but looking at the learned bias tells us something much more interest‐
ing. It tells us not just whether a movie is of a kind that people tend not to enjoy
watching, but that people tend to not like watching it even if it is of a kind that they
would otherwise enjoy! By the same token, here are the movies with the highest bias:
idxs = movie_bias.argsort(descending=True)[:5]
[dls.classes['title'][i] <b>for</b> i <b>in</b> idxs]
['L.A. Confidential (1997)',
'Titanic (1997)',
'Silence of the Lambs, The (1991)',
'Shawshank Redemption, The (1994)',
'Star Wars (1977)']"|built from scratch; interpretting embeddings and biases; embedding from scratch
"The really interesting thing here is that this works just as well with more than two
columns. To see this, consider what would happen if we added an activation column
for every digit (0 through 9), and then targ contained a number from 0 to 9. As long
as the activation columns sum to 1 (as they will, if we use softmax), we’ll have a loss
function that shows how well we’re predicting each digit.
We’re picking the loss only from the column containing the correct label. We don’t
need to consider the other columns, because by the definition of softmax, they add up
to 1 minus the activation corresponding to the correct label. Therefore, making the
activation for the correct label as high as possible must mean we’re also decreasing the
activations of the remaining columns.
PyTorch provides a function that does exactly the same thing as sm_acts[range(n),
targ] (except it takes the negative, because when applying the log afterward, we will
have negative numbers), called nll_loss (NLL stands for <i>negative</i> <i>log</i> <i>likelihood):</i>
-sm_acts[idx, targ]
tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])
F.nll_loss(sm_acts, targ, reduction='none')
tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661])
Despite its name, this PyTorch function does not take the log. We’ll see why in the
next section, but first, let’s see why taking the logarithm can be useful.
<header><largefont><b>Taking</b></largefont> <largefont><b>the</b></largefont> <largefont><b>log</b></largefont></header>
The function we saw in the previous section works quite well as a loss function, but
we can make it a bit better. The problem is that we are using probabilities, and proba‐
bilities cannot be smaller than 0 or greater than 1. That means our model will not
care whether it predicts 0.99 or 0.999. Indeed, those numbers are very close together
—but in another sense, 0.999 is 10 times more confident than 0.99. So, we want to
transform our numbers between 0 and 1 to instead be between negative infinity and
infinity. There is a mathematical function that does exactly this: the <i>logarithm</i> (avail‐
able as torch.log). It is not defined for numbers less than 0 and looks like this:
plot_function(torch.log, min=0,max=4)"|cross-entropy loss; image classifier model training; logarithmic scale; loss; MNIST loss function; pet breeds image classifier; binary to multiple categories
"• Cardiovascular disease
• Accidental injury
• Benign breast lump
• Colonoscopy
• Sinusitis
However, only the top two have anything to do with a stroke! Based on what we’ve
studied so far, you can probably guess why. We haven’t really measured <i>stroke,</i> which
occurs when a region of the brain is denied oxygen due to an interruption in the
blood supply. What we’ve measured is who had symptoms, went to a doctor, got the
appropriate tests, <i>and</i> received a diagnosis of stroke. Actually having a stroke is not
the only thing correlated with this complete list—it’s also correlated with being the
kind of person who goes to the doctor (which is influenced by who has access to
healthcare, can afford their co-pay, doesn’t experience racial or gender-based medical
discrimination, and more)! If you are likely to go to the doctor for an <i>accidental</i>
<i>injury,</i> you are likely to also go the doctor when you are having a stroke.
This is an example of <i>measurement</i> <i>bias.</i> It occurs when our models make mistakes
because we are measuring the wrong thing, or measuring it in the wrong way, or
incorporating that measurement into the model inappropriately.
<b>Aggregationbias</b>
<i>Aggregation</i> <i>bias</i> occurs when models do not aggregate data in a way that incorporates
all of the appropriate factors, or when a model does not include the necessary interac‐
tion terms, nonlinearities, or so forth. This can particularly occur in medical settings.
For instance, the way diabetes is treated is often based on simple univariate statistics
and studies involving small groups of heterogeneous people. Analysis of results is
often done in a way that does not take into account different ethnicities or genders.
However, it turns out that diabetes patients have different complications across eth‐
nicities, and HbA1c levels (widely used to diagnose and monitor diabetes) differ in
complex ways across ethnicities and genders. This can result in people being misdiag‐
nosed or incorrectly treated because medical decisions are based on a model that does
not include these important variables and interactions.
<b>Representationbias</b>
The abstract of the paper “Bias in Bios: A Case Study of Semantic Representation Bias
in a High-Stakes Setting” by Maria De-Arteaga et al. notes that there is gender imbal‐
ance in occupations (e.g., females are more likely to be nurses, and males are more
likely to be pastors), and says that “differences in true positive rates between genders
are correlated with existing gender imbalances in occupations, which may compound
these imbalances.”"|aggregation bias; bias; ethics; De-Arteaga; diabetes data aggregation bias; gender; jobs and gender; medicine; occupations and gender; representation bias; research papers
"<i>Tokenization</i>
Convert the text into a list of words (or characters, or substrings, depending on
the granularity of your model).
<i>Numericalization</i>
List all of the unique words that appear (the vocab), and convert each word into a
number by looking up its index in the vocab.
<i>Language</i> <i>model</i> <i>data</i> <i>loader</i> <i>creation</i>
LMDataLoader
fastai provides an class that automatically handles creating a
dependent variable that is offset from the independent variable by one token. It
also handles some important details, such as how to shuffle the training data in
such a way that the dependent and independent variables maintain their struc‐
ture as required.
<i>Language</i> <i>model</i> <i>creation</i>
We need a special kind of model that does something we haven’t seen before:
handles input lists that could be arbitrarily big or small. There are a number of
ways to do this; in this chapter, we will be using a <i>recurrent</i> <i>neural</i> <i>network</i>
(RNN). We will get to the details of RNNs in Chapter 12, but for now, you can
think of it as just another deep neural network.
Let’s take a look at how each step works in detail.
<header><largefont><b>Tokenization</b></largefont></header>
When we said “convert the text into a list of words,” we left out a lot of details. For
instance, what do we do with punctuation? How do we deal with a word like “don’t”?
Is it one word or two? What about long medical or chemical words? Should they be
split into their separate pieces of meaning? How about hyphenated words? What
about languages like German and Polish, which can create really long words from
many, many pieces? What about languages like Japanese and Chinese that don’t use
bases at all, and don’t really have a well-defined idea of <i>word?</i>
Because there is no one correct answer to these questions, there is no one approach to
tokenization. There are three main approaches:
<i>Word-based</i>
Split a sentence on spaces, as well as applying language-specific rules to try to
separate parts of meaning even when there are no spaces (such as turning “don’t”
into “do n’t”). Generally, punctuation marks are also split into separate tokens.
<i>Subword</i> <i>based</i>
Split words into smaller parts, based on the most commonly occurring sub‐
strings. For instance, “occasion” might be tokenized as “o c ca sion”."|recurrent neural network; natural language processing (NLP); numericalization; natural language processing using; tokenization
"From time to time, you will hear directly from us in sidebars, like this one from
Jeremy:
<b>JeremySays</b>
Hi, everybody; I’m Jeremy! You might be interested to know that I
do not have any formal technical education. I completed a BA with
a major in philosophy, and didn’t have great grades. I was much
more interested in doing real projects than theoretical studies, so I
worked full time at a management consulting firm called McKinsey
and Company throughout my university years. If you’re somebody
who would rather get their hands dirty building stuff than spend
years learning abstract concepts, you will understand where I am
coming from! Look out for sidebars from me to find information
most suited to people with a less mathematical or formal technical
background—that is, people like me…
Sylvain, on the other hand, knows a lot about formal technical education. He has
written 10 math textbooks, covering the entire advanced French math curriculum!
<b>SylvainSays</b>
Unlike Jeremy, I have not spent many years coding and applying
machine learning algorithms. Rather, I recently came to the
machine learning world by watching Jeremy’s fast.ai course videos.
So, if you are somebody who has not opened a terminal and writ‐
ten commands at the command line, you will understand where I
am coming from! Look out for sidebars from me to find informa‐
tion most suited to people with a more mathematical or formal
technical background, but less real-world coding experience—that
is, people like me…
The fast.ai course has been studied by hundreds of thousands of students, from all
walks of life, from all parts of the world. Sylvain stood out as the most impressive stu‐
dent of the course that Jeremy had ever seen, which led to him joining fast.ai and then
becoming the coauthor, along with Jeremy, of the fastai software library.
All this means that between us, you have the best of both worlds: the people who
know more about the software than anybody else, because they wrote it; an expert on
math, and an expert on coding and machine learning; and also people who under‐
stand both what it feels like to be a relative outsider in math, and a relative outsider in
coding and machine learning.
Anybody who has watched sports knows that if you have a two-person commentary
team, you also need a third person to do “special comments.” Our special"|fast.ai ML courses
"thousands of items, using the default 20% validation set size may be more than you
need. On the other hand, if you have lots of data, using some of it for validation prob‐
ably doesn’t have any downsides.
Having two levels of “reserved data”—a validation set and a test set, with one level
representing data that you are virtually hiding from yourself—may seem a bit
extreme. But it is often necessary because models tend to gravitate toward the sim‐
plest way to do good predictions (memorization), and we as fallible humans tend to
gravitate toward fooling ourselves about how well our models are performing. The
discipline of the test set helps us keep ourselves intellectually honest. That doesn’t
mean we <i>always</i> need a separate test set—if you have very little data, you may need
just a validation set—but generally it’s best to use one if at all possible.
This same discipline can be critical if you intend to hire a third party to perform
modeling work on your behalf. A third party might not understand your require‐
ments accurately, or their incentives might even encourage them to misunderstand
them. A good test set can greatly mitigate these risks and let you evaluate whether
their work solves your actual problem.
To put it bluntly, if you’re a senior decision maker in your organization (or you’re
advising senior decision makers), the most important takeaway is this: if you ensure
that you really understand what test and validation sets are and why they’re impor‐
tant, you’ll avoid the single biggest source of failures we’ve seen when organizations
decide to use AI. For instance, if you’re considering bringing in an external vendor or
service, make sure that you hold out some test data that the vendor <i>never</i> <i>gets</i> <i>to</i> <i>see.</i>
Then <i>you</i> check their model on your test data, using a metric that <i>you</i> choose based
on what actually matters to you in practice, and <i>you</i> decide what level of performance
is adequate. (It’s also a good idea for you to try out simple baseline yourself, so you
know what a really simple model can achieve. Often it’ll turn out that your simple
model performs just as well as one produced by an external “expert”!)
<header><largefont><b>Use</b></largefont> <largefont><b>Judgment</b></largefont> <largefont><b>in</b></largefont> <largefont><b>Defining</b></largefont> <largefont><b>Test</b></largefont> <largefont><b>Sets</b></largefont></header>
To do a good job of defining a validation set (and possibly a test set), you will some‐
times want to do more than just randomly grab a fraction of your original dataset.
Remember: a key property of the validation and test sets is that they must be repre‐
sentative of the new data you will see in the future. This may sound like an impossible
order! By definition, you haven’t seen this data yet. But you usually still do know
some things."|validation set; testing models; training; model memorizing data
"<b>a</b>
<b>0</b> 1
<b>1</b> 2
<b>2</b>
3
<b>3</b>
4
df1['b'] = [10, 20, 30, 40]
df1['a'] + df1['b']
0 11
1 22
2 33
3 44
dtype: int64
Pandas is a fast and flexible library, and an important part of every data scientist’s
Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it
takes a while to get familiar with it. If you haven’t used Pandas before, we suggest
going through a tutorial; we are particularly fond of <i>Python</i> <i>for</i> <i>Data</i> <i>Analysis</i>
(O’Reilly) by Wes McKinney, the creator of Pandas. It also covers other important
libraries like matplotlib and NumPy. We will try to briefly describe Pandas functional‐
ity we use as we come across it, but will not go into the level of detail of McKinney’s
book.
Now that we have seen what the data looks like, let’s make it ready for model training.
<header><largefont><b>Constructing</b></largefont> <largefont><b>a</b></largefont> <largefont><b>DataBlock</b></largefont></header>
How do we convert from a DataFrame object to a DataLoaders object? We generally
suggest using the data block API for creating a DataLoaders object, where possible,
since it provides a good mix of flexibility and simplicity. Here we will show you the
steps that we take to use the data block API to construct a DataLoaders object in
practice, using this dataset as an example.
As we have seen, PyTorch and fastai have two main classes for representing and
accessing a training set or validation set:
Dataset
A collection that returns a tuple of your independent and dependent variable for
a single item
DataLoader
An iterator that provides a stream of mini-batches, where each mini-batch is a
couple of a batch of independent variables and a batch of dependent variables"|validation set; multi-label classification; DataFrame to DataLoaders; DataLoaders object from; DataLoader iterator; DataFrame converted to; Dataset collection; labels; McKinney; DataLoader variables; Pandas library; Python; Python for Data Analysis book (McKinney); training; tutorials
"Instead of complaining about shapes not matching, it returned the distance for every
single image as a vector (i.e., a rank-1 tensor) of length 1,010 (the number of 3s in our
validation set). How did that happen?
Take another look at our function mnist_distance , and you’ll see we have there the
subtraction (a-b). The magic trick is that PyTorch, when it tries to perform a simple
subtraction operation between two tensors of different ranks, will use <i>broadcasting:</i> it
will automatically expand the tensor with the smaller rank to have the same size as
the one with the larger rank. Broadcasting is an important capability that makes ten‐
sor code much easier to write.
After broadcasting so the two argument tensors have the same rank, PyTorch applies
its usual logic for two tensors of the same rank: it performs the operation on each
corresponding element of the two tensors, and returns the tensor result. For instance:
tensor([1,2,3]) + tensor([1,1,1])
tensor([2, 3, 4])
So in this case, PyTorch treats mean3, a rank-2 tensor representing a single image, as if
it were 1,010 copies of the same image, and then subtracts each of those copies from
each 3 in our validation set. What shape would you expect this tensor to have? Try to
figure it out yourself before you look at the answer here:
(valid_3_tens-mean3).shape
torch.Size([1010, 28, 28])
We are calculating the difference between our ideal 3 and each of the 1,010 3s in the
validation set, for each of 28×28 images, resulting in the shape [1010,28,28].
There are a couple of important points about how broadcasting is implemented,
which make it valuable not just for expressivity but also for performance:
• PyTorch doesn’t <i>actually</i> copy mean3 1,010 times. It <i>pretends</i> it were a tensor of
that shape, but doesn’t allocate any additional memory.
• It does the whole calculation in C (or, if you’re using a GPU, in CUDA, the equiv‐
alent of C on the GPU), tens of thousands of times faster than pure Python (up to
millions of times faster on a GPU!).
This is true of all broadcasting and elementwise operations and functions done in
PyTorch. <i>It’s</i> <i>the</i> <i>most</i> <i>important</i> <i>technique</i> <i>for</i> <i>you</i> <i>to</i> <i>know</i> <i>to</i> <i>create</i> <i>efficient</i> <i>PyTorch</i>
<i>code.</i>
Next in mnist_distance we see abs. You might be able to guess now what this does
when applied to a tensor. It applies the method to each individual element in the ten‐
sor, and returns a tensor of the results (that is, it applies the method <i>elementwise).</i> So
in this case, we’ll get back 1,010 absolute values."|broadcasting; PyTorch; most important technique; tensors
"experiments, or even just to see an unexpected result, say, “Hmmm, that’s interesting,”
and then, most importantly, set about figuring out what on earth is going on, with
great tenacity, is at the heart of many scientific discoveries. Deep learning is not like
pure mathematics. It is a heavily experimental field, so it’s important to be a strong
practitioner, not just a theoretician.
Since the ResNet was introduced, it’s been widely studied and applied to many
domains. One of the most interesting papers, published in 2018, is “Visualizing the
Loss Landscape of Neural Nets” by Hao Li et al. It shows that using skip connections
helps smooth the loss function, which makes training easier as it avoids falling into a
very sharp area. Figure 14-3 shows a stunning picture from the paper, illustrating the
difference between the bumpy terrain that SGD has to navigate to optimize a regular
CNN (left) versus the smooth surface of a ResNet (right).
<i>Figure</i> <i>14-3.</i> <i>Impact</i> <i>of</i> <i>ResNet</i> <i>on</i> <i>loss</i> <i>landscape</i> <i>(courtesy</i> <i>of</i> <i>Hao</i> <i>Li</i> <i>et</i> <i>al.)</i>
Our first model is already good, but further research has discovered more tricks we
can apply to make it better. We’ll look at those next.
<header><largefont><b>A</b></largefont> <largefont><b>State-of-the-Art</b></largefont> <largefont><b>ResNet</b></largefont></header>
In “Bag of Tricks for Image Classification with Convolutional Neural Networks”,
Tong He et al. study variations of the ResNet architecture that come at almost no
additional cost in terms of number of parameters or computation. By using a tweaked
ResNet-50 architecture and Mixup, they achieved 94.6% top-5 accuracy on ImageNet,
in comparison to 92.2% with a regular ResNet-50 without Mixup. This result is better
than that achieved by regular ResNet models that are twice as deep (and twice as slow,
and much more likely to overfit)."|building ResNet CNN; top 5 accuracy; He; ResNet architecture; Li; ResNet improved; skip connections smoothing loss; building state-of-the-art ResNet; skip connections
"<header><largefont><b>fastai’s</b></largefont> <largefont><b>Tabular</b></largefont> <largefont><b>Classes</b></largefont></header>
In fastai, a tabular model is simply a model that takes columns of continuous or cate‐
gorical data, and predicts a category (a classification model) or a continuous value (a
regression model). Categorical independent variables are passed through an embed‐
ding and concatenated, as we saw in the neural net we used for collaborative filtering,
and then continuous variables are concatenated as well.
tabular_learner TabularModel.
The model created in is an object of class Take a
look at the source for tabular_learner now (remember, that’s tabular_learner?? in
Jupyter). You’ll see that like collab_learner, it first calls get_emb_sz to calculate
appropriate embedding sizes (you can override these by using the emb_szs parameter,
which is a dictionary containing any column names you want to set sizes for man‐
ually), and it sets a few other defaults. Other than that, it creates the TabularModel
TabularLearner TabularLearner Learner,
and passes that to (note that is identical to
except for a customized predict method).
That means that really all the work is happening in TabularModel, so take a look at
the source for that now. With the exception of the BatchNorm1d and Dropout layers
(which we’ll be learning about shortly), you now have the knowledge required to
understand this whole class. Take a look at the discussion of EmbeddingNN at the end
n_cont=0 TabularModel.
of the preceding chapter. Recall that it passed to We now
can see why that was: because there are zero continuous variables (in fastai, the n_
prefix means “number of,” and cont is an abbreviation for “continuous”).
Another thing that can help with generalization is to use several models and average
their predictions—a technique, as mentioned earlier, known as <i>ensembling.</i>
<header><largefont><b>Ensembling</b></largefont></header>
Think back to the original reasoning behind why random forests work so well: each
tree has errors, but those errors are not correlated with each other, so the average of
those errors should tend toward zero once there are enough trees. Similar reasoning
could be used to consider averaging the predictions of models trained using different
algorithms.
In our case, we have two very different models, trained using very different algo‐
rithms: a random forest and a neural network. It would be reasonable to expect that
the kinds of errors that each one makes would be quite different. Therefore, we might
expect that the average of their predictions would be better than either one’s individ‐
ual predictions."|bagging; decision trees; ensembling random forests; fastai software library; machine learning (ML); predictions; random forests; Tabular classes; tabular data for models; training
"So, how do we know if this model is any good? In the last column of the table, you
can see the <i>error</i> <i>rate,</i> which is the proportion of images that were incorrectly identi‐
fied. The error rate serves as our metric—our measure of model quality, chosen to be
intuitive and comprehensible. As you can see, the model is nearly perfect, even
though the training time was only a few seconds (not including the one-time down‐
loading of the dataset and the pretrained model). In fact, the accuracy you’ve
achieved already is far better than anybody had ever achieved just 10 years ago!
Finally, let’s check that this model actually works. Go and get a photo of a dog or a cat;
if you don’t have one handy, just search Google Images and download an image that
you find there. Now execute the cell with uploader defined. It will output a button
you can click, so you can select the image you want to classify:
uploader = widgets.FileUpload()
uploader
Now you can pass the uploaded file to the model. Make sure that it is a clear photo of
a single dog or a cat, and not a line drawing, cartoon, or similar. The notebook will
tell you whether it thinks it is a dog or a cat, and how confident it is. Hopefully, you’ll
find that your model did a great job:
img = PILImage.create(uploader.data[0])
is_cat,_,probs = learn.predict(img)
<b>print(f""Is</b> this a cat?: {is_cat}."")
<b>print(f""Probability</b> it's a cat: {probs[1].item():.6f}"")
Is this a cat?: True.
Probability it's a cat: 0.999986
Congratulations on your first classifier!
But what does this mean? What did you actually do? In order to explain this, let’s
zoom out again to take in the big picture."|beginning; error rate; notebooks; testing models
"With elementwise arithmetic, we can remove one of our three nested loops: we can
multiply the tensors that correspond to the i-th row of a and the j-th column of b
before summing all the elements, which will speed things up because the inner loop
will now be executed by PyTorch at C speed.
To access one column or row, we can simply write a[i,:] or b[:,j]. The : means
take everything in that dimension. We could restrict this and take only a slice of that
dimension by passing a range, like 1:5, instead of just :. In that case, we would take
the elements in columns 1 to 4 (the second number is noninclusive).
One simplification is that we can always omit a trailing colon, so a[i,:] can be
abbreviated to a[i] . With all of that in mind, we can write a new version of our
matrix multiplication:
<b>def</b> matmul(a,b):
ar,ac = a.shape
br,bc = b.shape
<b>assert</b> ac==br
c = torch.zeros(ar, bc)
<b>for</b> i <b>in</b> range(ar):
<b>for</b> j <b>in</b> range(bc): c[i,j] = (a[i] * b[:,j]).sum()
<b>return</b> c
%timeit -n 20 t3 = matmul(m1,m2)
1.7 ms ± 88.1 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)
We’re already ~700 times faster, just by removing that inner for loop! And that’s just
the beginning—with broadcasting, we can remove another loop and get an even
more important speedup.
<header><largefont><b>Broadcasting</b></largefont></header>
As we discussed in Chapter 4, <i>broadcasting</i> is a term introduced by the Numpy
Library that describes how tensors of different ranks are treated during arithmetic
operations. For instance, it’s obvious there is no way to add a 3×3 matrix with a 4×5
matrix, but what if we want to add one scalar (which can be represented as a 1×1 ten‐
sor) with a matrix? Or a vector of size 3 with a 3×4 matrix? In both cases, we can find
a way to make sense of this operation.
Broadcasting gives specific rules to codify when shapes are compatible when trying to
do an elementwise operation, and how the tensor of the smaller shape is expanded to
match the tensor of the bigger shape. It’s essential to master those rules if you want to
be able to write code that executes quickly. In this section, we’ll expand our previous
treatment of broadcasting to understand these rules."|broadcasting; neural networks; building layer from scratch
"We can leverage the work we did to trim unwanted columns in the random forest by
using the same set of columns for our neural network:
df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]
Categorical columns are handled very differently in neural networks, compared to
decision tree approaches. As we saw in Chapter 8, in a neutral net, a great way to han‐
dle categorical variables is by using embeddings. To create embeddings, fastai needs
to determine which columns should be treated as categorical variables. It does this by
comparing the number of distinct levels in the variable to the value of the max_card
parameter. If it’s lower, fastai will treat the variable as categorical. Embedding sizes
larger than 10,000 should generally be used only after you’ve tested whether there are
better ways to group the variable, so we’ll use 9,000 as our max_card value:
cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)
In this case, however, there’s one variable that we absolutely do not want to treat as
categorical: saleElapsed. A categorical variable cannot, by definition, extrapolate
outside the range of values that it has seen, but we want to be able to predict auction
sale prices in the future. Therefore, we need to make this a continuous variable:
cont_nn.append('saleElapsed')
cat_nn.remove('saleElapsed')
Let’s take a look at the cardinality of each of the categorical variables that we have
chosen so far:
df_nn_final[cat_nn].nunique()
YearMade 73
ProductSize 6
Coupler_System 2
fiProductClassDesc 74
ModelID 5281
Hydraulics_Flow 3
fiSecondaryDesc 177
fiModelDesc 5059
ProductGroup 6
Enclosure 6
fiModelDescriptor 140
Drive_System 4
Hydraulics 12
Tire_Size 17
dtype: int64
The fact that there are two variables pertaining to the “model” of the equipment, both
with similar very high cardinalities, suggests that they may contain similar, redundant
information. Note that we would not necessarily catch this when analyzing redundant
features, since that relies on similar variables being sorted in the same order (that is,
they need to have similarly named levels). Having a column with 5,000 levels means
needing 5,000 columns in our embedding matrix, which would be nice to avoid if"|bagging; decision trees; tabular data with categorical columns; machine learning (ML); neural networks; predictions; random forests; tabular data for models; training
"The second tweak is that each of these three layers will use the same weight matrix.
The way that one word impacts the activations from previous words should not
change depending on the position of a word. In other words, activation values will
change as data moves through the layers, but the layer weights themselves will not
change from layer to layer. So, a layer does not learn one sequence position; it must
learn to handle all positions.
Since layer weights do not change, you might think of the sequential layers as “the
same layer” repeated. In fact, PyTorch makes this concrete; we can create just one
layer and use it multiple times.
<header><largefont><b>Our</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>in</b></largefont> <largefont><b>PyTorch</b></largefont></header>
We can now create the language model module that we described earlier:
<b>class</b> <b>LMModel1(Module):</b>
<b>def</b> <b>__init__(self,</b> vocab_sz, n_hidden):
self.i_h = nn.Embedding(vocab_sz, n_hidden)
self.h_h = nn.Linear(n_hidden, n_hidden)
self.h_o = nn.Linear(n_hidden,vocab_sz)
<b>def</b> forward(self, x):
h = F.relu(self.h_h(self.i_h(x[:,0])))
h = h + self.i_h(x[:,1])
h = F.relu(self.h_h(h))
h = h + self.i_h(x[:,2])
h = F.relu(self.h_h(h))
<b>return</b> self.h_o(h)
As you see, we have created three layers:
• The embedding layer (i_h, for <i>input</i> to <i>hidden)</i>
• The linear layer to create the activations for the next word (h_h, for <i>hidden</i> to
<i>hidden)</i>
• A final linear layer to predict the fourth word (h_o, for <i>hidden</i> to <i>output)</i>"|language model; natural language processing (NLP); building NLP model
"Rosenblatt further developed the artificial neuron to give it the ability to learn. Even
more importantly, he worked on building the first device that used these principles,
the Mark I Perceptron. In “The Design of an Intelligent Automaton,” Rosenblatt
wrote about this work: “We are now about to witness the birth of such a machine—a
machine capable of perceiving, recognizing and identifying its surroundings without
any human training or control.” The perceptron was built and was able to successfully
recognize simple shapes.
An MIT professor named Marvin Minsky (who was a grade behind Rosenblatt at the
same high school!), along with Seymour Papert, wrote a book called <i>Perceptrons</i> (MIT
Press) about Rosenblatt’s invention. They showed that a single layer of these devices
was unable to learn some simple but critical mathematical functions (such as XOR).
In the same book, they also showed that using multiple layers of the devices would
allow these limitations to be addressed. Unfortunately, only the first of these insights
was widely recognized. As a result, the global academic community nearly entirely
gave up on neural networks for the next two decades.
Perhaps the most pivotal work in neural networks in the last 50 years was the multi-
volume <i>Parallel</i> <i>Distributed</i> <i>Processing</i> (PDP) by David Rumelhart, James McClelland,
and the PDP Research Group, released in 1986 by MIT Press. Chapter 1 lays out a
similar hope to that shown by Rosenblatt:
People are smarter than today’s computers because the brain employs a basic computa‐
tional architecture that is more suited to deal with a central aspect of the natural infor‐
mation processing tasks that people are so good at.…We will introduce a
computational framework for modeling cognitive processes that seems…closer than
other frameworks to the style of computation as it might be done by the brain.
The premise that PDP is using here is that traditional computer programs work very
differently from brains, and that might be why computer programs had been (at that
point) so bad at doing things that brains find easy (such as recognizing objects in pic‐
tures). The authors claimed that the PDP approach was “closer than other frame‐
works” to how the brain works, and therefore it might be better able to handle these
kinds of tasks.
In fact, the approach laid out in PDP is very similar to the approach used in today’s
neural networks. The book defined parallel distributed processing as requiring the
following:
• A set of <i>processing</i> <i>units</i>
• A <i>state</i> <i>of</i> <i>activation</i>
• An <i>output</i> <i>function</i> for each unit
• A <i>pattern</i> <i>of</i> <i>connectivity</i> among units"|Mark I Perceptron; McClelland; Minsky; neural networks; Papert; Group); PDP Research Group; Perceptrons book (Minsky and Papert); Rumelhart
"solve it itself. This turned out to be very effective: by 1961, his checkers-playing pro‐
gram had learned so much that it beat the Connecticut state champion! Here’s how he
described his idea (from the same essay as noted previously):
Suppose we arrange for some automatic means of testing the effectiveness of any cur‐
rent weight assignment in terms of actual performance and provide a mechanism for
altering the weight assignment so as to maximize the performance. We need not go
into the details of such a procedure to see that it could be made entirely automatic and
to see that a machine so programmed would “learn” from its experience.
There are a number of powerful concepts embedded in this short statement:
• The idea of a “weight assignment”
• The fact that every weight assignment has some “actual performance”
• The requirement that there be an “automatic means” of testing that performance
• The need for a “mechanism” (i.e., another automatic process) for improving the
performance by changing the weight assignments
Let’s take these concepts one by one, in order to understand how they fit together in
practice. First, we need to understand what Samuel means by a <i>weight</i> <i>assignment.</i>
Weights are just variables, and a weight assignment is a particular choice of values for
those variables. The program’s inputs are values that it processes in order to produce
its results—for instance, taking image pixels as inputs, and returning the classification
“dog” as a result. The program’s weight assignments are other values that define how
the program will operate.
Because they will affect the program, they are in a sense another kind of input. We
will update our basic picture in Figure 1-4 and replace it with Figure 1-5 in order to
take this into account.
<i>Figure</i> <i>1-5.</i> <i>A</i> <i>program</i> <i>using</i> <i>weight</i> <i>assignment</i>
We’ve changed the name of our box from <i>program</i> to <i>model.</i> This is to follow modern
terminology and to reflect that the <i>model</i> is a special kind of program: it’s one that can
do <i>many</i> <i>different</i> <i>things,</i> depending on the <i>weights.</i> It can be implemented in many
different ways. For instance, in Samuel’s checkers program, different values of the
weights would result in different checkers-playing strategies."|machine learning (ML); models; programs versus models; weights
"A Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python
provides a zip function that, when combined with list, provides a simple way to get
this functionality:
dset = list(zip(train_x,train_y))
x,y = dset[0]
x.shape,y
(torch.Size([784]), tensor([1]))
valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)
valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)
valid_dset = list(zip(valid_x,valid_y))
Now we need an (initially random) weight for every pixel (this is the <i>initialize</i> step in
our seven-step process):
<b>def</b> init_params(size, std=1.0): <b>return</b> (torch.randn(size)*std).requires_grad_()
weights = init_params((28*28,1))
The function weights*pixels won’t be flexible enough—it is always equal to 0 when
the pixels are equal to 0 (i.e., its <i>intercept</i> is 0). You might remember from high school
math that the formula for a line is y=w*x+b ; we still need the b . We’ll initialize it to a
random number too:
bias = init_params(1)
In neural networks, the w in the equation y=w*x+b is called the <i>weights,</i> and the b is
called the <i>bias.</i> Together, the weights and bias make up the <i>parameters.</i>
<b>Jargon:Parameters</b>
w
The <i>weights</i> and <i>biases</i> of a model. The weights are the in the
w*x+b, b
equation and the biases are the in that equation.
We can now calculate a prediction for one image:
(train_x[0]*weights.T).sum() + bias
tensor([20.2336], grad_fn=<AddBackward0>)
While we could use a Python for loop to calculate the prediction for each image, that
would be very slow. Because Python loops don’t run on the GPU, and because Python
is a slow language for loops in general, we need to represent as much of the computa‐
tion in a model as possible using higher-level functions.
In this case, there’s an extremely convenient mathematical operation that calculates
w*x for every row of a matrix—it’s called <i>matrix</i> <i>multiplication.</i> Figure 4-6 shows what
matrix multiplication looks like."|MNIST loss function; numerical digit image classifier; matrix multiplication; Python; tensors
"This seems to have worked nicely, so let’s use fastai’s download_images to download
all the URLs for each of our search terms. We’ll put each in a separate folder:
bear_types = 'grizzly','black','teddy'
path = Path('bears')
<b>if</b> <b>not</b> path.exists():
path.mkdir()
<b>for</b> o <b>in</b> bear_types:
dest = (path/o)
dest.mkdir(exist_ok=True)
results = search_images_bing(key, f'{o} bear')
download_images(dest, urls=results.attrgot('content_url'))
Our folder has image files, as we’d expect:
fns = get_image_files(path)
fns
(#421) [Path('bears/black/00000095.jpg'),Path('bears/black/00000133.jpg'),Path('
> bears/black/00000062.jpg'),Path('bears/black/00000023.jpg'),Path('bears/black
> /00000029.jpg'),Path('bears/black/00000094.jpg'),Path('bears/black/00000124.j
> pg'),Path('bears/black/00000056.jpeg'),Path('bears/black/00000046.jpg'),Path(
> 'bears/black/00000045.jpg')...]
<b>JeremySays</b>
I just love this about working in Jupyter notebooks! It’s so easy to
gradually build what I want, and check my work every step of the
way. I make a <i>lot</i> of mistakes, so this is really helpful to me.
Often when we download files from the internet, a few are corrupt. Let’s check:
failed = verify_images(fns)
failed
(#0) []
unlink.
To remove all the failed images, you can use Like most fastai functions that
return a collection, verify_images returns an object of type L, which includes the
map method. This calls the passed function on each element of the collection:
failed.map(Path.unlink);
<header><largefont><b>Getting</b></largefont> <largefont><b>Help</b></largefont> <largefont><b>in</b></largefont> <largefont><b>Jupyter</b></largefont> <largefont><b>Notebooks</b></largefont></header>
Jupyter notebooks are great for experimenting and immediately seeing the results of
each function, but there is also a lot of functionality to help you figure out how to use
different functions, or even directly look at their source code. For instance, say you
type this in a cell:
??verify_images"|gathering data; download_images; process end-to-end; source code display; signature of function; verify_images
"The final step prior to training the classifier is to load the encoder from our fine-
tuned language model. We use load_encoder instead of load because we have only
pretrained weights available for the encoder; load by default raises an exception if an
incomplete model is loaded:
learn = learn.load_encoder('finetuned')
<header><largefont><b>Fine-Tuning</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Classifier</b></largefont></header>
The last step is to train with discriminative learning rates and <i>gradual</i> <i>unfreezing.</i> In
computer vision, we often unfreeze the model all at once, but for NLP classifiers, we
find that unfreezing a few layers at a time makes a real difference:
learn.fit_one_cycle(1, 2e-2)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.347427 0.184480 0.929320 00:33
In just one epoch, we get the same result as our training in Chapter 1—not too bad!
We can pass -2 to freeze_to to freeze all except the last two parameter groups:
learn.freeze_to(-2)
learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.247763 0.171683 0.934640 00:37
Then we can unfreeze a bit more and continue training:
learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.193377 0.156696 0.941200 00:45
And finally, the whole model!
learn.unfreeze()
learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.172888 0.153770 0.943120 01:01
1 0.161492 0.155567 0.942640 00:57"|fine-tuning models; natural language processing (NLP); fine-tuning classifier; gradual unfreezing NLP classifier
"fastai is a bit different from most libraries in that by default it adds two linear layers,
rather than one, in the CNN head. The reason is that transfer learning can still be
useful even, as we have seen, when transferring the pretrained model to very different
domains. However, just using a single linear layer is unlikely to be enough in these
cases; we have found that using two linear layers can allow transfer learning to be
used more quickly and easily, in more situations.
<b>OneLastBatchnorm</b>
create_head
One parameter to that is worth looking at is
bn_final. True
Setting this to will cause a batchnorm layer to be
added as your final layer. This can be useful in helping your model
scale appropriately for your output activations. We haven’t seen this
approach published anywhere as yet, but we have found that it
works well in practice wherever we have used it.
Let’s now take a look at what unet_learner did in the segmentation problem we
showed in Chapter 1.
<header><largefont><b>unet_learner</b></largefont></header>
One of the most interesting architectures in deep learning is the one that we used for
segmentation in Chapter 1. Segmentation is a challenging task, because the output
required is really an image, or a pixel grid, containing the predicted label for every
pixel. Other tasks share a similar basic design, such as increasing the resolution of an
image (super-resolution), adding color to a black-and-white image (colorization), or
converting a photo into a synthetic painting (style <i>transfer)—these</i> tasks are covered
by an online chapter of this book, so be sure to check it out after you’ve read this
chapter. In each case, we are starting with an image and converting it to another
image of the same dimensions or aspect ratio, but with the pixels altered in some way.
We refer to these as <i>generative</i> <i>vision</i> <i>models.</i>
The way we do this is to start with the exact same approach to developing a CNN
head as we saw in the previous section. We start with a ResNet, for instance, and cut
off the adaptive pooling layer and everything after that. Then we replace those layers
with our custom head, which does the generative task.
There was a lot of handwaving in that last sentence! How on earth do we create a
CNN head that generates an image? If we start with, say, a 224-pixel input image,
then at the end of the ResNet body we will have a 7×7 grid of convolutional activa‐
tions. How can we convert that into a 224-pixel segmentation mask?
Naturally, we do this with a neural network! So we need some kind of layer that can
increase the grid size in a CNN. One simple approach is to replace every pixel in the
7×7 grid with four pixels in a 2×2 square. Each of those four pixels will have the same"|architecture of model; unet_learner architecture
"To get rid of this trailing 1 dimension, we use the squeeze function:
<b>def</b> mse(output, targ): <b>return</b> (output.squeeze(-1) - targ).pow(2).mean()
And now we are ready to compute our loss:
loss = mse(out, y)
That’s all for the forward pass—let’s now look at the gradients.
<header><largefont><b>Gradients</b></largefont> <largefont><b>and</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Backward</b></largefont> <largefont><b>Pass</b></largefont></header>
We’ve seen that PyTorch computes all the gradients we need with a magic call to
loss.backward, but let’s explore what’s happening behind the scenes.
Now comes the part where we need to compute the gradients of the loss with respect
to all the weights of our model, so all the floats in w1 , b1 , w2 , and b2 . For this, we will
need a bit of math—specifically, the <i>chain</i> <i>rule.</i> This is the rule of calculus that guides
how we can compute the derivative of a composed function:
<i>g</i> ∘ <i>f</i> ′ <i>x</i> = <i>g′</i> <i>f</i> <i>x</i> <i>f′</i> <i>x</i>
<b>JeremySays</b>
I find this notation hard to wrap my head around, so instead I like
to think of it as follows: if y = g(u) and u=f(x), then dy/dx =
dy/du * du/dx. The two notations mean the same thing, so use
whatever works for you.
Our loss is a big composition of different functions: mean squared error (which is, in
turn, the composition of a mean and a power of two), the second linear layer, a ReLU,
and the first linear layer. For instance, if we want the gradients of the loss with respect
to b2 and our loss is defined by the following:
loss = mse(out,y) = mse(lin(l2, w2, b2), y)
The chain rule tells us that we have this:
dloss dloss dout d d
= × = <i>mse</i> <i>out,</i> <i>y</i> × <i>lin</i> <i>l</i> ,w ,b
2 2 2
db dout db dout db
2 2 2
To compute the gradients of the loss with respect to <i>b</i> , we first need the gradients of
2
the loss with respect to our output <i>out.</i> It’s the same if we want the gradients of the
loss with respect to <i>w</i> . Then, to get the gradients of the loss with respect to <i>b</i> or <i>w</i> ,
2 1 1
we will need the gradients of the loss with respect to <i>l</i> , which in turn requires the
1"|defining and initializing a layer; backward pass and; gradients and backward pass
"<header><largefont><b>From</b></largefont> <largefont><b>Data</b></largefont> <largefont><b>to</b></largefont> <largefont><b>DataLoaders</b></largefont></header>
DataLoaders is a thin class that just stores whatever DataLoader objects you pass to it
and makes them available as train and valid . Although it’s a simple class, it’s impor‐
DataLoad
tant in fastai: it provides the data for your model. The key functionality in
ers is provided with just these four lines of code (it has some other minor
functionality we’ll skip over for now):
<b>class</b> <b>DataLoaders(GetAttr):</b>
<b>def</b> <b>__init__(self,</b> *loaders): self.loaders = loaders
<b>def</b> <b>__getitem__(self,</b> i): <b>return</b> self.loaders[i]
train,valid = add_props(lambda i,self: self[i])
<b>Jargon:DataLoaders</b>
A fastai class that stores multiple DataLoader objects you pass to it
—normally a train and a valid , although it’s possible to have as
many as you like. The first two are made available as properties.
Later in the book, you’ll also learn about the Dataset and Datasets classes, which
have the same relationship. To turn our downloaded data into a DataLoaders object,
we need to tell fastai at least four things:
• What kinds of data we are working with
• How to get the list of items
• How to label these items
• How to create the validation set
So far we have seen a number of <i>factory</i> <i>methods</i> for particular combinations of these
things, which are convenient when you have an application and data structure that
happen to fit into those predefined methods. For when you don’t, fastai has an
extremely flexible system called the <i>data</i> <i>block</i> <i>API.</i> With this API, you can fully cus‐
tomize every stage of the creation of your DataLoaders. Here is what we need to cre‐
DataLoaders
ate a for the dataset that we just downloaded:
bears = DataBlock(
blocks=(ImageBlock, CategoryBlock),
get_items=get_image_files,
splitter=RandomSplitter(valid_pct=0.2, seed=42),
get_y=parent_label,
item_tfms=Resize(128))
Let’s look at each of these arguments in turn. First we provide a tuple specifying the
types we want for the independent and dependent variables:
blocks=(ImageBlock, CategoryBlock)"|CategoryBlock; DataLoaders; factory methods versus customization; process end-to-end; DataLoaders customization
"<header><largefont><b>The</b></largefont> <largefont><b>Power</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Diversity</b></largefont></header>
Currently, less than 12% of AI researchers are women, according to a study from Ele‐
ment AI. The statistics are similarly dire when it comes to race and age. When every‐
body on a team has similar backgrounds, they are likely to have similar blind spots
around ethical risks. The <i>Harvard</i> <i>Business</i> <i>Review</i> (HBR) has published a number of
studies showing many benefits of diverse teams, including the following:
• “How Diversity Can Drive Innovation”
• “Teams Solve Problems Faster When They’re More Cognitively Diverse”
• “Why Diverse Teams Are Smarter”
• “Defend Your Research: What Makes a Team Smarter? More Women”
Diversity can lead to problems being identified earlier, and a wider range of solutions
being considered. For instance, Tracy Chou was an early engineer at Quora. She
wrote of her experiences, describing how she advocated internally for adding a fea‐
ture that would allow trolls and other bad actors to be blocked. Chou recounts, “I was
eager to work on the feature because I personally felt antagonized and abused on the
site (gender isn’t an unlikely reason as to why)…But if I hadn’t had that personal per‐
spective, it’s possible that the Quora team wouldn’t have prioritized building a block
button so early in its existence.” Harassment often drives people from marginalized
groups off online platforms, so this functionality has been important for maintaining
the health of Quora’s community.
A crucial aspect to understand is that women leave the tech industry at over twice the
rate that men do. According to the Harvard Business Review, 41% of women working
in tech leave, compared to 17% of men. An analysis of over 200 books, whitepapers,
and articles found that the reason they leave is that “they’re treated unfairly; under‐
paid, less likely to be fast-tracked than their male colleagues, and unable to advance.”
Studies have confirmed a number of the factors that make it harder for women to
advance in the workplace. Women receive more vague feedback and personality criti‐
cism in performance evaluations, whereas men receive actionable advice tied to busi‐
ness outcomes (which is more useful). Women frequently experience being excluded
from more creative and innovative roles, and not receiving high-visibility “stretch”
assignments that are helpful in getting promoted. One study found that men’s voices
are perceived as more persuasive, fact-based, and logical than women’s voices, even
when reading identical scripts.
Receiving mentorship has been statistically shown to help men advance, but not
women. The reason behind this is that when women receive mentorship, it’s advice
on how they should change and gain more self-knowledge. When men receive men‐
torship, it’s public endorsement of their authority. Guess which is more useful in get‐
ting promoted?"|bias; Chou; ethics; diversity against ethical risks; gender; racial bias; tech industry and gender
"subtracting the predictions of each new tree from the residuals from the previous
tree, the residuals will get smaller and smaller.
To make predictions with an ensemble of boosted trees, we calculate the predictions
from each tree and then add them all together. There are many models following this
basic approach, and many names for the same models. <i>Gradient</i> <i>boosting</i> <i>machines</i>
(GBMs) and <i>gradient</i> <i>boosted</i> <i>decision</i> <i>trees</i> (GBDTs) are the terms you’re most likely
to come across, or you may see the names of specific libraries implementing these; at
the time of writing, <i>XGBoost</i> is the most popular.
Note that, unlike with random forests, with this approach, there is nothing to stop us
from overfitting. Using more trees in a random forest does not lead to overfitting,
because each tree is independent of the others. But in a boosted ensemble, the more
trees you have, the better the training error becomes, and eventually you will see
overfitting on the validation set.
We are not going to go into detail on how to train a gradient boosted tree ensemble
here, because the field is moving rapidly, and any guidance we give will almost cer‐
tainly be outdated by the time you read this. As we write this, sklearn has just added a
HistGradientBoostingRegressor class that provides excellent performance. There
are many hyperparameters to tweak for this class, and for all gradient boosted tree
methods we have seen. Unlike random forests, gradient boosted trees are extremely
sensitive to the choices of these hyperparameters; in practice, most people use a loop
that tries a range of hyperparameters to find the ones that work best.
One more technique that has gotten great results is to use embeddings learned by a
neural net in a machine learning model.
<header><largefont><b>Combining</b></largefont> <largefont><b>Embeddings</b></largefont> <largefont><b>with</b></largefont> <largefont><b>Other</b></largefont> <largefont><b>Methods</b></largefont></header>
The abstract of the entity embedding paper we mentioned at the start of this chapter
states: “The embeddings obtained from the trained neural network boost the perfor‐
mance of all tested machine learning methods considerably when used as the input
features instead.” It includes the very interesting table shown in Figure 9-8.
<i>Figure</i> <i>9-8.</i> <i>Effects</i> <i>of</i> <i>using</i> <i>neural</i> <i>network</i> <i>embeddings</i> <i>as</i> <i>input</i> <i>to</i> <i>other</i> <i>machine</i> <i>learn‐</i>
<i>ing</i> <i>methods</i> <i>(courtesy</i> <i>of</i> <i>Cheng</i> <i>Guo</i> <i>and</i> <i>Felix</i> <i>Berkhahn)</i>"|Berkhahn; combining with other methods; gradient boosted decision trees (GBDTs); gradient boosting machines (GBMs); Guo; XGBoost library
"<i>Figure</i> <i>9-6.</i> <i>An</i> <i>example</i> <i>of</i> <i>decision</i> <i>tree</i>
Let’s consider how we find the right questions to ask. Of course, we wouldn’t want to
have to create all these questions ourselves—that’s what computers are for! The basic
steps to train a decision tree can be written down very easily:
1. Loop through each column of the dataset in turn.
2. For each column, loop through each possible level of that column in turn.
3. Try splitting the data into two groups, based on whether they are greater than or
less than that value (or if it is a categorical variable, based on whether they are
equal to or not equal to that level of that categorical variable).
4. Find the average sale price for each of those two groups, and see how close that is
to the actual sale price of each of the items of equipment in that group. Treat this
as a very simple “model” in which our predictions are simply the average sale
price of the item’s group.
5. After looping through all of the columns and all the possible levels for each, pick
the split point that gave the best predictions using that simple model.
6. We now have two groups for our data, based on this selected split. Treat each
group as a separate dataset, and find the best split for each by going back to step 1
for each group.
7. Continue this process recursively, until you have reached some stopping criterion
for each group—for instance, stop splitting a group further when it has only 20
items in it."|decision trees; tabular data for models; training
"this at the start of 2020, things are just starting to change, but it’s likely to take a while.
So be careful: most people you speak to will probably greatly underestimate what you
can do in deep learning with few resources, because they probably won’t deeply
understand how to use pretrained models.
Using a pretrained model for a task different from what it was originally trained for is
known as <i>transfer</i> <i>learning.</i> Unfortunately, because transfer learning is so under-
studied, few domains have pretrained models available. For instance, few pretrained
models are currently available in medicine, making transfer learning challenging to
use in that domain. In addition, it is not yet well understood how to use transfer
learning for tasks such as time series analysis.
<b>Jargon:TransferLearning</b>
Using a pretrained model for a task different from what it was orig‐
inally trained for.
The sixth line of our code tells fastai how to <i>fit</i> the model:
learn.fine_tune(1)
As we’ve discussed, the architecture only describes a <i>template</i> for a mathematical
function; it doesn’t actually do anything until we provide values for the millions of
parameters it contains.
This is the key to deep learning—determining how to fit the parameters of a model to
get it to solve your problem. To fit a model, we have to provide at least one piece of
information: how many times to look at each image (known as number of <i>epochs).</i>
The number of epochs you select will largely depend on how much time you have
available, and how long you find it takes in practice to fit your model. If you select a
number that is too small, you can always train for more epochs later.
But why is the method called fine_tune, and not fit? fastai <i>does</i> have a method
called fit , which does indeed fit a model (i.e., look at images in the training set mul‐
tiple times, each time updating the parameters to make the predictions closer and
closer to the target labels). But in this case, we’ve started with a pretrained model, and
we don’t want to throw away all those capabilities that it already has. As you’ll learn in
this book, there are some important tricks to adapt a pretrained model for a new
dataset—a process called <i>fine-tuning.</i>
<b>Jargon:Fine-Tuning</b>
A transfer learning technique that updates the parameters of a pre‐
trained model by training for additional epochs using a different
task from that used for pretraining."|beginning; deep learning; epochs; fine-tuning models; fine-tuning; fitting models; pretrained model availability; models; notebooks; parameters; architecture requiring many; pretrained models; training; transfer learning; fine-tuning as
"Then we can put everything in a model that we initiate with our tensors w1 , b1 , w2 ,
and b2:
<b>class</b> <b>Model():</b>
<b>def</b> <b>__init__(self,</b> w1, b1, w2, b2):
self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]
self.loss = Mse()
<b>def</b> <b>__call__(self,</b> x, targ):
<b>for</b> l <b>in</b> self.layers: x = l(x)
<b>return</b> self.loss(x, targ)
<b>def</b> backward(self):
self.loss.backward()
<b>for</b> l <b>in</b> reversed(self.layers): l.backward()
What is nice about this refactoring and registering things as layers of our model is
that the forward and backward passes are now really easy to write. If we want to
instantiate our model, we just need to write this:
model = Model(w1, b1, w2, b2)
The forward pass can then be executed as follows:
loss = model(x, y)
And the backward pass with this:
model.backward()
<header><largefont><b>Going</b></largefont> <largefont><b>to</b></largefont> <largefont><b>PyTorch</b></largefont></header>
The Lin, Mse, and Relu classes we wrote have a lot in common, so we could make
them all inherit from the same base class:
<b>class</b> <b>LayerFunction():</b>
<b>def</b> <b>__call__(self,</b> *args):
self.args = args
self.out = self.forward(*args)
<b>return</b> self.out
<b>def</b> forward(self): <b>raise</b> <b>Exception('not</b> implemented')
<b>def</b> bwd(self): <b>raise</b> <b>Exception('not</b> implemented')
<b>def</b> backward(self): self.bwd(self.out, *self.args)
Then we just need to implement forward and bwd in each of our subclasses:
<b>class</b> <b>Relu(LayerFunction):</b>
<b>def</b> forward(self, inp): <b>return</b> inp.clamp_min(0.)
<b>def</b> bwd(self, out, inp): inp.g = (inp>0).float() * out.g"|PyTorch
"Let’s do the same thing for YearMade . Since this is a numeric feature, we’ll need to
draw a histogram, which groups the year values into a few discrete bins:
ax = valid_xs_final['YearMade'].hist()
Other than the special value 1950, which we used for coding missing year values,
most of the data is from after 1990.
Now we’re ready to look at <i>partial</i> <i>dependence</i> <i>plots.</i> Partial dependence plots try to
answer the question: if a row varied on nothing other than the feature in question,
how would it impact the dependent variable?
For instance, how does YearMade impact sale price, all other things being equal? To
YearMade.
answer this question, we can’t just take the average sale price for each The
problem with that approach is that many other things vary from year to year as well,
such as which products are sold, how many products have air-conditioning, inflation,
and so forth. So, merely averaging over all the auctions that have the same YearMade
would also capture the effect of how every other field also changed along with Year
Made and how that overall change affected price.
YearMade
Instead, what we do is replace every single value in the column with 1950,
and then calculate the predicted sale price for every auction, and take the average
over all auctions. Then we do the same for 1951, 1952, and so forth until our final
year of 2011. This isolates the effect of only YearMade (even if it does so by averaging
over some imagined records where we assign a YearMade value that might never
actually exist alongside some other values)."|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"validation set is used to measure the accuracy of the model. By default, the 20% that is
held out is selected randomly. The parameter seed=42 sets the <i>random</i> <i>seed</i> to the
same value every time we run this code, which means we get the same validation set
every time we run it—this way, if we change our model and retrain it, we know that
any differences are due to the changes to the model, not due to having a different ran‐
dom validation set.
fastai will <i>always</i> show you your model’s accuracy using <i>only</i> the validation set, <i>never</i>
the training set. This is absolutely critical, because if you train a large enough model
for a long enough time, it will eventually memorize the label of every item in your
dataset! The result will not be a useful model, because what we care about is how well
our model works on <i>previously</i> <i>unseen</i> <i>images.</i> That is always our goal when creating a
model: for it to be useful on data that the model sees only in the future, after it has
been trained.
Even when your model has not fully memorized all your data, earlier on in training it
may have memorized certain parts of it. As a result, the longer you train for, the bet‐
ter your accuracy will get on the training set; the validation set accuracy will also
improve for a while, but eventually it will start getting worse as the model starts to
memorize the training set rather than finding generalizable underlying patterns in the
data. When this happens, we say that the model is <i>overfitting.</i>
Figure 1-9 shows what happens when you overfit, using a simplified example where
we have just one parameter and some randomly generated data based on the function
x**2. As you see, although the predictions in the overfit model are accurate for data
near the observed data points, they are way off when outside of that range.
<i>Figure</i> <i>1-9.</i> <i>Example</i> <i>of</i> <i>overfitting</i>"|beginning; accuracy with validation set; notebooks; model memorizing training set; training; model memorizing data
"So, here’s our commitment to you. Throughout this book, we follow these principles:
<i>Teaching</i> <i>the</i> <i>whole</i> <i>game</i>
We’ll start off by showing you how to use a complete, working, usable, state-of-
the-art deep learning network to solve real-world problems using simple, expres‐
sive tools. And then we’ll gradually dig deeper and deeper into understanding
how those tools are made, and how the tools that make those tools are made, and
so on…
<i>Always</i> <i>teaching</i> <i>through</i> <i>examples</i>
We’ll ensure that there is a context and a purpose that you can understand intui‐
tively, rather than starting with algebraic symbol manipulation.
<i>Simplifying</i> <i>as</i> <i>much</i> <i>as</i> <i>possible</i>
We’ve spent years building tools and teaching methods that make previously
complex topics simple.
<i>Removing</i> <i>barriers</i>
Deep learning has, until now, been an exclusive game. We’re breaking it open and
ensuring that everyone can play.
The hardest part of deep learning is artisanal: how do you know if you’ve got enough
data, whether it is in the right format, if your model is training properly, and, if it’s
not, what you should do about it? That is why we believe in learning by doing. As
with basic data science skills, with deep learning you get better only through practical
experience. Trying to spend too much time on the theory can be counterproductive.
The key is to just code and try to solve problems: the theory can come later, when you
have context and motivation.
There will be times when the journey feels hard. Times when you feel stuck. Don’t
give up! Rewind through the book to find the last bit where you definitely weren’t
stuck, and then read slowly through from there to find the first thing that isn’t clear.
Then try some code experiments yourself, and Google around for more tutorials on
whatever the issue you’re stuck with is—often you’ll find a different angle on the
material that might help it to click. Also, it’s expected and normal to not understand
everything (especially the code) on first reading. Trying to understand the material
serially before proceeding can sometimes be hard. Sometimes things click into place
after you get more context from parts down the road, from having a bigger picture.
So if you do get stuck on a section, try moving on anyway and make a note to come
back to it later.
Remember, you don’t need any particular academic background to succeed at deep
learning. Many important breakthroughs are made in research and industry by folks
without a PhD, such as the paper “Unsupervised Representation Learning with Deep
Convolutional Generative Adversarial Networks”—one of the most influential papers
of the last decade, with over 5,000 citations—which was written by Alec Radford"|how to learn
"<header><largefont><b>Decoupled</b></largefont> <largefont><b>Weight</b></largefont> <largefont><b>Decay</b></largefont></header>
Weight decay, which we’ve discussed in Chapter 8, is equivalent to (in the case of
vanilla SGD) updating the parameters with the following:
new_weight = weight - lr*weight.grad - lr*wd*weight
The last part of that formula explains the name of this technique: each weight is
decayed by a factor of lr * wd.
The other name for weight decay is <i>L2</i> <i>regularization,</i> which consists of adding the
sum of all squared weights to the loss (multiplied by the weight decay). As we saw in
Chapter 8, this can be directly expressed on the gradients:
weight.grad += wd*weight
For SGD, those two formulas are equivalent. However, this equivalence holds only for
standard SGD because, as we’ve seen with momentum, RMSProp, or in Adam, the
update has some additional formulas around the gradient.
Most libraries use the second formulation, but it was pointed out in “Decoupled
Weight Decay Regularization” by Ilya Loshchilov and Frank Hutter that the first one
is the only correct approach with the Adam optimizer or momentum, which is why
fastai makes it its default.
learn.fit_one_cycle!
Now you know everything that is hidden behind the line
Optimizers are only one part of the training process, however. When you need to
change the training loop with fastai, you can’t directly change the code inside the
library. Instead, we have designed a system of callbacks to let you write any tweaks
you like in independent blocks that you can then mix and match.
<header><largefont><b>Callbacks</b></largefont></header>
Sometimes you need to change how things work a little bit. In fact, we have already
seen examples of this: Mixup, fp16 training, resetting the model after each epoch for
training RNNs, and so forth. How do we go about making these kinds of tweaks to
the training process?
We’ve seen the basic training loop, which, with the help of the Optimizer class, looks
like this for a single epoch:
<b>for</b> xb,yb <b>in</b> dl:
loss = loss_func(model(xb), yb)
loss.backward()
opt.step()
opt.zero_grad()
Figure 16-3 shows how to picture that."|callbacks; training; decoupled weight decay; weights
"that we need to act to make digital signatures de rigueur as a means of authentication
of digital content.”
While we can’t hope to discuss all the ethical issues that deep learning, and algorithms
more generally, bring up, hopefully this brief introduction has been a useful starting
point you can build on. We’ll now move on to the questions of how to identify ethical
issues and what to do about them.
<header><largefont><b>Identifying</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Addressing</b></largefont> <largefont><b>Ethical</b></largefont> <largefont><b>Issues</b></largefont></header>
Mistakes happen. Finding out about them, and dealing with them, needs to be part of
the design of any system that includes machine learning (and many other systems
too). The issues raised within data ethics are often complex and interdisciplinary, but
it is crucial that we work to address them.
So what can we do? This is a big topic, but here are a few steps toward addressing
ethical issues:
• Analyze a project you are working on.
• Implement processes at your company to find and address ethical risks.
• Support good policy.
• Increase diversity.
Let’s walk through each step, starting with analyzing a project you are working on.
<header><largefont><b>Analyze</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Project</b></largefont> <largefont><b>You</b></largefont> <largefont><b>Are</b></largefont> <largefont><b>Working</b></largefont> <largefont><b>On</b></largefont></header>
It’s easy to miss important issues when considering ethical implications of your work.
One thing that helps enormously is simply asking the right questions. Rachel Thomas
recommends considering the following questions throughout the development of a
data project:
• Should we even be doing this?
• What bias is in the data?
• Can the code and data be audited?
• What are the error rates for different subgroups?
• What is the accuracy of a simple rule-based alternative?
• What processes are in place to handle appeals or mistakes?
• How diverse is the team that built it?"|addressing ethical issues; identifying ethical issues; Thomas
"<b>cols</b> <b>imp</b>
<b>69</b>
YearMade 0.182890
<b>6</b> ProductSize 0.127268
<b>30</b> Coupler_System 0.117698
<b>7</b> fiProductClassDesc 0.069939
<b>66</b> ModelID 0.057263
<b>77</b> saleElapsed 0.050113
<b>32</b>
Hydraulics_Flow 0.047091
<b>3</b>
fiSecondaryDesc 0.041225
<b>31</b> Grouser_Tracks 0.031988
<b>1</b> fiModelDesc 0.031838
A plot of the feature importances shows the relative importances more clearly:
<b>def</b> plot_fi(fi):
<b>return</b> fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)
plot_fi(fi[:30]);
The way these importances are calculated is quite simple yet elegant. The feature
importance algorithm loops through each tree, and then recursively explores each
branch. At each branch, it looks to see what feature was used for that split, and how
much the model improves as a result of that split. The improvement (weighted by the
number of rows in that group) is added to the importance score for that feature. This
is summed across all branches of all trees, and finally the scores are normalized such
that they add to 1."|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"With this, we can remove another for loop in our matrix multiplication function.
Now, instead of multiplying a[i] with b[:,j], we can multiply a[i] with the whole
matrix b using broadcasting, and then sum the results:
<b>def</b> matmul(a,b):
ar,ac = a.shape
br,bc = b.shape
<b>assert</b> ac==br
c = torch.zeros(ar, bc)
<b>for</b> i <b>in</b> range(ar):
<i>#</i> <i>c[i,j]</i> <i>=</i> <i>(a[i,:]</i> <i>*</i> <i>b[:,j]).sum()</i> <i>#</i> <i>previous</i>
c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0)
<b>return</b> c
%timeit -n 20 t4 = matmul(m1,m2)
357 µs ± 7.2 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)
We’re now 3,700 times faster than our first implementation! Before we move on, let’s
discuss the rules of broadcasting in a little more detail.
<b>Broadcastingrules</b>
When operating on two tensors, PyTorch compares their shapes elementwise. It starts
with the <i>trailing</i> <i>dimensions</i> and works its way backward, adding 1 when it meets
empty dimensions. Two dimensions are <i>compatible</i> when one of the following is true:
• They are equal.
• One of them is 1, in which case that dimension is broadcast to make it the same
as the other.
Arrays do not need to have the same number of dimensions. For example, if you have
a 256×256×3 array of RGB values, and you want to scale each color in the image by a
different value, you can multiply the image by a one-dimensional array with three
values. Lining up the sizes of the trailing axes of these arrays according to the broad‐
cast rules shows that they are compatible:
Image (3d tensor): 256 x 256 x 3
Scale (1d tensor): (1) (1) 3
Result (3d tensor): 256 x 256 x 3
However, a 2D tensor of size 256×256 isn’t compatible with our image:
Image (3d tensor): 256 x 256 x 3
Scale (1d tensor): (1) 256 x 256
Error"|vector to matrix; neural networks; building layer from scratch; broadcasting vector to matrix
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>4</b></largefont></header>
<header><largefont><b>Under</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Hood:</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Digit</b></largefont> <largefont><b>Classifier</b></largefont></header>
Having seen what it looks like to train a variety of models in Chapter 2, let’s now look
under the hood and see exactly what is going on. We’ll start by using computer vision
to introduce fundamental tools and concepts for deep learning.
To be exact, we’ll discuss the roles of arrays and tensors and of broadcasting, a power‐
ful technique for using them expressively. We’ll explain stochastic gradient descent
(SGD), the mechanism for learning by updating weights automatically. We’ll discuss
the choice of a loss function for our basic classification task, and the role of mini-
batches. We’ll also describe the math that a basic neural network is doing. Finally,
we’ll put all these pieces together.
In future chapters, we’ll do deep dives into other applications as well, and see how
these concepts and tools generalize. But this chapter is about laying foundation
stones. To be frank, that also makes this one of the hardest chapters, because of how
these concepts all depend on each other. Like an arch, all the stones need to be in
place for the structure to stay up. Also like an arch, once that happens, it’s a powerful
structure that can support other things. But it requires some patience to assemble.
Let’s begin. The first step is to consider how images are represented in a computer.
<header><largefont><b>Pixels:</b></largefont> <largefont><b>The</b></largefont> <largefont><b>Foundations</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Computer</b></largefont> <largefont><b>Vision</b></largefont></header>
To understand what happens in a computer vision model, we first have to understand
how computers handle images. We’ll use one of the most famous datasets in com‐
puter vision, MNIST, for our experiments. MNIST contains images of handwritten
digits, collected by the National Institute of Standards and Technology and collated
into a machine learning dataset by Yann Lecun and his colleagues. Lecun used
MNIST in 1998 in LeNet-5, the first computer system to demonstrate practically use‐"|pixels as foundation; handwritten digits; MNIST handwritten digits dataset; handwritten digits dataset; Lecun; National Institute of Standards and Technology; pixels
"right, yet there are societal impacts to widespread surveillance (which would still be
the case even if it was possible for a few individuals to opt out).
Many of the issues we are seeing in tech are human rights issues, such as when a
biased algorithm recommends that Black defendants have longer prison sentences,
when particular job ads are shown only to young people, or when police use facial
recognition to identify protesters. The appropriate venue to address human rights
issues is typically through the law.
We need both regulatory and legal changes, <i>and</i> the ethical behavior of individuals.
Individual behavior change can’t address misaligned profit incentives, externalities
(where corporations reap large profits while offloading their costs and harms to the
broader society), or systemic failures. However, the law will never cover all edge cases,
and it is important that individual software developers and data scientists are equip‐
ped to make ethical decisions in practice.
<header><largefont><b>Cars:</b></largefont> <largefont><b>A</b></largefont> <largefont><b>Historical</b></largefont> <largefont><b>Precedent</b></largefont></header>
The problems we are facing are complex, and there are no simple solutions. This can
be discouraging, but we find hope in considering other large challenges that people
have tackled throughout history. One example is the movement to increase car safety,
covered as a case study in “Datasheets for Datasets” by Timnit Gebru et al. and in the
design podcast 99% Invisible. Early cars had no seatbelts, metal knobs on the dash‐
board that could lodge in people’s skulls during a crash, regular plate glass windows
that shattered in dangerous ways, and noncollapsible steering columns that impaled
drivers. However, car companies were resistant to even discussing safety as something
they could help address, and the widespread belief was that cars are just the way they
are, and that it was the people using them who caused problems.
It took consumer safety activists and advocates decades of work to change the
national conversation to consider that perhaps car companies had some responsibility
that should be addressed through regulation. When the collapsible steering column
was invented, it was not implemented for several years as there was no financial
incentive to do so. Major car company General Motors hired private detectives to try
to dig up dirt on consumer safety advocate Ralph Nader. The requirement of seat‐
belts, crash test dummies, and collapsible steering columns were major victories. It
was only in 2011 that car companies were required to start using crash test dummies
that would represent the average woman, and not just average men’s bodies; prior to
this, women were 40% more likely to be injured in a car crash of the same impact
compared to a man. This is a vivid example of the ways that bias, policy, and technol‐
ogy have important consequences."|car safety for ethics inspiration; crash test dummies and gender; ethics; car safety inspiration; Gebru; crash test dummies; Nader; policy’s role in ethics
"We can see that this dataset provides us with <i>images</i> and <i>annotations</i> directories. The
website for the dataset tells us that the <i>annotations</i> directory contains information
about where the pets are rather than what they are. In this chapter, we will be doing
classification, not localization, which is to say that we care about what the pets are,
not where they are. Therefore, we will ignore the <i>annotations</i> directory for now. So,
let’s have a look inside the <i>images</i> directory:
(path/""images"").ls()
(#7394) [Path('images/great_pyrenees_173.jpg'),Path('images/wheaten_terrier_46.j
> pg'),Path('images/Ragdoll_262.jpg'),Path('images/german_shorthaired_3.jpg'),P
> ath('images/american_bulldog_196.jpg'),Path('images/boxer_188.jpg'),Path('ima
> ges/staffordshire_bull_terrier_173.jpg'),Path('images/basset_hound_71.jpg'),P
> ath('images/staffordshire_bull_terrier_37.jpg'),Path('images/yorkshire_terrie
> r_18.jpg')...]
Most functions and methods in fastai that return a collection use a class called L. This
class can be thought of as an enhanced version of the ordinary Python list type,
with added conveniences for common operations. For instance, when we display an
object of this class in a notebook, it appears in the format shown here. The first thing
that is shown is the number of items in the collection, prefixed with a #. You’ll also
see in the preceding output that the list is suffixed with an ellipsis. This means that
only the first few items are displayed—which is a good thing, because we would not
want more than 7,000 filenames on our screen!
By examining these filenames, we can see how they appear to be structured. Each file‐
name contains the pet breed, then an underscore (_), a number, and finally the file
extension. We need to create a piece of code that extracts the breed from a single
Path . Jupyter notebooks make this easy, because we can gradually build up something
that works, and then use it for the entire dataset. We do have to be careful to not
make too many assumptions at this point. For instance, if you look carefully, you may
notice that some of the pet breeds contain multiple words, so we cannot simply break
at the first _ character that we find. To allow us to test our code, let’s pick out one of
these filenames:
fname = (path/""images"").ls()[0]
The most powerful and flexible way to extract information from strings like this is to
use a <i>regular</i> <i>expression,</i> also known as a <i>regex.</i> A regular expression is a special string,
written in the regular expression language, which specifies a general rule for deciding
whether another string passes a test (i.e., “matches” the regular expression), and also
possibly for plucking a particular part or parts out of that other string. In this case, we
need a regular expression that extracts the pet breed from the filename.
We do not have the space to give you a complete regular expression tutorial here, but
many excellent ones are online, and we know that many of you will already be famil‐
iar with this wonderful tool. If you’re not, that is totally fine—this is a great"|filename extraction; L class returning collections; labels; pet breeds dataset; list type as fastai L class; regular expressions (regex)
"We can apply this function to a single column of activations from a neural network
and get back a column of numbers between 0 and 1, so it’s a very useful activation
function for our final layer.
Now think about what happens if we want to have more categories in our target (such
as our 37 pet breeds). That means we’ll need more activations than just a single col‐
umn: we need an activation <i>per</i> <i>category.</i> We can create, for instance, a neural net that
predicts 3s and 7s that returns two activations, one for each class—this will be a good
first step toward creating the more general approach. Let’s just use some random
numbers with a standard deviation of 2 (so we multiply randn by 2) for this example,
assuming we have six images and two possible categories (where the first column rep‐
resents 3s and the second is 7s):
acts = torch.randn((6,2))*2
acts
tensor([[ 0.6734, 0.2576],
[ 0.4689, 0.4607],
[-2.2457, -0.3727],
[ 4.4164, -1.2760],
[ 0.9233, 0.5347],
[ 1.0698, 1.6187]])
We can’t just take the sigmoid of this directly, since we don’t get rows that add to 1
(we want the probability of being a 3 plus the probability of being a 7 to add up to 1):
acts.sigmoid()
tensor([[0.6623, 0.5641],
[0.6151, 0.6132],
[0.0957, 0.4079],
[0.9881, 0.2182],
[0.7157, 0.6306],
[0.7446, 0.8346]])
In Chapter 4, our neural net created a single activation per image, which we passed
through the sigmoid function. That single activation represented the model’s"|binary problems; cross-entropy loss; pet breeds image classifier
"<header><largefont><b>The</b></largefont> <largefont><b>Model</b></largefont></header>
We can save some time by using PyTorch’s RNN class, which implements exactly what
we created earlier, but also gives us the option to stack multiple RNNs, as we have
discussed:
<b>class</b> <b>LMModel5(Module):</b>
<b>def</b> <b>__init__(self,</b> vocab_sz, n_hidden, n_layers):
self.i_h = nn.Embedding(vocab_sz, n_hidden)
self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True)
self.h_o = nn.Linear(n_hidden, vocab_sz)
self.h = torch.zeros(n_layers, bs, n_hidden)
<b>def</b> forward(self, x):
res,h = self.rnn(self.i_h(x), self.h)
self.h = h.detach()
<b>return</b> self.h_o(res)
<b>def</b> reset(self): self.h.zero_()
learn = Learner(dls, LMModel5(len(vocab), 64, 2),
loss_func=CrossEntropyLossFlat(),
metrics=accuracy, cbs=ModelResetter)
learn.fit_one_cycle(15, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 3.055853 2.591640 0.437907 00:01
1 2.162359 1.787310 0.471598 00:01
2 1.710663 1.941807 0.321777 00:01
3 1.520783 1.999726 0.312012 00:01
4 1.330846 2.012902 0.413249 00:01
5 1.163297 1.896192 0.450684 00:01
6 1.033813 2.005209 0.434814 00:01
7 0.919090 2.047083 0.456706 00:01
8 0.822939 2.068031 0.468831 00:01
9 0.750180 2.136064 0.475098 00:01
10 0.695120 2.139140 0.485433 00:01
11 0.655752 2.155081 0.493652 00:01
12 0.629650 2.162583 0.498535 00:01
13 0.613583 2.171649 0.491048 00:01
14 0.604309 2.180355 0.487874 00:01
Now that’s disappointing…our previous single-layer RNN performed better. Why?
The reason is that we have a deeper model, leading to exploding or vanishing
activations."|multilayer RNNs
"Why is that useful? 1×1 convolutions are much faster, so even if this seems to be a
more complex design, this block executes faster than the first ResNet block we saw.
This then lets us use more filters: as we see in the illustration, the number of filters in
and out is four times higher (256 instead of 64). The 1×1 convs diminish then restore
the number of channels (hence the name <i>bottleneck).</i> The overall impact is that we
can use more filters in the same amount of time.
Let’s try replacing our ResBlock with this bottleneck design:
<b>def</b> _conv_block(ni,nf,stride):
<b>return</b> nn.Sequential(
ConvLayer(ni, nf//4, 1),
ConvLayer(nf//4, nf//4, stride=stride),
ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero))
We’ll use this to create a ResNet-50 with group sizes of (3,4,6,3) . We now need to
pass 4 into the expansion parameter of ResNet, since we need to start with four times
fewer channels and we’ll end with four times more channels.
Deeper networks like this don’t generally show improvements when training for only
5 epochs, so we’ll bump it up to 20 epochs this time to make the most of our bigger
model. And to really get great results, let’s use bigger images too:
dls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224)
We don’t have to do anything to account for the larger 224-pixel images; thanks to
our fully convolutional network, it just works. This is also why we were able to do
<i>progressive</i> <i>resizing</i> earlier in the book—the models we used were fully convolutional,
so we were even able to fine-tune models trained with different sizes. We can now
train our model and see the effects:
rn = ResNet(dls.c, [3,4,6,3], 4)
learn = get_learner(rn)
learn.fit_one_cycle(20, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.613448 1.473355 0.514140 00:31
1 1.359604 2.050794 0.397452 00:31
2 1.253112 4.511735 0.387006 00:31
3 1.133450 2.575221 0.396178 00:31
4 1.054752 1.264525 0.613758 00:32
5 0.927930 2.670484 0.422675 00:32
6 0.838268 1.724588 0.528662 00:32
7 0.748289 1.180668 0.666497 00:31
8 0.688637 1.245039 0.650446 00:32
9 0.645530 1.053691 0.674904 00:31"|ResNet architecture; building state-of-the-art ResNet
"and stride-1 convolutions are useful for adding layers without changing the output
size.
<i>Figure</i> <i>13-6.</i> <i>A</i> <i>3×3</i> <i>kernel</i> <i>with</i> <i>5×5</i> <i>input,</i> <i>stride-2</i> <i>convolution,</i> <i>and</i> <i>1</i> <i>pixel</i> <i>of</i> <i>padding</i>
<i>(courtesy</i> <i>of</i> <i>Vincent</i> <i>Dumoulin</i> <i>and</i> <i>Francesco</i> <i>Visin)</i>
h w,
In an image of size by using a padding of 1 and a stride of 2 will give us a result of
size (h+1)//2 by (w+1)//2 . The general formula for each dimension is
(n + 2*pad - ks) // stride + 1
where pad is the padding, ks is the size of our kernel, and stride is the stride.
Let’s now take a look at how the pixel values of the result of our convolutions are
computed.
<header><largefont><b>Understanding</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Convolution</b></largefont> <largefont><b>Equations</b></largefont></header>
To explain the math behind convolutions, fast.ai student Matt Kleinsmith came up
with the very clever idea of showing CNNs from different viewpoints. In fact, it’s so
clever, and so helpful, we’re going to show it here too!
Here’s our 3×3-pixel image, with each pixel labeled with a letter:
And here’s our kernel, with each weight labeled with a Greek letter:
Since the filter fits in the image four times, we have four results:"|convolutional neural network (CNN); stride-1 convolutions
"Pipeline: partial -> Categorize
starting from
/home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg
applying partial gives
american_bulldog
applying Categorize gives
TensorCategory(12)
Final sample: (PILImage mode=RGB size=375x500, TensorCategory(12))
Setting up after_item: Pipeline: ToTensor
Setting up before_batch: Pipeline:
Setting up after_batch: Pipeline: IntToFloatTensor
Building one batch
Applying item_tfms to the first sample:
Pipeline: ToTensor
starting from
(PILImage mode=RGB size=375x500, TensorCategory(12))
applying ToTensor gives
(TensorImage of size 3x500x375, TensorCategory(12))
Adding the next 3 samples
No before_batch transform to apply
Collating items in a batch
Error! It's not possible to collate your items in a batch
Could not collate the 0-th members of your tuples because got the following
shapes:
torch.Size([3, 500, 375]),torch.Size([3, 375, 500]),torch.Size([3, 333, 500]),
torch.Size([3, 375, 500])
You can see exactly how we gathered the data and split it, how we went from a file‐
name to a <i>sample</i> (the tuple (image, category)), then what item transforms were
applied and how it failed to collate those samples in a batch (because of the different
shapes).
Once you think your data looks right, we generally recommend the next step should
be using it to train a simple model. We often see people put off the training of an
actual model for far too long. As a result, they don’t find out what their baseline
results look like. Perhaps your problem doesn’t require lots of fancy domain-specific
engineering. Or perhaps the data doesn’t seem to train the model at all. These are
things that you want to know as soon as possible."|begin with simple baseline model; baseline simple model; labels; models; training
"As you see, this returns the dependent and independent variables, as a mini-batch.
Let’s see what is contained in our dependent variable:
y
TensorCategory([11, 0, 0, 5, 20, 4, 22, 31, 23, 10, 20, 2, 3, 27, 18, 23,
> 33, 5, 24, 7, 6, 12, 9, 11, 35, 14, 10, 15, 3, 3, 21, 5, 19, 14, 12,
> 15, 27, 1, 17, 10, 7, 6, 15, 23, 36, 1, 35, 6,
4, 29, 24, 32, 2, 14, 26, 25, 21, 0, 29, 31, 18, 7, 7, 17],
> device='cuda:5')
Our batch size is 64, so we have 64 rows in this tensor. Each row is a single integer
between 0 and 36, representing our 37 possible pet breeds. We can view the predic‐
tions (the activations of the final layer of our neural network) by using
Learner.get_preds. This function takes either a dataset index (0 for train and 1 for
valid) or an iterator of batches. Thus, we can pass it a simple list with our batch to get
our predictions. It returns predictions and targets by default, but since we already
have the targets, we can effectively ignore them by assigning to the special variable _:
preds,_ = learn.get_preds(dl=[(x,y)])
preds[0]
tensor([7.9069e-04, 6.2350e-05, 3.7607e-05, 2.9260e-06, 1.3032e-05, 2.5760e-05,
> 6.2341e-08, 3.6400e-07, 4.1311e-06, 1.3310e-04, 2.3090e-03, 9.9281e-01,
> 4.6494e-05, 6.4266e-07, 1.9780e-06, 5.7005e-07,
3.3448e-06, 3.5691e-03, 3.4385e-06, 1.1578e-05, 1.5916e-06, 8.5567e-08,
> 5.0773e-08, 2.2978e-06, 1.4150e-06, 3.5459e-07, 1.4599e-04, 5.6198e-08,
> 3.4108e-07, 2.0813e-06, 8.0568e-07, 4.3381e-07,
1.0069e-05, 9.1020e-07, 4.8714e-06, 1.2734e-06, 2.4735e-06])
The actual predictions are 37 probabilities between 0 and 1, which add up to 1 in
total:
len(preds[0]),preds[0].sum()
(37, tensor(1.0000))
To transform the activations of our model into predictions like this, we used some‐
thing called the <i>softmax</i> activation function.
<header><largefont><b>Softmax</b></largefont></header>
In our classification model, we use the softmax activation function in the final layer
to ensure that the activations are all between 0 and 1, and that they sum to 1.
Softmax is similar to the sigmoid function, which we saw earlier. As a reminder, sig‐
moid looks like this:
plot_function(torch.sigmoid, min=-4,max=4)"|softmax as activation function; transforming into predictions; cross-entropy loss; dependent variable; get_preds function; image classifier model training; activations into predictions; softmax activation function; independent variable; layers; pet breeds image classifier; dependent and independent variables; predictions; activations transformed into; sigmoid function; viewing as mini-batch
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>16</b></largefont></header>
<header><largefont><b>The</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Process</b></largefont></header>
You now know how to create state-of-the-art architectures for computer vision, natu‐
ral image processing, tabular analysis, and collaborative filtering, and you know how
to train them quickly. So we’re done, right? Not quite yet. We still have to explore a
little bit more of the training process.
We explained in Chapter 4 the basis of stochastic gradient descent: pass a mini-batch
to the model, compare it to our target with the loss function, then compute the gradi‐
ents of this loss function with regard to each weight before updating the weights with
the formula:
new_weight = weight - lr * weight.grad
We implemented this from scratch in a training loop, and saw that PyTorch provides
nn.SGD
a simple class that does this calculation for each parameter for us. In this
chapter, we will build some faster optimizers, using a flexible foundation. But that’s
not all we might want to change in the training process. For any tweak of the training
loop, we will need a way to add some code to the basis of SGD. The fastai library has a
system of callbacks to do this, and we will teach you all about it.
Let’s start with standard SGD to get a baseline; then we will introduce the most com‐
monly used optimizers.
<header><largefont><b>Establishing</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Baseline</b></largefont></header>
First we’ll create a baseline using plain SGD and compare it to fastai’s default opti‐
mizer. We’ll start by grabbing Imagenette with the same get_data we used in
Chapter 14:
dls = get_data(URLs.IMAGENETTE_160, 160, 128)"|SGD class; stochastic gradient descent (SGD); training
"But picking a learning rate that’s too high is even worse—it can result in the loss get‐
ting <i>worse,</i> as we see in Figure 4-3!
<i>Figure</i> <i>4-3.</i> <i>Gradient</i> <i>descent</i> <i>with</i> <i>high</i> <i>LR</i>
If the learning rate is too high, it may also “bounce” around, rather than diverging;
Figure 4-4 shows how this results in taking many steps to train successfully.
<i>Figure</i> <i>4-4.</i> <i>Gradient</i> <i>descent</i> <i>with</i> <i>bouncy</i> <i>LR</i>
Now let’s apply all of this in an end-to-end example.
<header><largefont><b>An</b></largefont> <largefont><b>End-to-End</b></largefont> <largefont><b>SGD</b></largefont> <largefont><b>Example</b></largefont></header>
We’ve seen how to use gradients to minimize our loss. Now it’s time to look at an SGD
example and see how finding a minimum can be used to train a model to fit data
better.
Let’s start with a simple, synthetic example model. Imagine you were measuring the
speed of a roller coaster as it went over the top of a hump. It would start fast, and then
get slower as it went up the hill; it would be slowest at the top, and it would then"|learning rate (LR); stepping with learning rate; stochastic gradient descent (SGD); training; weights
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.677074 1.827367 0.467548 00:02
1 1.282722 1.870913 0.388942 00:02
2 1.090705 1.651793 0.462500 00:02
3 1.005092 1.613794 0.516587 00:02
4 0.965975 1.560775 0.551202 00:02
5 0.916182 1.595857 0.560577 00:02
6 0.897657 1.539733 0.574279 00:02
7 0.836274 1.585141 0.583173 00:02
8 0.805877 1.629808 0.586779 00:02
9 0.795096 1.651267 0.588942 00:02
This is already better! The next step is to use more targets and compare them to the
intermediate predictions.
<header><largefont><b>Creating</b></largefont> <largefont><b>More</b></largefont> <largefont><b>Signal</b></largefont></header>
Another problem with our current approach is that we predict only one output word
for each three input words. As a result, the amount of signal that we are feeding back
to update weights with is not as large as it could be. It would be better if we predicted
the next word after every single word, rather than every three words, as shown in
Figure 12-5.
<i>Figure</i> <i>12-5.</i> <i>RNN</i> <i>predicting</i> <i>after</i> <i>every</i> <i>token</i>"|creating more signal; improved RNN
"We’re using **kwargs in EmbeddingNN to avoid having to write all the arguments to
TabularModel a second time, and keep them in sync. However, this makes our API
quite difficult to work with, because now Jupyter Notebook doesn’t know what
parameters are available. Consequently, things like tab completion of parameter
names and pop-up lists of signatures won’t work.
fastai resolves this by providing a special @delegates decorator, which automatically
changes the signature of the class or function ( EmbeddingNN in this case) to insert all
of its keyword arguments into the signature.
Although the results of EmbeddingNN are a bit worse than the dot product approach
(which shows the power of carefully constructing an architecture for a domain), it
does allow us to do something very important: we can now directly incorporate other
user and movie information, date and time information, or any other information
that may be relevant to the recommendation. That’s exactly what TabularModel does.
In fact, we’ve now seen that EmbeddingNN is just a TabularModel , with n_cont=0 and
out_sz=1. So, we’d better spend some time learning about TabularModel, and how to
use it to get great results! We’ll do that in the next chapter.
<header><largefont><b>Conclusion</b></largefont></header>
For our first non–computer vision application, we looked at recommendation sys‐
tems and saw how gradient descent can learn intrinsic factors or biases about items
from a history of ratings. Those can then give us information about the data.
We also built our first model in PyTorch. We will do a lot more of this in the next
section of the book, but first, let’s finish our dive into the other general applications of
deep learning, continuing with tabular data.
<header><largefont><b>Questionnaire</b></largefont></header>
1. What problem does collaborative filtering solve?
2. How does it solve it?
3. Why might a collaborative filtering predictive model fail to be a very useful rec‐
ommendation system?
4. What does a crosstab representation of collaborative filtering data look like?
5. Write the code to create a crosstab representation of the MovieLens data (you
might need to do some web searching!).
6. What is a latent factor? Why is it “latent”?"|built from scratch; delegates; embedding from scratch; signature of function
"With a large beta , we might miss that the gradients have changed directions and roll
over a small local minima. This is a desired side effect: intuitively, when we show a
new input to our model, it will look like something in the training set but won’t be
<i>exactly</i> like it. It will correspond to a point in the loss function that is close to the
minimum we ended up with at the end of training, but not exactly <i>at</i> that minimum.
So, we would rather end up training in a wide minimum, where nearby points have
approximately the same loss (or if you prefer, a point where the loss is as flat as possi‐
ble). Figure 16-2 shows how the chart in Figure 16-1 varies as we change beta.
<i>Figure</i> <i>16-2.</i> <i>Momentum</i> <i>with</i> <i>different</i> <i>beta</i> <i>values</i>
beta
We can see in these examples that a that’s too high results in the overall changes
in gradient getting ignored. In SGD with momentum, a value of beta that is often
used is 0.9.
fit_one_cycle beta
by default starts with a of 0.95, gradually adjusts it to 0.85, and
then gradually moves it back to 0.95 at the end of training. Let’s see how our training
goes with momentum added to plain SGD.
To add momentum to our optimizer, we’ll first need to keep track of the moving aver‐
age gradient, which we can do with another callback. When an optimizer callback
returns a dict, it is used to update the state of the optimizer and is passed back to the
optimizer on the next step. So this callback will keep track of the gradient averages in
a parameter called grad_avg:"|momentum in SGD; stochastic gradient descent (SGD); training
"When we multiply two vectors together and add up the results, this is known as the
<i>dot</i> <i>product.</i> It is used a lot in machine learning and forms the basis of matrix multi‐
plication. We will be looking a lot more at matrix multiplication and dot products in
Chapter 17.
<b>Jargon:DotProduct</b>
The mathematical operation of multiplying the elements of two
vectors together, and then summing up the result.
On the other hand, we might represent the movie <i>Casablanca</i> as follows:
casablanca = np.array([-0.99,-0.3,0.8])
The match between this combination is shown here:
(user1*casablanca).sum()
-1.611
Since we don’t know what the latent factors are, and we don’t know how to score
them for each user and movie, we should learn them.
<header><largefont><b>Learning</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Latent</b></largefont> <largefont><b>Factors</b></largefont></header>
There is surprisingly little difference between specifying the structure of a model, as
we did in the preceding section, and learning one, since we can just use our general
gradient descent approach.
Step 1 of this approach is to randomly initialize some parameters. These parameters
will be a set of latent factors for each user and movie. We will have to decide how
many to use. We will discuss how to select this shortly, but for illustrative purposes,
let’s use 5 for now. Because each user will have a set of these factors, and each movie
will have a set of these factors, we can show these randomly initialized values right
next to the users and movies in our crosstab, and we can then fill in the dot products
for each of these combinations in the middle. For example, Figure 8-2 shows what it
looks like in Microsoft Excel, with the top-left cell formula displayed as an example.
Step 2 of this approach is to calculate our predictions. As we’ve discussed, we can do
this by simply taking the dot product of each movie with each user. If, for instance,
the first latent user factor represents how much the user likes action movies and the
first latent movie factor represents whether the movie has a lot of action or not, the
product of those will be particularly high if either the user likes action movies and the
movie has a lot of action in it, or the user doesn’t like action movies and the movie
doesn’t have any action in it. On the other hand, if we have a mismatch (a user loves"|learning latent factors; dot product of vectors; vector dot product
"These questions may be able to help you identify outstanding issues, and possible
alternatives that are easier to understand and control. In addition to asking the right
questions, it’s also important to consider practices and processes to implement.
One thing to consider at this stage is what data you are collecting and storing. Data
often ends up being used for different purposes the original intent. For instance, IBM
began selling to Nazi Germany well before the Holocaust, including helping with
Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying
far more Jewish people than had previously been recognized in Germany. Similarly,
US census data was used to round up Japanese-Americans (who were US citizens) for
internment during World War II. It is important to recognize how data and images
collected can be weaponized later. Columbia professor Tim Wu wrote “You must
assume that any personal data that Facebook or Android keeps are data that govern‐
ments around the world will try to get or that thieves will try to steal.”
<header><largefont><b>Processes</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Implement</b></largefont></header>
The Markkula Center has released An Ethical Toolkit for Engineering/Design Prac‐
tice that includes concrete practices to implement at your company, including regu‐
larly scheduled sweeps to proactively search for ethical risks (in a manner similar to
cybersecurity penetration testing), expanding the ethical circle to include the perspec‐
tives of a variety of stakeholders, and considering the terrible people (how could bad
actors abuse, steal, misinterpret, hack, destroy, or weaponize what you are building?).
Even if you don’t have a diverse team, you can still try to proactively include the per‐
spectives of a wider group, considering questions such as these (provided by the
Markkula Center):
• Whose interests, desires, skills, experiences, and values have we simply assumed,
rather than actually consulted?
• Who are all the stakeholders who will be directly affected by our product? How
have their interests been protected? How do we know what their inter‐
ests really are—have we asked?
• Who/which groups and individuals will be indirectly affected in significant ways?
• Who might use this product that we didn’t expect to use it, or for purposes we
didn’t initially intend?
<b>Ethicallenses</b>
Another useful resource from the Markkula Center is its Conceptual Frameworks in
Technology and Engineering Practice. This considers how different foundational eth‐
ical lenses can help identify concrete issues, and lays out the following approaches
and key questions:"|census data weaponization; ethics; processes to implement; IBM and Nazi Germany; Hitler; Nazi Germany and IBM; web resources
"learn.activation_stats.plot_layer_stats(0)
Generally our model should have a consistent, or at least smooth, mean and standard
deviation of layer activations during training. Activations near zero are particularly
problematic, because it means we have computation in the model that’s doing noth‐
ing at all (since multiplying by zero gives zero). When you have some zeros in one
layer, they will therefore generally carry over to the next layer…which will then create
more zeros. Here’s the penultimate layer of our network:
learn.activation_stats.plot_layer_stats(-2)
As expected, the problems get worse toward the end of the network, as the instability
and zero activations compound over layers. Let’s look at what we can do to make
training more stable.
<header><largefont><b>Increase</b></largefont> <largefont><b>Batch</b></largefont> <largefont><b>Size</b></largefont></header>
One way to make training more stable is to increase the batch size. Larger batches
have gradients that are more accurate, since they’re calculated from more data. On
the downside, though, a larger batch size means fewer batches per epoch, which
means fewer opportunities for your model to update weights. Let’s see if a batch size
of 512 helps:
dls = get_dls(512)
learn = fit()"|convolutional neural network (CNN); batch size increased; building a CNN; training more stable; training on all digits
"Let’s see if the loss has improved:
preds = f(time,params)
mse(preds, speed)
tensor(5435.5366, grad_fn=<MeanBackward0>)
And take a look at the plot:
show_preds(preds)
We need to repeat this a few times, so we’ll create a function to apply one step:
<b>def</b> apply_step(params, prn=True):
preds = f(time, params)
loss = mse(preds, speed)
loss.backward()
params.data -= lr * params.grad.data
params.grad = None
<b>if</b> prn: <b>print(loss.item())</b>
<b>return</b> preds
<b>Step6:Repeattheprocess</b>
Now we iterate. By looping and performing many improvements, we hope to reach a
good result:
<b>for</b> i <b>in</b> range(10): apply_step(params)
5435.53662109375
1577.4495849609375
847.3780517578125
709.22265625
683.0757446289062
678.12451171875
677.1839599609375
677.0025024414062
676.96435546875
676.9537353515625"|stochastic gradient descent (SGD); training; weights
"color images) and output an image with a different number of channels. As with our
hidden size that represented the numbers of neurons in a linear layer, we can decide
to have as many filters as we want, and each will be able to specialize (some to detect
horizontal edges, others to detect vertical edges, and so forth) to give something like
the examples we studied in Chapter 2.
In one sliding window, we have a certain number of channels and we need as many
filters (we don’t use the same kernel for all the channels). So our kernel doesn’t have a
size of 3×3, but ch_in (for channels in) by 3×3. On each channel, we multiply the ele‐
ments of our window by the elements of the corresponding filter, and then sum the
results (as we saw before) and sum over all the filters. In the example given in
Figure 13-12, the result of our conv layer on that window is red + green + blue.
<i>Figure</i> <i>13-12.</i> <i>Convolution</i> <i>over</i> <i>an</i> <i>RGB</i> <i>image</i>
So, in order to apply a convolution to a color picture, we require a kernel tensor with
a size that matches the first axis. At each location, the corresponding parts of the ker‐
nel and the image patch are multiplied together."|convolutional neural network (CNN); building a CNN
"<b>ReadtheDocs</b>
It’s important to learn about PyTorch functions like this, because
looping over tensors in Python performs at Python speed, not C/
CUDA speed! Try running help(torch.where) now to read the
docs for this function, or, better still, look it up on the PyTorch
documentation site.
Let’s try it on our prds and trgts:
torch.where(trgts==1, 1-prds, prds)
tensor([0.1000, 0.4000, 0.8000])
You can see that this function returns a lower number when predictions are more
accurate, when accurate predictions are more confident (higher absolute values), and
when inaccurate predictions are less confident. In PyTorch, we always assume that a
lower value of a loss function is better. Since we need a scalar for the final loss,
mnist_loss takes the mean of the previous tensor:
mnist_loss(prds,trgts)
tensor(0.4333)
For instance, if we change our prediction for the one “false” target from 0.2 to 0.8,
the loss will go down, indicating that this is a better prediction:
mnist_loss(tensor([0.9, 0.4, 0.8]),trgts)
tensor(0.2333)
One problem with mnist_loss as currently defined is that it assumes that predictions
are always between 0 and 1. We need to ensure, then, that this is actually the case! As
it happens, there is a function that does exactly that—let’s take a look.
<header><largefont><b>Sigmoid</b></largefont></header>
The sigmoid function always outputs a number between 0 and 1. It’s defined as
follows:
<b>def</b> sigmoid(x): <b>return</b> 1/(1+torch.exp(-x))
PyTorch defines an accelerated version for us, so we don’t really need our own. This is
an important function in deep learning, since we often want to ensure that values are
between 0 and 1. This is what it looks like:
plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"|loss; MNIST loss function; numerical digit image classifier; sigmoid function
"As you can see, rather than defining a function in the usual way, we are using Python’s
lambda keyword. This is just a shortcut for defining and then referring to a function.
The following more verbose approach is identical:
<b>def</b> get_x(r): <b>return</b> r['fname']
<b>def</b> get_y(r): <b>return</b> r['labels']
dblock = DataBlock(get_x = get_x, get_y = get_y)
dsets = dblock.datasets(df)
dsets.train[0]
('002549.jpg', 'tvmonitor')
Lambda functions are great for quickly iterating, but they are not compatible with
serialization, so we advise you to use the more verbose approach if you want to export
your Learner after training (lambdas are fine if you are just experimenting).
We can see that the independent variable will need to be converted into a complete
path so that we can open it as an image, and the dependent variable will need to be
split on the space character (which is the default for Python’s split function) so that
it becomes a list:
<b>def</b> get_x(r): <b>return</b> path/'train'/r['fname']
<b>def</b> get_y(r): <b>return</b> r['labels'].split(' ')
dblock = DataBlock(get_x = get_x, get_y = get_y)
dsets = dblock.datasets(df)
dsets.train[0]
(Path('/home/sgugger/.fastai/data/pascal_2007/train/008663.jpg'),
['car', 'person'])
To actually open the image and do the conversion to tensors, we will need to use a set
of transforms; block types will provide us with those. We can use the same block
types that we have used previously, with one exception: the ImageBlock will work fine
again, because we have a path that points to a valid image, but the CategoryBlock is
not going to work. The problem is that block returns a single integer, but we need to
be able to have multiple labels for each item. To solve this, we use a MultiCategory
Block . This type of block expects to receive a list of strings, as we have in this case, so
let’s test it out:
dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),
get_x = get_x, get_y = get_y)
dsets = dblock.datasets(df)
dsets.train[0]
(PILImage mode=RGB size=500x375,
TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,
> 0., 0., 0., 0., 0., 0.]))
As you can see, our list of categories is not encoded in the same way that it was for the
regular CategoryBlock . In that case, we had a single integer representing which cate‐
gory was present, based on its location in our vocab. In this case, however, we instead"|MultiCategoryBlock; DataFrame to DataLoaders; DataLoaders object from; DataFrame converted to; lambda functions; lambda functions and exporting; Python
"<header><largefont><b>Creating</b></largefont> <largefont><b>the</b></largefont> <largefont><b>CNN</b></largefont></header>
Let’s go back to the basic neural network we had in Chapter 4. It was defined like this:
simple_net = nn.Sequential(
nn.Linear(28*28,30),
nn.ReLU(),
nn.Linear(30,1)
)
We can view a model’s definition:
simple_net
Sequential(
(0): Linear(in_features=784, out_features=30, bias=True)
(1): ReLU()
(2): Linear(in_features=30, out_features=1, bias=True)
)
We now want to create a similar architecture to this linear model, but using convolu‐
tional layers instead of linear. nn.Conv2d is the module equivalent of F.conv2d. It’s
more convenient than F.conv2d when creating an architecture, because it creates the
weight matrix for us automatically when we instantiate it.
Here’s a possible architecture:
broken_cnn = sequential(
nn.Conv2d(1,30, kernel_size=3, padding=1),
nn.ReLU(),
nn.Conv2d(30,1, kernel_size=3, padding=1)
)
One thing to note here is that we didn’t need to specify 28*28 as the input size. That’s
because a linear layer needs a weight in the weight matrix for every pixel, so it needs
to know how many pixels there are, but a convolution is applied over each pixel auto‐
matically. The weights depend only on the number of input and output channels and
the kernel size, as we saw in the previous section.
Think about what the output shape is going to be; then let’s try it and see:
broken_cnn(xb).shape
torch.Size([64, 1, 28, 28])
This is not something we can use to do classification, since we need a single output
activation per image, not a 28×28 map of activations. One way to deal with this is to
use enough stride-2 convolutions such that the final layer is size 1. After one stride-2
convolution, the size will be 14×14; after two, it will be 7×7; then 4×4, 2×2, and finally
size 1."|building a CNN
"A helpful exercise prior to rolling out a significant machine learning system is to con‐
sider this question: “What would happen if it went really, really well?” In other words,
what if the predictive power was extremely high, and its ability to influence behavior
was extremely significant? In that case, who would be most impacted? What would
the most extreme results potentially look like? How would you know what was really
going on?
Such a thought exercise might help you to construct a more careful rollout plan, with
ongoing monitoring systems and human oversight. Of course, human oversight isn’t
useful if it isn’t listened to, so make sure that reliable and resilient communication
channels exist so that the right people will be aware of issues and will have the power
to fix them.
<header><largefont><b>Get</b></largefont> <largefont><b>Writing!</b></largefont></header>
One of the things our students have found most helpful to solidify their understand‐
ing of this material is to write it down. There is no better test of your understanding
of a topic than attempting to teach it to somebody else. This is helpful even if you
never show your writing to anybody—but it’s even better if you share it! So we rec‐
ommend that, if you haven’t already, you start a blog. Now that you’ve completed this
chapter and have learned how to train and deploy models, you’re well placed to write
your first blog post about your deep learning journey. What’s surprised you? What
opportunities do you see for deep learning in your field? What obstacles do you see?
Rachel Thomas, cofounder of fast.ai, wrote in the article “Why You (Yes, You) Should
Blog”:
The top advice I would give my younger self would be to start blogging sooner. Here
are some reasons to blog:
• It’s like a resume, only better. I know of a few people who have had blog posts lead
to job offers!
• Helps you learn. Organizing knowledge always helps me synthesize my own ideas.
One of the tests of whether you understand something is whether you can explain
it to someone else. A blog post is a great way to do that.
• I’ve gotten invitations to conferences and invitations to speak from my blog posts.
I was invited to the TensorFlow Dev Summit (which was awesome!) for writing a
blog post about how I don’t like TensorFlow.
• Meet new people. I’ve met several people who have responded to blog posts I
wrote.
• Saves time. Any time you answer a question multiple times through email, you
should turn it into a blog post, which makes it easier for you to share the next
time someone asks.
Perhaps her most important tip is this:"|blogging about deep learning journey; Thomas; web resources
"• nll_loss , as we saw, returns the value of just one activation: the single activation
corresponding with the single label for an item. This doesn’t make sense when we
have multiple labels.
binary_cross_entropy mnist_loss
On the other hand, the function, which is just
along with log , provides just what we need, thanks to the magic of PyTorch’s element‐
wise operations. Each activation will be compared to each target for each column, so
we don’t have to do anything to make this function work for multiple columns.
<b>JeremySays</b>
One of the things I really like about working with libraries like
PyTorch, with broadcasting and elementwise operations, is that
quite frequently I find I can write code that works equally well for a
single item or a batch of items, without changes.
binary_cross_entropy is a great example of this. By using these
operations, we don’t have to write loops ourselves, and can rely on
PyTorch to do the looping we need as appropriate for the rank of
the tensors we’re working with.
PyTorch already provides this function for us. In fact, it provides a number of ver‐
sions, with rather confusing names!
F.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross
entropy on a one-hot-encoded target, but do not include the initial sigmoid. Nor‐
mally, for one-hot-encoded targets you’ll want F.binary_cross_entropy_with_log
its (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross entropy in
a single function, as in the preceding example.
The equivalent for single-label datasets (like MNIST or the Pet dataset), where the
target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version
without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the
version with the initial softmax.
Since we have a one-hot-encoded target, we will use BCEWithLogitsLoss:
loss_func = nn.BCEWithLogitsLoss()
loss = loss_func(activs, y)
loss
tensor(1.0082, device='cuda:5', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)
We don’t need to tell fastai to use this loss function (although we can if we want) since
it will be automatically chosen for us. fastai knows that the DataLoaders has multiple
nn.BCEWithLogitsLoss
category labels, so it will use by default."|PyTorch single item or batch same code; BCELoss; BCEWithLogitsLoss; binary cross entropy loss function; loss function selected by; labels; binary cross entropy; fastai selecting function; multi-label classifier loss function; single item or batch same code; one-hot-encoded targets
"<header><largefont><b>Our</b></largefont> <largefont><b>First</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>from</b></largefont> <largefont><b>Scratch</b></largefont></header>
One simple way to turn this into a neural network would be to specify that we are
going to predict each word based on the previous three words. We could create a list
of every sequence of three words as our independent variables, and the next word
after each sequence as the dependent variable.
We can do that with plain Python. Let’s do it first with tokens just to confirm what it
looks like:
L((tokens[i:i+3], tokens[i+3]) <b>for</b> i <b>in</b> range(0,len(tokens)-4,3))
(#21031) [(['one', '.', 'two'], '.'),(['.', 'three', '.'], 'four'),(['four',
> '.', 'five'], '.'),(['.', 'six', '.'], 'seven'),(['seven', '.', 'eight'],
> '.'),(['.', 'nine', '.'], 'ten'),(['ten', '.', 'eleven'], '.'),(['.',
> 'twelve', '.'], 'thirteen'),(['thirteen', '.', 'fourteen'], '.'),(['.',
> 'fifteen', '.'], 'sixteen')...]
Now we will do it with tensors of the numericalized values, which is what the model
will actually use:
seqs = L((tensor(nums[i:i+3]), nums[i+3]) <b>for</b> i <b>in</b> range(0,len(nums)-4,3))
seqs
(#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]),
> 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]),
> 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1,
> 14]), 1),(tensor([ 1, 15, 1]), 16)...]
We can batch those easily using the DataLoader class. For now, we will split the
sequences randomly:
bs = 64
cut = int(len(seqs) * 0.8)
dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False)
We can now create a neural network architecture that takes three words as input, and
returns a prediction of the probability of each possible next word in the vocab. We
will use three standard linear layers, but with two tweaks.
The first tweak is that the first linear layer will use only the first word’s embedding as
activations, the second layer will use the second word’s embedding plus the first lay‐
er’s output activations, and the third layer will use the third word’s embedding plus
the second layer’s output activations. The key effect is that every word is interpreted
in the information context of any words preceding it."|language model; natural language processing (NLP)
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.738273 0.541828 0.150880 00:24
1 0.401544 0.266623 0.081867 00:24
<b>LogarithmicScale</b>
The learning rate finder plot has a logarithmic scale, which is why
the middle point between 1e-3 and 1e-2 is between 3e-3 and 4e-3.
This is because we care mostly about the order of magnitude of the
learning rate.
It’s interesting that the learning rate finder was discovered only in 2015, while neural
networks have been under development since the 1950s. Throughout that time, find‐
ing a good learning rate has been, perhaps, the most important and challenging issue
for practitioners. The solution does not require any advanced math, giant computing
resources, huge datasets, or anything else that would make it inaccessible to any curi‐
ous researcher. Furthermore, Smith was not part of some exclusive Silicon Valley lab,
but was working as a naval researcher. All of this is to say: breakthrough work in deep
learning absolutely does not require access to vast resources, elite teams, or advanced
mathematical ideas. Lots of work remains to be done that requires just a bit of com‐
mon sense, creativity, and tenacity.
Now that we have a good learning rate to train our model, let’s look at how we can
fine-tune the weights of a pretrained model.
<header><largefont><b>Unfreezing</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Transfer</b></largefont> <largefont><b>Learning</b></largefont></header>
We discussed briefly in Chapter 1 how transfer learning works. We saw that the basic
idea is that a pretrained model, trained potentially on millions of data points (such as
ImageNet), is fine-tuned for another task. But what does this really mean?
We now know that a convolutional neural network consists of many linear layers with
a nonlinear activation function between each pair, followed by one or more final lin‐
ear layers with an activation function such as softmax at the very end. The final linear
layer uses a matrix with enough columns such that the output size is the same as the
number of classes in our model (assuming that we are doing classification).
This final linear layer is unlikely to be of any use for us when we are fine-tuning in a
transfer learning setting, because it is specifically designed to classify the categories in
the original pretraining dataset. So when we do transfer learning, we remove it, throw
it away, and replace it with a new linear layer with the correct number of outputs for
our desired task (in this case, there would be 37 activations).
This newly added linear layer will have entirely random weights. Therefore, our
model prior to fine-tuning has entirely random outputs. But that does not mean that"|softmax as activation function; convolutional neural network (CNN); last layer and; fine-tuning models; freezing pretrained models; history; freezing pretrained layers; softmax activation function; final layer matrix; nonlinear function between linears; linear and nonlinear layers; learning rate finder plot; neural networks; nonlinear and linear layers; pretrained models; Smith; training; transfer learning; fine-tuning as; unfreezing
"Now we can grab the image:
img = PILImage.create(btn_upload.data[-1])
Output
We can use an widget to display it:
out_pl = widgets.Output()
out_pl.clear_output()
<b>with</b> out_pl: display(img.to_thumb(128,128))
out_pl
Then we can get our predictions:
pred,pred_idx,probs = learn_inf.predict(img)
And use a Label to display them:
lbl_pred = widgets.Label()
lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'
lbl_pred
Prediction: grizzly; Probability: 1.0000
We’ll need a button to do the classification. It looks exactly like the Upload button:
btn_run = widgets.Button(description='Classify')
btn_run"|button click event handler; deployment; process end-to-end; web application from model; web application from; web display Output widget; web applications
"In practice, though, it would be very inefficient (and maybe numerically unstable) to
compute that big sum and add it to the loss. If you remember a little bit of high
school math, you might recall that the derivative of p**2 with respect to p is 2*p, so
adding that big sum to our loss is exactly the same as doing this:
parameters.grad += wd * 2 * parameters
In practice, since wd is a parameter that we choose, we can make it twice as big, so we
don’t even need the *2 in this equation. To use weight decay in fastai, pass wd in your
call to fit or fit_one_cycle (it can be passed on both):
model = DotProductBias(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 0.972090 0.962366 00:13
1 0.875591 0.885106 00:13
2 0.723798 0.839880 00:13
3 0.586002 0.823225 00:13
4 0.490980 0.823060 00:13
Much better!
<header><largefont><b>Creating</b></largefont> <largefont><b>Our</b></largefont> <largefont><b>Own</b></largefont> <largefont><b>Embedding</b></largefont> <largefont><b>Module</b></largefont></header>
So far, we’ve used Embedding without thinking about how it really works. Let’s re-
create DotProductBias <i>without</i> using this class. We’ll need a randomly initialized
weight matrix for each of the embeddings. We have to be careful, however. Recall
from Chapter 4 that optimizers require that they can get all the parameters of a mod‐
parameters
ule from the module’s method. However, this does not happen fully auto‐
matically. If we just add a tensor as an attribute to a Module , it will not be included in
parameters:
<b>class</b> <b>T(Module):</b>
<b>def</b> <b>__init__(self):</b> self.a = torch.ones(3)
L(T().parameters())
(#0) []"|categorical variables; collaborative filtering; built from scratch; embedding; embedding from scratch; embedding categorical variables; optimization
"You can do this using the show_batch method:
dls.show_batch(nrows=1, ncols=3)
Take a look at each image, and check that each one seems to have the correct label for
that breed of pet. Often, data scientists work with data with which they are not as
familiar as domain experts may be: for instance, I actually don’t know what a lot of
these pet breeds are. Since I am not an expert on pet breeds, I would use Google
images at this point to search for a few of these breeds, and make sure the images
look similar to what I see in this output.
If you made a mistake while building your DataBlock, you likely won’t see it before
summary
this step. To debug this, we encourage you to use the method. It will attempt
to create a batch from the source you give it, with a lot of details. Also, if it fails, you
will see exactly at which point the error happens, and the library will try to give you
some help. For instance, one common mistake is to forget to use a Resize transform,
so you end up with pictures of different sizes and are not able to batch them. Here is
what the summary would look like in that case (note that the exact text may have
changed since the time of writing, but it will give you an idea):
pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),
get_items=get_image_files,
splitter=RandomSplitter(seed=42),
get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'))
pets1.summary(path/""images"")
Setting-up type transforms pipelines
Collecting items from /home/sgugger/.fastai/data/oxford-iiit-pet/images
Found 7390 items
2 datasets of sizes 5912,1478
Setting up Pipeline: PILBase.create
Setting up Pipeline: partial -> Categorize
Building one sample
Pipeline: PILBase.create
starting from
/home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg
applying PILBase.create gives
PILImage mode=RGB size=375x500"|batch operations; debugging; labels; show_batch method; debugging image dataset
"Let’s take a look at our first image:
im = PILImage.create(img_files[0])
im.shape
(480, 640)
im.to_thumb(160)
The Biwi dataset website used to explain the format of the pose text file associated
with each image, which shows the location of the center of the head. The details of
this aren’t important for our purposes, so we’ll just show the function we use to
extract the head center point:
cal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)
<b>def</b> get_ctr(f):
ctr = np.genfromtxt(img2pose(f), skip_header=3)
c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]
c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]
<b>return</b> tensor([c1,c2])
This function returns the coordinates as a tensor of two items:
get_ctr(img_files[0])
tensor([384.6370, 259.4787])
We can pass this function to DataBlock as get_y, since it is responsible for labeling
each item. We’ll resize the images to half their input size, to speed up training a bit.
One important point to note is that we should not just use a random splitter. The
same people appear in multiple images in this dataset, but we want to ensure that our
model can generalize to people that it hasn’t seen yet. Each folder in the dataset con‐
tains the images for one person. Therefore, we can create a splitter function that
returns True for just one person, resulting in a validation set containing just that per‐
son’s images.
The only other difference from the previous data block examples is that the second
block is a PointBlock. This is necessary so that fastai knows that the labels represent
coordinates; that way, it knows that when doing data augmentation, it should do the
same augmentation to these coordinates as it does to the images:"|applied to coordinates; fastai software library; image regression; extracting head center point; key point model; key point model of image regression; PointBlock
"ful recognition of handwritten digit sequences. This was one of the most important
breakthroughs in the history of AI.
<header><largefont><b>Tenacity</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont></header>
The story of deep learning is one of tenacity and grit by a handful of dedicated
researchers. After early hopes (and hype!), neural networks went out of favor in the
1990s and 2000s, and just a handful of researchers kept trying to make them work
well. Three of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded
the highest honor in computer science, the Turing Award (generally considered the
“Nobel Prize of computer science”), in 2018 after triumphing despite the deep skepti‐
cism and disinterest of the wider machine learning and statistics community.
Hinton has told of how academic papers showing dramatically better results than
anything previously published would be rejected by top journals and conferences, just
because they used a neural network. Lecun’s work on convolutional neural networks,
which we will study in the next section, showed that these models could read hand‐
written text—something that had never been achieved before. However, his break‐
through was ignored by most researchers, even as it was used commercially to read
10% of the checks in the US!
In addition to these three Turing Award winners, many other researchers have battled
to get us to where we are today. For instance, Jurgen Schmidhuber (who many believe
should have shared in the Turing Award) pioneered many important ideas, including
working with his student Sepp Hochreiter on the long short-term memory (LSTM)
architecture (widely used for speech recognition and other text modeling tasks, and
used in the IMDb example in Chapter 1). Perhaps most important of all, Paul Werbos
in 1974 invented backpropagation for neural networks, the technique shown in this
chapter and used universally for training neural networks (Werbos 1994). His devel‐
opment was almost entirely ignored for decades, but today it is considered the most
important foundation of modern AI.
There is a lesson here for all of us! On your deep learning journey, you will face many
obstacles, both technical and (even more difficult) posed by people around you who
don’t believe you’ll be successful. There’s one <i>guaranteed</i> way to fail, and that’s to stop
trying. We’ve seen that the only consistent trait among every fast.ai student who’s
gone on to be a world-class practitioner is that they are all very tenacious.
For this initial tutorial, we are just going to try to create a model that can classify any
image as a 3 or a 7. So let’s download a sample of MNIST that contains images of just
these digits:
path = untar_data(URLs.MNIST_SAMPLE)"|long short-term memory; training neural networks; Bengio; pixels as foundation; Yann Lecun’s work; deep learning; handwritten text read by models; Hinton; history; Hochreiter; Lecun; long short-term memory (LSTM); machine learning (ML); read by models; training via backpropagation; number-related datasets; pixels; Schmidhuber; training; Turing Award; Werbos
"back even more deeply, from us ourselves, in order to ensure that <i>we</i> don’t overfit on
the validation data as we explore various model architectures and hyperparameters.
We don’t get to see the test set. But we do want to define our validation data so that it
has the same sort of relationship to the training data as the test set will have.
In some cases, just randomly choosing a subset of your data points will do that. This
is not one of those cases, because it is a time series.
If you look at the date range represented in the test set, you will discover that it covers
a six-month period from May 2012, which is later in time than any date in the train‐
ing set. This is a good design, because the competition sponsor will want to ensure
that a model is able to predict the future. But it means that if we are going to have a
useful validation set, we also want the validation set to be later in time than the train‐
ing set. The Kaggle training data ends in April 2012, so we will define a narrower
training dataset that consists only of the Kaggle training data from before November
2011, and we’ll define a validation set consisting of data from after November 2011.
To do this we use np.where, a useful function that returns (as the first element of a
tuple) the indices of all True values:
cond = (df.saleYear<2011) | (df.saleMonth<10)
train_idx = np.where( cond)[0]
valid_idx = np.where(~cond)[0]
splits = (list(train_idx),list(valid_idx))
TabularPandas needs to be told which columns are continuous and which are catego‐
rical. We can handle that automatically using the helper function cont_cat_split:
cont,cat = cont_cat_split(df, 1, dep_var=dep_var)
to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)
A TabularPandas behaves a lot like a fastai Datasets object, including providing
train and valid attributes:
len(to.train),len(to.valid)
(404710, 7988)
We can see that the data is still displayed as strings for categories (we show only a few
columns here because the full table is too big to fit on a page):
to.show(3)
<b>state</b> <b>ProductGroup</b> <b>Drive_System</b> <b>Enclosure</b> <b>SalePrice</b>
<b>0</b> Alabama WL #na# EROPSwAC 11.097410
<b>1</b> NorthCarolina WL #na# EROPSwAC 10.950807
<b>2</b> NewYork SSL #na# OROPS 9.210340"|tabular dataset prep; decision trees; tabular data for models; training
"during backpropagation (backward hook). A forward hook is a function that takes
three things—a module, its input, and its output—and it can perform any behavior
you want. (fastai also provides a handy HookCallback that we won’t cover here, but
take a look at the fastai docs; it makes working with hooks a little easier.)
To illustrate, we’ll use the same cats and dogs model we trained in Chapter 1:
path = untar_data(URLs.PETS)/'images'
<b>def</b> is_cat(x): <b>return</b> x[0].isupper()
dls = ImageDataLoaders.from_name_func(
path, get_image_files(path), valid_pct=0.2, seed=21,
label_func=is_cat, item_tfms=Resize(224))
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.141987 0.018823 0.007442 00:16
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.050934 0.015366 0.006766 00:21
To start, we’ll grab a cat picture and a batch of data:
img = PILImage.create('images/chapter1_cat_example.jpg')
x, = first(dls.test_dl([img]))
For CAM, we want to store the activations of the last convolutional layer. We put our
hook function in a class so it has a state that we can access later, and just store a copy
of the output:
<b>class</b> <b>Hook():</b>
<b>def</b> hook_func(self, m, i, o): self.stored = o.detach().clone()
Hook
We can then instantiate a and attach it to the layer we want, which is the last
layer of the CNN body:
hook_output = Hook()
hook = learn.model[0].register_forward_hook(hook_output.hook_func)
Now we can grab a batch and feed it through our model:
<b>with</b> torch.no_grad(): output = learn.model.eval()(x)
And we can access our stored activations:
act = hook_output.stored[0]
Let’s also double-check our predictions:
F.softmax(output, dim=-1)
tensor([[7.3566e-07, 1.0000e+00]], device='cuda:0')"|HookCallback; class activation map (CAM); hooks in PyTorch; interpretation via class activation map; PyTorch
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>7</b></largefont></header>
<header><largefont><b>Training</b></largefont> <largefont><b>a</b></largefont> <largefont><b>State-of-the-Art</b></largefont> <largefont><b>Model</b></largefont></header>
This chapter introduces more advanced techniques for training an image classifica‐
tion model and getting state-of-the-art results. You can skip it if you want to learn
more about other applications of deep learning and come back to it later—knowledge
of this material will not be assumed in later chapters.
We will look at what normalization is, a powerful data augmentation technique called
Mixup, the progressive resizing approach, and test time augmentation. To show all of
this, we are going to train a model from scratch (not using transfer learning) by using
a subset of ImageNet called Imagenette. It contains a subset of 10 very different cate‐
gories from the original ImageNet dataset, making for quicker training when we want
to experiment.
This is going to be much harder to do well than with our previous datasets because
we’re using full-size, full-color images, which are photos of objects of different sizes,
in different orientations, in different lighting, and so forth. So, in this chapter we’re
going to introduce important techniques for getting the most out of your dataset,
especially when you’re training from scratch, or using transfer learning to train a
model on a very different kind of dataset than the pretrained model used.
<header><largefont><b>Imagenette</b></largefont></header>
When fast.ai first started, people used three main datasets for building and testing
computer vision models:
<i>ImageNet</i>
1.3 million images of various sizes, around 500 pixels across, in 1,000 categories,
which took a few days to train"|datasets; ImageNet dataset; image classifier model training
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>12</b></largefont></header>
<header><largefont><b>A</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>from</b></largefont> <largefont><b>Scratch</b></largefont></header>
We’re now ready to go deep…deep into deep learning! You already learned how to
train a basic neural network, but how do you go from there to creating state-of-the-
art models? In this part of the book, we’re going to uncover all of the mysteries, start‐
ing with language models.
You saw in Chapter 10 how to fine-tune a pretrained language model to build a text
classifier. In this chapter, we will explain exactly what is inside that model and what
an RNN is. First, let’s gather some data that will allow us to quickly prototype our var‐
ious models.
<header><largefont><b>The</b></largefont> <largefont><b>Data</b></largefont></header>
Whenever we start working on a new problem, we always first try to think of the sim‐
plest dataset we can that will allow us to try out methods quickly and easily, and inter‐
pret the results. When we started working on language modeling a few years ago, we
didn’t find any datasets that would allow for quick prototyping, so we made one. We
call it <i>Human</i> <i>Numbers,</i> and it simply contains the first 10,000 numbers written out in
English.
<b>JeremySays</b>
One of the most common practical mistakes I see even among
highly experienced practitioners is failing to use appropriate data‐
sets at appropriate times during the analysis process. In particular,
most people tend to start with datasets that are too big and too
complicated."|Human Numbers; Human Numbers dataset; language model; natural language processing (NLP)
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 0.940104 0.959786 00:15
1 0.893943 0.905222 00:14
2 0.865591 0.875238 00:14
3 0.800177 0.867468 00:14
4 0.760255 0.867455 00:14
fastai provides this model in fastai.collab if you pass use_nn=True in your call to
collab_learner (including calling get_emb_sz for you), and it lets you easily create
more layers. For instance, here we’re creating two hidden layers, of size 100 and 50,
respectively:
learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50])
learn.fit_one_cycle(5, 5e-3, wd=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 1.002747 0.972392 00:16
1 0.926903 0.922348 00:16
2 0.877160 0.893401 00:16
3 0.838334 0.865040 00:16
4 0.781666 0.864936 00:16
learn.model is an object of type EmbeddingNN . Let’s take a look at fastai’s code for this
class:
@delegates(TabularModel)
<b>class</b> <b>EmbeddingNN(TabularModel):</b>
<b>def</b> <b>__init__(self,</b> emb_szs, layers, **kwargs):
super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs)
TabularModel,
Wow, that’s not a lot of code! This class <i>inherits</i> from which is where it
gets all its functionality from. In __init__, it calls the same method in TabularModel,
passing n_cont=0 and out_sz=1; other than that, it passes along only whatever argu‐
ments it received.
<header><largefont><b>kwargs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Delegates</b></largefont></header>
EmbeddingNN includes **kwargs as a parameter to __init__ . In Python, **kwargs in
a parameter list means “put any additional keyword arguments into a dict called
kwargs.” And **kwargs in an argument list means “insert all key/value pairs in the
kwargs dict as named arguments here.” This approach is used in many popular libra‐
ries, such as matplotlib, in which the main plot function simply has the signature
plot(*args, **kwargs). The plot documentation says “The kwargs are Line2D
properties” and then lists those properties."|built from scratch; kwargs; Learner; embedding from scratch
"<i>MNIST</i>
50,000 28×28-pixel grayscale handwritten digits
<i>CIFAR10</i>
60,000 32×32-pixel color images in 10 classes
The problem was that the smaller datasets didn’t generalize effectively to the large
ImageNet dataset. The approaches that worked well on ImageNet generally had to be
developed and trained on ImageNet. This led to many people believing that only
researchers with access to giant computing resources could effectively contribute to
developing image classification algorithms.
We thought that seemed very unlikely to be true. We had never seen a study that
showed that ImageNet happens to be exactly the right size, and that other datasets
could not be developed that would provide useful insights. So we wanted to create a
new dataset that researchers could test their algorithms on quickly and cheaply, but
that would also provide insights likely to work on the full ImageNet dataset.
About three hours later, we had created Imagenette. We selected 10 classes from the
full ImageNet that looked very different from one another. As we had hoped, we were
able to quickly and cheaply create a classifier capable of recognizing these classes. We
then tried out a few algorithmic tweaks to see how they impacted Imagenette. We
found some that worked pretty well, and tested them on ImageNet as well—and we
were pleased to find that our tweaks worked well on ImageNet too!
There is an important message here: the dataset you are given is not necessarily the
dataset you want. It’s particularly unlikely to be the dataset that you want to do your
development and prototyping in. You should aim to have an iteration speed of no
more than a couple of minutes—that is, when you come up with a new idea you want
to try out, you should be able to train a model and see how it goes within a couple of
minutes. If it’s taking longer to do an experiment, think about how you could cut
down your dataset, or simplify your model, to improve your experimentation speed.
The more experiments you can do, the better!
Let’s get started with this dataset:
<b>from</b> <b>fastai.vision.all</b> <b>import</b> *
path = untar_data(URLs.IMAGENETTE)
First we’ll get our dataset into a DataLoaders object, using the <i>presizing</i> trick intro‐
duced in Chapter 5:
dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),
get_items=get_image_files,
get_y=parent_label,
item_tfms=Resize(460),
batch_tfms=aug_transforms(size=224, min_scale=0.75))
dls = dblock.dataloaders(path, bs=64)"|CIFAR10 dataset; handwritten digits; MNIST handwritten digits dataset; handwritten digits dataset
"The result is nans everywhere. So maybe the scale of our matrix was too big, and we
need to have smaller weights? But if we use too small weights, we will have the oppo‐
site problem—the scale of our activations will go from 1 to 0.1, and after 100 layers
we’ll be left with zeros everywhere:
x = torch.randn(200, 100)
<b>for</b> i <b>in</b> range(50): x = x @ (torch.randn(100,100) * 0.01)
x[0:5,0:5]
tensor([[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.],
[0., 0., 0., 0., 0.]])
So we have to scale our weight matrices exactly right so that the standard deviation of
our activations stays at 1. We can compute the exact value to use mathematically, as
illustrated by Xavier Glorot and Yoshua Bengio in “Understanding the Difficulty of
Training Deep Feedforward Neural Networks”. The right scale for a given layer is
1/ <i>n</i> , where <i>n</i> represents the number of inputs.
<i>in</i> <i>in</i>
In our case, if we have 100 inputs, we should scale our weight matrices by 0.1:
x = torch.randn(200, 100)
<b>for</b> i <b>in</b> range(50): x = x @ (torch.randn(100,100) * 0.1)
x[0:5,0:5]
tensor([[ 0.7554, 0.6167, -0.1757, -1.5662, 0.5644],
[-0.1987, 0.6292, 0.3283, -1.1538, 0.5416],
[ 0.6106, 0.2556, -0.0618, -0.9463, 0.4445],
[ 0.4484, 0.7144, 0.1164, -0.8626, 0.4413],
[ 0.3463, 0.5930, 0.3375, -0.9486, 0.5643]])
nan!
Finally, some numbers that are neither zeros nor Notice how stable the scale of
our activations is, even after those 50 fake layers:
x.std()
tensor(0.7042)
If you play a little bit with the value for scale, you’ll notice that even a slight variation
from 0.1 will get you either to very small or very large numbers, so initializing the
weights properly is extremely important.
Let’s go back to our neural net. Since we messed a bit with our inputs, we need to
redefine them:
x = torch.randn(200, 100)
y = torch.randn(200)"|Bengio; defining and initializing a layer; Glorot; training deep feedforward neural networks
"We’ll put that into a function
<b>def</b> logsumexp(x):
m = x.max(-1)[0]
<b>return</b> m + (x-m[:,None]).exp().sum(-1).log()
logsumexp(r)[0]
tensor(3.9784, grad_fn=<SelectBackward>)
so we can use it for our log_softmax function:
<b>def</b> log_softmax(x): <b>return</b> x - x.logsumexp(-1,keepdim=True)
Which gives the same result as before:
sm = log_softmax(r); sm[0][0]
tensor(-1.2790, grad_fn=<SelectBackward>)
We can use these to create cross_entropy:
<b>def</b> cross_entropy(preds, yb): <b>return</b> nll(log_softmax(preds), yb).mean()
Let’s now combine all those pieces to create a Learner .
<header><largefont><b>Learner</b></largefont></header>
We have data, a model, and a loss function; we need only one more thing before we
can fit a model, and that’s an optimizer! Here’s SGD:
<b>class</b> <b>SGD:</b>
<b>def</b> <b>__init__(self,</b> params, lr, wd=0.): store_attr(self, 'params,lr,wd')
<b>def</b> step(self):
<b>for</b> p <b>in</b> self.params:
p.data -= (p.grad.data + p.data*self.wd) * self.lr
p.grad.data.zero_()
As we’ve seen in this book, life is easier with a Learner. The Learner needs to know
our training and validation sets, which means we need DataLoaders to store them.
We don’t need any other functionality, just a place to store them and access them:
<b>class</b> <b>DataLoaders:</b>
<b>def</b> <b>__init__(self,</b> *dls): self.train,self.valid = dls
dls = DataLoaders(train_dl,valid_dl)
Now we’re ready to create our Learner class:
<b>class</b> <b>Learner:</b>
<b>def</b> <b>__init__(self,</b> model, dls, loss_func, lr, cbs, opt_func=SGD):
store_attr(self, 'model,dls,loss_func,lr,cbs,opt_func')
<b>for</b> cb <b>in</b> cbs: cb.learner = self"|building Learner class from scratch; Learner
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>9</b></largefont></header>
<header><largefont><b>Tabular</b></largefont> <largefont><b>Modeling</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Dive</b></largefont></header>
Tabular modeling takes data in the form of a table (like a spreadsheet or CSV). The
objective is to predict the value in one column based on the values in the other col‐
umns. In this chapter, we will look at not only deep learning, but also more general
machine learning techniques like random forests, as they can give better results
depending on your problem.
We will look at how we should preprocess and clean the data as well as how to inter‐
pret the result of our models after training, but first we will see how we can feed col‐
umns that contain categories into a model that expects numbers by using
embeddings.
<header><largefont><b>Categorical</b></largefont> <largefont><b>Embeddings</b></largefont></header>
In tabular data, some columns may contain numerical data, like “age,” while others
contain string values, like “sex.” The numerical data can be directly fed to the model
(with some optional preprocessing), but the other columns need to be converted to
numbers. Since the values in those correspond to different categories, we often call
this type of variables <i>categorical</i> <i>variables.</i> The first type are called <i>continuous</i>
<i>variables.</i>
<b>Jargon:ContinuousandCategoricalVariables</b>
Continuous variables are numerical data, such as “age,” that can be
directly fed to the model, since you can add and multiply them
directly. Categorical variables contain a number of discrete levels,
such as “movie ID,” for which addition and multiplication don’t
have meaning (even if they’re stored as numbers)."|categorical variables; continuous variables; tabular data for models; variables
"represents. Our goal is to produce a program, called a <i>model,</i> that, given a new image,
will make an accurate <i>prediction</i> regarding what that new image represents.
Every model starts with a choice of <i>architecture,</i> a general template for how that kind
of model works internally. The process of <i>training</i> (or <i>fitting)</i> the model is the process
of finding a set of <i>parameter</i> <i>values</i> (or <i>weights)</i> that specialize that general architec‐
ture into a model that works well for our particular kind of data. To define how well a
model does on a single prediction, we need to define a <i>loss</i> <i>function,</i> which deter‐
mines how we score a prediction as good or bad.
To make the training process go faster, we might start with a <i>pretrained</i> <i>model—a</i>
model that has already been trained on someone else’s data. We can then adapt it to
our data by training it a bit more on our data, a process called <i>fine-tuning.</i>
When we train a model, a key concern is to ensure that our model <i>generalizes:</i> it
learns general lessons from our data that also apply to new items it will encounter, so
it can make good predictions on those items. The risk is that if we train our model
badly, instead of learning general lessons, it effectively memorizes what it has already
seen, and then it will make poor predictions about new images. Such a failure is
called <i>overfitting.</i>
To avoid this, we always divide our data into two parts, the <i>training</i> <i>set</i> and the <i>valida‐</i>
<i>tion</i> <i>set.</i> We train the model by showing it only the training set, and then we evaluate
how well the model is doing by seeing how well it performs on items from the valida‐
tion set. In this way, we check if the lessons the model learns from the training set are
lessons that generalize to the validation set. In order for a person to assess how well
the model is doing on the validation set overall, we define a <i>metric.</i> During the train‐
ing process, when the model has seen every item in the training set, we call that an
<i>epoch.</i>
All these concepts apply to machine learning in general. They apply to all sorts of
schemes for defining a model by training it with data. What makes deep learning dis‐
tinctive is a particular class of architectures: the architectures based on <i>neural</i> <i>net‐</i>
<i>works.</i> In particular, tasks like image classification rely heavily on <i>convolutional</i> <i>neural</i>
<i>networks,</i> which we will discuss shortly.
<header><largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Is</b></largefont> <largefont><b>Not</b></largefont> <largefont><b>Just</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Image</b></largefont> <largefont><b>Classification</b></largefont></header>
Deep learning’s effectiveness for classifying images has been widely discussed in
recent years, even showing <i>superhuman</i> results on complex tasks like recognizing
malignant tumors in CT scans. But it can do a lot more than this, as we will show
here."|non-image tasks; generalization by models
"Finally, we tell PyTorch to calculate the gradients for us:
yt.backward()
The “backward” here refers to <i>backpropagation,</i> which is the name given to the pro‐
cess of calculating the derivative of each layer. We’ll see how this is done exactly in
Chapter 17, when we calculate the gradients of a deep neural net from scratch. This is
called the <i>backward</i> <i>pass</i> of the network, as opposed to the <i>forward</i> <i>pass,</i> which is
where the activations are calculated. Life would probably be easier if backward was
just called calculate_grad, but deep learning folks really do like to add jargon every‐
where they can!
We can now view the gradients by checking the grad attribute of our tensor:
xt.grad
tensor(6.)
If you remember your high school calculus rules, the derivative of x**2 is 2*x , and we
have x=3, so the gradients should be 2*3=6, which is what PyTorch calculated for us!
Now we’ll repeat the preceding steps, but with a vector argument for our function:
xt = tensor([3.,4.,10.]).requires_grad_()
xt
tensor([ 3., 4., 10.], requires_grad=True)
And we’ll add sum to our function so it can take a vector (i.e., a rank-1 tensor) and
return a scalar (i.e., a rank-0 tensor):
<b>def</b> f(x): <b>return</b> (x**2).sum()
yt = f(xt)
yt
tensor(125., grad_fn=<SumBackward0>)
Our gradients are 2*xt, as we’d expect!
yt.backward()
xt.grad
tensor([ 6., 8., 20.])
The gradients tell us only the slope of our function; they don’t tell us exactly how far
to adjust the parameters. But they do give us some idea of how far: if the slope is very
large, that may suggest that we have more adjustments to do, whereas if the slope is
very small, that may suggest that we are close to the optimal value."|forward pass; backward pass; gradients; layers; stochastic gradient descent (SGD); training; weights
"In this case, the areas in bright yellow correspond to high activations, and the areas in
purple to low activations. In this case, we can see the head and the front paw were the
two main areas that made the model decide it was a picture of a cat.
Once you’re done with your hook, you should remove it as otherwise it might leak
some memory:
hook.remove()
That’s why it’s usually a good idea to have the Hook class be a <i>context</i> <i>manager,</i> regis‐
tering the hook when you enter it and removing it when you exit. A context manager
is a Python construct that calls __enter__ when the object is created in a with clause,
and __exit__ at the end of the with clause. For instance, this is how Python handles
the with open(...) as f: construct that you’ll often see for opening files without
requiring an explicit close(f) at the end.
If we define Hook as follows
<b>class</b> <b>Hook():</b>
<b>def</b> <b>__init__(self,</b> m):
self.hook = m.register_forward_hook(self.hook_func)
<b>def</b> hook_func(self, m, i, o): self.stored = o.detach().clone()
<b>def</b> <b>__enter__(self,</b> *args): <b>return</b> self
<b>def</b> <b>__exit__(self,</b> *args): self.hook.remove()
we can safely use it this way:
<b>with</b> Hook(learn.model[0]) <b>as</b> hook:
<b>with</b> torch.no_grad(): output = learn.model.eval()(x.cuda())
act = hook.stored
fastai provides this Hook class for you, as well as some other handy classes to make
working with hooks easier.
This method is useful, but works for only the last layer. <i>Gradient</i> <i>CAM</i> is a variant
that addresses this problem.
<header><largefont><b>Gradient</b></largefont> <largefont><b>CAM</b></largefont></header>
The method we just saw lets us compute only a heatmap with the last activations,
since once we have our features, we have to multiply them by the last weight matrix.
This won’t work for inner layers in the network. A variant introduced in the 2016
paper “Grad-CAM: Why Did You Say That?” by Ramprasaath R. Selvaraju et al. uses
the gradients of the final activation for the desired class. If you remember a little bit
about the backward pass, the gradients of the output of the last layer with respect to
the input of that layer are equal to the layer weights, since it is a linear layer.
With deeper layers, we still want the gradients, but they won’t just be equal to the
weights anymore. We have to calculate them. The gradients of every layer are"|class activation map (CAM); gradient CAM; context manager; gradient class activation map; hooks in PyTorch; Hook class as context manager; interpretation via class activation map; hooks might leak; PyTorch; Python
"The TfmdLists is named with an “s” because it can handle a training and a validation
set with a splits argument. You just need to pass the indices of the elements that are
in the training set and the indices of the elements that are in the validation set:
cut = int(len(files)*0.8)
splits = [list(range(cut)), list(range(cut,len(files)))]
tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize],
splits=splits)
You can then access them through the train and valid attributes:
tls.valid[0][:20]
tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379,
> 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509])
If you have manually written a Transform that performs all of your preprocessing at
once, turning raw items into a tuple with inputs and targets, then TfmdLists is the
class you need. You can directly convert it to a DataLoaders object with the dataload
ers method. This is what we will do in our Siamese example later in this chapter.
In general, though, you will have two (or more) parallel pipelines of transforms: one
for processing your raw items into inputs and one to process your raw items into tar‐
gets. For instance, here, the pipeline we defined processes only the raw text into
inputs. If we want to do text classification, we also have to process the labels into
targets.
For this, we need to do two things. First we take the label name from the parent
folder. There is a function, parent_label , for this:
lbls = files.map(parent_label)
lbls
(#50000) ['pos','pos','pos','pos','pos','pos','pos','pos','pos','pos'...]
Then we need a Transform that will grab the unique items and build a vocab with
them during setup, then transform the string labels into integers when called. fastai
provides this for us; it’s called Categorize :
cat = Categorize()
cat.setup(lbls)
cat.vocab, cat(lbls[0])
((#2) ['neg','pos'], TensorCategory(1))
To do the whole setup automatically on our list of files, we can create a TfmdLists as
before:
tls_y = TfmdLists(files, [parent_label, Categorize()])
tls_y[0]
TensorCategory(1)"|TfmdLists; writing your own
"Note that we are writing weight.avg to highlight the fact that we need to store the
moving averages for each parameter of the model (they all their own independent
moving averages).
Figure 16-1 shows an example of noisy data for a single parameter with the momen‐
tum curve plotted in red, and the gradients of the parameter plotted in blue. The gra‐
dients increase, then decrease, and the momentum does a good job of following the
general trend without getting too influenced by noise.
<i>Figure</i> <i>16-1.</i> <i>An</i> <i>example</i> <i>of</i> <i>momentum</i>
It works particularly well if the loss function has narrow canyons we need to navigate:
vanilla SGD would send us bouncing from one side to the other, while SGD with
momentum will average those to roll smoothly down the side. The parameter beta
determines the strength of the momentum we are using: with a small beta , we stay
closer to the actual gradient values, whereas with a high beta, we will mostly go in the
direction of the average of the gradients and it will take a while before any change in
the gradients makes that trend move."|momentum in SGD; stochastic gradient descent (SGD); training
"<header><largefont><b>SymPy</b></largefont></header>
SymPy is a library for symbolic computation that is extremely useful when working
with calculus. Per the documentation:
Symbolic computation deals with the computation of mathematical objects symboli‐
cally. This means that the mathematical objects are represented exactly, not approxi‐
mately, and mathematical expressions with unevaluated variables are left in symbolic
form.
To do symbolic computation, we define a <i>symbol</i> and then do a computation, like so:
<b>from</b> <b>sympy</b> <b>import</b> symbols,diff
sx,sy = symbols('sx sy')
diff(sx**2, sx)
2*sx
Here, SymPy has taken the derivative of x**2 for us! It can take the derivative of com‐
plicated compound expressions, simplify and factor equations, and much more.
There’s really not much reason for anyone to do calculus manually nowadays—for
calculating gradients, PyTorch does it for us, and for showing the equations, SymPy
does it for us!
Once we have defined those functions, we can use them to write the backward pass.
Since each gradient is automatically populated in the right tensor, we don’t need to
store the results of those _grad functions anywhere—we just need to execute them in
out.g
the reverse order of the forward pass, to make sure that in each function exists:
<b>def</b> forward_and_backward(inp, targ):
<i>#</i> <i>forward</i> <i>pass:</i>
l1 = inp @ w1 + b1
l2 = relu(l1)
out = l2 @ w2 + b2
<i>#</i> <i>we</i> <i>don't</i> <i>actually</i> <i>need</i> <i>the</i> <i>loss</i> <i>in</i> <i>backward!</i>
loss = mse(out, targ)
<i>#</i> <i>backward</i> <i>pass:</i>
mse_grad(out, targ)
lin_grad(l2, out, w2, b2)
relu_grad(l1, l2)
lin_grad(inp, l1, w1, b1)
And now we can access the gradients of our model parameters in w1.g, b1.g, w2.g,
b2.g.
and We have sucessfuly defined our model—now let’s make it a bit more like a
PyTorch module."|calculus and SymPy; backward pass and; gradients and backward pass; symbolic computation library; SymPy library and calculus; SymPy library
"<b>KeepinginTouchwiththeLatestServices</b>
Services that can be used for creating datasets come and go all the
time, and their features, interfaces, and pricing change regularly
too. In this section, we’ll show how to use the Bing Image Search
API available as part of Azure Cognitive Services at the time this
book was written.
To download images with Bing Image Search, sign up at Microsoft for a free account.
You will be given a key, which you can copy and enter in a cell as follows (replacing
<i>XXX</i> with your key and executing it):
key = 'XXX'
Or, if you’re comfortable at the command line, you can set it in your terminal with
export AZURE_SEARCH_KEY=your_key_here
and then restart the Jupyter server, type this in a cell, and execute it:
key = os.environ['AZURE_SEARCH_KEY']
Once you’ve set key, you can use search_images_bing. This function is provided by
the small utils class included with the notebooks online (if you’re not sure where a
function is defined, you can just type it in your notebook to find out, as shown here):
search_images_bing
<function utils.search_images_bing(key, term, min_sz=128)>
Let’s try this function out:
results = search_images_bing(key, 'grizzly bear')
ims = results.attrgot('content_url')
len(ims)
150
We’ve successfully downloaded the URLs of 150 grizzly bears (or, at least, images that
Bing Image Search finds for that search term). Let’s look at one:
dest = 'images/grizzly.jpg'
download_url(ims[0], dest)
im = Image.open(dest)
im.to_thumb(128,128)"|Azure Cognitive Services (Microsoft); API; gathering data; process end-to-end; Azure Cognitive Services; notebooks; search_images_bing
"As long as qualified women keep dropping out of tech, teaching more girls to code
will not solve the diversity issues plaguing the field. Diversity initiatives often end up
focusing primarily on white women, even though women of color face many addi‐
tional barriers. In interviews with 60 women of color who work in STEM research,
100% had experienced discrimination.
The hiring process is particularly broken in tech. One study indicative of the disfunc‐
tion comes from Triplebyte, a company that helps place software engineers in compa‐
nies, conducting a standardized technical interview as part of this process. The
company has a fascinating dataset: the results of how over 300 engineers did on their
exam, coupled with the results of how those engineers did during the interview pro‐
cess for a variety of companies. The number one finding from Triplebyte’s research is
that “the types of programmers that each company looks for often have little to do
with what the company needs or does. Rather, they reflect company culture and the
backgrounds of the founders.”
This is a challenge for those trying to break into the world of deep learning, since
most companies’ deep learning groups today were founded by academics. These
groups tend to look for people “like them”—that is, people who can solve complex
math problems and understand dense jargon. They don’t always know how to spot
people who are actually good at solving real problems using deep learning.
This leaves a big opportunity for companies that are ready to look beyond status and
pedigree, and focus on results!
<header><largefont><b>Fairness,</b></largefont> <largefont><b>Accountability,</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Transparency</b></largefont></header>
The professional society for computer scientists, the ACM, runs a data ethics confer‐
ence called the Conference on Fairness, Accountability, and Transparency (ACM
FAccT), which used to go under the acronym FAT but now uses the less objectionable
FAccT. Microsoft also has a group focused on Fairness, Accountability, Transparency,
and Ethics in AI (FATE). In this section, we’ll use the acronym FAccT to refer to the
concepts of fairness, accountability, and transparency.
FAccT is a lens some people have used for considering ethical issues. One helpful
resource for this is the free online book <i>Fairness</i> <i>and</i> <i>Machine</i> <i>Learning:</i> <i>Limitations</i>
<i>and</i> <i>Opportunities</i> by Solon Barocas et al., which “gives a perspective on machine
learning that treats fairness as a central concern rather than an afterthought.” It also
warns, however, that it “is intentionally narrow in scope…A narrow framing of
machine learning ethics might be tempting to technologists and businesses as a way
to focus on technical interventions while sidestepping deeper questions about power
and accountability. We caution against this temptation.” Rather than provide an over‐
view of the FAccT approach to ethics (which is better done in books such as that one),
our focus here will be on the limitations of this kind of narrow framing."|accountability for ethics violations; Barocas; ethics; (Barocas; fairness; Hardt; machine learning (ML); Microsoft; Narayanan; Fairness and Machine Learning book
"<b>def</b> get_dls(bs, size):
dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),
get_items=get_image_files,
get_y=parent_label,
item_tfms=Resize(460),
batch_tfms=[*aug_transforms(size=size, min_scale=0.75),
Normalize.from_stats(*imagenet_stats)])
<b>return</b> dblock.dataloaders(path, bs=bs)
dls = get_dls(64, 224)
x,y = dls.one_batch()
x.mean(dim=[0,2,3]),x.std(dim=[0,2,3])
(TensorImage([-0.0787, 0.0525, 0.2136], device='cuda:5'),
TensorImage([1.2330, 1.2112, 1.3031], device='cuda:5'))
Let’s check what effect this had on training our model:
model = xresnet50()
learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)
learn.fit_one_cycle(5, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.632865 2.250024 0.391337 01:02
1 1.294041 1.579932 0.517177 01:02
2 0.960535 1.069164 0.657207 01:04
3 0.730220 0.767433 0.771845 01:05
4 0.577889 0.550673 0.824496 01:06
Although it helped only a little here, normalization becomes especially important
when using pretrained models. The pretrained model knows how to work with only
data of the type that it has seen before. If the average pixel value was 0 in the data it
was trained with, but your data has 0 as the minimum possible value of a pixel, then
the model is going to be seeing something very different from what is intended!
This means that when you distribute a model, you need to also distribute the statistics
used for normalization, since anyone using it for inference or transfer learning will
need to use the same statistics. By the same token, if you’re using a model that some‐
one else has trained, make sure you find out what normalization statistics they used,
and match them.
We didn’t have to handle normalization in previous chapters because when using a
pretrained model through cnn_learner, the fastai library automatically adds the
proper Normalize transform; the model has been pretrained with certain statistics in
Normalize (usually coming from the ImageNet dataset), so the library can fill those in
for you. Note that this applies to only pretrained models, which is why we need to
add this information manually here, when training from scratch."|cnn_learner; statistics distributed with model; image classifier model training; Learner; pretrained models
"We can tell Pandas about a suitable ordering of these levels like so:
sizes = 'Large','Large / Medium','Medium','Small','Mini','Compact'
df['ProductSize'] = df['ProductSize'].astype('category')
df['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)
The most important data column is the dependent variable—the one we want to pre‐
dict. Recall that a model’s metric is a function that reflects how good the predictions
are. It’s important to note what metric is being used for a project. Generally, selecting
the metric is an important part of the project setup. In many cases, choosing a good
metric will require more than just selecting a variable that already exists. It is more
like a design process. You should think carefully about which metric, or set of metric,
actually measures the notion of model quality that matters to you. If no variable rep‐
resents that metric, you should see if you can build the metric from the variables that
are available.
However, in this case, Kaggle tells us what metric to use: the root mean squared log
error (RMLSE) between the actual and predicted auction prices. We need do only a
small amount of processing to use this: we take the log of the prices, so that the
m_rmse of that value will give us what we ultimately need:
dep_var = 'SalePrice'
df[dep_var] = np.log(df[dep_var])
We are now ready to explore our first machine learning algorithm for tabular data:
decision trees.
<header><largefont><b>Decision</b></largefont> <largefont><b>Trees</b></largefont></header>
Decision tree ensembles, as the name suggests, rely on decision trees. So let’s start
there! A decision tree asks a series of binary (yes or no) questions about the data.
After each question, the data at that part of the tree is split between a Yes and a No
branch, as shown in Figure 9-6. After one or more questions, either a prediction can
be made on the basis of all previous answers or another question is required.
This sequence of questions is now a procedure for taking any data item, whether an
item from the training set or a new one, and assigning that item to a group. Namely,
after asking and answering the questions, we can say the item belongs to the same
group as all the other training data items that yielded the same set of answers to the
questions. But what good is this? The goal of our model is to predict values for items,
not to assign them into groups from the training dataset. The value is that we can
now assign a prediction value for each of these groups—for regression, we take the
target mean of the items in the group."|categorical variables; decision trees; prediction variable importance; metrics; root mean squared log error; predictions; root mean squared log error as metric; tabular data for models
"commentator is Alexis Gallagher. Alexis has a very diverse background: he has been a
researcher in mathematical biology, a screenplay writer, an improv performer, a
McKinsey consultant (like Jeremy!), a Swift coder, and a CTO.
<b>AlexisSays</b>
I’ve decided it’s time for me to learn about this AI stuff! After all,
I’ve tried pretty much everything else.…But I don’t really have a
background in building machine learning models. Still…how hard
can it be? I’m going to be learning throughout this book, just like
you are. Look out for my sidebars for learning tips that I found
helpful on my journey, and hopefully you will find helpful too.
<header><largefont><b>How</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Learn</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont></header>
Harvard professor David Perkins, who wrote <i>Making</i> <i>Learning</i> <i>Whole</i> (Jossey-Bass),
has much to say about teaching. The basic idea is to teach the <i>whole</i> <i>game.</i> That
means that if you’re teaching baseball, you first take people to a baseball game or get
them to play it. You don’t teach them how to wind twine to make a baseball from
scratch, the physics of a parabola, or the coefficient of friction of a ball on a bat.
Paul Lockhart, a Columbia math PhD, former Brown professor, and K–12 math
teacher, imagines in the influential essay “A Mathematician’s Lament” a nightmare
world where music and art are taught the way math is taught. Children are not
allowed to listen to or play music until they have spent over a decade mastering music
notation and theory, spending classes transposing sheet music into a different key. In
art class, students study colors and applicators, but aren’t allowed to actually paint
until college. Sound absurd? This is how math is taught—we require students to
spend years doing rote memorization and learning dry, disconnected <i>fundamentals</i>
that we claim will pay off later, long after most of them quit the subject.
Unfortunately, this is where many teaching resources on deep learning begin—asking
learners to follow along with the definition of the Hessian and theorems for the Tay‐
lor approximation of your loss functions, without ever giving examples of actual
working code. We’re not knocking calculus. We love calculus, and Sylvain has even
taught it at the college level, but we don’t think it’s the best place to start when learn‐
ing deep learning!
In deep learning, it really helps if you have the motivation to fix your model to get it
to do better. That’s when you start learning the relevant theory. But you need to have
the model in the first place. We teach almost everything through real examples. As we
build out those examples, we go deeper and deeper, and we’ll show you how to make
your projects better and better. This means that you’ll be gradually learning all the
theoretical foundations you need, in context, in such a way that you’ll see why it mat‐
ters and how it works."|how to learn; Lockhart; Making Learning Whole book (Perkins); Perkins
"path = untar_data(URLs.MNIST_SAMPLE)
im3 = Image.open(path/'train'/'3'/'12.png')
show_image(im3);
Now we’re going to take the top 3×3-pixel square of our image, and multiply each of
those values by each item in our kernel. Then we’ll add them up, like so:
im3_t = tensor(im3)
im3_t[0:3,0:3] * top_edge
tensor([[-0., -0., -0.],
[0., 0., 0.],
[0., 0., 0.]])
(im3_t[0:3,0:3] * top_edge).sum()
tensor(0.)
Not very interesting so far—all the pixels in the top-left corner are white. But let’s
pick a couple of more interesting spots:
df = pd.DataFrame(im3_t[:10,:20])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"|convolutional neural network (CNN); kernel of convolution
"Note that the method called and the method implemented are different, for each of
these methods:
<b>Class</b> <b>Tocall</b> <b>Toimplement</b>
nn.Module(PyTorch) ()(i.e.,callasfunction) forward
Transform () encodes
Transform decode() decodes
Transform setup() setups
So, for instance, you would never call setups directly, but instead would call setup.
setup setups
The reason is that does some work before and after calling for you. To
learn more about Transforms and how you can use them to implement different
behavior depending on the type of input, be sure to check the tutorials in the fastai
docs.
<header><largefont><b>Pipeline</b></largefont></header>
To compose several transforms together, fastai provides the Pipeline class. We define
a Pipeline by passing it a list of Transforms; it will then compose the transforms
Pipeline
inside it. When you call a on an object, it will automatically call the trans‐
forms inside, in order:
tfms = Pipeline([tok, num])
t = tfms(txts[0]); t[:20]
tensor([ 2, 8, 76, 10, 23, 3112, 23, 34, 3113, 33, 10, 8,
> 4477, 22, 88, 32, 10, 27, 42, 14])
And you can call decode on the result of your encoding, to get back something you
can display and analyze:
tfms.decode(t)[:100]
'xxbos xxmaj well , "" cube "" ( 1997 ) , xxmaj vincenzo \'s first movie , was one
> of the most interesti'
Transform
The only part that doesn’t work the same way as in is the setup. To prop‐
erly set up a Pipeline of Transforms on some data, you need to use a TfmdLists.
<header><largefont><b>TfmdLists</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Datasets:</b></largefont> <largefont><b>Transformed</b></largefont> <largefont><b>Collections</b></largefont></header>
Your data is usually a set of raw items (like filenames, or rows in a DataFrame) to
which you want to apply a succession of transformations. We just saw that a succes‐
sion of transformations is represented by a Pipeline in fastai. The class that groups
Pipeline TfmdLists.
this with your raw items is called"|mid-level API; TfmdLists; Pipeline class; Transforms
"That’s our starting point. Let’s train for one epoch and see if the accuracy improves:
lr = 1.
params = weights,bias
train_epoch(linear1, lr, params)
validate_epoch(linear1)
0.6883
Then do a few more:
<b>for</b> i <b>in</b> range(20):
train_epoch(linear1, lr, params)
<b>print(validate_epoch(linear1),</b> end=' ')
0.8314 0.9017 0.9227 0.9349 0.9438 0.9501 0.9535 0.9564 0.9594 0.9618 0.9613
> 0.9638 0.9643 0.9652 0.9662 0.9677 0.9687 0.9691 0.9691 0.9696
Looking good! We’re already about at the same accuracy as our “pixel similarity”
approach, and we’ve created a general-purpose foundation we can build on. Our next
step will be to create an object that will handle the SGD step for us. In PyTorch, it’s
called an <i>optimizer.</i>
<header><largefont><b>Creating</b></largefont> <largefont><b>an</b></largefont> <largefont><b>Optimizer</b></largefont></header>
Because this is such a general foundation, PyTorch provides some useful classes to
make it easier to implement. The first thing we can do is replace our linear function
with PyTorch’s nn.Linear module. A <i>module</i> is an object of a class that inherits from
nn.Module
the PyTorch class. Objects of this class behave identically to standard
Python functions, in that you can call them using parentheses, and they will return
the activations of a model.
nn.Linear does the same thing as our init_params and linear together. It contains
both the <i>weights</i> and <i>biases</i> in a single class. Here’s how we replicate our model from
the previous section:
linear_model = nn.Linear(28*28,1)
Every PyTorch module knows what parameters it has that can be trained; they are
available through the parameters method:
w,b = linear_model.parameters()
w.shape,b.shape
(torch.Size([1, 784]), torch.Size([1]))
We can use this information to create an optimizer:
<b>class</b> <b>BasicOptim:</b>
<b>def</b> <b>__init__(self,params,lr):</b> self.params,self.lr = list(params),lr
<b>def</b> step(self, *args, **kwargs):
<b>for</b> p <b>in</b> self.params: p.data -= p.grad.data * self.lr"|models returning; numerical digit classifier; modules; PyTorch; creating an optimizer; stochastic gradient descent (SGD)
"corrects = (preds>0.0).float() == train_y
corrects
tensor([[ True],
[ True],
[ True],
...,
[False],
[False],
[False]])
corrects.float().mean().item()
0.4912068545818329
Now let’s see what the change in accuracy is for a small change in one of the weights:
weights[0] *= 1.0001
preds = linear1(train_x)
((preds>0.0).float() == train_y).float().mean().item()
0.4912068545818329
As we’ve seen, we need gradients in order to improve our model using SGD, and in
order to calculate gradients we need a <i>loss</i> <i>function</i> that represents how good our
model is. That is because the gradients are a measure of how that loss function
changes with small tweaks to the weights.
So, we need to choose a loss function. The obvious approach would be to use accu‐
racy, which is our metric, as our loss function as well. In this case, we would calculate
our prediction for each image, collect these values to calculate an overall accuracy,
and then calculate the gradients of each weight with respect to that overall accuracy.
Unfortunately, we have a significant technical problem here. The gradient of a func‐
tion is its <i>slope,</i> or its steepness, which can be defined as <i>rise</i> <i>over</i> <i>run—that</i> is, how
much the value of the function goes up or down, divided by how much we changed
the input. We can write this mathematically as:
(y_new – y_old) / (x_new – x_old)
This gives a good approximation of the gradient when x_new is very similar to x_old,
meaning that their difference is very small. But accuracy changes at all only when a
prediction changes from a 3 to a 7, or vice versa. The problem is that a small change
in weights from x_old to x_new isn’t likely to cause any prediction to change, so
(y_new – y_old) will almost always be 0. In other words, the gradient is 0 almost
everywhere.
A very small change in the value of a weight will often not change the accuracy at all.
This means it is not useful to use accuracy as a loss function—if we do, most of the
time our gradients will be 0, and the model will not be able to learn from that
number."|gradients; MNIST loss function; numerical digit image classifier
"confidence that the input was a 3. Binary problems are a special case of classification
problem, because the target can be treated as a single Boolean value, as we did in
mnist_loss. But binary problems can also be thought of in the context of the more
general group of classifiers with any number of categories: in this case, we happen to
have two categories. As we saw in the bear classifier, our neural net will return one
activation per category.
So in the binary case, what do those activations really indicate? A single pair of acti‐
vations simply indicates the <i>relative</i> confidence of the input being a 3 versus being a 7.
The overall values, whether they are both high or both low, don’t matter—all that
matters is which is higher, and by how much.
We would expect that since this is just another way of representing the same problem,
we would be able to use sigmoid directly on the two-activation version of our neural
net. And indeed we can! We can just take the <i>difference</i> between the neural net activa‐
tions, because that reflects how much more sure we are of the input being a 3 than a
7, and then take the sigmoid of that:
(acts[:,0]-acts[:,1]).sigmoid()
tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661])
The second column (the probability of it being a 7) will then just be that value sub‐
tracted from 1. Now, we need a way to do all this that also works for more than two
columns. It turns out that this function, called softmax , is exactly that:
<b>def</b> softmax(x): <b>return</b> exp(x) / exp(x).sum(dim=1, keepdim=True)
<b>Jargon:ExponentialFunction(exp)</b>
Defined as e**x , where e is a special number approximately equal
to 2.718. It is the inverse of the natural logarithm function. Note
exp
that is always positive and increases <i>very</i> rapidly!
Let’s check that softmax returns the same values as sigmoid for the first column, and
those values subtracted from 1 for the second column:
sm_acts = torch.softmax(acts, dim=1)
sm_acts
tensor([[0.6025, 0.3975],
[0.5021, 0.4979],
[0.1332, 0.8668],
[0.9966, 0.0034],
[0.5959, 0.4041],
[0.3661, 0.6339]])
softmax sigmoid—we
is the multi-category equivalent of have to use it anytime we
have more than two categories and the probabilities of the categories must add to 1,"|cross-entropy loss; exponential function (exp); pet breeds image classifier; sigmoid function; two-activation version
"We seem to have a good baseline. What can we do now to make it even better?
<header><largefont><b>Improving</b></largefont> <largefont><b>Our</b></largefont> <largefont><b>Model</b></largefont></header>
We will now look at a range of techniques to improve the training of our model and
make it better. While doing so, we will explain a little bit more about transfer learning
and how to fine-tune our pretrained model as best as possible, without breaking the
pretrained weights.
The first thing we need to set when training a model is the learning rate. We saw in
the previous chapter that it needs to be just right to train as efficiently as possible, so
how do we pick a good one? fastai provides a tool for this.
<header><largefont><b>The</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Rate</b></largefont> <largefont><b>Finder</b></largefont></header>
One of the most important things we can do when training a model is to make sure
that we have the right learning rate. If our learning rate is too low, it can take many,
many epochs to train our model. Not only does this waste time, but it also means that
we may have problems with overfitting, because every time we do a complete pass
through the data, we give our model a chance to memorize it.
So let’s just make our learning rate really high, right? Sure, let’s try that and see what
happens:
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(1, base_lr=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 8.946717 47.954632 0.893775 00:20
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 7.231843 4.119265 0.954668 00:24
That doesn’t look good. Here’s what happened. The optimizer stepped in the correct
direction, but it stepped so far that it totally overshot the minimum loss. Repeating
that multiple times makes it get further and further away, not closer and closer!
What do we do to find the perfect learning rate—not too high and not too low? In
2015, researcher Leslie Smith came up with a brilliant idea, called the <i>learning</i> <i>rate</i>
<i>finder.</i> His idea was to start with a very, very small learning rate, something so small
that we would never expect it to be too big to handle. We use that for one mini-batch,
find what the losses are afterward, and then increase the learning rate by a certain
percentage (e.g., doubling it each time). Then we do another mini-batch, track the
loss, and double the learning rate again. We keep doing this until the loss gets worse,"|image classifier model training; learning rate finder; Smith
"By the end of the book, you’ll understand nearly all the code that’s inside fastai (and
much of PyTorch too), because in each chapter we’ll be digging a level deeper to show
you exactly what’s going on as we build and train our models. This means that you’ll
have learned the most important best practices used in modern deep learning—not
just how to use them, but how they really work and are implemented. If you want to
use those approaches in another framework, you’ll have the knowledge you need to
do so if needed.
Since the most important thing for learning deep learning is writing code and experi‐
menting, it’s important that you have a great platform for experimenting with code.
The most popular programming experimentation platform is called Jupyter. This is
what we will be using throughout this book. We will show you how you can use
Jupyter to train and experiment with models and introspect every stage of the data
preprocessing and model development pipeline. Jupyter is the most popular tool for
doing data science in Python, for good reason. It is powerful, flexible, and easy to use.
We think you will love it!
Let’s see it in practice and train our first model.
<header><largefont><b>Your</b></largefont> <largefont><b>First</b></largefont> <largefont><b>Model</b></largefont></header>
As we said before, we will teach you how to do things before we explain why they
work. Following this top-down approach, we will begin by actually training an image
classifier to recognize dogs and cats with almost 100% accuracy. To train this model
and run our experiments, you will need to do some initial setup. Don’t worry; it’s not
as hard as it looks.
<b>SylvainSays</b>
Do not skip the setup part even if it looks intimidating at first,
especially if you have little or no experience using things like a ter‐
minal or the command line. Most of that is not necessary, and you
will find that the easiest servers can be set up with just your usual
web browser. It is crucial that you run your own experiments in
parallel with this book in order to learn."|beginning; Jupyter Notebook; cats and dogs first model; dogs and cats first model; first model; notebooks; Python; setup; web resources
"saledate
The date of the sale.
In any sort of data science work, it’s important to <i>look</i> <i>at</i> <i>your</i> <i>data</i> <i>directly</i> to make
sure you understand the format, how it’s stored, what types of values it holds, etc.
Even if you’ve read a description of the data, the actual data may not be what you
expect. We’ll start by reading the training set into a Pandas DataFrame. Generally, it’s
a good idea to also specify low_memory=False unless Pandas actually runs out of
memory and returns an error. The low_memory parameter, which is True by default,
tells Pandas to look at only a few rows of data at a time to figure out what type of data
is in each column. This means that Pandas can end up using different data types for
different rows, which generally leads to data processing errors or model training
problems later.
Let’s load our data and have a look at the columns:
df = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)
df.columns
Index(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource',
'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand',
'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc',
'fiModelSeries', 'fiModelDescriptor', 'ProductSize',
'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc',
'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control',
'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension',
'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics',
'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size',
'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow',
'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb',
'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type',
'Travel_Controls', 'Differential_Type', 'Steering_Controls'],
dtype='object')
That’s a lot of columns for us to look at! Try looking through the dataset to get a sense
of what kind of information is in each one. We’ll shortly see how to “zero in” on the
most interesting bits.
At this point, a good next step is to handle <i>ordinal</i> <i>columns.</i> This refers to columns
containing strings or similar, but where those strings have a natural ordering. For
instance, here are the levels of ProductSize :
df['ProductSize'].unique()
array([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'],
> dtype=object)"|categorical variables; examining data importance; ordinal columns in tabular data; Pandas library; tabular data for models
"As we will discuss shortly, in addition, the vast majority of AI researchers and devel‐
opers are young white men. Most projects that we have seen do most user testing
using friends and families of the immediate product development group. Given this,
the kinds of problems we just discussed should not be surprising.
Similar historical bias is found in the texts used as data for natural language process‐
ing models. This crops up in downstream machine learning tasks in many ways. For
instance, it was widely reported that until last year, Google Translate showed system‐
atic bias in how it translated the Turkish gender-neutral pronoun “o” into English:
when applied to jobs that are often associated with males, it used “he,” and when
applied to jobs that are often associated with females, it used “she” (Figure 3-13).
<i>Figure</i> <i>3-13.</i> <i>Gender</i> <i>bias</i> <i>in</i> <i>text</i> <i>datasets</i>
We also see this kind of bias in online advertisements. For instance, a study in 2019
by Muhammad Ali et al. found that even when the person placing the ad does not
intentionally discriminate, Facebook will show ads to very different audiences based
on race and gender. Housing ads with the same text but picturing either a white or a
Black family were shown to racially different audiences.
<b>Measurementbias</b>
In “Does Machine Learning Automate Moral Hazard and Error” in <i>American</i> <i>Eco‐</i>
<i>nomic</i> <i>Review,</i> Sendhil Mullainathan and Ziad Obermeyer look at a model that tries
to answer this question: using historical electronic health record (EHR) data, what
factors are most predictive of stroke? These are the top predictors from the model:
• Prior stroke"|Ali; bias; ethics; electronic health record measurement bias; Facebook; gender; Google; historical bias; jobs and gender; measurement bias; medicine; Mullainathan; natural language processing (NLP); Obermeyer; occupations and gender; online advertisement bias; predictions; racial bias; research papers; stroke prediction; translation of languages
"Out-of-domain data and domain shift are examples of a larger problem: that you can
never fully understand all the possible behaviors of a neural network, because they
have far too many parameters. This is the natural downside of their best feature—
their flexibility, which enables them to solve complex problems where we may not
even be able to fully specify our preferred solution approaches. The good news, how‐
ever, is that there are ways to mitigate these risks using a carefully thought-out pro‐
cess. The details of this will vary depending on the details of the problem you are
solving, but we will attempt to lay out a high-level approach, summarized in
Figure 2-5, which we hope will provide useful guidance.
<i>Figure</i> <i>2-5.</i> <i>Deployment</i> <i>process</i>
Where possible, the first step is to use an entirely manual process, with your deep
learning model approach running in parallel but not being used directly to drive any
actions. The humans involved in the manual process should look at the deep learning
outputs and check whether they make sense. For instance, with our bear classifier, a
park ranger could have a screen displaying video feeds from all the cameras, with any
possible bear sightings simply highlighted in red. The park ranger would still be
expected to be just as alert as before the model was deployed; the model is simply
helping to check for problems at this point.
The second step is to try to limit the scope of the model, and have it carefully super‐
vised by people. For instance, do a small geographically and time-constrained trial of
the model-driven approach. Rather than rolling out our bear classifier in every
national park throughout the country, we could pick a single observation post, for a
one-week period, and have a park ranger check each alert before it goes out."|deep learning; model and human interaction; neural networks beyond understanding; deployment; process end-to-end; machine learning (ML); manual process in parallel; neural networks; production
"One other area where deep learning has dramatically improved in the last couple of
years is natural language processing (NLP). Computers can now generate text, trans‐
late automatically from one language to another, analyze comments, label words in
sentences, and much more. Here is all of the code necessary to train a model that can
classify the sentiment of a movie review better than anything that existed in the world
just five years ago:
<b>from</b> <b>fastai.text.all</b> <b>import</b> *
dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')
learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)
learn.fine_tune(4, 1e-2)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.594912 0.407416 0.823640 01:35
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.268259 0.316242 0.876000 03:03
1 0.184861 0.246242 0.898080 03:10
2 0.136392 0.220086 0.918200 03:16
3 0.106423 0.191092 0.931360 03:15"|movie review sentiment model; natural language processing (NLP)
"This is showing the mean average percent error (MAPE) compared among four mod‐
eling techniques, three of which we have already seen, along with <i>k-nearest</i> neighbors
(KNN), which is a very simple baseline method. The first numeric column contains
the results of using the methods on the data provided in the competition; the second
column shows what happens if you first train a neural network with categorical
embeddings, and then use those categorical embeddings instead of the raw categori‐
cal columns in the model. As you see, in every case, the models are dramatically
improved by using the embeddings instead of the raw categories.
This is a really important result, because it shows that you can get much of the perfor‐
mance improvement of a neural network without having to use a neural network at
inference time. You could just use an embedding, which is literally just an array
lookup, along with a small decision tree ensemble.
These embeddings need not even be necessarily learned separately for each model or
task in an organization. Instead, once a set of embeddings are learned for a column
for a particular task, they could be stored in a central place and reused across multiple
models. In fact, we know from private communication with other practitioners at
large companies that this is already happening in many places.
<header><largefont><b>Conclusion</b></largefont></header>
We have discussed two approaches to tabular modeling: decision tree ensembles and
neural networks. We’ve also mentioned two decision tree ensembles: random forests
and gradient boosting machines. Each is effective but also requires compromises:
• <i>Random</i> <i>forests</i> are the easiest to train, because they are extremely resilient to
hyperparameter choices and require little preprocessing. They are fast to train,
and should not overfit if you have enough trees. But they can be a little less accu‐
rate, especially if extrapolation is required, such as predicting future time periods.
• <i>Gradient</i> <i>boosting</i> <i>machines</i> in theory are just as fast to train as random forests,
but in practice you will have to try lots of hyperparameters. They can overfit, but
they are often a little more accurate than random forests.
• <i>Neural</i> <i>networks</i> take the longest time to train and require extra preprocessing,
such as normalization; this normalization needs to be used at inference time as
well. They can provide great results and extrapolate well, but only if you are care‐
ful with your hyperparameters and take care to avoid overfitting.
We suggest starting your analysis with a random forest. This will give you a strong
baseline, and you can be confident that it’s a reasonable starting point. You can then
use that model for feature selection and partial dependence analysis, to get a better
understanding of your data."|mean average percent error metric; mean average percent error; models; tabular data for models
"<header><largefont><b>From</b></largefont> <largefont><b>Dogs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Cats</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Pet</b></largefont> <largefont><b>Breeds</b></largefont></header>
In our very first model, we learned how to classify dogs versus cats. Just a few years
ago, this was considered a very challenging task—but today, it’s far too easy! We will
not be able to show you the nuances of training models with this problem, because
we get a nearly perfect result without worrying about any of the details. But it turns
out that the same dataset also allows us to work on a much more challenging prob‐
lem: figuring out what breed of pet is shown in each image.
In Chapter 1, we presented the applications as already-solved problems. But this is
not how things work in real life. We start with a dataset that we know nothing about.
We then have to figure out how it is put together, how to extract the data we need
from it, and what that data looks like. For the rest of this book, we will be showing
you how to solve these problems in practice, including all of the intermediate steps
necessary to understand the data that we are working with and test your modeling as
you go.
We already downloaded the Pets dataset, and we can get a path to this dataset using
the same code as in Chapter 1:
<b>from</b> <b>fastai2.vision.all</b> <b>import</b> *
path = untar_data(URLs.PETS)
Now if we are going to understand how to extract the breed of each pet from each
image, we’re going to need to understand how this data is laid out. Such details of
data layout are a vital piece of the deep learning puzzle. Data is usually provided in
one of these two ways:
• Individual files representing items of data, such as text documents or images,
possibly organized into folders or with filenames representing information about
those items
• A table of data (e.g., in CSV format) in which each row is an item and may
include filenames providing connections between the data in the table and data
in other formats, such as text documents and images
There are exceptions to these rules—particularly in domains such as genomics, where
there can be binary database formats or even network streams—but overall the vast
majority of the datasets you’ll work with will use some combination of these two
formats.
To see what is in our dataset, we can use the ls method:
path.ls()
(#3) [Path('annotations'),Path('images'),Path('models')]"|binary database format as data type; path to dataset; pet images; as data type; files as data type; pet images dataset
"returns the transpose of the matrix a . You can also have three or more members:
torch.einsum('bi,ij,bj->b', a, b, c)
This will return a vector of size b, where the k-th coordinate is the sum of a[k,i]
b[i,j] c[k,j]. This notation is particularly convenient when you have more dimen‐
sions because of batches. For example, if you have two batches of matrices and want
to compute the matrix product per batch, you could do this:
torch.einsum('bik,bkj->bij', a, b)
Let’s go back to our new matmul implementation using einsum and look at its speed:
%timeit -n 20 t5 = matmul(m1,m2)
68.7 µs ± 4.06 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)
As you can see, not only is it practical, but it’s <i>very</i> fast. einsum is often the fastest way
to do custom operations in PyTorch, without diving into C++ and CUDA. (But it’s
generally not as fast as carefully optimized CUDA code, as you see from the results in
“Matrix Multiplication from Scratch” on page 495.)
Now that we know how to implement a matrix multiplication from scratch, we are
ready to build our neural net—specifically, its forward and backward passes—using
just matrix multiplication.
<header><largefont><b>The</b></largefont> <largefont><b>Forward</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Backward</b></largefont> <largefont><b>Passes</b></largefont></header>
As we saw in Chapter 4, to train a model, we will need to compute all the gradients of
a given loss with respect to its parameters, which is known as the <i>backward</i> <i>pass.</i> In a
<i>forward</i> <i>pass,</i> where we compute the output of the model on a given input, based on
the matrix products. As we define our first neural net, we will also delve into the
problem of properly initializing the weights, which is crucial for making training start
properly.
<header><largefont><b>Defining</b></largefont></header>
<header><largefont><b>and</b></largefont> <largefont><b>Initializing</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Layer</b></largefont></header>
We will take the example of a two-layer neural net first. As we’ve seen, one layer can
be expressed as y = x @ w + b , with x our inputs, y our outputs, w the weights of the
layer (which is of size number of inputs by number of neurons if we don’t transpose
as before), and b is the bias vector:
<b>def</b> lin(x, w, b): <b>return</b> x @ w + b
We can stack the second layer on top of the first, but since mathematically the com‐
position of two linear operations is another linear operation, this makes sense only if
we put something nonlinear in the middle, called an activation function. As men‐
tioned at the beginning of this chapter, in deep learning applications the activation
function most commonly used is a ReLU, which returns the maximum of x and 0 ."|forward pass; backward pass; defining and initializing a layer; neural networks; building layer from scratch
"This has two problems, however: it can’t handle a stride other than 1, and it requires
that ni==nf. Stop for a moment to think carefully about why this is.
The issue is that with a stride of, say, 2 on one of the convolutions, the grid size of the
output activations will be half the size on each axis of the input. So then we can’t add
that back to x in forward because x and the output activations have different dimen‐
sions. The same basic issue occurs if ni!=nf: the shapes of the input and output con‐
nections won’t allow us to add them together.
To fix this, we need a way to change the shape of x to match the result of self.convs .
Halving the grid size can be done using an average pooling layer with a stride of 2:
that is, a layer that takes 2×2 patches from the input and replaces them with their
average.
Changing the number of channels can be done by using a convolution. We want this
skip connection to be as close to an identity map as possible, however, which means
making this convolution as simple as possible. The simplest possible convolution is
one with a kernel size of 1. That means that the kernel is size ni × nf × 1 × 1, so it’s
only doing a dot product over the channels of each input pixel—it’s not combining
across pixels at all. This kind of <i>1x1</i> <i>convolution</i> is widely used in modern CNNs, so
take a moment to think about how it works.
<b>Jargon:1x1Convolution</b>
A convolution with a kernel size of 1.
Here’s a ResBlock using these tricks to handle changing shape in the skip connection:
<b>def</b> _conv_block(ni,nf,stride):
<b>return</b> nn.Sequential(
ConvLayer(ni, nf, stride=stride),
ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero))
<b>class</b> <b>ResBlock(Module):</b>
<b>def</b> <b>__init__(self,</b> ni, nf, stride=1):
self.convs = _conv_block(ni,nf,stride)
self.idconv = noop <b>if</b> ni==nf <b>else</b> ConvLayer(ni, nf, 1, act_cls=None)
self.pool = noop <b>if</b> stride==1 <b>else</b> nn.AvgPool2d(2, ceil_mode=True)
<b>def</b> forward(self, x):
<b>return</b> F.relu(self.convs(x) + self.idconv(self.pool(x)))
Note that we’re using the noop function here, which simply returns its input
unchanged (noop is a computer science term that stands for “no operation”). In this
case, idconv does nothing at all if nf==nf, and pool does nothing if stride==1, which
is what we wanted in our skip connection."|building ResNet CNN; ResNet architecture; skip connections
"We also saw that the model in a Learner is generally an object of a class inheriting
from nn.Module, and that we can call it using parentheses and it will return the acti‐
vations of a model. You should pass it your independent variable, as a mini-batch. We
can try it out by grabbing a mini-batch from our DataLoader and then passing it to
the model:
x,y = dls.train.one_batch()
activs = learn.model(x)
activs.shape
torch.Size([64, 20])
Think about why activs has this shape—we have a batch size of 64, and we need to
calculate the probability of each of 20 categories. Here’s what one of those activations
looks like:
activs[0]
tensor([ 2.0258, -1.3543, 1.4640, 1.7754, -1.2820, -5.8053, 3.6130, 0.7193,
> -4.3683, -2.5001, -2.8373, -1.8037, 2.0122, 0.6189, 1.9729, 0.8999,
> -2.6769, -0.3829, 1.2212, 1.6073],
device='cuda:0', grad_fn=<SelectBackward>)
<b>GettingModelActivations</b>
Knowing how to manually get a mini-batch and pass it into a
model, and look at the activations and loss, is really important for
debugging your model. It is also very helpful for learning, so that
you can see exactly what is going on.
They aren’t yet scaled to between 0 and 1, but we learned how to do that in Chapter 4,
using the sigmoid function. We also saw how to calculate a loss based on this—this is
our loss function from Chapter 4, with the addition of log as discussed in the preced‐
ing chapter:
<b>def</b> binary_cross_entropy(inputs, targets):
inputs = inputs.sigmoid()
<b>return</b> -torch.where(targets==1, inputs, 1-inputs).log().mean()
Note that because we have a one-hot-encoded dependent variable, we can’t directly
use nll_loss or softmax (and therefore we can’t use cross_entropy ):
• softmax, as we saw, requires that all predictions sum to 1, and tends to push one
exp);
activation to be much larger than the others (because of the use of however,
we may well have multiple objects that we’re confident appear in an image, so
restricting the maximum sum of activations to 1 is not a good idea. By the same
reasoning, we may want the sum to be <i>less</i> than 1, if we don’t think <i>any</i> of the
categories appear in an image."|multi-label classifier; models returning; binary cross entropy loss function; labels; binary cross entropy; multi-label classifier loss function; models returning activations; Module class; predictions
"This is easy enough to add. We need to first change our data so that the dependent
variable has each of the three next words after each of our three input words. Instead
of 3, we use an attribute, sl (for sequence length), and make it a bit bigger:
sl = 16
seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1]))
<b>for</b> i <b>in</b> range(0,len(nums)-sl-1,sl))
cut = int(len(seqs) * 0.8)
dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs),
group_chunks(seqs[cut:], bs),
bs=bs, drop_last=True, shuffle=False)
Looking at the first element of seqs , we can see that it contains two lists of the same
size. The second list is the same as the first, but offset by one element:
[L(vocab[o] <b>for</b> o <b>in</b> s) <b>for</b> s <b>in</b> seqs[0]]
[(#16) ['one','.','two','.','three','.','four','.','five','.'...],
(#16) ['.','two','.','three','.','four','.','five','.','six'...]]
Now we need to modify our model so that it outputs a prediction after every word,
rather than just at the end of a three-word sequence:
<b>class</b> <b>LMModel4(Module):</b>
<b>def</b> <b>__init__(self,</b> vocab_sz, n_hidden):
self.i_h = nn.Embedding(vocab_sz, n_hidden)
self.h_h = nn.Linear(n_hidden, n_hidden)
self.h_o = nn.Linear(n_hidden,vocab_sz)
self.h = 0
<b>def</b> forward(self, x):
outs = []
<b>for</b> i <b>in</b> range(sl):
self.h = self.h + self.i_h(x[:,i])
self.h = F.relu(self.h_h(self.h))
outs.append(self.h_o(self.h))
self.h = self.h.detach()
<b>return</b> torch.stack(outs, dim=1)
<b>def</b> reset(self): self.h = 0
This model will return outputs of shape bs x sl x vocab_sz (since we stacked on
dim=1 ). Our targets are of shape bs x sl , so we need to flatten those before using
them in F.cross_entropy:
<b>def</b> loss_func(inp, targ):
<b>return</b> F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1))
We can now use this loss function to train the model:
learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func,
metrics=accuracy, cbs=ModelResetter)
learn.fit_one_cycle(15, 3e-3)"|improved RNN
"a huge model on all of your data for a really long time. But the reason that deep learn‐
ing is not straightforward is that your data, memory, and time are typically limited. If
you are running out of memory or time, the solution is to train a smaller model. If
you are not able to train for long enough to overfit, you are not taking advantage of
the capacity of your model.
So, step 1 is to get to the point where you can overfit. Then the question is how to
reduce that overfitting. Figure 15-3 shows how we recommend prioritizing the steps
from there.
<i>Figure</i> <i>15-3.</i> <i>Steps</i> <i>to</i> <i>reducing</i> <i>overfitting</i>
Many practitioners, when faced with an overfitting model, start at exactly the wrong
end of this diagram. Their starting point is to use a smaller model or more regulariza‐
tion. Using a smaller model should be absolutely the last step you take, unless train‐
ing your model is taking up too much time or memory. Reducing the size of your
model reduces the ability of your model to learn subtle relationships in your data.
Instead, your first step should be to seek to <i>create</i> <i>more</i> <i>data.</i> That could involve
adding more labels to data that you already have, finding additional tasks that your
model could be asked to solve (or, to think of it another way, identifying different
kinds of labels that you could model), or creating additional synthetic data by using
more or different data augmentation techniques. Thanks to the development of
Mixup and similar approaches, effective data augmentation is now available for
nearly all kinds of data."|models; overfitting; training
"most colored images have three values per pixel to define their color. We’ll look at
working with color images next.
<header><largefont><b>Color</b></largefont> <largefont><b>Images</b></largefont></header>
A color picture is a rank-3 tensor:
im = image2tensor(Image.open('images/grizzly.jpg'))
im.shape
torch.Size([3, 1000, 846])
show_image(im);
The first axis contains the channels red, green, and blue:
_,axs = subplots(1,3)
<b>for</b> bear,ax,color <b>in</b> zip(im,axs,('Reds','Greens','Blues')):
show_image(255-bear, ax=ax, cmap=color)
We saw what the convolution operation was for one filter on one channel of the
image (our examples were done on a square). A convolutional layer will take an
image with a certain number of channels (three for the first layer for regular RGB"|color image as rank-3 tensor; convolutional neural network (CNN); building a CNN
"On top of these, fastai provides two classes for bringing your training and validation
sets together:
Datasets
Dataset Dataset
An iterator that contains a training and a validation
DataLoaders
An object that contains a training DataLoader and a validation DataLoader
Since a DataLoader builds on top of a Dataset and adds additional functionality to it
(collating multiple items into a mini-batch), it’s often easiest to start by creating and
testing Datasets , and then look at DataLoaders after that’s working.
When we create a DataBlock, we build up gradually, step by step, and use the note‐
book to check our data along the way. This is a great way to make sure that you main‐
tain momentum as you are coding, and that you keep an eye out for any problems. It’s
easy to debug, because you know that if a problem arises, it is in the line of code you
just typed!
Let’s start with the simplest case, which is a data block created with no parameters:
dblock = DataBlock()
We can create a Datasets object from this. The only thing needed is a source—in this
case, our DataFrame:
dsets = dblock.datasets(df)
This contains a train and a valid dataset, which we can index into:
dsets.train[0]
(fname 008663.jpg
labels car person
is_valid False
Name: 4346, dtype: object,
fname 008663.jpg
labels car person
is_valid False
Name: 4346, dtype: object)
As you can see, this simply returns a row of the DataFrame, twice. This is because by
default, the data block assumes we have two things: input and target. We are going to
need to grab the appropriate fields from the DataFrame, which we can do by passing
get_x get_y
and functions:
dblock = DataBlock(get_x = <b>lambda</b> r: r['fname'], get_y = <b>lambda</b> r: r['labels'])
dsets = dblock.datasets(df)
dsets.train[0]
('005620.jpg', 'aeroplane')"|DataFrame to DataLoaders; DataLoaders object from; DataFrame converted to; Datasets iterator
"they broke frequently. IBM set up categorizations on its punch card system for the
way that each person was killed, which group they were assigned to, and the logistical
information necessary to track them through the vast Holocaust system (see
Figure 3-3). IBM’s code for Jews in the concentration camps was 8: some 6,000,000
were killed. Its code for Romanis was 12 (they were labeled by the Nazis as “asocials,”
with over 300,000 killed in the <i>Zigeunerlager,</i> or “Gypsy camp”). General executions
were coded as 4, death in the gas chambers as 6.
<i>Figure</i> <i>3-3.</i> <i>A</i> <i>punch</i> <i>card</i> <i>used</i> <i>by</i> <i>IBM</i> <i>in</i> <i>concentration</i> <i>camps</i>
Of course, the project managers and engineers and technicians involved were just liv‐
ing their ordinary lives. Caring for their families, going to the church on Sunday,
doing their jobs the best they could. Following orders. The marketers were just doing
what they could to meet their business development goals. As Edwin Black, author of
<i>IBM</i> <i>and</i> <i>the</i> <i>Holocaust</i> (Dialog Press) observed: “To the blind technocrat, the means
were more important than the ends. The destruction of the Jewish people became
even less important because the invigorating nature of IBM’s technical achievement
was only heightened by the fantastical profits to be made at a time when bread lines
stretched across the world.”
Step back for a moment and consider: How would you feel if you discovered that you
had been part of a system that ended up hurting society? Would you be open to find‐
ing out? How can you help make sure this doesn’t happen? We have described the
most extreme situation here, but there are many negative societal consequences
linked to AI and machine learning being observed today, some of which we’ll
describe in this chapter.
It’s not just a moral burden, either. Sometimes technologists pay very directly for their
actions. For instance, the first person who was jailed as a result of the Volkswagen"|Black; IBM and the Holocaust book (Black)
"You need an API key to use the Kaggle API; to get one, click your profile picture on
the Kaggle website and choose My Account; then click Create New API Token. This
will save a file called <i>kaggle.json</i> to your PC. You need to copy this key on your GPU
server. To do so, open the file you downloaded, copy the contents, and paste them
inside the single quotes in the following cell in the notebook associated with this
chapter (e.g., creds = '{""username"":""xxx"",""key"":""xxx""}'):
creds = ''
Then execute this cell (this needs to be run only once):
cred_path = Path('~/.kaggle/kaggle.json').expanduser()
<b>if</b> <b>not</b> cred_path.exists():
cred_path.parent.mkdir(exist_ok=True)
cred_path.write(creds)
cred_path.chmod(0o600)
Now you can download datasets from Kaggle! Pick a path to download the dataset to:
path = URLs.path('bluebook')
path
Path('/home/sgugger/.fastai/archive/bluebook')
And use the Kaggle API to download the dataset to that path and extract it:
<b>if</b> <b>not</b> path.exists():
path.mkdir()
api.competition_download_cli('bluebook-for-bulldozers', path=path)
file_extract(path/'bluebook-for-bulldozers.zip')
path.ls(file_type='text')
(#7) [Path('Valid.csv'),Path('Machine_Appendix.csv'),Path('ValidSolution.csv'),P
> ath('TrainAndValid.csv'),Path('random_forest_benchmark_test.csv'),Path('Test.
> csv'),Path('median_benchmark.csv')]
Now that we have downloaded our dataset, let’s take a look at it!
<header><largefont><b>Look</b></largefont> <largefont><b>at</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Data</b></largefont></header>
Kaggle provides information about some of the fields of our dataset. The Data page
explains that the key fields in <i>train.csv</i> are as follows:
SalesID
The unique identifier of the sale.
MachineID
The unique identifier of a machine. A machine can be sold multiple times.
saleprice
What the machine sold for at auction (provided only in <i>train.csv).</i>"|categorical variables; tabular data for models
"are classes, we have to instantiate them, which is why you see nn.ReLU in this
example.
Because nn.Sequential is a module, we can get its parameters, which will return a
list of all the parameters of all the modules it contains. Let’s try it out! As this is a
deeper model, we’ll use a lower learning rate and a few more epochs:
learn = Learner(dls, simple_net, opt_func=SGD,
loss_func=mnist_loss, metrics=batch_accuracy)
learn.fit(40, 0.1)
We’re not showing the 40 lines of output here to save room; the training process is
recorded in learn.recorder, with the table of output stored in the values attribute,
so we can plot the accuracy over training:
plt.plot(L(learn.recorder.values).itemgot(2));
And we can view the final accuracy:
learn.recorder.values[-1][2]
0.982826292514801
At this point, we have something that is rather magical:
• A function that can solve any problem to any level of accuracy (the neural net‐
work) given the correct set of parameters
• A way to find the best set of parameters for any function (stochastic gradient
descent)
This is why deep learning can do such fantastic things. Believing that this combina‐
tion of simple techniques can really solve any problem is one of the biggest steps that
we find many students have to take. It seems too good to be true—surely things
should be more difficult and complicated than this? Our recommendation: try it out!
We just tried it on the MNIST dataset, and you’ve seen the results. And since we are"|numerical digit classifier; PyTorch; creating an optimizer; stochastic gradient descent (SGD)
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>8</b></largefont></header>
<header><largefont><b>Collaborative</b></largefont> <largefont><b>Filtering</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Dive</b></largefont></header>
One common problem to solve is having a number of users and a number of prod‐
ucts, and you want to recommend which products are most likely to be useful for
which users. Many variations exist: for example, recommending movies (such as on
Netflix), figuring out what to highlight for a user on a home page, deciding what sto‐
ries to show in a social media feed, and so forth. A general solution to this problem,
called <i>collaborative</i> <i>filtering,</i> works like this: look at which products the current user
has used or liked, find other users who have used or liked similar products, and then
recommend other products that those users have used or liked.
For example, on Netflix, you may have watched lots of movies that are science fiction,
full of action, and were made in the 1970s. Netflix may not know these particular
properties of the films you have watched, but it will be able to see that other people
who have watched the same movies that you watched also tended to watch other
movies that are science fiction, full of action, and were made in the 1970s. In other
words, to use this approach, we don’t necessarily need to know anything about the
movies except who likes to watch them.
There is a more general class of problems that this approach can solve, not necessarily
involving users and products. Indeed, for collaborative filtering, we more commonly
refer to <i>items,</i> rather than <i>products.</i> Items could be links that people click, diagnoses
that are selected for patients, and so forth.
The key foundational idea is that of <i>latent</i> <i>factors.</i> In the Netflix example, we started
with the assumption that you like old, action-packed sci-fi movies. But you never told
Netflix that you like these kinds of movies. And Netflix never needed to add columns
to its movies table saying which movies are of these types. Still, there must be some
underlying concept of sci-fi, action, and movie age, and these concepts must be rele‐
vant for at least some people’s movie-watching decisions."|collaborative filtering; items rather than products; latent factors
"We begin by comparing the outputs the model gives us with our targets (we have
labeled data, so we know what result the model should give) using a <i>loss</i> <i>function,</i>
which returns a number that we want to make as low as possible by improving our
weights. To do this, we take a few data items (such as images) from the training set
and feed them to our model. We compare the corresponding targets using our loss
function, and the score we get tells us how wrong our predictions were. We then
change the weights a little bit to make it slightly better.
To find how to change the weights to make the loss a bit better, we use calculus to
calculate the <i>gradients.</i> (Actually, we let PyTorch do it for us!) Let’s consider an anal‐
ogy. Imagine you are lost in the mountains with your car parked at the lowest point.
To find your way back to it, you might wander in a random direction, but that proba‐
bly wouldn’t help much. Since you know your vehicle is at the lowest point, you
would be better off going downhill. By always taking a step in the direction of the
steepest downward slope, you should eventually arrive at your destination. We use the
magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to
take; specifically, we multiply the gradient by a number we choose called the <i>learning</i>
<i>rate</i> to decide on the step size. We then <i>iterate</i> until we have reached the lowest point,
which will be our parking lot; then we can <i>stop.</i>
All of what we just saw can be transposed directly to the MNIST dataset, except for
the loss function. Let’s now see how we can define a good training objective.
<header><largefont><b>The</b></largefont> <largefont><b>MNIST</b></largefont> <largefont><b>Loss</b></largefont> <largefont><b>Function</b></largefont></header>
We already have our xs—that is, our independent variables, the images themselves.
We’ll concatenate them all into a single tensor, and also change them from a list of
matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). We can do this using
view , which is a PyTorch method that changes the shape of a tensor without changing
its contents. -1 is a special parameter to view that means “make this axis as big as
necessary to fit all the data”:
train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)
We need a label for each image. We’ll use 1 for 3s and 0 for 7s:
train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)
train_x.shape,train_y.shape
(torch.Size([12396, 784]), torch.Size([12396, 1]))"|MNIST loss function; numerical digit image classifier
"What is remarkable is that deep learning has such varied applications, yet nearly all of
deep learning is based on a single innovative type of model: the neural network.
But neural networks are not, in fact, completely new. In order to have a wider per‐
spective on the field, it is worth starting with a bit of history.
<header><largefont><b>Neural</b></largefont> <largefont><b>Networks:</b></largefont> <largefont><b>A</b></largefont> <largefont><b>Brief</b></largefont> <largefont><b>History</b></largefont></header>
In 1943 Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, teamed
up to develop a mathematical model of an artificial neuron. In their paper “A Logical
Calculus of the Ideas Immanent in Nervous Activity,” they declared the following:
Because of the “all-or-none” character of nervous activity, neural events and the rela‐
tions among them can be treated by means of propositional logic. It is found that the
behavior of every net can be described in these terms.
McCulloch and Pitts realized that a simplified model of a real neuron could be repre‐
sented using simple addition and thresholding, as shown in Figure 1-1. Pitts was self-
taught, and by age 12, had received an offer to study at Cambridge University with
the great Bertrand Russell. He did not take up this invitation, and indeed throughout
his life did not accept any offers of advanced degrees or positions of authority. Most
of his famous work was done while he was homeless. Despite his lack of an officially
recognized position and increasing social isolation, his work with McCulloch was
influential and was taken up by a psychologist named Frank Rosenblatt.
<i>Figure</i> <i>1-1.</i> <i>Natural</i> <i>and</i> <i>artificial</i> <i>neurons</i>"|deep learning; history; McCulloch; neural networks; Pitts; Rosenblatt
"project goals being worked on, if they know that employees are not going to like the
answers. This is sometimes done by compartmentalizing pieces as much as possible.
In other words, we’re not saying that any of this is easy. It’s hard. It’s really hard. We all
have to do our best. And we have often seen that the people who do get involved in
the higher-level context of these projects, and attempt to develop cross-disciplinary
capabilities and teams, become some of the most important and well rewarded mem‐
bers of their organizations. It’s the kind of work that tends to be highly appreciated by
senior executives, even if it is sometimes considered rather uncomfortable by middle
management.
<header><largefont><b>Topics</b></largefont> <largefont><b>in</b></largefont> <largefont><b>Data</b></largefont> <largefont><b>Ethics</b></largefont></header>
Data ethics is a big field, and we can’t cover everything. Instead, we’re going to pick a
few topics that we think are particularly relevant:
• The need for recourse and accountability
• Feedback loops
• Bias
• Disinformation
Let’s look at each in turn.
<header><largefont><b>Recourse</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Accountability</b></largefont></header>
In a complex system, it is easy for no one person to feel responsible for outcomes.
While this is understandable, it does not lead to good results. In the earlier example
of the Arkansas healthcare system in which a bug led to people with cerebral palsy
losing access to needed care, the creator of the algorithm blamed government offi‐
cials, and government officials blamed those who implemented the software. NYU
professor Danah Boyd described this phenomenon: “Bureaucracy has often been
used to shift or evade responsibility….Today’s algorithmic systems are extending
bureaucracy.”
An additional reason why recourse is so necessary is that data often contains errors.
Mechanisms for audits and error correction are crucial. A database of suspected gang
members maintained by California law enforcement officials was found to be full of
errors, including 42 babies who had been added to the database when they were less
than 1 year old (28 of whom were marked as “admitting to being gang members”). In
this case, there was no process in place for correcting mistakes or removing people
after they’d been added. Another example is the US credit report system: a large-scale
study of credit reports by the Federal Trade Commission (FTC) in 2012 found that"|accountability for ethics violations; algorithm buggy; Arkansas healthcare buggy algorithm (ethics); buggy algorithm ethics; California gang database (ethics); credit report system errors (ethics); ethics; errors in data (ethics); healthcare benefits buggy algorithm; healthcare benefits buggy algorithm (ethics); database error ethics; recourse for ethics violations
"Much better! Now we just have to bring these ideas together, and we have Adam, fas‐
tai’s default optimizer.
<header><largefont><b>Adam</b></largefont></header>
Adam mixes the ideas of SGD with momentum and RMSProp together: it uses the
moving average of the gradients as a direction and divides by the square root of the
moving average of the gradients squared to give an adaptive learning rate to each
parameter.
There is one other difference in how Adam calculates moving averages. It takes the
<i>unbiased</i> moving average, which is
w.avg = beta * w.avg + (1-beta) * w.grad
unbias_avg = w.avg / (1 - (beta**(i+1)))
if we are the i-th iteration (starting at 0 as Python does). This divisor of
1 - (beta**(i+1)) makes sure the unbiased average looks more like the gradients at
the beginning (since beta < 1, the denominator is very quickly close to 1).
Putting everything together, our update step looks like this:
w.avg = beta1 * w.avg + (1-beta1) * w.grad
unbias_avg = w.avg / (1 - (beta1**(i+1)))
w.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2)
new_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps)
As for RMSProp, eps is usually set to 1e-8, and the default for (beta1,beta2) sug‐
gested by the literature is (0.9,0.999).
In fastai, Adam is the default optimizer we use since it allows faster training, but
we’ve found that beta2=0.99 is better suited to the type of schedule we are using.
beta1 is the momentum parameter, which we specify with the argument moms in our
call to fit_one_cycle. As for eps, fastai uses a default of 1e-5. eps is not just useful
eps
for numerical stability. A higher limits the maximum value of the adjusted learn‐
ing rate. To take an extreme example, if eps is 1, then the adjusted learning will never
be higher than the base learning rate.
Rather than show all the code for this in the book, we’ll let you look at the optimizer
notebook in fastai’s <i>https://oreil.ly/24_O[GitHub</i> <i>repository]</i> <i>(browse</i> <i>the</i> <i>_nbs</i> folder
and search for the notebook called <i>optimizer).</i> You’ll see all the code we’ve shown so
far, along with Adam and other optimizers, and lots of examples and tests.
One thing that changes when we go from SGD to Adam is the way we apply weight
decay, and it can have important consequences."|Adam; Adam as default; training
"Now let’s see whether the mistakes the model is making are mainly thinking that griz‐
zlies are teddies (that would be bad for safety!), or that grizzlies are black bears, or
something else. To visualize this, we can create a <i>confusion</i> <i>matrix:</i>
interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix()
The rows represent all the black, grizzly, and teddy bears in our dataset, respectively.
The columns represent the images that the model predicted as black, grizzly, and
teddy bears, respectively. Therefore, the diagonal of the matrix shows the images that
were classified correctly, and the off-diagonal cells represent those that were classified
incorrectly. This is one of the many ways that fastai allows you to view the results of
your model. It is (of course!) calculated using the validation set. With the color-
coding, the goal is to have white everywhere except the diagonal, where we want dark
blue. Our bear classifier isn’t making many mistakes!
It’s helpful to see where exactly our errors are occurring, to see whether they’re due to
a dataset problem (e.g., images that aren’t bears at all, or are labeled incorrectly) or a
model problem (perhaps it isn’t handling images taken with unusual lighting, or from
a different angle, etc.). To do this, we can sort our images by their loss.
The <i>loss</i> is a number that is higher if the model is incorrect (especially if it’s also confi‐
dent of its incorrect answer), or if it’s correct but not confident of its correct answer.
In the beginning of Part II, we’ll learn in depth how loss is calculated and used in the
plot_top_losses
training process. For now, shows us the images with the highest
loss in our dataset. As the title of the output says, each image is labeled with four
things: prediction, actual (target label), loss, and probability. The <i>probability</i> here is
the confidence level, from zero to one, that the model has assigned to its prediction:
interp.plot_top_losses(5, nrows=1)"|confusion matrix with image classifiers; process end-to-end; testing with confusion matrix; bear image classifier; probability as confidence level; testing models
"We won’t actually train our model in this chapter, so we’ll use random tensors for our
inputs and targets. Let’s say our inputs are 200 vectors of size 100, which we group
into one batch, and our targets are 200 random floats:
x = torch.randn(200, 100)
y = torch.randn(200)
For our two-layer model, we will need two weight matrices and two bias vectors. Let’s
say we have a hidden size of 50 and the output size is 1 (for one of our inputs, the
corresponding output is one float in this toy example). We initialize the weights ran‐
domly and the bias at zero:
w1 = torch.randn(100,50)
b1 = torch.zeros(50)
w2 = torch.randn(50,1)
b2 = torch.zeros(1)
Then the result of our first layer is simply this:
l1 = lin(x, w1, b1)
l1.shape
torch.Size([200, 50])
Note that this formula works with our batch of inputs, and returns a batch of hidden
state: l1 is a matrix of size 200 (our batch size) by 50 (our hidden size).
There is a problem with the way our model was initialized, however. To understand
it, we need to look at the mean and standard deviation (std) of l1:
l1.mean(), l1.std()
(tensor(0.0019), tensor(10.1058))
The mean is close to zero, which is understandable since both our input and weight
matrices have means close to zero. But the standard deviation, which represents how
far away our activations go from the mean, went from 1 to 10. This is a really big
problem because that’s with just one layer. Modern neural nets can have hundreds of
layers, so if each of them multiplies the scale of our activations by 10, we won’t have
numbers representable by a computer by the end of the last layer.
Indeed, if we make just 50 multiplications between x and random matrices of size
100×100, we’ll have this:
x = torch.randn(200, 100)
<b>for</b> i <b>in</b> range(50): x = x @ torch.randn(100,100)
x[0:5,0:5]
tensor([[nan, nan, nan, nan, nan],
[nan, nan, nan, nan, nan],
[nan, nan, nan, nan, nan],
[nan, nan, nan, nan, nan],
[nan, nan, nan, nan, nan]])"|defining and initializing a layer
"We will use Jupyter to do some little checks of our work along the way—in this case,
making sure that the number of returned items seems reasonable:
seven_tensors = [tensor(Image.open(o)) <b>for</b> o <b>in</b> sevens]
three_tensors = [tensor(Image.open(o)) <b>for</b> o <b>in</b> threes]
len(three_tensors),len(seven_tensors)
(6131, 6265)
<b>ListComprehensions</b>
List and dictionary comprehensions are a wonderful feature of
Python. Many Python programmers use them every day, including
the authors of this book—they are part of “idiomatic Python.” But
programmers coming from other languages may have never seen
them before. A lot of great tutorials are just a web search away, so
we won’t spend a long time discussing them now. Here is a quick
explanation and example to get you started. A list comprehension
looks like this: new_list = [f(o) for o in a_list if o>0].
This will return every element of a_list that is greater than 0, after
passing it to the function f. There are three parts here: the collec‐
tion you are iterating over (a_list), an optional filter (if o>0),
and something to do to each element ( f(o) ). It’s not only shorter to
write, but also way faster than the alternative ways of creating the
same list with a loop.
We’ll also check that one of the images looks OK. Since we now have tensors (which
Jupyter by default will print as values), rather than PIL images (which Jupyter by
default will display images), we need to use fastai’s show_image function to display it:
show_image(three_tensors[1]);
For every pixel position, we want to compute the average over all the images of the
intensity of that pixel. To do this, we first combine all the images in this list into a
single three-dimensional tensor. The most common way to describe such a tensor is
to call it a <i>rank-3</i> <i>tensor.</i> We often need to stack up individual tensors in a collection
stack
into a single tensor. Unsurprisingly, PyTorch comes with a function called that
we can use for this purpose."|list comprehensions; ideal digit creation; pixels; Python; show_image function; displaying as images
"<i>Figure</i> <i>12-6.</i> <i>2-layer</i> <i>RNN</i>
The unrolled representation is shown in Figure 12-7 (similar to Figure 12-3).
<i>Figure</i> <i>12-7.</i> <i>2-layer</i> <i>unrolled</i> <i>RNN</i>
Let’s see how to implement this in practice."|multilayer RNNs
"There are positive examples of people and organizations attempting to combat these
problems. Evan Estola, lead machine learning engineer at Meetup, discussed the
example of men expressing more interest than women in tech meetups. Taking gen‐
der into account could therefore cause Meetup’s algorithm to recommend fewer tech
meetups to women, and as a result, fewer women would find out about and attend
tech meetups, which could cause the algorithm to suggest even fewer tech meetups to
women, and so on in a self-reinforcing feedback loop. So, Evan and his team made
the ethical decision for their recommendation algorithm to not create such a feed‐
back loop, by explicitly not using gender for that part of their model. It is encouraging
to see a company not just unthinkingly optimize a metric, but consider its impact.
According to Evan, “You need to decide which feature not to use in your algorithm…
the most optimal algorithm is perhaps not the best one to launch into production.”
While Meetup chose to avoid such an outcome, Facebook provides an example of
allowing a runaway feedback loop to run wild. Like YouTube, it tends to radicalize
users interested in one conspiracy theory by introducing them to more. As Renee
DiResta, a researcher on proliferation of disinformation, writes:
Once people join a single conspiracy-minded [Facebook] group, they are algorithmi‐
cally routed to a plethora of others. Join an anti-vaccine group, and your suggestions
will include anti-GMO, chemtrail watch, flat Earther (yes, really), and “curing cancer
naturally” groups. Rather than pulling a user out of the rabbit hole, the recommenda‐
tion engine pushes them further in.
It is extremely important to keep in mind that this kind of behavior can happen, and
to either anticipate a feedback loop or take positive action to break it when you see
the first signs of it in your own projects. Another thing to keep in mind is <i>bias,</i> which,
as we discussed briefly in the previous chapter, can interact with feedback loops in
very troublesome ways.
<header><largefont><b>Bias</b></largefont></header>
Discussions of bias online tend to get pretty confusing pretty fast. The word “bias”
means so many different things. Statisticians often think when data ethicists are talk‐
ing about bias that they’re talking about the statistical definition of the term bias—but
they’re not. And they’re certainly not talking about the biases that appear in the
weights and biases that are the parameters of your model!
What they’re talking about is the social science concept of bias. In “A Framework for
Understanding Unintended Consequences of Machine Learning” MIT’s Harini
Suresh and John Guttag describe six types of bias in machine learning, summarized
in Figure 3-6."|bias; conspiracy theory feedback loop; ethics; DiResta; Estola; Facebook; conspiracy theories fed by; Facebook and conspiracy theories; Meetup recommendation algorithm; Guttag; machine learning (ML); conspiracy theory feedback loops; Meetup and gender; research papers; Suresh; web resources
"parameters—such as a neural net. Let’s find the parameters for f first, and then we’ll
come back and do the same thing for the MNIST dataset with a neural net.
We need to define first what we mean by “best.” We define this precisely by choosing
a <i>loss</i> <i>function,</i> which will return a value based on a prediction and a target, where
lower values of the function correspond to “better” predictions. For continuous data,
it’s common to use <i>mean</i> <i>squared</i> <i>error:</i>
<b>def</b> mse(preds, targets): <b>return</b> ((preds-targets)**2).mean()
Now, let’s work through our seven-step process.
<b>Step1:Initializetheparameters</b>
First, we initialize the parameters to random values and tell PyTorch that we want to
requires_grad_:
track their gradients using
params = torch.randn(3).requires_grad_()
<b>Step2:Calculatethepredictions</b>
Next, we calculate the predictions:
preds = f(time, params)
Let’s create a little function to see how close our predictions are to our targets, and
take a look:
<b>def</b> show_preds(preds, ax=None):
<b>if</b> ax <b>is</b> None: ax=plt.subplots()[1]
ax.scatter(time, speed)
ax.scatter(time, to_np(preds), color='red')
ax.set_ylim(-300,100)
show_preds(preds)
This doesn’t look very close—our random parameters suggest that the roller coaster
will end up going backward, since we have negative speeds!"|stochastic gradient descent (SGD); training; weights
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>13</b></largefont></header>
<header><largefont><b>Convolutional</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Networks</b></largefont></header>
In Chapter 4, we learned how to create a neural network recognizing images. We
were able to achieve a bit over 98% accuracy at distinguishing 3s from 7s—but we
also saw that fastai’s built-in classes were able to get close to 100%. Let’s start trying to
close the gap.
In this chapter, we will begin by digging into what convolutions are and building a
CNN from scratch. We will then study a range of techniques to improve training sta‐
bility and learn all the tweaks the library usually applies for us to get great results.
<header><largefont><b>The</b></largefont> <largefont><b>Magic</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Convolutions</b></largefont></header>
One of the most powerful tools that machine learning practitioners have at their dis‐
posal is <i>feature</i> <i>engineering.</i> A <i>feature</i> is a transformation of the data that is designed
add_datepart
to make it easier to model. For instance, the function that we used for
our tabular dataset preprocessing in Chapter 9 added date features to the Bulldozers
dataset. What kinds of features might we be able to create from images?
<b>Jargon:FeatureEngineering</b>
Creating new transformations of the input data in order to make it
easier to model.
In the context of an image, a feature is a visually distinctive attribute. For example,
the number 7 is characterized by a horizontal edge near the top of the digit, and a
top-right to bottom-left diagonal edge underneath that. On the other hand, the num‐
ber 3 is characterized by a diagonal edge in one direction at the top left and bottom
right of the digit, the opposite diagonal at the bottom left and top right, horizontal"|feature engineering; machine learning (ML)
"This might be easier to represent in pictorial form, so let’s define a simple pictorial
representation of basic neural networks. Figure 12-1 shows how we’re going to repre‐
sent a neural net with one hidden layer.
<i>Figure</i> <i>12-1.</i> <i>Pictorial</i> <i>representation</i> <i>of</i> <i>a</i> <i>simple</i> <i>neural</i> <i>network</i>
Each shape represents activations: rectangle for input, circle for hidden (inner) layer
activations, and triangle for output activations. We will use those shapes (summarized
in Figure 12-2) in all the diagrams in this chapter.
<i>Figure</i> <i>12-2.</i> <i>Shapes</i> <i>used</i> <i>in</i> <i>our</i> <i>pictorial</i> <i>representations</i>"|language model; natural language processing (NLP)
"<header><largefont><b>Refactoring</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Model</b></largefont></header>
The three functions we used have two associated functions: a forward pass and a
backward pass. Instead of writing them separately, we can create a class to wrap them
together. That class can also store the inputs and outputs for the backward pass. This
way, we will just have to call backward:
<b>class</b> <b>Relu():</b>
<b>def</b> <b>__call__(self,</b> inp):
self.inp = inp
self.out = inp.clamp_min(0.)
<b>return</b> self.out
<b>def</b> backward(self): self.inp.g = (self.inp>0).float() * self.out.g
__call__ is a magic name in Python that will make our class callable. This is what
will be executed when we type y = Relu()(x). We can do the same for our linear
layer and the MSE loss:
<b>class</b> <b>Lin():</b>
<b>def</b> <b>__init__(self,</b> w, b): self.w,self.b = w,b
<b>def</b> <b>__call__(self,</b> inp):
self.inp = inp
self.out = inp@self.w + self.b
<b>return</b> self.out
<b>def</b> backward(self):
self.inp.g = self.out.g @ self.w.t()
self.w.g = self.inp.t() @ self.out.g
self.b.g = self.out.g.sum(0)
<b>class</b> <b>Mse():</b>
<b>def</b> <b>__call__(self,</b> inp, targ):
self.inp = inp
self.targ = targ
self.out = (inp.squeeze() - targ).pow(2).mean()
<b>return</b> self.out
<b>def</b> backward(self):
x = (self.inp.squeeze()-self.targ).unsqueeze(-1)
self.inp.g = 2.*x/self.targ.shape[0]"|refactoring the model; refactoring parts of neural networks
"<i>Figure</i> <i>3-6.</i> <i>Bias</i> <i>in</i> <i>machine</i> <i>learning</i> <i>can</i> <i>come</i> <i>from</i> <i>multiple</i> <i>sources</i> <i>(courtesy</i> <i>of</i> <i>Harini</i>
<i>Suresh</i> <i>and</i> <i>John</i> <i>V.</i> <i>Guttag)</i>
We’ll discuss four of these types of bias, those that we’ve found most helpful in our
own work (see the paper for details on the others).
<b>Historicalbias</b>
<i>Historical</i> <i>bias</i> comes from the fact that people are biased, processes are biased, and
society is biased. Suresh and Guttag say: “Historical bias is a fundamental, structural
issue with the first step of the data generation process and can exist even given perfect
sampling and feature selection.”
For instance, here are a few examples of historical <i>race</i> <i>bias</i> in the US, from the <i>New</i>
<i>York</i> <i>Times</i> article “Racial Bias, Even When We Have Good Intentions” by the Univer‐
sity of Chicago’s Sendhil Mullainathan:
• When doctors were shown identical files, they were much less likely to recom‐
mend cardiac catheterization (a helpful procedure) to Black patients.
• When bargaining for a used car, Black people were offered initial prices $700
higher and received far smaller concessions."|bias; ethics; historical bias; racial bias
"Then we’ll test the model on the full dataset. The blue dots are the training data, and
the red dots are the predictions:
plt.scatter(x_lin, y_lin, 20)
plt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5);
We have a big problem! Our predictions outside the domain that our training data
covered are all too low. Why do you suppose this is?
Remember, a random forest just averages the predictions of a number of trees. And a
tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a
random forest can never predict values outside the range of the training data. This is
particularly problematic for data indicating a trend over time, such as inflation, and
you wish to make predictions for a future time. Your predictions will be systemati‐
cally too low.
But the problem extends beyond time variables. Random forests are not able to
extrapolate outside the types of data they have seen, in a more general sense. That’s
why we need to make sure our validation set does not contain out-of-domain data.
<header><largefont><b>Finding</b></largefont> <largefont><b>Out-of-Domain</b></largefont> <largefont><b>Data</b></largefont></header>
Sometimes it is hard to know whether your test set is distributed in the same way as
your training data, or, if it is different, which columns reflect that difference. There’s
an easy way to figure this out, which is to use a random forest!
But in this case, we don’t use the random forest to predict our actual dependent vari‐
able. Instead, we try to predict whether a row is in the validation set or the training
set. To see this in action, let’s combine our training and validation sets, create a
dependent variable that represents which dataset each row comes from, build a ran‐
dom forest using that data, and get its feature importance:"|bagging; out-of-domain data; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"Now that we have assembled our data in a format fit for model training, let’s train an
image classifier using it.
<header><largefont><b>Training</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Model,</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Using</b></largefont> <largefont><b>It</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Clean</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Data</b></largefont></header>
Time to use the same lines of code as in Chapter 1 to train our bear classifier. We
don’t have a lot of data for our problem (150 pictures of each sort of bear at most), so
to train our model, we’ll use RandomResizedCrop , an image size of 224 pixels, which is
fairly standard for image classification, and the default aug_transforms:
bears = bears.new(
item_tfms=RandomResizedCrop(224, min_scale=0.5),
batch_tfms=aug_transforms())
dls = bears.dataloaders(path)
We can now create our Learner and fine-tune it in the usual way:
learn = cnn_learner(dls, resnet18, metrics=error_rate)
learn.fine_tune(4)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 1.235733 0.212541 0.087302 00:05
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.213371 0.112450 0.023810 00:05
1 0.173855 0.072306 0.023810 00:06
2 0.147096 0.039068 0.015873 00:06
3 0.123984 0.026801 0.015873 00:06"|process end-to-end; training the model
"<i>recommendation</i> <i>system</i> that can predict what products a user might purchase. This is
often used in ecommerce, such as to customize products shown on a home page by
showing the highest-ranked items. But such a model is generally created by looking at
a user and their buying history (inputs) and what they went on to buy or look at
(labels), which means that the model is likely to tell you about products the user
already has, or already knows about, rather than new products that they are most
likely to be interested in hearing about. That’s very different from what, say, an expert
at your local bookseller might do, where they ask questions to figure out your taste,
and then tell you about authors or series that you’ve never heard of before.
Another critical insight comes from considering how a model interacts with its envi‐
ronment. This can create <i>feedback</i> <i>loops,</i> as described here:
1. A <i>predictive</i> <i>policing</i> model is created based on where arrests have been made in
the past. In practice, this is not actually predicting crime, but rather predicting
arrests, and is therefore partially simply reflecting biases in existing policing
processes.
2. Law enforcement officers then might use that model to decide where to focus
their policing activity, resulting in increased arrests in those areas.
3. Data on these additional arrests would then be fed back in to retrain future ver‐
sions of the model.
This is a <i>positive</i> <i>feedback</i> <i>loop:</i> the more the model is used, the more biased the data
becomes, making the model even more biased, and so forth.
Feedback loops can also create problems in commercial settings. For instance, a video
recommendation system might be biased toward recommending content consumed
by the biggest watchers of video (e.g., conspiracy theorists and extremists tend to
watch more online video content than the average), resulting in those users increas‐
ing their video consumption, resulting in more of those kinds of videos being recom‐
mended. We’ll consider this topic in more detail in Chapter 3.
Now that you have seen the base of the theory, let’s go back to our code example and
see in detail how the code corresponds to the process we just described.
<header><largefont><b>How</b></largefont> <largefont><b>Our</b></largefont> <largefont><b>Image</b></largefont> <largefont><b>Recognizer</b></largefont> <largefont><b>Works</b></largefont></header>
Let’s see just how our image recognizer code maps to these ideas. We’ll put each line
into a separate cell, and look at what each one is doing (we won’t explain every detail
of every parameter yet, but will give a description of the important bits; full details
will come later in the book). The first line imports all of the fastai.vision library:
<b>from</b> <b>fastai.vision.all</b> <b>import</b> *"|beginning; bias; feedback loops; limitations inherent to; notebooks; positive feedback loop; recommendation systems
"For instance, let’s talk about something that is critically important for autonomous
vehicles: localizing objects in a picture. If a self-driving car doesn’t know where a
pedestrian is, then it doesn’t know how to avoid one! Creating a model that can rec‐
ognize the content of every individual pixel in an image is called <i>segmentation.</i> Here
is how we can train a segmentation model with fastai, using a subset of the <i>CamVid</i>
dataset from the paper “Semantic Object Classes in Video: A High-Definition
Ground Truth Database” by Gabriel J. Brostow et al.:
path = untar_data(URLs.CAMVID_TINY)
dls = SegmentationDataLoaders.from_label_func(
path, bs=8, fnames = get_image_files(path/""images""),
label_func = <b>lambda</b> o: path/'labels'/f'{o.stem}_P{o.suffix}',
codes = np.loadtxt(path/'codes.txt', dtype=str)
)
learn = unet_learner(dls, resnet34)
learn.fine_tune(8)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 2.906601 2.347491 00:02
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 1.988776 1.765969 00:02
1 1.703356 1.265247 00:02
2 1.591550 1.309860 00:02
3 1.459745 1.102660 00:02
4 1.324229 0.948472 00:02
5 1.205859 0.894631 00:02
6 1.102528 0.809563 00:02
7 1.020853 0.805135 00:02
We are not even going to walk through this code line by line, because it is nearly iden‐
tical to our previous example! (We will be doing a deep dive into segmentation mod‐
els in Chapter 15, along with all of the other models that we are briefly introducing in
this chapter and many, many more.)
We can visualize how well it achieved its task by asking the model to color-code each
pixel of an image. As you can see, it nearly perfectly classifies every pixel in every
object. For instance, notice that all of the cars are overlaid with the same color, and all
of the trees are overlaid with the same color (in each pair of images, the lefthand
image is the ground truth label, and the right is the prediction from the model):
learn.show_results(max_n=6, figsize=(7,8))"|autonomous vehicles; Brostow; Cipolla; autonomous vehicles localizing objects; Fauqueur; training a segmentation model; autonomous vehicle training; self-driving cars; web resources
"<b>Step3:Calculatetheloss</b>
We calculate the loss as follows:
loss = mse(preds, speed)
loss
tensor(25823.8086, grad_fn=<MeanBackward0>)
Our goal is now to improve this. To do that, we’ll need to know the gradients.
<b>Step4:Calculatethegradients</b>
The next step is to calculate the gradients, or an approximation of how the parame‐
ters need to change:
loss.backward()
params.grad
tensor([-53195.8594, -3419.7146, -253.8908])
params.grad * 1e-5
tensor([-0.5320, -0.0342, -0.0025])
We can use these gradients to improve our parameters. We’ll need to pick a learning
rate (we’ll discuss how to do that in practice in the next chapter; for now, we’ll just use
1e-5 or 0.00001):
params
tensor([-0.7658, -0.7506, 1.3525], requires_grad=True)
<b>Step5:Steptheweights</b>
Now we need to update the parameters based on the gradients we just calculated:
lr = 1e-5
params.data -= lr * params.grad.data
params.grad = None
<b>AlexisSays</b>
Understanding this bit depends on remembering recent history. To
calculate the gradients, we call backward on the loss . But this loss
was itself calculated by mse , which in turn took preds as an input,
which was calculated using f taking as an input params , which was
the object on which we originally called required_grads_ —which
is the original call that now allows us to call backward on loss.
This chain of function calls represents the mathematical composi‐
tion of functions, which enables PyTorch to use calculus’s chain
rule under the hood to calculate these gradients."|stochastic gradient descent (SGD); training; weights
"So, now you’ve seen what an image looks like to a computer, let’s recall our goal: cre‐
ate a model that can recognize 3s and 7s. How might you go about getting a computer
to do that?
<b>StopandThink!</b>
Before you read on, take a moment to think about how a computer
might be able to recognize these two digits. What kinds of features
might it be able to look at? How might it be able to identify these
features? How could it combine them? Learning works best when
you try to solve problems yourself, rather than just reading some‐
body else’s answers; so step away from this book for a few minutes,
grab a piece of paper and pen, and jot some ideas down.
<header><largefont><b>First</b></largefont> <largefont><b>Try:</b></largefont> <largefont><b>Pixel</b></largefont> <largefont><b>Similarity</b></largefont></header>
So, here is a first idea: how about we find the average pixel value for every pixel of the
3s, then do the same for the 7s. This will give us two group averages, defining what
we might call the “ideal” 3 and 7. Then, to classify an image as one digit or the other,
we see which of these two ideal digits the image is most similar to. This certainly
seems like it should be better than nothing, so it will make a good baseline.
<b>Jargon:Baseline</b>
A simple model that you are confident should perform reasonably
well. It should be simple to implement and easy to test, so that you
can then test each of your improved ideas and make sure they are
always better than your baseline. Without starting with a sensible
baseline, it is difficult to know whether your super-fancy models
are any good. One good approach to creating a baseline is doing
what we have done here: think of a simple, easy-to-implement
model. Another good approach is to search around to find other
people who have solved problems similar to yours, and download
and run their code on your dataset. Ideally, try both of these!
Step 1 for our simple model is to get the average of pixel values for each of our two
groups. In the process of doing this, we will learn a lot of neat Python numeric pro‐
gramming tricks!
Let’s create a tensor containing all of our 3s stacked together. We already know how to
create a tensor containing a single image. To create a tensor containing all the images
in a directory, we will first use a Python list comprehension to create a plain list of the
single image tensors."|begin with simple baseline model; models; ideal digit creation; pixels; tensors; training
"<b>SylvainSays</b>
An interesting feature about cross-entropy loss appears when we
consider its gradient. The gradient of cross_entropy(a,b) is
softmax(a)-b. Since softmax(a) is the final activation of the
model, that means that the gradient is proportional to the differ‐
ence between the prediction and the target. This is the same as
mean squared error in regression (assuming there’s no final activa‐
tion function such as that added by y_range ), since the gradient of
(a-b)**2 is 2*(a-b) . Because the gradient is linear, we won’t see
sudden jumps or exponential increases in gradients, which should
lead to smoother training of models.
We have now seen all the pieces hidden behind our loss function. But while this puts
a number on how well (or badly) our model is doing, it does nothing to help us know
if it’s any good. Let’s now see some ways to interpret our model’s predictions.
<header><largefont><b>Model</b></largefont> <largefont><b>Interpretation</b></largefont></header>
It’s very hard to interpret loss functions directly, because they are designed to be
things computers can differentiate and optimize, not things that people can under‐
stand. That’s why we have metrics. These are not used in the optimization process,
but just to help us poor humans understand what’s going on. In this case, our accu‐
racy is looking pretty good already! So where are we making mistakes?
We saw in Chapter 1 that we can use a confusion matrix to see where our model is
doing well and where it’s doing badly:
interp = ClassificationInterpretation.from_learner(learn)
interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"|confusion matrix with image classifiers; cross-entropy loss; pet breeds image classifier
"We can now use this approach to find the best threshold level:
xs = torch.linspace(0.05,0.95,29)
accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) <b>for</b> i <b>in</b> xs]
plt.plot(xs,accs);
In this case, we’re using the validation set to pick a hyperparameter (the threshold),
which is the purpose of the validation set. Sometimes students have expressed their
concern that we might be <i>overfitting</i> to the validation set, since we’re trying lots of
values to see which is the best. However, as you see in the plot, changing the thres‐
hold in this case results in a smooth curve, so we’re clearly not picking an inappropri‐
ate outlier. This is a good example of where you have to be careful of the difference
between theory (don’t try lots of hyperparameter values or you might overfit the vali‐
dation set) versus practice (if the relationship is smooth, it’s fine to do this).
This concludes the part of this chapter dedicated to multi-label classification. Next,
we’ll take a look at a regression problem.
<header><largefont><b>Regression</b></largefont></header>
It’s easy to think of deep learning models as being classified into domains, like <i>com‐</i>
<i>puter</i> <i>vision,</i> <i>NLP,</i> and so forth. And indeed, that’s how fastai classifies its applications
—largely because that’s how most people are used to thinking of things.
But really, that’s hiding a more interesting and deeper perspective. A model is defined
by its independent and dependent variables, along with its loss function. That means
that there’s really a far wider array of models than just the simple domain-based split.
Perhaps we have an independent variable that’s an image, and a dependent that’s text
(e.g., generating a caption from an image); or perhaps we have an independent vari‐
able that’s text and a dependent that’s an image (e.g., generating an image from a cap‐
tion—which is actually possible for deep learning to do!); or perhaps we’ve got
images, texts, and tabular data as independent variables, and we’re trying to predict
product purchases…the possibilities really are endless."|binary cross entropy loss function; dependent variable; validation set picking threshold; independent variable; loss; binary cross entropy; models; hyperparameter picked by
"Taking the mean of the positive or negative log of our probabilities (depending on
whether it’s the correct or incorrect class) gives us the <i>negative</i> <i>log</i> <i>likelihood</i> loss. In
PyTorch, nll_loss assumes that you already took the log of the softmax, so it doesn’t
do the logarithm for you.
<b>ConfusingName,Beware</b>
The “nll” in nll_loss stands for “negative log likelihood,” but it
doesn’t actually take the log at all! It assumes you have <i>already</i>
taken the log. PyTorch has a function called log_softmax that
combines log and softmax in a fast and accurate way. nll_loss is
designed to be used after log_softmax.
When we first take the softmax, and then the log likelihood of that, that combination
nn.CrossEntropyLoss
is called <i>cross-entropy</i> <i>loss.</i> In PyTorch, this is available as
(which, in practice, does log_softmax and then nll_loss):
loss_func = nn.CrossEntropyLoss()
As you see, this is a class. Instantiating it gives you an object that behaves like a
function:
loss_func(acts, targ)
tensor(1.8045)
All PyTorch loss functions are provided in two forms, the class form just shown as
well as a plain functional form, available in the F namespace:
F.cross_entropy(acts, targ)
tensor(1.8045)
Either one works fine and can be used in any situation. We’ve noticed that most peo‐
ple tend to use the class version, and that’s more often used in PyTorch’s official docs
and examples, so we’ll tend to use that too.
By default, PyTorch loss functions take the mean of the loss of all items. You can use
reduction='none' to disable that:
nn.CrossEntropyLoss(reduction='none')(acts, targ)
tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048])"|cross-entropy loss; class versus plain functional form; pet breeds image classifier; negative log likelihood; negative log likelihood loss (nll_loss)
"bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))
dls = bears.dataloaders(path)
dls.train.show_batch(max_n=4, nrows=1, unique=True)
Here, we used unique=True to have the same image repeated with different versions
of this RandomResizedCrop transform.
RandomResizedCrop is a specific example of a more general technique, called data
augmentation.
<header><largefont><b>Data</b></largefont> <largefont><b>Augmentation</b></largefont></header>
<i>Data</i> <i>augmentation</i> refers to creating random variations of our input data, such that
they appear different but do not change the meaning of the data. Examples of com‐
mon data augmentation techniques for images are rotation, flipping, perspective
warping, brightness changes, and contrast changes. For natural photo images such as
the ones we are using here, a standard set of augmentations that we have found work
aug_transforms
pretty well are provided with the function.
Because our images are now all the same size, we can apply these augmentations to an
entire batch of them using the GPU, which will save a lot of time. To tell fastai we
want to use these transforms on a batch, we use the batch_tfms parameter (note that
we’re not using RandomResizedCrop in this example, so you can see the differences
more clearly; we’re also using double the amount of augmentation compared to the
default, for the same reason):
bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))
dls = bears.dataloaders(path)
dls.train.show_batch(max_n=8, nrows=2, unique=True)"|batch operations; data augmentation; data augmentation definition
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>15</b></largefont></header>
<header><largefont><b>Application</b></largefont> <largefont><b>Architectures</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Dive</b></largefont></header>
We are now in the exciting position that we can fully understand the architectures
that we have been using for our state-of-the-art models for computer vision, natural
language processing, and tabular analysis. In this chapter, we’re going to fill in all the
missing details on how fastai’s application models work and show you how to build
them.
We will also go back to the custom data preprocessing pipeline we saw in Chapter 11
for Siamese networks and show you how to use the components in the fastai library
to build custom pretrained models for new tasks.
We’ll start with computer vision.
<header><largefont><b>Computer</b></largefont> <largefont><b>Vision</b></largefont></header>
cnn_learner
For computer vision applications, we use the functions and
unet_learner to build our models, depending on the task. In this section, we’ll
explore how to build the Learner objects we used in Parts I and II of this book.
<header><largefont><b>cnn_learner</b></largefont></header>
Let’s take a look at what happens when we use the cnn_learner function. We begin by
passing this function an architecture to use for the <i>body</i> of the network. Most of the
time, we use a ResNet, which you already know how to create, so we don’t need to
delve into that any further. Pretrained weights are downloaded as required and
loaded into the ResNet.
Then, for transfer learning, the network needs to be <i>cut.</i> This refers to slicing off the
final layer, which is responsible only for ImageNet-specific categorization. In fact, we
do not slice off only this layer, but everything from the adaptive average pooling layer"|architecture of model; cnn_learner; cutting network; head of model; Learner; transfer learning
"We’ve handled four of these already; only the second question remains. To answer
this question, we need to use the <i>treeinterpreter</i> library. We’ll also use the <i>waterfall‐</i>
<i>charts</i> library to draw the chart of the results. You can install these by running these
commands in a notebook cell:
!pip install treeinterpreter
!pip install waterfallcharts
We have already seen how to compute feature importances across the entire random
forest. The basic idea was to look at the contribution of each variable to improving
the model, at each branch of every tree, and then add up all of these contributions per
variable.
We can do exactly the same thing, but for just a single row of data. For instance, let’s
say we are looking at a particular item at auction. Our model might predict that this
item will be very expensive, and we want to know why. So, we take that one row of
data and put it through the first decision tree, looking to see what split is used at each
point throughout the tree. For each split, we find the increase or decrease in the addi‐
tion, compared to the parent node of the tree. We do this for every tree, and add up
the total change in importance by split variable.
For instance, let’s pick the first few rows of our validation set:
row = valid_xs_final.iloc[:5]
We can then pass these to treeinterpreter:
prediction,bias,contributions = treeinterpreter.predict(m, row.values)
prediction is simply the prediction that the random forest makes. bias is the predic‐
tion based on taking the mean of the dependent variable (i.e., the <i>model</i> that is the
root of every tree). contributions is the most interesting bit—it tells us the total
change in prediction due to each of the independent variables. Therefore, the sum of
contributions plus bias must equal the prediction, for each row. Let’s look at just
the first row:
prediction[0], bias[0], contributions[0].sum()
(array([9.98234598]), 10.104309759725059, -0.12196378442186026)"|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"One thing that makes this harder to interpret is that there seem to be some variables
with very similar meanings: for example, ProductGroup and ProductGroupDesc. Let’s
try to remove any redundant features.
<header><largefont><b>Removing</b></largefont> <largefont><b>Redundant</b></largefont> <largefont><b>Features</b></largefont></header>
Let’s start with this:
cluster_columns(xs_imp)
In this chart, the pairs of columns that are most similar are the ones that were merged
together early, far from the “root” of the tree at the left. Unsurprisingly, the fields Pro
ductGroup ProductGroupDesc saleYear
and were merged quite early, as were and
saleElapsed, and fiModelDesc and fiBaseModel. These might be so closely correla‐
ted they are practically synonyms for each other.
<b>DeterminingSimilarity</b>
The most similar pairs are found by calculating the <i>rank</i> <i>correla‐</i>
<i>tion,</i> which means that all the values are replaced with their <i>rank</i>
(first, second, third, etc. within the column), and then the <i>correla‐</i>
<i>tion</i> is calculated. (Feel free to skip over this minor detail though,
since it’s not going to come up again in the book!)
Let’s try removing some of these closely related features to see if the model can be
simplified without impacting the accuracy. First, we create a function that quickly
trains a random forest and returns the OOB score, by using a lower max_samples and
min_samples_leaf.
higher The OOB score is a number returned by sklearn that
ranges between 1.0 for a perfect model and 0.0 for a random model. (In statistics it’s"|bagging; decision trees; removing redundant features; machine learning (ML); predictions; random forests; rank correlation; tabular data for models; training
"Let’s create a first SiameseImage and check that our show method works:
img = PILImage.create(files[0])
s = SiameseImage(img, img, True)
s.show();
We can also try with a second image that’s not from the same class:
img1 = PILImage.create(files[1])
s1 = SiameseImage(img, img1, False)
s1.show();
The important thing with transforms that we saw before is that they dispatch over
tuples or their subclasses. That’s precisely why we chose to subclass Tuple in this
instance—this way, we can apply any transform that works on images to our Siamese
Image, and it will be applied on each image in the tuple:
s2 = Resize(224)(s1)
s2.show();
Here the Resize transform is applied to each of the two images, but not the Boolean
flag. Even if we have a custom type, we can thus benefit from all the data augmenta‐
tion transforms inside the library."|Siamese model image comparison
"To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels,
25 pixels are being used at each kernel application. Creating eight filters from this will
mean the neural net will have to find some useful features:
<b>def</b> simple_cnn():
<b>return</b> sequential(
conv(1 ,8, ks=5), <i>#14x14</i>
conv(8 ,16), <i>#7x7</i>
conv(16,32), <i>#4x4</i>
conv(32,64), <i>#2x2</i>
conv(64,10, act=False), <i>#1x1</i>
Flatten(),
)
As you’ll see in a moment, we can look inside our models while they’re training in
order to try to find ways to make them train better. To do this, we use the Activation
Stats callback, which records the mean, standard deviation, and histogram of activa‐
tions of every trainable layer (as we’ve seen, callbacks are used to add behavior to the
training loop; we’ll explore how they work in Chapter 16):
<b>from</b> <b>fastai.callback.hook</b> <b>import</b> *
We want to train quickly, so that means training at a high learning rate. Let’s see how
we go at 0.06:
<b>def</b> fit(epochs=1):
learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,
metrics=accuracy, cbs=ActivationStats(with_hist=True))
learn.fit(epochs, 0.06)
<b>return</b> learn
learn = fit()
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 2.307071 2.305865 0.113500 00:16
This didn’t train at all well! Let’s find out why.
One handy feature of the callbacks passed to Learner is that they are made available
automatically, with the same name as the callback class, except in camel_case. So, our
ActivationStats callback can be accessed through activation_stats. I’m sure you
learn.recorder…can
remember you guess how that is implemented? That’s right, it’s
a callback called Recorder !
ActivationStats includes some handy utilities for plotting the activations during
training. plot_layer_stats(idx) plots the mean and standard deviation of the acti‐
vations of layer number <i>idx,</i> along with the percentage of activations near zero. Here’s
the first layer’s plot:"|plotting during training; ActivationStats; callbacks; convolutional neural network (CNN); building a CNN; Learner; training on all digits
"because we’re decreasing the number of activations in the activation map by a factor
of 4; we don’t want to decrease the capacity of a layer by too much at a time.”
There is one bias for each channel. (Sometimes channels are called <i>features</i> or <i>filters</i>
when they are not input channels.) The output shape is 64x4x14x14 , and this will
therefore become the input shape to the next layer. The next layer, according to
summary , has 296 parameters. Let’s ignore the batch axis to keep things simple. So, for
each of 14*14=196 locations, we are multiplying 296-8=288 weights (ignoring the bias
for simplicity), so that’s 196*288=56_448 multiplications at this layer. The next layer
will have 7*7*(1168-16)=56_448 multiplications.
What happened here is that our stride-2 convolution halved the <i>grid</i> <i>size</i> from 14x14
to 7x7 , and we doubled the <i>number</i> <i>of</i> <i>filters</i> from 8 to 16, resulting in no overall
change in the amount of computation. If we left the number of channels the same in
each stride-2 layer, the amount of computation being done in the net would get less
and less as it gets deeper. But we know that the deeper layers have to compute seman‐
tically rich features (such as eyes or fur), so we wouldn’t expect that doing <i>less</i> compu‐
tation would make sense.
Another way to think of this is based on receptive fields.
<header><largefont><b>Receptive</b></largefont> <largefont><b>Fields</b></largefont></header>
The <i>receptive</i> <i>field</i> is the area of an image that is involved in the calculation of a layer.
On the book’s website, you’ll find an Excel spreadsheet called <i>conv-example.xlsx</i> that
shows the calculation of two stride-2 convolutional layers using an MNIST digit.
Each layer has a single kernel. Figure 13-10 shows what we see if we click one of the
cells in the <i>conv2</i> section, which shows the output of the second convolutional layer,
and click <i>trace</i> <i>precedents.</i>
<i>Figure</i> <i>13-10.</i> <i>Immediate</i> <i>precedents</i> <i>of</i> <i>Conv2</i> <i>layer</i>"|convolutional neural network (CNN); building a CNN
"Understanding this picture is one of the best ways to understand decision trees, so we
will start at the top and explain each part step by step.
The top node represents the <i>initial</i> <i>model</i> before any splits have been done, when all
the data is in one group. This is the simplest possible model. It is the result of asking
zero questions and will always predict the value to be the average value of the whole
dataset. In this case, we can see it predicts a value of 10.1 for the logarithm of the sales
price. It gives a mean squared error of 0.48. The square root of this is 0.69. (Remem‐
m_rmse,
ber that unless you see or a <i>root</i> <i>mean</i> <i>squared</i> <i>error,</i> the value you are looking
at is before taking the square root, so it is just the average of the square of the differ‐
ences.) We can also see that there are 404,710 auction records in this group—that is
the total size of our training set. The final piece of information shown here is the
decision criterion for the best split that was found, which is to split based on the
coupler_system column.
Moving down and to the left, this node shows us that there were 360,847 auction
records for equipment where coupler_system was less than 0.5. The average value of
our dependent variable in this group is 10.21. Moving down and to the right from the
initial model takes us to the records where coupler_system was greater than 0.5.
The bottom row contains our <i>leaf</i> <i>nodes:</i> the nodes with no answers coming out of
them, because there are no more questions to be answered. At the far right of this row
is the node containing records where coupler_system was greater than 0.5. The aver‐
age value is 9.21, so we can see the decision tree algorithm did find a single binary"|decision trees; tabular data for models; training
"We can also get a tensor’s rank directly with ndim :
stacked_threes.ndim
3
Finally, we can compute what the ideal 3 looks like. We calculate the mean of all the
image tensors by taking the mean along dimension 0 of our stacked, rank-3 tensor.
This is the dimension that indexes over all the images.
In other words, for every pixel position, this will compute the average of that pixel
over all images. The result will be one value for every pixel position, or a single
image. Here it is:
mean3 = stacked_threes.mean(0)
show_image(mean3);
According to this dataset, this is the ideal number 3! (You may not like it, but this is
what peak number 3 performance looks like.) You can see how it’s very dark where all
the images agree it should be dark, but it becomes wispy and blurry where the images
disagree.
Let’s do the same thing for the 7s, but put all the steps together at once to save time:
mean7 = stacked_sevens.mean(0)
show_image(mean7);
Let’s now pick an arbitrary 3 and measure its <i>distance</i> from our “ideal digits.”
<b>StopandThink!</b>
How would you calculate how similar a particular image is to each
of our ideal digits? Remember to step away from this book and jot
down some ideas before you move on! Research shows that recall
and understanding improve dramatically when you are engaged
with the learning process by solving problems, experimenting, and
trying new ideas yourself."|ideal digit creation; pixels
"The loss is going down, just as we hoped! But looking only at these loss numbers dis‐
guises the fact that each iteration represents an entirely different quadratic function
being tried, on the way to finding the best possible quadratic function. We can see
this process visually if, instead of printing out the loss function, we plot the function
at every step. Then we can see how the shape is approaching the best possible quad‐
ratic function for our data:
_,axs = plt.subplots(1,4,figsize=(12,3))
<b>for</b> ax <b>in</b> axs: show_preds(apply_step(params, False), ax)
plt.tight_layout()
<b>Step7:Stop</b>
We just decided to stop after 10 epochs arbitrarily. In practice, we would watch the
training and validation losses and our metrics to decide when to stop, as we’ve
discussed.
<header><largefont><b>Summarizing</b></largefont> <largefont><b>Gradient</b></largefont> <largefont><b>Descent</b></largefont></header>
Now that you’ve seen what happens in each step, let’s take another look at our graphi‐
cal representation of the gradient descent process (Figure 4-5) and do a quick recap.
<i>Figure</i> <i>4-5.</i> <i>The</i> <i>gradient</i> <i>descent</i> <i>process</i>
At the beginning, the weights of our model can be random (training <i>from</i> <i>scratch)</i> or
come from a pretrained model (transfer <i>learning).</i> In the first case, the output we will
get from our inputs won’t have anything to do with what we want, and even in the
second case, it’s likely the pretrained model won’t be very good at the specific task we
are targeting. So the model will need to <i>learn</i> better weights."|gradient descent; optimization; pretrained models; stochastic gradient descent (SGD); training; transfer learning; weights
"We can create a regularized Learner using the RNNRegularizer callback:
learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5),
loss_func=CrossEntropyLossFlat(), metrics=accuracy,
cbs=[ModelResetter, RNNRegularizer(alpha=2, beta=1)])
A TextLearner automatically adds those two callbacks for us (with those values for
alpha and beta as defaults), so we can simplify the preceding line:
learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4),
loss_func=CrossEntropyLossFlat(), metrics=accuracy)
We can then train the model, and add additional regularization by increasing the
weight decay to 0.1:
learn.fit_one_cycle(15, 1e-2, wd=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 2.693885 2.013484 0.466634 00:02
1 1.685549 1.187310 0.629313 00:02
2 0.973307 0.791398 0.745605 00:02
3 0.555823 0.640412 0.794108 00:02
4 0.351802 0.557247 0.836100 00:02
5 0.244986 0.594977 0.807292 00:02
6 0.192231 0.511690 0.846761 00:02
7 0.162456 0.520370 0.858073 00:02
8 0.142664 0.525918 0.842285 00:02
9 0.128493 0.495029 0.858073 00:02
10 0.117589 0.464236 0.867188 00:02
11 0.109808 0.466550 0.869303 00:02
12 0.104216 0.455151 0.871826 00:02
13 0.100271 0.452659 0.873617 00:02
14 0.098121 0.458372 0.869385 00:02
Now this is far better than our previous model!
<header><largefont><b>Conclusion</b></largefont></header>
You have now seen everything that is inside the AWD-LSTM architecture we used in
text classification in Chapter 10. It uses dropout in a lot more places:
• Embedding dropout (just after the embedding layer)
• Input dropout (after the embedding layer)
• Weight dropout (applied to the weights of the LSTM at each training step)"|training weight-tied regularized LSTM; LSTM model; LSTM training; training a regularized LSTM; recurrent neural networks (RNNs)
"<b>AlexisSays</b>
If you are philosophically minded, it is somewhat dizzying to con‐
template the different kinds of hypotheticality that we are juggling
to make this calculation. First, there’s the fact that <i>every</i> prediction
is hypothetical, because we are not noting empirical data. Second,
there’s the point that we’re <i>not</i> merely interested in asking how sale
YearMade
price would change if we changed and everything else
along with it. Rather, we’re very specifically asking how sale price
would change in a hypothetical world where only YearMade
changed. Phew! It is impressive that we can ask such questions. I
recommend Judea Pearl and Dana Mackenzie’s recent book on cau‐
sality, <i>The</i> <i>Book</i> <i>of</i> <i>Why</i> (Basic Books), if you’re interested in more
deeply exploring formalisms for analyzing these subtleties.
With these averages, we can then plot each year on the x-axis, and each prediction on
the y-axis. This, finally, is a partial dependence plot. Let’s take a look:
<b>from</b> <b>sklearn.inspection</b> <b>import</b> plot_partial_dependence
fig,ax = plt.subplots(figsize=(12, 4))
plot_partial_dependence(m, valid_xs_final, ['YearMade','ProductSize'],
grid_resolution=20, ax=ax);
Looking first of all at the YearMade plot, and specifically at the section covering the
years after 1990 (since, as we noted, this is where we have the most data), we can see a
nearly linear relationship between year and price. Remember that our dependent
variable is after taking the logarithm, so this means that in practice there is an expo‐
nential increase in price. This is what we would expect: depreciation is generally rec‐
ognized as being a multiplicative factor over time, so for a given sale date, varying the
year made ought to show an exponential relationship with sale price.
The ProductSize partial plot is a bit concerning. It shows that the final group, which
we saw is for missing values, has the lowest price. To use this insight in practice, we"|bagging; The Book of Why (Pearl and Mackenzie); decision trees; machine learning (ML); Mackenzie; Pearl; predictions; random forests; tabular data for models; training
"To represent collaborative filtering in PyTorch, we can’t just use the crosstab repre‐
sentation directly, especially if we want it to fit into our deep learning framework. We
can represent our movie and user latent factor tables as simple matrices:
n_users = len(dls.classes['user'])
n_movies = len(dls.classes['title'])
n_factors = 5
user_factors = torch.randn(n_users, n_factors)
movie_factors = torch.randn(n_movies, n_factors)
To calculate the result for a particular movie and user combination, we have to look
up the index of the movie in our movie latent factor matrix, and the index of the user
in our user latent factor matrix; then we can do our dot product between the two
latent factor vectors. But <i>look</i> <i>up</i> <i>in</i> <i>an</i> <i>index</i> is not an operation our deep learning
models know how to do. They know how to do matrix products and activation
functions.
Fortunately, it turns out that we can represent <i>look</i> <i>up</i> <i>in</i> <i>an</i> <i>index</i> as a matrix product.
The trick is to replace our indices with one-hot-encoded vectors. Here is an example
of what happens if we multiply a vector by a one-hot-encoded vector representing the
index 3:
one_hot_3 = one_hot(3, n_users).float()
user_factors.t() @ one_hot_3
tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])
It gives us the same vector as the one at index 3 in the matrix:
user_factors[3]
tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908])
If we do that for a few indices at once, we will have a matrix of one-hot-encoded vec‐
tors, and that operation will be a matrix multiplication! This would be a perfectly
acceptable way to build models using this kind of architecture, except that it would
use a lot more memory and time than necessary. We know that there is no real under‐
lying reason to store the one-hot-encoded vector, or to search through it to find the
occurrence of the number 1—we should just be able to index into an array directly
with an integer. Therefore, most deep learning libraries, including PyTorch, include a
special layer that does just this; it indexes into a vector using an integer, but has its
derivative calculated in such a way that it is identical to what it would have been if it
had done a matrix multiplication with a one-hot-encoded vector. This is called an
<i>embedding.</i>"|categorical variables; collaborative filtering; tables as matrices; look-up index; embedding; look-up index as one-hot-encoded vector; embedding categorical variables
"Instead of being better, it ends up being worse (at least at the end of training). Why is
that? If we look at both trainings carefully, we can see the validation loss stopped
improving in the middle and started to get worse. As we’ve seen, this is a clear indica‐
tion of overfitting. In this case, there is no way to use data augmentation, so we will
have to use another regularization technique. One approach that can be helpful is
<i>weight</i> <i>decay.</i>
<header><largefont><b>Weight</b></largefont> <largefont><b>Decay</b></largefont></header>
Weight decay, or <i>L2</i> <i>regularization,</i> consists of adding to your loss function the sum of
all the weights squared. Why do that? Because when we compute the gradients, it will
add a contribution to them that will encourage the weights to be as small as possible.
Why would it prevent overfitting? The idea is that the larger the coefficients are, the
sharper canyons we will have in the loss function. If we take the basic example of a
y = a * (x**2), a
parabola, the larger is, the more <i>narrow</i> the parabola is:
So, letting our model learn high parameters might cause it to fit all the data points in
the training set with an overcomplex function that has very sharp changes, which will
lead to overfitting.
Limiting our weights from growing too much is going to hinder the training of the
model, but it will yield a state where it generalizes better. Going back to the theory
briefly, weight decay (or just wd) is a parameter that controls that sum of squares we
add to our loss (assuming parameters is a tensor of all parameters):
loss_with_wd = loss + wd * (parameters**2).sum()"|categorical variables; collaborative filtering; embedding; L2 regularization; embedding categorical variables; weight decay against; weights
"<i>Bias</i>
When a traditionally African-American name is searched for on Google, it dis‐
plays ads for criminal background checks.
In fact, for every concept that we introduce in this chapter, we are going to provide at
least one specific example. For each one, think about what you could have done in
this situation, and what kinds of obstructions there might have been to you getting
that done. How would you deal with them? What would you look out for?
<header><largefont><b>Bugs</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Recourse:</b></largefont> <largefont><b>Buggy</b></largefont> <largefont><b>Algorithm</b></largefont> <largefont><b>Used</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Healthcare</b></largefont> <largefont><b>Benefits</b></largefont></header>
The Verge investigated software used in over half of the US states to determine how
much healthcare people receive, and documented its findings in the article “What
Happens When an Algorithm Cuts Your Healthcare”. After implementation of the
algorithm in Arkansas, hundreds of people (many with severe disabilities) had their
healthcare drastically cut.
For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aide to help
her to get out of bed, to go to the bathroom, to get food, and more, had her hours of
help suddenly reduced by 20 hours a week. She couldn’t get any explanation for why
her healthcare was cut. Eventually, a court case revealed that there were mistakes in
the software implementation of the algorithm, negatively impacting people with dia‐
betes or cerebral palsy. However, Dobbs and many other people reliant on these
health-care benefits live in fear that their benefits could again be cut suddenly and
inexplicably.
<header><largefont><b>Feedback</b></largefont> <largefont><b>Loops:</b></largefont> <largefont><b>YouTube’s</b></largefont> <largefont><b>Recommendation</b></largefont> <largefont><b>System</b></largefont></header>
Feedback loops can occur when your model is controlling the next round of data you
get. The data that is returned quickly becomes flawed by the software itself.
For instance, YouTube has 1.9 billion users, who watch over 1 billion hours of You‐
Tube videos a day. Its recommendation algorithm (built by Google), which was
designed to optimize watch time, is responsible for around 70% of the content that is
watched. But there was a problem: it led to out-of-control feedback loops, leading the
<i>New</i> <i>York</i> <i>Times</i> to run the headline “YouTube Unleashed a Conspiracy Theory
Boom. Can It Be Contained?” in February 2019. Ostensibly, recommendation systems
are predicting what content people will like, but they also have a lot of power in deter‐
mining what content people even see.
<header><largefont><b>Bias:</b></largefont> <largefont><b>Professor</b></largefont> <largefont><b>Latanya</b></largefont> <largefont><b>Sweeney</b></largefont> <largefont><b>“Arrested”</b></largefont></header>
Dr. Latanya Sweeney is a professor at Harvard and director of the university’s data
privacy lab. In the paper “Discrimination in Online Ad Delivery” (see Figure 3-1), she
describes her discovery that Googling her name resulted in advertisements saying"|algorithm buggy; Arkansas healthcare buggy algorithm (ethics); arrest record Google bias; bias; buggy algorithm ethics; conspiracy theory feedback loop; ethics; healthcare benefits buggy algorithm; YouTube recommendation feedback loops; conspiracy theories fed by; recommendation system ethics; Google; healthcare benefits buggy algorithm (ethics); data seen changing over time; racial bias; conspiracy theory feedback loops; feedback loop ethics; YouTube feedback loop ethics; research papers; Sweeney; recommendation feedback loops
"<b>def</b> get_data(url, presize, resize):
path = untar_data(url)
<b>return</b> DataBlock(
blocks=(ImageBlock, CategoryBlock), get_items=get_image_files,
splitter=GrandparentSplitter(valid_name='val'),
get_y=parent_label, item_tfms=Resize(presize),
batch_tfms=[*aug_transforms(min_scale=0.5, size=resize),
Normalize.from_stats(*imagenet_stats)],
).dataloaders(path, bs=128)
dls = get_data(URLs.IMAGENETTE_160, 160, 128)
dls.show_batch(max_n=4)
When we looked at MNIST, we were dealing with 28×28-pixel images. For Image‐
nette, we are going to be training with 128×128-pixel images. Later, we would like to
be able to use larger images as well—at least as big as 224×224-pixels, the ImageNet
standard. Do you recall how we managed to get a single vector of activations for each
image out of the MNIST convolutional neural network?
The approach we used was to ensure that there were enough stride-2 convolutions
such that the final layer would have a grid size of 1. Then we just flattened out the
unit axes that we ended up with, to get a vector for each image (so, a matrix of activa‐
tions for a mini-batch). We could do the same thing for Imagenette, but that would
cause two problems:
• We’d need lots of stride-2 layers to make our grid 1×1 at the end—perhaps more
than we would otherwise choose.
• The model would not work on images of any size other than the size we origi‐
nally trained on."|handwritten digits; ImageNet dataset; MNIST handwritten digits dataset; handwritten digits dataset; ResNet architecture
"1s with 1 −  + . This way, we don’t encourage the model to predict something over‐
<i>N</i>
confidently. In our Imagenette example that has 10 classes, the targets become some‐
thing like this (here for a target that corresponds to the index 3):
[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
In practice, we don’t want to one-hot encode the labels, and fortunately we won’t need
to (the one-hot encoding is just good to explain label smoothing and visualize it).
<header><largefont><b>Label</b></largefont> <largefont><b>Smoothing,</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Paper</b></largefont></header>
Here is how the reasoning behind label smoothing was explained in the paper by
Christian Szegedy et al.:
This maximum is not achievable for finite <i>z</i> but is approached if <i>z</i> ≫ <i>z</i> for all
<i>k</i> <i>y</i> <i>k</i>
<i>k</i> ≠ <i>y</i>
—that is, if the logit corresponding to the ground-truth label is much [greater]
than all other logits. This, however, can cause two problems. First, it may result in
over-fitting: if the model learns to assign full probability to the ground-truth label for
each training example, it is not guaranteed to generalize. Second, it encourages the
differences between the largest logit and all others to become large, and this, com‐
∂ℓ
bined with the bounded gradient , reduces the ability of the model to adapt. Intui‐
∂z
<i>k</i>
tively, this happens because the model becomes too confident about its predictions.
Let’s practice our paper-reading skills to try to interpret this. “This maximum” is
referring to the previous part of the paragraph, which talked about the fact that 1 is
the value of the label for the positive class. So, it’s not possible for any value (except
infinity) to result in 1 after sigmoid or softmax. In a paper, you won’t normally see
“any value” written; instead, it will get a symbol, which in this case is <i>z</i> . This short‐
<i>k</i>
hand is helpful in a paper, because it can be referred to again later, and the reader will
know which value is being discussed.
Then it says: “if <i>z</i> ≫ <i>z</i> for all <i>k</i> ≠ <i>y.”</i> In this case, the paper immediately follows the
<i>y</i> <i>k</i>
math with an English description, which is handy because you can just read that. In
the math, the <i>y</i> is referring to the target (y is defined earlier in the paper; sometimes
it’s hard to find where symbols are defined, but nearly all papers will define all their
symbols somewhere), and <i>z</i> is the activation corresponding to the target. So to get
<i>y</i>
close to 1, this activation needs to be much higher than all the others for that
prediction.
Next, consider the statement “if the model learns to assign full probability to the
ground-truth label for each training example, it is not guaranteed to generalize.” This
is saying that making <i>z</i> really big means we’ll need large weights and large activations
<i>y</i>
throughout our model. Large weights lead to “bumpy” functions, where a small
change in input results in a big change to predictions. This is really bad for
generalization, because it means just one pixel changing a bit could change our pre‐
diction entirely!"|image classifier model training; research papers; Szegedy
"to work on). In addition, PyTorch can automatically calculate derivatives of these
operations, including combinations of operations. As you’ll see, it would be impossi‐
ble to do deep learning in practice without this capability.
<b>SylvainSays</b>
If you don’t know what C is, don’t worry: you won’t need it at all. In
a nutshell, it’s a low-level (low-level means more similar to the lan‐
guage that computers use internally) language that is very fast com‐
pared to Python. To take advantage of its speed while
programming in Python, try to avoid as much as possible writing
loops, and replace them by commands that work directly on arrays
or tensors.
Perhaps the most important new coding skill for a Python programmer to learn is
how to effectively use the array/tensor APIs. We will be showing lots more tricks later
in this book, but here’s a summary of the key things you need to know for now.
To create an array or tensor, pass a list (or list of lists, or list of lists of lists, etc.) to
array or tensor:
data = [[1,2,3],[4,5,6]]
arr = array (data)
tns = tensor(data)
arr <i>#</i> <i>numpy</i>
array([[1, 2, 3],
[4, 5, 6]])
tns <i>#</i> <i>pytorch</i>
tensor([[1, 2, 3],
[4, 5, 6]])
All the operations that follow are shown on tensors, but the syntax and results for
NumPy arrays are identical.
You can select a row (note that, like lists in Python, tensors are 0-indexed, so 1 refers
to the second row/column):
tns[1]
tensor([4, 5, 6])
Or a column, by using : to indicate <i>all</i> <i>of</i> <i>the</i> <i>first</i> <i>axis</i> (we sometimes refer to the
dimensions of tensors/arrays as <i>axes):</i>
tns[:,1]
tensor([2, 5])"|arrays; APIs; creating an array; C programming language; Python; array APIs; tensor APIs; tensors; creating a tensor
"One approach to dealing with the first issue would be to flatten the final convolu‐
tional layer in a way that handles a grid size other than 1×1. We could simply flatten a
matrix into a vector as we have done before, by laying out each row after the previous
row. In fact, this is the approach that convolutional neural networks up until 2013
nearly always took. The most famous example is the 2013 ImageNet winner VGG,
still sometimes used today. But there was another problem with this architecture: it
not only did not work with images other than those of the same size used in the train‐
ing set, but also required a lot of memory, because flattening out the convolutional
layer resulted in many activations being fed into the final layers. Therefore, the weight
matrices of the final layers were enormous.
This problem was solved through the creation of <i>fully</i> <i>convolutional</i> <i>networks.</i> The
trick in fully convolutional networks is to take the average of activations across a con‐
volutional grid. In other words, we can simply use this function:
<b>def</b> avg_pool(x): <b>return</b> x.mean((2,3))
As you see, it is taking the mean over the x- and y-axes. This function will always
convert a grid of activations into a single activation per image. PyTorch provides a
slightly more versatile module called nn.AdaptiveAvgPool2d, which averages a grid
of activations into whatever sized destination you require (although we nearly always
use a size of 1).
A fully convolutional network, therefore, has a number of convolutional layers, some
of which will be stride 2, at the end of which is an adaptive average pooling layer, a
flatten layer to remove the unit axes, and finally a linear layer. Here is our first fully
convolutional network:
<b>def</b> block(ni, nf): <b>return</b> ConvLayer(ni, nf, stride=2)
<b>def</b> get_model():
<b>return</b> nn.Sequential(
block(3, 16),
block(16, 32),
block(32, 64),
block(64, 128),
block(128, 256),
nn.AdaptiveAvgPool2d(1),
Flatten(),
nn.Linear(256, dls.c))
We’re going to be replacing the implementation of block in the network with other
variants in a moment, which is why we’re not calling it conv anymore. We’re also sav‐
ing some time by taking advantage of fastai’s ConvLayer, which already provides the
functionality of conv from the preceding chapter (plus a lot more!)."|AdaptiveAvgPool2d; fully convolutional networks; ResNet architecture
"<b>Jargon:Top-5Accuracy</b>
A metric testing how often the label we want is in the top 5 predic‐
tions of our model. It was used in the ImageNet competition
because many of the images contained multiple objects, or con‐
tained objects that could be easily confused or may even have been
mislabeled with a similar label. In these situations, looking at top-1
accuracy may be inappropriate. However, recently CNNs have been
getting so good that top-5 accuracy is nearly 100%, so some
researchers are using top-1 accuracy for ImageNet too now.
We’ll use this tweaked version as we scale up to the full ResNet, because it’s substan‐
tially better. It differs a little bit from our previous implementation, in that instead of
just starting with ResNet blocks, it begins with a few convolutional layers followed by
a max pooling layer. This is what the first layers, called the <i>stem</i> of the network, look
like:
<b>def</b> _resnet_stem(*sizes):
<b>return</b> [
ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 <b>if</b> i==0 <b>else</b> 1)
<b>for</b> i <b>in</b> range(len(sizes)-1)
] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)]
_resnet_stem(3,32,32,64)
[ConvLayer(
(0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
(1): BatchNorm2d(32, eps=1e-05, momentum=0.1)
(2): ReLU()
), ConvLayer(
(0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(1): BatchNorm2d(32, eps=1e-05, momentum=0.1)
(2): ReLU()
), ConvLayer(
(0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(1): BatchNorm2d(64, eps=1e-05, momentum=0.1)
(2): ReLU()
), MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)]
<b>Jargon:Stem</b>
The first few layers of a CNN. Generally, the stem has a different
structure than the main body of the CNN.
The reason that we have a stem of plain convolutional layers, instead of ResNet
blocks, is based on an important insight about all deep convolutional neural net‐
works: the vast majority of the computation occurs in the early layers. Therefore, we
should keep the early layers as fast and simple as possible."|convolutional neural network (CNN); building state-of-the-art ResNet; stem in convolutional neural network
"popularity has soared, and it seems likely to become the most common tokenization
approach (it may well already be, by the time you read this!).
Once our texts have been split into tokens, we need to convert them to numbers.
We’ll look at that next.
<header><largefont><b>Numericalization</b></largefont> <largefont><b>with</b></largefont> <largefont><b>fastai</b></largefont></header>
<i>Numericalization</i> is the process of mapping tokens to integers. The steps are basically
identical to those necessary to create a Category variable, such as the dependent vari‐
able of digits in MNIST:
1. Make a list of all possible levels of that categorical variable (the vocab).
2. Replace each level with its index in the vocab.
Let’s take a look at this in action on the word-tokenized text we saw earlier:
toks = tkn(txt)
<b>print(coll_repr(tkn(txt),</b> 31))
(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',
> 'the','video','store',',','has','apparently','sit','around','for','a','couple
> ','of','years','without','a','distributor','.','xxmaj','it',""'s"",'easy'...]
SubwordTokenizer, setup Numericalize;
Just as with we need to call on this is how
we create the vocab. That means we’ll need our tokenized corpus first. Since tokeniza‐
tion takes a while, it’s done in parallel by fastai; but for this manual walk-through,
we’ll use a small subset:
toks200 = txts[:200].map(tkn)
toks200[0]
(#228)
> ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at'...]
We can pass this to setup to create our vocab:
num = Numericalize()
num.setup(toks200)
coll_repr(num.vocab,20)
""(#2000) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj
> ','the','.',',','a','and','of','to','is','in','i','it'...]""
Our special rules tokens appear first, and then every word appears once, in frequency
order. The defaults to Numericalize are min_freq=3 and max_vocab=60000.
max_vocab=60000 results in fastai replacing all words other than the most common
xxunk.
60,000 with a special <i>unknown</i> <i>word</i> token, This is useful to avoid having an
overly large embedding matrix, since that can slow down training and use up too
much memory, and can also mean that there isn’t enough data to train useful"|natural language processing (NLP); numericalization; word-tokenized text; tokenization; unknown word token
"<header><largefont><b>Dropout</b></largefont></header>
<i>Dropout</i> is a regularization technique that was introduced by Geoffrey Hinton et al. in
“Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors”.
The basic idea is to randomly change some activations to zero at training time. This
makes sure all neurons actively work toward the output, as seen in Figure 12-10
(from “Dropout: A Simple Way to Prevent Neural Networks from Overfitting” by
Nitish Srivastava et al.).
<i>Figure</i> <i>12-10.</i> <i>Applying</i> <i>dropout</i> <i>in</i> <i>a</i> <i>neural</i> <i>network</i> <i>(courtesy</i> <i>of</i> <i>Nitish</i> <i>Srivastava</i> <i>et</i> <i>al.)</i>
Hinton used a nice metaphor when he explained, in an interview, the inspiration for
dropout:
I went to my bank. The tellers kept changing, and I asked one of them why. He said he
didn’t know but they got moved around a lot. I figured it must be because it would
require cooperation between employees to successfully defraud the bank. This made
me realize that randomly removing a different subset of neurons on each example
would prevent conspiracies and thus reduce overfitting.
In the same interview, he also explained that neuroscience provided additional
inspiration:
We don’t really know why neurons spike. One theory is that they want to be noisy so as
to regularize, because we have many more parameters than we have data points. The
idea of dropout is that if you have noisy activations, you can afford to use a much big‐
ger model.
This explains the idea behind why dropout helps to generalize: first it helps the neu‐
rons to cooperate better together; then it makes the activations more noisy, thus mak‐
ing the model more robust."|dropout; Hinton; LSTM model; recurrent neural networks (RNNs)
"The sklearn docs show an example of the effects of different max_features choices,
with increasing numbers of trees. In the plot, the blue plot line uses the fewest fea‐
tures, and the green line uses the most (it uses all the features). As you can see in
Figure 9-7, the models with the lowest error result from using a subset of features but
with a larger number of trees.
<i>Figure</i> <i>9-7.</i> <i>Error</i> <i>based</i> <i>on</i> <i>max</i> <i>features</i> <i>and</i> <i>number</i> <i>of</i> <i>trees</i> <i>(source:</i> <i>https://oreil.ly/</i>
<i>E0Och)</i>
To see the impact of n_estimators, let’s get the predictions from each individual tree
in our forest (these are in the estimators_ attribute):
preds = np.stack([t.predict(valid_xs) <b>for</b> t <b>in</b> m.estimators_])
As you can see, preds.mean(0) gives the same results as our random forest:
r_mse(preds.mean(0), valid_y)
0.233502"|bagging; decision trees; machine learning (ML); predictions; random forests; sklearn; tabular data for models; training; web resources
"We can download, extract, and take a look at our dataset in the usual way:
<b>from</b> <b>fastai.text.all</b> <b>import</b> *
path = untar_data(URLs.HUMAN_NUMBERS)
path.ls()
(#2) [Path('train.txt'),Path('valid.txt')]
Let’s open those two files and see what’s inside. At first, we’ll join all of the texts
together and ignore the train/valid split given by the dataset (we’ll come back to that
later):
lines = L()
<b>with</b> open(path/'train.txt') <b>as</b> f: lines += L(*f.readlines())
<b>with</b> open(path/'valid.txt') <b>as</b> f: lines += L(*f.readlines())
lines
(#9998) ['one \n','two \n','three \n','four \n','five \n','six \n','seven
> \n','eight \n','nine \n','ten \n'...]
We take all those lines and concatenate them in one big stream. To mark when we go
from one number to the next, we use a . as a separator:
text = ' . '.join([l.strip() <b>for</b> l <b>in</b> lines])
text[:100]
'one . two . three . four . five . six . seven . eight . nine . ten . eleven .
> twelve . thirteen . fo'
We can tokenize this dataset by splitting on spaces:
tokens = text.split(' ')
tokens[:10]
['one', '.', 'two', '.', 'three', '.', 'four', '.', 'five', '.']
To numericalize, we have to create a list of all the unique tokens (our <i>vocab):</i>
vocab = L(*tokens).unique()
vocab
(#30) ['one','.','two','three','four','five','six','seven','eight','nine'...]
Then we can convert our tokens into numbers by looking up the index of each in the
vocab:
word2idx = {w:i <b>for</b> i,w <b>in</b> enumerate(vocab)}
nums = L(word2idx[i] <b>for</b> i <b>in</b> tokens)
nums
(#63095) [0,1,2,1,3,1,4,1,5,1...]
Now that we have a small dataset on which language modeling should be an easy task,
we can build our first model."|language model; natural language processing (NLP)
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
10 0.593401 1.180786 0.676433 00:32
11 0.536634 0.879937 0.713885 00:32
12 0.479208 0.798356 0.741656 00:32
13 0.440071 0.600644 0.806879 00:32
14 0.402952 0.450296 0.858599 00:32
15 0.359117 0.486126 0.846369 00:32
16 0.313642 0.442215 0.861911 00:32
17 0.294050 0.485967 0.853503 00:32
18 0.270583 0.408566 0.875924 00:32
19 0.266003 0.411752 0.872611 00:33
We’re getting a great result now! Try adding Mixup, and then training this for a hun‐
dred epochs while you go get lunch. You’ll have yourself a very accurate image classi‐
fier, trained from scratch.
The bottleneck design we’ve shown here is typically used in only ResNet-50, -101, and
-152 models. ResNet-18 and -34 models usually use the non-bottleneck design seen
in the previous section. However, we’ve noticed that the bottleneck layer generally
works better even for the shallower networks. This just goes to show that the little
details in papers tend to stick around for years, even if they’re not quite the best
design! Questioning assumptions and “stuff everyone knows” is always a good idea,
because this is still a new field, and lots of details aren’t always done well.
<header><largefont><b>Conclusion</b></largefont></header>
You have now seen how the models we have been using for computer vision since the
first chapter are built, using skip connections to allow deeper models to be trained.
Even though there has been a lot of research into better architectures, they all use one
version or another of this trick to make a direct path from the input to the end of the
network. When using transfer learning, the ResNet is the pretrained model. In the
next chapter, we will look at the final details of how the models we used were built
from it.
<header><largefont><b>Questionnaire</b></largefont></header>
1. How did we get to a single vector of activations in the CNNs used for MNIST in
previous chapters? Why isn’t that suitable for Imagenette?
2. What do we do for Imagenette instead?
3. What is adaptive pooling?
4. What is average pooling?"|ResNet architecture; building state-of-the-art ResNet
"Kao estimated that “less than 800,000 of the 22M+ comments…could be considered
truly unique” and that “more than 99% of the truly unique comments were in favor of
keeping net neutrality.”
Given advances in language modeling that have occurred since 2017, such fraudulent
campaigns could be nearly impossible to catch now. You now have all the necessary
tools at your disposal to create a compelling language model—something that can
generate context-appropriate, believable text. It won’t necessarily be perfectly accurate
or correct, but it will be plausible. Think about what this technology would mean
when put together with the kinds of disinformation campaigns we have learned about
in recent years. Take a look at the Reddit dialogue shown in Figure 10-3, where a lan‐
guage model based on OpenAI’s GPT-2 algorithm is having a conversation with itself
about whether the US government should cut defense spending.
<i>Figure</i> <i>10-3.</i> <i>An</i> <i>algorithm</i> <i>talking</i> <i>to</i> <i>itself</i> <i>on</i> <i>Reddit</i>
In this case, it was explained that an algorithm was being used to generate the dia‐
logue. But imagine what would happen if a bad actor decided to release such an algo‐
rithm across social networks—they could do it slowly and carefully, allowing the
algorithm to gradually develop followers and trust over time. It would not take many
resources to have literally millions of accounts doing this. In such a situation, we
could easily imagine getting to a point where the vast majority of discourse online
was from bots, and nobody would have any idea that it was happening."|ethics; identity generation by ML
"One great way to consider whether an ethical lens is complete is to try to come up
with an example in which the lens and our own ethical intuitions give diverging
results. Os Keyes et al. explored this in a graphic way in their paper “A Mulching Pro‐
posal: Analysing and Improving an Algorithmic System for Turning the Elderly into
High-Nutrient Slurry”. The paper’s abstract says:
The ethical implications of algorithmic systems have been much discussed in both HCI
and the broader community of those interested in technology design, development,
and policy. In this paper, we explore the application of one prominent ethical frame‐
work—Fairness, Accountability, and Transparency—to a proposed algorithm that
resolves various societal issues around food security and population aging. Using vari‐
ous standardised forms of algorithmic audit and evaluation, we drastically increase the
algorithm’s adherence to the FAT framework, resulting in a more ethical and benefi‐
cent system. We discuss how this might serve as a guide to other researchers or practi‐
tioners looking to ensure better ethical outcomes from algorithmic systems in their
line of work.
In this paper, the rather controversial proposal (“Turning the Elderly into High-
Nutrient Slurry”) and the results (“drastically increase the algorithm’s adherence to
the FAT framework, resulting in a more ethical and beneficent system”) are at odds…
to say the least!
In philosophy, and especially philosophy of ethics, this is one of the most effective
tools: first, come up with a process, definition, set of questions, etc., which is designed
to resolve a problem. Then try to come up with an example in which that apparent
solution results in a proposal that no one would consider acceptable. This can then
lead to a further refinement of the solution.
So far, we’ve focused on things that you and your organization can do. But sometimes
individual or organizational action is not enough. Sometimes governments also need
to consider policy implications.
<header><largefont><b>Role</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Policy</b></largefont></header>
We often talk to people who are eager for technical or design fixes to be a full solution
to the kinds of problems that we’ve been discussing; for instance, a technical
approach to debias data, or design guidelines for making technology less addictive.
While such measures can be useful, they will not be sufficient to address the underly‐
ing problems that have led to our current state. For example, as long as it is profitable
to create addictive technology, companies will continue to do so, regardless of
whether this has the side effect of promoting conspiracy theories and polluting our
information ecosystem. While individual designers may try to tweak product designs,
we will not see substantial changes until the underlying profit incentives change."|ethics; Durbin; Hutson; Keyes; policy’s role in ethics; ethical lens versus ethical intuitions
"Just as we moved from sigmoid to softmax, we need to extend the loss function to
work with more than just binary classification—it needs to be able to classify any
number of categories (in this case, we have 37 categories). Our activations, after soft‐
max, are between 0 and 1, and sum to 1 for each row in the batch of predictions. Our
targets are integers between 0 and 36.
In the binary case, we used torch.where to select between inputs and 1-inputs.
When we treat a binary classification as a general classification problem with two cat‐
egories, it becomes even easier, because (as we saw in the previous section) we now
have two columns containing the equivalent of inputs and 1-inputs. So, all we need
to do is select from the appropriate column. Let’s try to implement this in PyTorch.
For our synthetic 3s and 7s example, let’s say these are our labels:
targ = tensor([0,1,0,1,1,0])
And these are the softmax activations:
sm_acts
tensor([[0.6025, 0.3975],
[0.5021, 0.4979],
[0.1332, 0.8668],
[0.9966, 0.0034],
[0.5959, 0.4041],
[0.3661, 0.6339]])
Then for each item of targ , we can use that to select the appropriate column of
sm_acts using tensor indexing, like so:
idx = range(6)
sm_acts[idx, targ]
tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661])
To see exactly what’s happening here, let’s put all the columns together in a table.
Here, the first two columns are our activations, then we have the targets, the row
index, and finally the result shown in the preceding code:
<b>3</b> <b>7</b> <b>targ</b> <b>idx</b> <b>loss</b>
0.602469 0.397531 0 0 0.602469
0.502065 0.497935 1 1 0.497935
0.133188 0.866811 0 2 0.133188
0.99664 0.00336017 1 3 0.00336017
0.595949 0.404051 1 4 0.404051
0.366118 0.633882 0 5 0.366118
Looking at this table, you can see that the final column can be calculated by taking the
targ and idx columns as indices into the two-column matrix containing the 3 and 7
columns. That’s what sm_acts[idx, targ] is doing."|cross-entropy loss; loss; MNIST loss function; pet breeds image classifier; binary to multiple categories
"<header><largefont><b>The</b></largefont> <largefont><b>Extrapolation</b></largefont> <largefont><b>Problem</b></largefont></header>
Let’s consider the simple task of making predictions from 40 data points showing a
slightly noisy linear relationship:
x_lin = torch.linspace(0,20, steps=40)
y_lin = x_lin + torch.randn_like(x_lin)
plt.scatter(x_lin, y_lin);
Although we have only a single independent variable, sklearn expects a matrix of
independent variables, not a single vector. So we have to turn our vector into a matrix
with one column. In other words, we have to change the <i>shape</i> from [40] to [40,1].
unsqueeze
One way to do that is with the method, which adds a new unit axis to a
tensor at the requested dimension:
xs_lin = x_lin.unsqueeze(1)
x_lin.shape,xs_lin.shape
(torch.Size([40]), torch.Size([40, 1]))
A more flexible approach is to slice an array or tensor with the special value None,
which introduces an additional unit axis at that location:
x_lin[:,None].shape
torch.Size([40, 1])
We can now create a random forest for this data. We’ll use only the first 30 rows to
train the model:
m_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30])"|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"Because accelerating SGD with momentum is such a good idea, fastai does this by
default in fit_one_cycle, so we turn it off with moms=(0,0,0). We’ll be discussing
momentum shortly.
Clearly, plain SGD isn’t training as fast as we’d like. So let’s learn some tricks to get
accelerated training!
<header><largefont><b>A</b></largefont> <largefont><b>Generic</b></largefont> <largefont><b>Optimizer</b></largefont></header>
To build up our accelerated SGD tricks, we’ll need to start with a nice flexible opti‐
mizer foundation. No library prior to fastai provided such a foundation, but during
fastai’s development, we realized that all the optimizer improvements we’d seen in the
academic literature could be handled using <i>optimizer</i> <i>callbacks.</i> These are small pieces
of code that we can compose, mix, and match in an optimizer to build the optimizer
step. They are called by fastai’s lightweight Optimizer class. These are the definitions
Optimizer
in of the two key methods that we’ve been using in this book:
<b>def</b> zero_grad(self):
<b>for</b> p,*_ <b>in</b> self.all_params():
p.grad.detach_()
p.grad.zero_()
<b>def</b> step(self):
<b>for</b> p,pg,state,hyper <b>in</b> self.all_params():
<b>for</b> cb <b>in</b> self.cbs:
state = _update(state, cb(p, **{**state, **hyper}))
self.state[p] = state
As we saw when training an MNIST model from scratch, zero_grad just loops
through the parameters of the model and sets the gradients to zero. It also calls
detach_,
which removes any history of gradient computation, since it won’t be
needed after zero_grad.
The more interesting method is step, which loops through the callbacks (cbs) and
calls them to update the parameters (the _update function just calls state.update if
there’s anything returned by cb). As you can see, Optimizer doesn’t do any SGD steps
Optimizer.
itself. Let’s see how we can add SGD to
Here’s an optimizer callback that does a single SGD step, by multiplying -lr by the
gradients and adding that to the parameter (when Tensor.add_ in PyTorch is passed
two parameters, they are multiplied together before the addition):
<b>def</b> sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data)
We can pass this to Optimizer using the cbs parameter; we’ll need to use partial
since Learner will call this function to create our optimizer later:
opt_func = partial(Optimizer, cbs=[sgd_cb])"|optimization; SGD class; training
"representations for rare words. However, this last issue is better handled by setting
min_freq; the default min_freq=3 means that any word appearing fewer than three
times is replaced with xxunk .
fastai can also numericalize your dataset using a vocab that you provide, by passing a
list of words as the vocab parameter.
Once we’ve created our Numericalize object, we can use it as if it were a function:
nums = num(toks)[:20]; nums
tensor([ 2, 8, 21, 28, 11, 90, 18, 59, 0, 45, 9, 351, 499, 11,
> 72, 533, 584, 146, 29, 12])
This time, our tokens have been converted to a tensor of integers that our model can
receive. We can check that they map back to the original text:
' '.join(num.vocab[o] <b>for</b> o <b>in</b> nums)
'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently
> sit around for a'
Now that we have numbers, we need to put them in batches for our model.
<header><largefont><b>Putting</b></largefont> <largefont><b>Our</b></largefont> <largefont><b>Texts</b></largefont> <largefont><b>into</b></largefont> <largefont><b>Batches</b></largefont> <largefont><b>for</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Model</b></largefont></header>
When dealing with images, we needed to resize them all to the same height and width
before grouping them together in a mini-batch so they could stack together efficiently
in a single tensor. Here it’s going to be a little different, because one cannot simply
resize text to a desired length. Also, we want our language model to read text in order,
so that it can efficiently predict what the next word is. This means each new batch
should begin precisely where the previous one left off.
Suppose we have the following text:
In this chapter, we will go back over the example of classifying movie reviews we stud‐
ied in chapter 1 and dig deeper under the surface. First we will look at the processing
steps necessary to convert text into numbers and how to customize it. By doing this,
we’ll have another example of the PreProcessor used in the data block API.
Then we will study how we build a language model and train it for a while.
The tokenization process will add special tokens and deal with punctuation to return
this text:
xxbos xxmaj in this chapter , we will go back over the example of classifying movie
reviews we studied in chapter 1 and dig deeper under the surface . xxmaj first we will
look at the processing steps necessary to convert text into numbers and how to cus‐
tomize it . xxmaj by doing this , we ‘ll have another example of the preprocessor used
in the data block xxup api . \n xxmaj then we will study how we build a language
model and train it for a while ."|batch operations; natural language processing (NLP); tokenization
"2. Define any parameters of the model as attributes with nn.Parameter .
3. Define a forward function that returns the output of your model.
As an example, here is the linear layer from scratch:
<b>import</b> <b>torch.nn</b> <b>as</b> <b>nn</b>
<b>class</b> <b>LinearLayer(nn.Module):</b>
<b>def</b> <b>__init__(self,</b> n_in, n_out):
super().__init__()
self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in))
self.bias = nn.Parameter(torch.zeros(n_out))
<b>def</b> forward(self, x): <b>return</b> x @ self.weight.t() + self.bias
As you see, this class automatically keeps track of what parameters have been defined:
lin = LinearLayer(10,2)
p1,p2 = lin.parameters()
p1.shape,p2.shape
(torch.Size([2, 10]), torch.Size([2]))
nn.Module opt.step
It is thanks to this feature of that we can just say and have an
optimizer loop through the parameters and update each one.
Note that in PyTorch, the weights are stored as an n_out x n_in matrix, which is why
we have the transpose in the forward pass.
By using the linear layer from PyTorch (which uses the Kaiming initialization as
well), the model we have been building up during this chapter can be written like this:
<b>class</b> <b>Model(nn.Module):</b>
<b>def</b> <b>__init__(self,</b> n_in, nh, n_out):
super().__init__()
self.layers = nn.Sequential(
nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))
self.loss = mse
<b>def</b> forward(self, x, targ): <b>return</b> self.loss(self.layers(x).squeeze(), targ)
fastai provides its own variant of Module that is identical to nn.Module, but doesn’t
require you to call super().__init__() (it does that for you automatically):
<b>class</b> <b>Model(Module):</b>
<b>def</b> <b>__init__(self,</b> n_in, nh, n_out):
self.layers = nn.Sequential(
nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out))
self.loss = mse
<b>def</b> forward(self, x, targ): <b>return</b> self.loss(self.layers(x).squeeze(), targ)"|PyTorch
"An arrow represents the actual layer computation—i.e., the linear layer followed by
the activation function. Using this notation, Figure 12-3 shows what our simple lan‐
guage model looks like.
<i>Figure</i> <i>12-3.</i> <i>Representation</i> <i>of</i> <i>our</i> <i>basic</i> <i>language</i> <i>model</i>
To simplify things, we’ve removed the details of the layer computation from each
arrow. We’ve also color-coded the arrows, such that all arrows with the same color
have the same weight matrix. For instance, all the input layers use the same embed‐
ding matrix, so they all have the same color (green).
Let’s try training this model and see how it goes:
learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy,
metrics=accuracy)
learn.fit_one_cycle(4, 1e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.824297 1.970941 0.467554 00:02
1 1.386973 1.823242 0.467554 00:02
2 1.417556 1.654497 0.494414 00:02
3 1.376440 1.650849 0.494414 00:02"|language model; natural language processing (NLP)
"<b>JeremySays</b>
No matter how many models I train, I never stop getting moved
and surprised by how these randomly initialized bunches of num‐
bers, trained with such simple mechanics, manage to discover
things about my data all by themselves. It almost seems like cheat‐
ing that I can create code that does useful things without ever
actually telling it how to do those things!
We defined our model from scratch to teach you what is inside, but you can directly
use the fastai library to build it. We’ll look at how to do that next.
<header><largefont><b>Using</b></largefont> <largefont><b>fastai.collab</b></largefont></header>
We can create and train a collaborative filtering model using the exact structure
shown earlier by using fastai’s collab_learner:
learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))
learn.fit_one_cycle(5, 5e-3, wd=0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 0.931751 0.953806 00:13
1 0.851826 0.878119 00:13
2 0.715254 0.834711 00:13
3 0.583173 0.821470 00:13
4 0.496625 0.821688 00:13
The names of the layers can be seen by printing the model:
learn.model
EmbeddingDotBias(
(u_weight): Embedding(944, 50)
(i_weight): Embedding(1635, 50)
(u_bias): Embedding(944, 1)
(i_bias): Embedding(1635, 1)
)
We can use these to replicate any of the analyses we did in the previous section—for
instance:
movie_bias = learn.model.i_bias.weight.squeeze()
idxs = movie_bias.argsort(descending=True)[:5]
[dls.classes['title'][i] <b>for</b> i <b>in</b> idxs]
['Titanic (1997)',
""Schindler's List (1993)"",
'Shawshank Redemption, The (1994)',"|collaborative filtering; built from scratch; layers via printing model; collab_learner; layers; Learner; models; embedding from scratch
"Removing these variables has slightly improved the model’s accuracy; but more
importantly, it should make it more resilient over time, and easier to maintain and
understand. We recommend that for all datasets, you try building a model in which
your dependent variable is is_valid , as we did here. It can often uncover subtle
<i>domain</i> <i>shift</i> issues that you may otherwise miss.
One thing that might help in our case is to simply avoid using old data. Often, old
data shows relationships that just aren’t valid anymore. Let’s try just using the most
recent few years of the data:
xs['saleYear'].hist();
Here’s the result of training on this subset:
filt = xs['saleYear']>2004
xs_filt = xs_final_time[filt]
y_filt = y[filt]
m = rf(xs_filt, y_filt)
m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)
(0.17768, 0.230631)
It’s a tiny bit better, which shows that you shouldn’t always use your entire dataset;
sometimes a subset can be better.
Let’s see if using a neural network helps.
<header><largefont><b>Using</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Network</b></largefont></header>
We can use the same approach to build a neural network model. Let’s first replicate
the steps we took to set up the TabularPandas object:
df_nn = pd.read_csv(path/'TrainAndValid.csv', low_memory=False)
df_nn['ProductSize'] = df_nn['ProductSize'].astype('category')
df_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)
df_nn[dep_var] = np.log(df_nn[dep_var])
df_nn = add_datepart(df_nn, 'saledate')"|bagging; decision trees; machine learning (ML); neural networks; fastai TabularPandas class; tabular data processing; predictions; random forests; tabular data for models; neural network model; training
"doing everything from scratch ourselves (except for calculating the gradients), you
know that there is no special magic hiding behind the scenes.
<header><largefont><b>Going</b></largefont> <largefont><b>Deeper</b></largefont></header>
There is no need to stop at just two linear layers. We can add as many as we want, as
long as we add a nonlinearity between each pair of linear layers. As you will learn,
however, the deeper the model gets, the harder it is to optimize the parameters in
practice. Later in this book, you will learn about some simple but brilliantly effective
techniques for training deeper models.
We already know that a single nonlinearity with two linear layers is enough to
approximate any function. So why would we use deeper models? The reason is per‐
formance. With a deeper model (one with more layers), we do not need to use as
many parameters; it turns out that we can use smaller matrices, with more layers, and
get better results than we would get with larger matrices and few layers.
That means that we can train the model more quickly, and it will take up less mem‐
ory. In the 1990s, researchers were so focused on the universal approximation theo‐
rem that few were experimenting with more than one nonlinearity. This theoretical
but not practical foundation held back the field for years. Some researchers, however,
did experiment with deep models, and eventually were able to show that these models
could perform much better in practice. Eventually, theoretical results were developed
that showed why this happens. Today, it is extremely unusual to find anybody using a
neural network with just one nonlinearity.
Here is what happens when we train an 18-layer model using the same approach we
saw in Chapter 1:
dls = ImageDataLoaders.from_folder(path)
learn = cnn_learner(dls, resnet18, pretrained=False,
loss_func=F.cross_entropy, metrics=accuracy)
learn.fit_one_cycle(1, 0.1)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.082089 0.009578 0.997056 00:11
Nearly 100% accuracy! That’s a big difference compared to our simple neural net. But
as you’ll learn in the remainder of this book, there are just a few little tricks you need
to use to get such great results from scratch yourself. You already know the key foun‐
dational pieces. (Of course, even when you know all the tricks, you’ll nearly always
want to work with the prebuilt classes provided by PyTorch and fastai, because they
save you from having to think about all the little details yourself.)"|deeper models; deeper models having more layers; layers; more linear layers; nonlinear function between linears; numerical digit classifier; PyTorch; optimization; creating an optimizer; deeper models and; stochastic gradient descent (SGD); training
"The sequence of steps we described earlier starts by picking a random value for a
parameter, and calculating the value of the loss:
plot_function(f, 'x', 'x**2')
plt.scatter(-1.5, f(-1.5), color='red');
Now we look to see what would happen if we increased or decreased our parameter
by a little bit—the <i>adjustment.</i> This is simply the slope at a particular point:"|stochastic gradient descent; stochastic gradient descent (SGD)
"As we have discussed, a DataLoader collates the items from a Dataset into a mini-
batch. This is a tuple of tensors, where each tensor simply stacks the items from that
location in the Dataset item.
Now that we have confirmed that the individual items look OK, there’s one more step,
we need to ensure we can create our DataLoaders , which is to ensure that every item
is of the same size. To do this, we can use RandomResizedCrop:
dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),
splitter=splitter,
get_x=get_x,
get_y=get_y,
item_tfms = RandomResizedCrop(128, min_scale=0.35))
dls = dblock.dataloaders(df)
And now we can display a sample of our data:
dls.show_batch(nrows=1, ncols=3)
DataLoaders
Remember that if anything goes wrong when you create your from your
DataBlock, or if you want to view exactly what happens with your DataBlock, you
can use the summary method we presented in the previous chapter.
Our data is now ready for training a model. As we will see, nothing is going to change
when we create our Learner, but behind the scenes the fastai library will pick a new
loss function for us: binary cross entropy.
<header><largefont><b>Binary</b></largefont> <largefont><b>Cross</b></largefont> <largefont><b>Entropy</b></largefont></header>
Now we’ll create our Learner. We saw in Chapter 4 that a Learner object contains
four main things: the model, a DataLoaders object, an Optimizer, and the loss func‐
tion to use. We already have our DataLoaders, we can leverage fastai’s resnet models
(which we’ll learn how to create from scratch later), and we know how to create an
SGD optimizer. So let’s focus on ensuring we have a suitable loss function. To do this,
let’s use cnn_learner to create a Learner, so we can look at its activations:
learn = cnn_learner(dls, resnet18)"|binary cross entropy loss function; DataFrame to DataLoaders; DataLoaders object from; DataFrame converted to; debugging; labels; Learner; binary cross entropy; multi-label classifier loss function; debugging tabular dataset
"We have nearly as many leaf nodes as data points! That seems a little over-
enthusiastic. Indeed, sklearn’s default settings allow it to continue splitting nodes until
there is only one item in each leaf node. Let’s change the stopping rule to tell sklearn
to ensure every leaf node contains at least 25 auction records:
m = DecisionTreeRegressor(min_samples_leaf=25)
m.fit(to.train.xs, to.train.y)
m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)
(0.248562, 0.32368)
That looks much better. Let’s check the number of leaves again:
m.get_n_leaves()
12397
Much more reasonable!
<b>AlexisSays</b>
Here’s my intuition for an overfitting decision tree with more leaf
nodes than data items. Consider the game Twenty Questions. In
that game, the chooser secretly imagines an object (like, “our televi‐
sion set”), and the guesser gets to pose 20 yes or no questions to try
to guess what the object is (like “Is it bigger than a breadbox?”).
The guesser is not trying to predict a numerical value, but just to
identify a particular object out of the set of all imaginable objects.
When your decision tree has more leaves than there are possible
objects in your domain, it is essentially a well-trained guesser. It has
learned the sequence of questions needed to identify a particular
data item in the training set, and it is “predicting” only by describ‐
ing that item’s value. This is a way of memorizing the training set—
i.e., of overfitting.
Building a decision tree is a good way to create a model of our data. It is very flexible,
since it can clearly handle nonlinear relationships and interactions between variables.
But we can see there is a fundamental compromise between how well it generalizes
(which we can achieve by creating small trees) and how accurate it is on the training
set (which we can achieve by using large trees).
So how do we get the best of both worlds? We’ll show you right after we handle an
important missing detail: how to handle categorical variables."|decision trees; default leaf node splitting; tabular data for models; training
"So, for instance, even if you don’t normally enjoy detective movies, you might enjoy
<i>LA</i> <i>Confidential!</i>
It is not quite so easy to directly interpret the embedding matrices. There are just too
many factors for a human to look at. But there is a technique that can pull out the
most important underlying <i>directions</i> in such a matrix, called <i>principal</i> <i>component</i>
<i>analysis</i> (PCA). We will not be going into this in detail in this book, because it is not
particularly important for you to understand to be a deep learning practitioner, but if
you are interested, we suggest you check out the fast.ai course Computational Linear
Algebra for Coders. Figure 8-3 shows what our movies look like based on two of the
strongest PCA components.
<i>Figure</i> <i>8-3.</i> <i>Representation</i> <i>of</i> <i>movies</i> <i>based</i> <i>on</i> <i>two</i> <i>strongest</i> <i>PCA</i> <i>components</i>
We can see here that the model seems to have discovered a concept of <i>classic</i> versus
<i>pop</i> <i>culture</i> movies, or perhaps it is <i>critically</i> <i>acclaimed</i> that is represented here."|built from scratch; embedding from scratch
"<header><largefont><b>Conclusion</b></largefont></header>
Coming from a background of working with binary logic, the lack of clear answers in
ethics can be frustrating at first. Yet, the implications of how our work impacts the
world, including unintended consequences and the work becoming weaponized by
bad actors, are some of the most important questions we can (and should!) consider.
Even though there aren’t any easy answers, there are definite pitfalls to avoid and
practices to follow to move toward more ethical behavior.
Many people (including us!) are looking for more satisfying, solid answers about how
to address harmful impacts of technology. However, given the complex, far-reaching,
and interdisciplinary nature of the problems we are facing, there are no simple solu‐
tions. Julia Angwin, former senior reporter at ProPublica who focuses on issues of
algorithmic bias and surveillance (and one of the 2016 investigators of the COMPAS
recidivism algorithm that helped spark the field of FAccT) said in a 2019 interview:
I strongly believe that in order to solve a problem, you have to diagnose it, and that
we’re still in the diagnosis phase of this. If you think about the turn of the century and
industrialization, we had, I don’t know, 30 years of child labor, unlimited work hours,
terrible working conditions, and it took a lot of journalist muckraking and advocacy to
diagnose the problem and have some understanding of what it was, and then the acti‐
vism to get laws changed. I feel like we’re in a second industrialization of data informa‐
tion… I see my role as trying to make as clear as possible what the downsides are, and
diagnosing them really accurately so that they can be solvable. That’s hard work, and
lots more people need to be doing it.
It’s reassuring that Angwin thinks we are largely still in the diagnosis phase: if your
understanding of these problems feels incomplete, that is normal and natural.
Nobody has a “cure” yet, although it is vital that we continue working to better under‐
stand and address the problems we are facing.
One of our reviewers for this book, Fred Monroe, used to work in hedge fund trad‐
ing. He told us, after reading this chapter, that many of the issues discussed here (dis‐
tribution of data being dramatically different from what a model was trained on, the
impact of feedback loops on a model once deployed and at scale, and so forth) were
also key issues for building profitable trading models. The kinds of things you need to
do to consider societal consequences are going to have a lot of overlap with things
you need to do to consider organizational, market, and customer consequences—so
thinking carefully about ethics can also help you think carefully about how to make
your data product successful more generally!
<header><largefont><b>Questionnaire</b></largefont></header>
1. Does ethics provide a list of “right answers”?"|Angwin; COMPAS algorithm; ethics; Monroe
"We can see what’s in this directory by using ls , a method added by fastai. This
method returns an object of a special fastai class called L, which has all the same func‐
tionality of Python’s built-in list, plus a lot more. One of its handy features is that,
when printed, it displays the count of items before listing the items themselves (if
there are more than 10 items, it shows just the first few):
path.ls()
(#9) [Path('cleaned.csv'),Path('item_list.txt'),Path('trained_model.pkl'),Path('
> models'),Path('valid'),Path('labels.csv'),Path('export.pkl'),Path('history.cs
> v'),Path('train')]
The MNIST dataset follows a common layout for machine learning datasets: separate
folders for the training set and the validation (and/or test) set. Let’s see what’s inside
the training set:
(path/'train').ls()
(#2) [Path('train/7'),Path('train/3')]
There’s a folder of 3s, and a folder of 7s. In machine learning parlance, we say that “3”
and “7” are the <i>labels</i> (or targets) in this dataset. Let’s take a look in one of these fold‐
ers (using sorted to ensure we all get the same order of files):
threes = (path/'train'/'3').ls().sorted()
sevens = (path/'train'/'7').ls().sorted()
threes
(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.pn
> g'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.p
> ng'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.p
> ng'),Path('train/3/10091.png')...]
As we might expect, it’s full of image files. Let’s take a look at one now. Here’s an
image of a handwritten number 3, taken from the famous MNIST dataset of hand‐
written numbers:
im3_path = threes[1]
im3 = Image.open(im3_path)
im3
Image
Here we are using the class from the <i>Python</i> <i>Imaging</i> <i>Library</i> (PIL), which is the
most widely used Python package for opening, manipulating, and viewing images.
Jupyter knows about PIL images, so it displays the image for us automatically.
In a computer, everything is represented as a number. To view the numbers that make
up this image, we have to convert it to a <i>NumPy</i> <i>array</i> or a <i>PyTorch</i> <i>tensor.</i> For
instance, here’s what a section of the image looks like converted to a NumPy array:"|arrays; pixels as foundation; Python Imaging Library; path to dataset; Image class; ls method in Path class; image as array or tensor; viewing dataset images; NumPy; PIL images; pixels; Python Imaging Library (PIL)
"<b>Jargon:Embedding</b>
Multiplying by a one-hot-encoded matrix, using the computational
shortcut that it can be implemented by simply indexing directly.
This is quite a fancy word for a very simple concept. The thing that
you multiply the one-hot-encoded matrix by (or, using the compu‐
tational shortcut, index into directly) is called the <i>embedding</i>
<i>matrix.</i>
In computer vision, we have a very easy way to get all the information of a pixel
through its RGB values: each pixel in a colored image is represented by three num‐
bers. Those three numbers give us the redness, the greenness, and the blueness, which
is enough to get our model to work afterward.
For the problem at hand, we don’t have the same easy way to characterize a user or a
movie. There are probably relations with genres: if a given user likes romance, they
are likely to give higher scores to romance movies. Other factors might be whether
the movie is more action-oriented versus heavy on dialogue, or the presence of a spe‐
cific actor whom a user might particularly like.
How do we determine numbers to characterize those? The answer is, we don’t. We
will let our model <i>learn</i> them. By analyzing the existing relations between users and
movies, our model can figure out itself the features that seem important or not.
This is what embeddings are. We will attribute to each of our users and each of our
movies a random vector of a certain length (here, n_factors=5 ), and we will make
those learnable parameters. That means that at each step, when we compute the loss
by comparing our predictions to our targets, we will compute the gradients of the loss
with respect to those embedding vectors and update them with the rules of SGD (or
another optimizer).
At the beginning, those numbers don’t mean anything since we have chosen them
randomly, but by the end of training, they will. By learning on existing data about the
relations between users and movies, without having any other information, we will
see that they still get some important features, and can isolate blockbusters from
independent films, action movies from romance, and so on.
We are now in a position to create our whole model from scratch.
<header><largefont><b>Collaborative</b></largefont> <largefont><b>Filtering</b></largefont> <largefont><b>from</b></largefont> <largefont><b>Scratch</b></largefont></header>
Before we can write a model in PyTorch, we first need to learn the basics of object-
oriented programming and Python. If you haven’t done any object-oriented program‐
ming before, we will give you a quick introduction here, but we would recommend
looking up a tutorial and getting some practice before moving on."|categorical variables; collaborative filtering; embedding; object-oriented programming; embedding categorical variables
"Also, you’ll see that we’ve removed the ReLU ( act_cls=None ) from the final convolu‐
tion in convs and from idconv, and moved it to <i>after</i> we add the skip connection.
The thinking behind this is that the whole ResNet block is like a layer, and you want
your activation to be after your layer.
Let’s replace our block with ResBlock and try it out:
<b>def</b> block(ni,nf): <b>return</b> ResBlock(ni, nf, stride=2)
learn = get_learner(get_model())
learn.fit_one_cycle(5, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.973174 1.845491 0.373248 00:08
1 1.678627 1.778713 0.439236 00:08
2 1.386163 1.596503 0.507261 00:08
3 1.177839 1.102993 0.644841 00:09
4 1.052435 1.038013 0.667771 00:09
It’s not much better. But the whole point of this was to allow us to train <i>deeper</i> mod‐
els, and we’re not really taking advantage of that yet. To create a model that’s, say,
twice as deep, all we need to do is replace our block with two ResBlocks in a row:
<b>def</b> block(ni, nf):
<b>return</b> nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf))
learn = get_learner(get_model())
learn.fit_one_cycle(5, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.964076 1.864578 0.355159 00:12
1 1.636880 1.596789 0.502675 00:12
2 1.335378 1.304472 0.588535 00:12
3 1.089160 1.065063 0.663185 00:12
4 0.942904 0.963589 0.692739 00:12
Now we’re making good progress!
The authors of the ResNet paper went on to win the 2015 ImageNet challenge. At the
time, this was by far the most important annual event in computer vision. We have
already seen another ImageNet winner: the 2013 winners, Zeiler and Fergus. It is
interesting to note that in both cases, the starting points for the breakthroughs were
experimental observations: observations about what layers actually learn, in the case
of Zeiler and Fergus, and observations about which kinds of networks can be trained,
in the case of the ResNet authors. This ability to design and analyze thoughtful"|building ResNet CNN; ResNet architecture; skip connections
"labels. Before we move on to fine-tuning the classifier, however, let’s quickly try
something different: using our model to generate random reviews.
<header><largefont><b>Text</b></largefont> <largefont><b>Generation</b></largefont></header>
Because our model is trained to guess the next word of the sentence, we can use it to
write new reviews:
TEXT = ""I liked this movie because""
N_WORDS = 40
N_SENTENCES = 2
preds = [learn.predict(TEXT, N_WORDS, temperature=0.75)
<b>for</b> _ <b>in</b> range(N_SENTENCES)]
<b>print(""\n"".join(preds))</b>
i liked this movie because of its story and characters . The story line was very
> strong , very good for a sci - fi film . The main character , Alucard , was
> very well developed and brought the whole story
i liked this movie because i like the idea of the premise of the movie , the (
> very ) convenient virus ( which , when you have to kill a few people , the ""
> evil "" machine has to be used to protect
As you can see, we add some randomness (we pick a random word based on the
probabilities returned by the model) so we don’t get exactly the same review twice.
Our model doesn’t have any programmed knowledge of the structure of a sentence or
grammar rules, yet it has clearly learned a lot about English sentences: we can see it
capitalizes properly (I is transformed to <i>i</i> because our rules require two characters or
more to consider a word as capitalized, so it’s normal to see it lowercased) and is
using consistent tense. The general review makes sense at first glance, and it’s only if
you read carefully that you can notice something is a bit off. Not bad for a model
trained in a couple of hours!
But our end goal wasn’t to train a model to generate reviews, but to classify them…so
let’s use this model to do just that.
<header><largefont><b>Classifier</b></largefont></header>
<header><largefont><b>Creating</b></largefont> <largefont><b>the</b></largefont> <largefont><b>DataLoaders</b></largefont></header>
We’re now moving from language model fine-tuning to classifier fine-tuning. To re-
cap, a language model predicts the next word of a document, so it doesn’t need any
external labels. A classifier, however, predicts an external label—in the case of IMDb,
it’s the sentiment of a document.
This means that the structure of our DataBlock for NLP classification will look very
familiar. It’s nearly the same as we’ve seen for the many image classification datasets
we’ve worked with:
dls_clas = DataBlock(
blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),
get_y = parent_label,"|movie review classifier; natural language processing (NLP); classifier DataLoaders; NLP
"Let’s try both of these now:
dist_3_abs = (a_3 - mean3).abs().mean()
dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()
dist_3_abs,dist_3_sqr
(tensor(0.1114), tensor(0.2021))
dist_7_abs = (a_3 - mean7).abs().mean()
dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()
dist_7_abs,dist_7_sqr
(tensor(0.1586), tensor(0.3021))
In both cases, the distance between our 3 and the “ideal” 3 is less than the distance to
the ideal 7, so our simple model will give the right prediction in this case.
PyTorch already provides both of these as <i>loss</i> <i>functions.</i> You’ll find these inside
torch.nn.functional , which the PyTorch team recommends importing as F (and is
available by default under that name in fastai):
F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()
(tensor(0.1586), tensor(0.3021))
Here, MSE stands for <i>mean</i> <i>squared</i> <i>error,</i> and l1 refers to the standard mathematical
jargon for <i>mean</i> <i>absolute</i> <i>value</i> (in math it’s called the <i>L1</i> <i>norm).</i>
<b>SylvainSays</b>
Intuitively, the difference between L1 norm and mean squared
error (MSE) is that the latter will penalize bigger mistakes more
heavily than the former (and be more lenient with small mistakes).
<b>JeremySays</b>
When I first came across this L1 thingie, I looked it up to see what
on earth it meant. I found on Google that it is a <i>vector</i> <i>norm</i> using
<i>absolute</i> <i>value,</i> so I looked up “vector norm” and started reading:
<i>Given</i> <i>a</i> <i>vector</i> <i>space</i> <i>V</i> <i>over</i> <i>a</i> <i>field</i> <i>F</i> <i>of</i> <i>the</i> <i>real</i> <i>or</i> <i>complex</i> <i>numbers,</i>
<i>a</i> <i>norm</i> <i>on</i> <i>V</i> <i>is</i> <i>a</i> <i>nonnegative-valued</i> <i>any</i> <i>function</i> <i>p:</i> <i>V</i> <i>→</i> <i>\[0,+∞)</i>
<i>with</i> <i>the</i> <i>following</i> <i>properties:</i> <i>For</i> <i>all</i> <i>a∈F</i> <i>and</i> <i>all</i> <i>u,</i> <i>v∈V,</i> <i>p(u</i> <i>+</i> <i>v)</i>
<i>≤</i> <i>p(u)</i> <i>+</i> <i>p(v)…Then</i> I stopped reading. “Ugh, I’ll never understand
math!” I thought, for the thousandth time. Since then, I’ve learned
that every time these complex mathy bits of jargon come up in
practice, it turns out I can replace them with a tiny bit of code!
Like, the <i>L1</i> <i>loss</i> is just equal to (a-b).abs().mean(), where a and
b are tensors. I guess mathy folks just think differently than I do…
I’ll make sure in this book that every time some mathy jargon
comes up, I’ll give you the little bit of code it’s equal to as well, and
explain in common-sense terms what’s going on."|F (torch.nn.functional); L1 norm (mean absolute difference); loss; mean absolute difference (L1 norm); mean squared error (MSE); PyTorch; pixels; fastai torch.nn.functional import; torch.nn.functional
"To create color_dim , we take the histogram shown on the left here and convert it into
just the colored representation shown at the bottom. Then we flip it on its side, as
shown on the right. We found that the distribution is clearer if we take the log of the
histogram values. Then, Giomo describes:
The final plot for each layer is made by stacking the histogram of the activations from
each batch along the horizontal axis. So each vertical slice in the visualisation repre‐
sents the histogram of activations for a single batch. The color intensity corresponds to
the height of the histogram; in other words, the number of activations in each histo‐
gram bin.
Figure 13-15 shows how this all fits together.
<i>Figure</i> <i>13-15.</i> <i>Summary</i> <i>of</i> <i>the</i> <i>colorful</i> <i>dimension</i> <i>(courtesy</i> <i>of</i> <i>Stefano</i> <i>Giomo)</i>
This illustrates why log(f) is more colorful than <i>f</i> when <i>f</i> follows a normal distribu‐
tion, because taking a log changes the Gaussian curve in a quadratic, which isn’t as
narrow.
So with that in mind, let’s take another look at the result for the penultimate layer:
learn.activation_stats.color_dim(-2)"|convolutional neural network (CNN); building a CNN; training more stable; training on all digits
"<i>Figure</i> <i>3-15.</i> <i>Event</i> <i>organized</i> <i>by</i> <i>the</i> <i>group</i> <i>Heart</i> <i>of</i> <i>Texas</i>
Disinformation often involves coordinated campaigns of inauthentic behavior. For
instance, fraudulent accounts may try to make it seem like many people hold a partic‐
ular viewpoint. While most of us like to think of ourselves as independent-minded, in
reality we evolved to be influenced by others in our in-group, and in opposition to
those in our out-group. Online discussions can influence our viewpoints, or alter the
range of what we consider acceptable viewpoints. Humans are social animals, and as
social animals, we are extremely influenced by the people around us. Increasingly,
radicalization occurs in online environments; so influence is coming from people in
the virtual space of online forums and social networks.
Disinformation through autogenerated text is a particularly significant issue, due to
the greatly increased capability provided by deep learning. We discuss this issue in
depth when we delve into creating language models in Chapter 10.
One proposed approach is to develop some form of digital signature, to implement it
in a seamless way, and to create norms that we should trust only content that has been
verified. The head of the Allen Institute on AI, Oren Etzioni, wrote such a proposal in
an article titled “How Will We Prevent AI-Based Forgery?”: “AI is poised to make
high-fidelity forgery inexpensive and automated, leading to potentially disastrous
consequences for democracy, security, and society. The specter of AI forgery means"|digital signature; Etzioni; forgery via AI; natural language processing (NLP); text generation
"<header><largefont><b>Exploding</b></largefont> <largefont><b>or</b></largefont> <largefont><b>Disappearing</b></largefont> <largefont><b>Activations</b></largefont></header>
In practice, creating accurate models from this kind of RNN is difficult. We will get
better results if we call detach less often, and have more layers—this gives our RNN a
longer time horizon to learn from and richer features to create. But it also means we
have a deeper model to train. The key challenge in the development of deep learning
has been figuring out how to train these kinds of models.
This is challenging because of what happens when you multiply by a matrix many
times. Think about what happens when you multiply by a number many times. For
example, if you multiply by 2, starting at 1, you get the sequence 1, 2, 4, 8,…and after
32 steps, you are already at 4,294,967,296. A similar issue happens if you multiply by
0.5: you get 0.5, 0.25, 0.125…and after 32 steps, it’s 0.00000000023. As you can see,
multiplying by a number even slightly higher or lower than 1 results in an explosion
or disappearance of our starting number, after just a few repeated multiplications.
Because matrix multiplication is just multiplying numbers and adding them up,
exactly the same thing happens with repeated matrix multiplications. And that’s all a
deep neural network is—each extra layer is another matrix multiplication. This
means that it is very easy for a deep neural network to end up with extremely large or
extremely small numbers.
This is a problem, because the way computers store numbers (known as <i>floating</i>
<i>point)</i> means that they become less and less accurate the further away the numbers
get from zero. The diagram in Figure 12-8, from the excellent article “What You
Never Wanted to Know about Floating Point but Will Be Forced to Find Out”, shows
how the precision of floating-point numbers varies over the number line.
<i>Figure</i> <i>12-8.</i> <i>Precision</i> <i>of</i> <i>floating-point</i> <i>numbers</i>
This inaccuracy means that often the gradients calculated for updating the weights
end up as zero or infinity for deep networks. This is commonly referred to as the
<i>vanishing</i> <i>gradients</i> or <i>exploding</i> <i>gradients</i> problem. It means that in SGD, the weights
are either not updated at all or jump to infinity. Either way, they won’t improve with
training.
Researchers have developed ways to tackle this problem, which we will be discussing
later in the book. One option is to change the definition of a layer in a way that makes
it less likely to have exploding activations. We’ll look at the details of how this is done
in Chapter 13, when we discuss batch normalization, and Chapter 14, when we dis‐
cuss ResNets, although these details don’t generally matter in practice (unless you are
a researcher who is creating new approaches to solving this problem). Another"|multilayer RNNs
"x = torch.randn(200, 100)
<b>for</b> i <b>in</b> range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100)))
x[0:5,0:5]
tensor([[0.2871, 0.0000, 0.0000, 0.0000, 0.0026],
[0.4546, 0.0000, 0.0000, 0.0000, 0.0015],
[0.6178, 0.0000, 0.0000, 0.0180, 0.0079],
[0.3333, 0.0000, 0.0000, 0.0545, 0.0000],
[0.1940, 0.0000, 0.0000, 0.0000, 0.0096]])
That’s better: our numbers aren’t all zeroed this time. So let’s go back to the definition
of our neural net and use this initialization (which is named <i>Kaiming</i> <i>initialization</i> or
<i>He</i> <i>initialization):</i>
x = torch.randn(200, 100)
y = torch.randn(200)
w1 = torch.randn(100,50) * sqrt(2 / 100)
b1 = torch.zeros(50)
w2 = torch.randn(50,1) * sqrt(2 / 50)
b2 = torch.zeros(1)
Let’s look at the scale of our activations after going through the first linear layer and
ReLU:
l1 = lin(x, w1, b1)
l2 = relu(l1)
l2.mean(), l2.std()
(tensor(0.5661), tensor(0.8339))
Much better! Now that our weights are properly initialized, we can define our whole
model:
<b>def</b> model(x):
l1 = lin(x, w1, b1)
l2 = relu(l1)
l3 = lin(l2, w2, b2)
<b>return</b> l3
This is the forward pass. Now all that’s left to do is to compare our output to the
labels we have (random numbers, in this example) with a loss function. In this case,
we will use the mean squared error. (It’s a toy problem, and this is the easiest loss
function to use for what is next, computing the gradients.)
The only subtlety is that our outputs and targets don’t have exactly the same shape—
after going though the model, we get an output like this:
out = model(x)
out.shape
torch.Size([200, 1])"|defining and initializing a layer
"<header><largefont><b>Stochastic</b></largefont> <largefont><b>Gradient</b></largefont> <largefont><b>Descent</b></largefont></header>
Do you remember the way that Arthur Samuel described machine learning, which we
quoted in Chapter 1?
Suppose we arrange for some automatic means of testing the effectiveness of any cur‐
rent weight assignment in terms of actual performance and provide a mechanism for
altering the weight assignment so as to maximize the performance. We need not go
into the details of such a procedure to see that it could be made entirely automatic and
to see that a machine so programmed would “learn” from its experience.
As we discussed, this is the key to allowing us to have a model that can get better and
better—that can learn. But our pixel similarity approach does not really do this. We
do not have any kind of weight assignment, or any way of improving based on testing
the effectiveness of a weight assignment. In other words, we can’t really improve our
pixel similarity approach by modifying a set of parameters. To take advantage of the
power of deep learning, we will first have to represent our task in the way that Samuel
described it.
Instead of trying to find the similarity between an image and an “ideal image,” we
could instead look at each individual pixel and come up with a set of weights for each,
such that the highest weights are associated with those pixels most likely to be black
for a particular category. For instance, pixels toward the bottom right are not very
likely to be activated for a 7, so they should have a low weight for a 7, but they are
likely to be activated for an 8, so they should have a high weight for an 8. This can be
represented as a function and set of weight values for each possible category—for
instance, the probability of being the number 8:
def pr_eight(x,w) = (x*w).sum()
Here we are assuming that X is the image, represented as a vector—in other words,
with all of the rows stacked up end to end into a single long line. And we are assum‐
W.
ing that the weights are a vector If we have this function, we just need some way to
update the weights to make them a little bit better. With such an approach, we can
repeat that step a number of times, making the weights better and better, until they
are as good as we can make them.
W
We want to find the specific values for the vector that cause the result of our func‐
tion to be high for those images that are 8s, and low for those images that are not.
Searching for the best vector W is a way to search for the best function for recognizing
8s. (Because we are not yet using a deep neural network, we are limited by what our
function can do—we are going to fix that constraint later in this chapter.)"|stochastic gradient descent; stochastic gradient descent (SGD)
"The solution to this problem is to tell PyTorch that we do not want to backpropagate
the derivatives through the entire implicit neural network. Instead, we will keep just
the last three layers of gradients. To remove all of the gradient history in PyTorch, we
use the detach method.
Here is the new version of our RNN. It is now stateful, because it remembers its acti‐
vations between different calls to forward , which represent its use for different sam‐
ples in the batch:
<b>class</b> <b>LMModel3(Module):</b>
<b>def</b> <b>__init__(self,</b> vocab_sz, n_hidden):
self.i_h = nn.Embedding(vocab_sz, n_hidden)
self.h_h = nn.Linear(n_hidden, n_hidden)
self.h_o = nn.Linear(n_hidden,vocab_sz)
self.h = 0
<b>def</b> forward(self, x):
<b>for</b> i <b>in</b> range(3):
self.h = self.h + self.i_h(x[:,i])
self.h = F.relu(self.h_h(self.h))
out = self.h_o(self.h)
self.h = self.h.detach()
<b>return</b> out
<b>def</b> reset(self): self.h = 0
This model will have the same activations whatever sequence length we pick, because
the hidden state will remember the last activation from the previous batch. The only
thing that will be different is the gradients computed at each step: they will be calcula‐
ted on only sequence length tokens in the past, instead of the whole stream. This
approach is called <i>backpropagation</i> <i>through</i> <i>time</i> (BPTT).
<b>Jargon:BackpropagationThroughTime</b>
Treating a neural net with effectively one layer per time step (usu‐
ally refactored using a loop) as one big model, and calculating gra‐
dients on it in the usual way. To avoid running out of memory and
time, we usually use <i>truncated</i> BPTT, which “detaches” the history
of computation steps in the hidden state every few time steps.
To use LMModel3, we need to make sure the samples are going to be seen in a certain
order. As we saw in Chapter 10, if the first line of the first batch is our dset[0], the
dset[1]
second batch should have as the first line, so that the model sees the text
flowing.
LMDataLoader was doing this for us in Chapter 10. This time we’re going to do it
ourselves."|backpropagation through time (BPTT); truncated BPTT; backpropagation through time; improved RNN
"To avoid this, our first step was to split our dataset into two sets: the <i>training</i> <i>set</i>
(which our model sees in training) and the <i>validation</i> <i>set,</i> also known as the <i>develop‐</i>
<i>ment</i> <i>set</i> (which is used only for evaluation). This lets us test that the model learns
lessons from the training data that generalize to new data, the validation data.
One way to understand this situation is that, in a sense, we don’t want our model to
get good results by “cheating.” If it makes an accurate prediction for a data item, that
should be because it has learned characteristics of that kind of item, and not because
the model has been shaped by <i>actually</i> <i>having</i> <i>seen</i> <i>that</i> <i>particular</i> <i>item.</i>
Splitting off our validation data means our model never sees it in training and so is
completely untainted by it, and is not cheating in any way. Right?
In fact, not necessarily. The situation is more subtle. This is because in realistic sce‐
narios we rarely build a model just by training its parameters once. Instead, we are
likely to explore many versions of a model through various modeling choices regard‐
ing network architecture, learning rates, data augmentation strategies, and other fac‐
tors we will discuss in upcoming chapters. Many of these choices can be described as
choices of <i>hyperparameters.</i> The word reflects that they are parameters about parame‐
ters, since they are the higher-level choices that govern the meaning of the weight
parameters.
The problem is that even though the ordinary training process is looking at only pre‐
dictions on the training data when it learns values for the weight parameters, the
same is not true of us. We, as modelers, are evaluating the model by looking at pre‐
dictions on the validation data when we decide to explore new hyperparameter val‐
ues! So subsequent versions of the model are, indirectly, shaped by us having seen the
validation data. Just as the automatic training process is in danger of overfitting the
training data, we are in danger of overfitting the validation data through human trial
and error and exploration.
The solution to this conundrum is to introduce another level of even more highly
reserved data: the <i>test</i> <i>set.</i> Just as we hold back the validation data from the training
process, we must hold back the test set data even from ourselves. It cannot be used to
improve the model; it can be used only to evaluate the model at the very end of our
efforts. In effect, we define a hierarchy of cuts of our data, based on how fully we want
to hide it from training and modeling processes: training data is fully exposed, the
validation data is less exposed, and test data is totally hidden. This hierarchy parallels
the different kinds of modeling and evaluation processes themselves—the automatic
training process with backpropagation, the more manual process of trying different
hyperparameters between training sessions, and the assessment of our final result.
The test and validation sets should have enough data to ensure that you get a good
estimate of your accuracy. If you’re creating a cat detector, for instance, you generally
want at least 30 cats in your validation set. That means that if you have a dataset with"|validation set; validation set size; hyperparameters; overfitting; parameters; testing models
"<header><largefont><b>Matrix</b></largefont> <largefont><b>Multiplication</b></largefont> <largefont><b>from</b></largefont> <largefont><b>Scratch</b></largefont></header>
Let’s write a function that computes the matrix product of two tensors, before we
allow ourselves to use the PyTorch version of it. We will use only the indexing in
PyTorch tensors:
<b>import</b> <b>torch</b>
<b>from</b> <b>torch</b> <b>import</b> tensor
We’ll need three nested for loops: one for the row indices, one for the column indi‐
ces, and one for the inner sum. ac and ar stand for number of columns of a and
number of rows of a , respectively (the same convention is followed for b ), and we
make sure calculating the matrix product is possible by checking that a has as many
columns as b has rows:
<b>def</b> matmul(a,b):
ar,ac = a.shape <i>#</i> <i>n_rows</i> <i>*</i> <i>n_cols</i>
br,bc = b.shape
<b>assert</b> ac==br
c = torch.zeros(ar, bc)
<b>for</b> i <b>in</b> range(ar):
<b>for</b> j <b>in</b> range(bc):
<b>for</b> k <b>in</b> range(ac): c[i,j] += a[i,k] * b[k,j]
<b>return</b> c
To test this out, we’ll pretend (using random matrices) that we’re working with a
small batch of 5 MNIST images, flattened into 28*28 vectors, with a linear model to
turn them into 10 activations:
m1 = torch.randn(5,28*28)
m2 = torch.randn(784,10)
Let’s time our function, using the Jupyter “magic” command %time :
%time t1=matmul(m1, m2)
CPU times: user 1.15 s, sys: 4.09 ms, total: 1.15 s
Wall time: 1.15 s
And see how that compares to PyTorch’s built-in @ ?
%timeit -n 20 t2=m1@m2
14 µs ± 8.95 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)
As we can see, in Python three nested loops is a bad idea! Python is a slow language,
and this isn’t going to be efficient. We see here that PyTorch is around 100,000 times
faster than Python—and that’s before we even start using the GPU!
Where does this difference come from? PyTorch didn’t write its matrix multiplication
in Python, but rather in C++ to make it fast. In general, whenever we do computa‐
tions on tensors, we will need to <i>vectorize</i> them so that we can take advantage of the"|function from scratch; neural networks; building layer from scratch
"This command has given us a DataBlock object. This is like a <i>template</i> for creating a
DataLoaders. We still need to tell fastai the actual source of our data—in this case, the
path where the images can be found:
dls = bears.dataloaders(path)
A DataLoaders includes validation and training DataLoaders. A DataLoader is a class
that provides batches of a few items at a time to the GPU. We’ll be learning a lot more
about this class in the next chapter. When you loop through a DataLoader, fastai will
give you 64 (by default) items at a time, all stacked up into a single tensor. We can
take a look at a few of those items by calling the show_batch method on a
DataLoader :
dls.valid.show_batch(max_n=4, nrows=1)
By default, Resize <i>crops</i> the images to fit a square shape of the size requested, using
the full width or height. This can result in losing some important details. Alterna‐
tively, you can ask fastai to pad the images with zeros (black), or squish/stretch them:
bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish))
dls = bears.dataloaders(path)
dls.valid.show_batch(max_n=4, nrows=1)"|image classifier model; DataLoaders; Resize
"There’s no need to use fine_tune, so we’ll train with fit_one_cycle for a few epochs
and see how it looks:
learn.fit_one_cycle(5, 1e-2)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 0.069705 0.062389 00:11
1 0.056253 0.058489 00:11
2 0.048385 0.052256 00:11
3 0.043400 0.050743 00:11
4 0.040358 0.050986 00:11
We can use our r_mse function to compare the result to the random forest result we
got earlier:
preds,targs = learn.get_preds()
r_mse(preds,targs)
0.2258
It’s quite a bit better than the random forest (although it took longer to train, and it’s
fussier about hyperparameter tuning).
Before we move on, let’s save our model in case we want to come back to it again
later:
learn.save('nn')"|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"<b>JeremySays</b>
At university, philosophy of ethics was my main thing (it would
have been the topic of my thesis, if I’d finished it, instead of drop‐
ping out to join the real world). Based on the years I spent studying
ethics, I can tell you this: no one really agrees on what right and
wrong are, whether they exist, how to spot them, which people are
good and which bad, or pretty much anything else. So don’t expect
too much from the theory! We’re going to focus on examples and
thought starters here, not theory.
In answering the question “What Is Ethics?” the Markkula Center for Applied Ethics
says that the term refers to the following:
• Well-founded standards of right and wrong that prescribe what humans should
do
• The study and development of one’s ethical standards
There is no list of right answers. There is no list of dos and don’ts. Ethics is compli‐
cated and context-dependent. It involves the perspectives of many stakeholders. Eth‐
ics is a muscle that you have to develop and practice. In this chapter, our goal is to
provide some signposts to help you on that journey.
Spotting ethical issues is best to do as part of a collaborative team. This is the only
way you can really incorporate different perspectives. Different people’s backgrounds
will help them to see things that may not be obvious to you. Working with a team is
helpful for many “muscle-building” activities, including this one.
This chapter is certainly not the only part of the book where we talk about data ethics,
but it’s good to have a place where we focus on it for a while. To get oriented, it’s per‐
haps easiest to look at a few examples. So, we picked out three that we think illustrate
effectively some of the key topics.
<header><largefont><b>Key</b></largefont> <largefont><b>Examples</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Data</b></largefont> <largefont><b>Ethics</b></largefont></header>
We are going to start with three specific examples that illustrate three common ethi‐
cal issues in tech (we’ll study these issues in more depth later in the chapter):
<i>Recourse</i> <i>processes</i>
Arkansas’s buggy healthcare algorithms left patients stranded.
<i>Feedback</i> <i>loops</i>
YouTube’s recommendation system helped unleash a conspiracy theory boom."|ethics; web resources
"etc. On the right is a map of Germany. The actual physical locations of the German
states were not part of the provided data, yet the model itself learned where they must
be, based only on the behavior of store sales!
Do you remember how we talked about <i>distance</i> between embeddings? The authors
of the paper plotted the distance between store embeddings against the actual geo‐
graphic distance between the stores (see Figure 9-3). They found that they matched
very closely!
<i>Figure</i> <i>9-3.</i> <i>Store</i> <i>distances</i> <i>(courtesy</i> <i>of</i> <i>Cheng</i> <i>Guo</i> <i>and</i> <i>Felix</i> <i>Berkhahn)</i>
We’ve even tried plotting the embeddings for days of the week and months of the
year, and found that days and months that are near each other on the calendar ended
up close as embeddings too, as shown in Figure 9-4.
What stands out in these two examples is that we provide the model fundamentally
categorical data about discrete entities (e.g., German states or days of the week), and
then the model learns an embedding for these entities that defines a continuous
notion of distance between them. Because the embedding distance was learned based
on real patterns in the data, that distance tends to match up with our intuitions."|categorical variables; collaborative filtering; dates on calendar and; geographic distance matching; embedding distance and store distance
"it is an entirely random model! All of the layers prior to the last one have been care‐
fully trained to be good at image classification tasks in general. As we saw in the
images from the Zeiler and Fergus paper in Chapter 1 (see Figures 1-10 through
1-13), the first few layers encode general concepts, such as finding gradients and
edges, and later layers encode concepts that are still useful for us, such as finding eye‐
balls and fur.
We want to train a model in such a way that we allow it to remember all of these gen‐
erally useful ideas from the pretrained model, use them to solve our particular task
(classify pet breeds), and adjust them only as required for the specifics of our particu‐
lar task.
Our challenge when fine-tuning is to replace the random weights in our added linear
layers with weights that correctly achieve our desired task (classifying pet breeds)
without breaking the carefully pretrained weights and the other layers. A simple trick
can allow this to happen: tell the optimizer to update the weights in only those ran‐
domly added final layers. Don’t change the weights in the rest of the neural network
at all. This is called <i>freezing</i> those pretrained layers.
When we create a model from a pretrained network, fastai automatically freezes all of
the pretrained layers for us. When we call the fine_tune method, fastai does two
things:
• Trains the randomly added layers for one epoch, with all other layers frozen
• Unfreezes all the layers, and trains them for the number of epochs requested
Although this is a reasonable default approach, it is likely that for your particular
dataset, you may get better results by doing things slightly differently. The fine_tune
method has parameters you can use to change its behavior, but it might be easiest for
you to just call the underlying methods directly if you want to get custom behavior.
Remember that you can see the source code for the method by using the following
syntax:
learn.fine_tune??
So let’s try doing this manually ourselves. First of all, we will train the randomly
added layers for three epochs, using fit_one_cycle. As mentioned in Chapter 1,
fit_one_cycle is the suggested way to train models without using fine_tune. We’ll
see why later in the book; in short, what fit_one_cycle does is to start training at a
low learning rate, gradually increase it for the first section of training, and then grad‐
ually decrease it again for the last section of training:
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fit_one_cycle(3, 3e-3)"|Fergus; fine-tuning models; layers; visualizing neural network weights; Zeiler
"<b>def</b> zero_grad(self, *args, **kwargs):
<b>for</b> p <b>in</b> self.params: p.grad = None
We can create our optimizer by passing in the model’s parameters:
opt = BasicOptim(linear_model.parameters(), lr)
Our training loop can now be simplified:
<b>def</b> train_epoch(model):
<b>for</b> xb,yb <b>in</b> dl:
calc_grad(xb, yb, model)
opt.step()
opt.zero_grad()
Our validation function doesn’t need to change at all:
validate_epoch(linear_model)
0.4157
Let’s put our little training loop in a function, to make things simpler:
<b>def</b> train_model(model, epochs):
<b>for</b> i <b>in</b> range(epochs):
train_epoch(model)
<b>print(validate_epoch(model),</b> end=' ')
The results are the same as in the previous section:
train_model(linear_model, 20)
0.4932 0.8618 0.8203 0.9102 0.9331 0.9468 0.9555 0.9629 0.9658 0.9673 0.9687
> 0.9707 0.9726 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785
fastai provides the SGD class that, by default, does the same thing as our BasicOptim:
linear_model = nn.Linear(28*28,1)
opt = SGD(linear_model.parameters(), lr)
train_model(linear_model, 20)
0.4932 0.852 0.8335 0.9116 0.9326 0.9473 0.9555 0.9624 0.9648 0.9668 0.9692
> 0.9712 0.9731 0.9746 0.9761 0.9765 0.9775 0.978 0.9785 0.9785
fastai also provides Learner.fit, which we can use instead of train_model. To create
a Learner , we first need to create a DataLoaders , by passing in our training and vali‐
dation DataLoaders:
dls = DataLoaders(dl, valid_dl)
To create a Learner without using an application (such as cnn_learner), we need to
pass in all the elements that we’ve created in this chapter: the DataLoaders , the
model, the optimization function (which will be passed the parameters), the loss
function, and optionally any metrics to print:"|Learner; numerical digit classifier; PyTorch; creating an optimizer; stochastic gradient descent (SGD)
"<i>Figure</i> <i>1-2.</i> <i>Initial</i> <i>view</i> <i>of</i> <i>Jupyter</i> <i>Notebook</i>
You are now ready to run your first Jupyter notebook!
<b>Jargon:JupyterNotebook</b>
A piece of software that allows you to include formatted text, code,
images, videos, and much more, all within a single interactive
document. Jupyter received the highest honor for software, the
ACM Software System Award, thanks to its wide use and enormous
impact in many academic fields and in industry. Jupyter Notebook
is the software most widely used by data scientists for developing
and interacting with deep learning models.
<header><largefont><b>Running</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>First</b></largefont> <largefont><b>Notebook</b></largefont></header>
The notebooks are numbered by chapter in the same order as they are presented in
this book. So, the very first notebook you will see listed is the notebook that you need
to use now. You will be using this notebook to train a model that can recognize dog
and cat photos. To do this, you’ll be downloading a dataset of dog and cat photos, and
using that to <i>train</i> <i>a</i> <i>model.</i>
A <i>dataset</i> is simply a bunch of data—it could be images, emails, financial indicators,
sounds, or anything else. There are many datasets made freely available that are suit‐
able for training models. Many of these datasets are created by academics to help
advance research, many are made available for competitions (there are competitions
where data scientists can compete to see who has the most accurate model!), and
some are byproducts of other processes (such as financial filings)."|beginning; Jupyter Notebook; cats and dogs first model; definition; download first model; freely available; dogs and cats first model; first model; notebooks; training
"they often (as here) beat previously state-of-the-art results. These really are the right
ways to think about these problem domains.
<header><largefont><b>Jargon</b></largefont> <largefont><b>Recap</b></largefont></header>
We just covered a lot of information, so let’s recap briefly. Table 1-3 provides a handy
vocabulary list.
<i>Table</i> <i>1-3.</i> <i>Deep</i> <i>learning</i> <i>vocabulary</i>
<b>Term</b> <b>Meaning</b>
Label Thedatathatwe’retryingtopredict,suchas“dog”or“cat”
Architecture Thetemplateofthemodelthatwe’retryingtofit;i.e.,theactualmathematicalfunctionthatwe’repassing
theinputdataandparametersto
Model Thecombinationofthearchitecturewithaparticularsetofparameters
Parameters Thevaluesinthemodelthatchangewhattaskitcandoandthatareupdatedthroughmodeltraining
Fit Updatetheparametersofthemodelsuchthatthepredictionsofthemodelusingtheinputdatamatchthe
targetlabels
Train Asynonymforfit
Pretrained Amodelthathasalreadybeentrained,generallyusingalargedataset,andwillbefine-tuned
model
Fine-tune Updateapretrainedmodelforadifferenttask
Epoch Onecompletepassthroughtheinputdata
Loss Ameasureofhowgoodthemodelis,chosentodrivetrainingviaSGD
Metric Ameasurementofhowgoodthemodelisusingthevalidationset,chosenforhumanconsumption
Validationset Asetofdataheldoutfromtraining,usedonlyformeasuringhowgoodthemodelis
Trainingset Thedatausedforfittingthemodel;doesnotincludeanydatafromthevalidationset
Overfitting Trainingamodelinsuchawaythatitremembersspecificfeaturesoftheinputdata,ratherthan
generalizingwelltodatanotseenduringtraining
CNN Convolutionalneuralnetwork;atypeofneuralnetworkthatworksparticularlywellforcomputervision
tasks
With this vocabulary in hand, we are now in a position to bring together all the key
concepts introduced so far. Take a moment to review those definitions and read the
following summary. If you can follow the explanation, you’re well equipped to under‐
stand the discussions to come.
<i>Machine</i> <i>learning</i> is a discipline in which we define a program not by writing it
entirely ourselves, but by learning from data. <i>Deep</i> <i>learning</i> is a specialty within
machine learning that uses <i>neural</i> <i>networks</i> with multiple <i>layers.</i> <i>Image</i> <i>classification</i> is
a representative example (also known as <i>image</i> <i>recognition).</i> We start with <i>labeled</i> <i>data</i>
—a set of images for which we have assigned a <i>label</i> to each image, indicating what it"|validation set; architecture of model; non-image tasks; convolutional neural network (CNN); deep learning; as machine learning; epochs; fine-tuning models; fitting models; labels; loss; machine learning (ML); metrics; models; deep learning using; overfitting; parameters; performance of model as loss; pretrained models; terminology for deep learning; training
"longer to train and are more prone to overfitting (i.e., you can’t train them for as
many epochs before the accuracy on the validation set starts getting worse). On the
other hand, when using more data, they can be quite a bit more accurate.
What is a metric? A <i>metric</i> is a function that measures the quality of the model’s pre‐
dictions using the validation set, and will be printed at the end of each epoch. In this
case, we’re using error_rate, which is a function provided by fastai that does just
what it says: tells you what percentage of images in the validation set are being classi‐
fied incorrectly. Another common metric for classification is accuracy (which is just
1.0 - error_rate ). fastai provides many more, which will be discussed throughout
this book.
The concept of a metric may remind you of <i>loss,</i> but there is an important distinction.
The entire purpose of loss is to define a “measure of performance” that the training
system can use to update weights automatically. In other words, a good choice for loss
is a choice that is easy for stochastic gradient descent to use. But a metric is defined
for human consumption, so a good metric is one that is easy for you to understand,
and that hews as closely as possible to what you want the model to do. At times, you
might decide that the loss function is a suitable metric, but that is not necessarily the
case.
cnn_learner also has a parameter pretrained, which defaults to True (so it’s used in
this case, even though we haven’t specified it), which sets the weights in your model
to values that have already been trained by experts to recognize a thousand different
categories across 1.3 million photos (using the famous <i>ImageNet</i> dataset). A model
that has weights that have already been trained on another dataset is called a <i>pre‐</i>
<i>trained</i> <i>model.</i> You should nearly always use a pretrained model, because it means
that your model, before you’ve even shown it any of your data, is already very capa‐
ble. And as you’ll see, in a deep learning model, many of these capabilities are things
you’ll need, almost regardless of the details of your project. For instance, parts of pre‐
trained models will handle edge, gradient, and color detection, which are needed for
many tasks.
When using a pretrained model, cnn_learner will remove the last layer, since that is
always specifically customized to the original training task (i.e., ImageNet dataset
classification), and replace it with one or more new layers with randomized weights,
of an appropriate size for the dataset you are working with. This last part of the
model is known as the <i>head.</i>
Using pretrained models is the <i>most</i> important method we have to allow us to train
more accurate models, more quickly, with less data and less time and money. You
might think that would mean that using pretrained models would be the most studied
area in academic deep learning…but you’d be very, very wrong! The importance of
pretrained models is generally not recognized or discussed in most courses, books, or
software library features, and is rarely considered in academic papers. As we write"|validation set; beginning; pretrained model accuracy; pretrained model weight values; convolutional neural network (CNN); last layer and; error rate; fastai software library; head of model; pretrained models and; last layer and pretrained models; loss; metrics; first model declaration; head and pretrained models; notebooks; metric measuring quality; pretrained models; convolutional neural network parameter; training; weights
"One important piece of this DataBlock call that we haven’t seen before is in these two
lines:
item_tfms=Resize(460),
batch_tfms=aug_transforms(size=224, min_scale=0.75)
These lines implement a fastai data augmentation strategy that we call <i>presizing.</i> Pre‐
sizing is a particular way to do image augmentation that is designed to minimize data
destruction while maintaining good performance.
<header><largefont><b>Presizing</b></largefont></header>
We need our images to have the same dimensions, so that they can collate into ten‐
sors to be passed to the GPU. We also want to minimize the number of distinct aug‐
mentation computations we perform. The performance requirement suggests that we
should, where possible, compose our augmentation transforms into fewer transforms
(to reduce the number of computations and the number of lossy operations) and
transform the images into uniform sizes (for more efficient processing on the GPU).
The challenge is that, if performed after resizing down to the augmented size, various
common data augmentation transforms might introduce spurious empty zones,
degrade data, or both. For instance, rotating an image by 45 degrees fills corner
regions of the new bounds with emptiness, which will not teach the model anything.
Many rotation and zooming operations will require interpolating to create pixels.
These interpolated pixels are derived from the original image data but are still of
lower quality.
To work around these challenges, presizing adopts two strategies that are shown in
Figure 5-1:
1. Resize images to relatively “large” dimensions—that is, dimensions significantly
larger than the target training dimensions.
2. Compose all of the common augmentation operations (including a resize to the
final target size) into one, and perform the combined operation on the GPU only
once at the end of processing, rather than performing the operations individually
and interpolating multiple times.
The first step, the resize, creates images large enough that they have spare margin to
allow further augmentation transforms on their inner regions without creating empty
zones. This transformation works by resizing to a square, using a large crop size. On
the training set, the crop area is chosen randomly, and the size of the crop is selected
to cover the entire width or height of the image, whichever is smaller. In the second
step, the GPU is used for all data augmentation, and all of the potentially destructive
operations are done together, with a single interpolation at the end."|data augmentation; process end-to-end; image sizes same; presizing; training; Transforms
"speed up again as it went downhill. You want to build a model of how the speed
changes over time. If you were measuring the speed manually every second for 20
seconds, it might look something like this:
time = torch.arange(0,20).float(); time
tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13.,
> 14., 15., 16., 17., 18., 19.])
speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1
plt.scatter(time,speed);
We’ve added a bit of random noise, since measuring things manually isn’t precise.
This means it’s not that easy to answer the question: what was the roller coaster’s
speed? Using SGD, we can try to find a function that matches our observations. We
can’t consider every possible function, so let’s use a guess that it will be quadratic;
a*(time**2)+(b*time)+c.
i.e., a function of the form
We want to distinguish clearly between the function’s input (the time when we are
measuring the coaster’s speed) and its parameters (the values that define <i>which</i> quad‐
ratic we’re trying). So let’s collect the parameters in one argument and thus separate
t, params,
the input, and the parameters, in the function’s signature:
<b>def</b> f(t, params):
a,b,c = params
<b>return</b> a*(t**2) + (b*t) + c
In other words, we’ve restricted the problem of finding the best imaginable function
that fits the data to finding the best <i>quadratic</i> function. This greatly simplifies the
a, b,
problem, since every quadratic function is fully defined by the three parameters
and c . Thus, to find the best quadratic function, we need to find only the best values
for a, b, and c.
If we can solve this problem for the three parameters of a quadratic function, we’ll be
able to apply the same approach for other, more complex functions with more"|signature of function; stochastic gradient descent (SGD); training; weights
"<header><largefont><b>A</b></largefont> <largefont><b>Note</b></largefont> <largefont><b>About</b></largefont> <largefont><b>Twitter</b></largefont></header>
We are not, to say the least, big users of social networks in general. But our goal in
writing this book is to help you become the best deep learning practitioner you can,
and we would be remiss not to mention how important Twitter has been in our own
deep learning journeys.
You see, there’s another part of Twitter, far away from Donald Trump and the Karda‐
shians, where deep learning researchers and practitioners talk shop every day. As we
were writing this section, Jeremy wanted to double-check that what we were saying
about stride-2 convolutions was accurate, so he asked on Twitter:
A few minutes later, this answer popped up:
Christian Szegedy is the first author of Inception, the 2014 ImageNet winner, and
source of many key insights used in modern neural networks. Two hours later, this
appeared:"|building a CNN; deep learning; machine learning (ML); Twitter for deep learning help
"Let’s add a batchnorm layer to conv :
<b>def</b> conv(ni, nf, ks=3, act=True):
layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]
layers.append(nn.BatchNorm2d(nf))
<b>if</b> act: layers.append(nn.ReLU())
<b>return</b> nn.Sequential(*layers)
and fit our model:
learn = fit()
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.130036 0.055021 0.986400 00:10
That’s a great result! Let’s take a look at color_dim:
learn.activation_stats.color_dim(-4)
This is just what we hope to see: a smooth development of activations, with no
“crashes.” Batchnorm has really delivered on its promise here! In fact, batchnorm has
been so successful that we see it (or something very similar) in nearly all modern
neural networks.
An interesting observation about models containing batch normalization layers is
that they tend to generalize better than models that don’t contain them. Although we
haven’t as yet seen a rigorous analysis of what’s going on here, most researchers
believe that the reason is that batch normalization adds some extra randomness to the
training process. Each mini-batch will have a somewhat different mean and standard
deviation than other mini-batches. Therefore, the activations will be normalized by
different values each time. In order for the model to make accurate predictions, it will
have to learn to become robust to these variations. In general, adding additional ran‐
domization to the training process often helps.
Since things are going so well, let’s train for a few more epochs and see how it goes. In
fact, let’s <i>increase</i> the learning rate, since the abstract of the batchnorm paper claimed
we should be able to “train at much higher learning rates”:"|convolutional neural network (CNN); building a CNN; training more stable; training on all digits
"For this chapter, we are going to work on this movie recommendation problem. We’ll
start by getting some data suitable for a collaborative filtering model.
<header><largefont><b>A</b></largefont> <largefont><b>First</b></largefont> <largefont><b>Look</b></largefont> <largefont><b>at</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Data</b></largefont></header>
We do not have access to Netflix’s entire dataset of movie watching history, but there
is a great dataset that we can use, called MovieLens. This dataset contains tens of mil‐
lions of movie rankings (a combination of a movie ID, a user ID, and a numeric rat‐
ing), although we will just use a subset of 100,000 of them for our example. If you’re
interested, it would be a great learning project to try to replicate this approach on the
full 25-million recommendation dataset, which you can get from their website.
The dataset is available through the usual fastai function:
<b>from</b> <b>fastai.collab</b> <b>import</b> *
<b>from</b> <b>fastai.tabular.all</b> <b>import</b> *
path = untar_data(URLs.ML_100k)
According to the <i>README,</i> the main table is in the file <i>u.data.</i> It is tab-separated and
the columns are, respectively, user, movie, rating, and timestamp. Since those names
are not encoded, we need to indicate them when reading the file with Pandas. Here is
a way to open this table and take a look:
ratings = pd.read_csv(path/'u.data', delimiter='\t', header=None,
names=['user','movie','rating','timestamp'])
ratings.head()
<b>user</b> <b>movie</b> <b>rating</b> <b>timestamp</b>
<b>0</b> 196 242 3 881250949
<b>1</b> 186 302 3 891717742
<b>2</b> 22 377 1 878887116
<b>3</b> 244 51 2 880606923
<b>4</b>
166 346 1 886397596
Although this has all the information we need, it is not a particularly helpful way for
humans to look at this data. Figure 8-1 shows the same data cross-tabulated into a
human-friendly table."|collaborative filtering; MovieLens; MovieLens dataset
"<i>Figure</i> <i>3-5.</i> <i>Coverage</i> <i>of</i> <i>the</i> <i>Mueller</i> <i>report</i>
Russia Today’s coverage of the Mueller report was an extreme outlier in terms of how
many channels were recommending it. This suggests the possibility that Russia
Today, a state-owned Russia media outlet, has been successful in gaming YouTube’s
recommendation algorithm. Unfortunately, the lack of transparency of systems like
this makes it hard to uncover the kinds of problems that we’re discussing.
One of our reviewers for this book, Aurélien Géron, led YouTube’s video classification
team from 2013 to 2016 (well before the events discussed here). He pointed out that
it’s not just feedback loops involving humans that are a problem. There can also be
feedback loops without humans! He told us about an example from YouTube:
One important signal to classify the main topic of a video is the channel it comes from.
For example, a video uploaded to a cooking channel is very likely to be a cooking
video. But how do we know what topic a channel is about? Well…in part by looking at
the topics of the videos it contains! Do you see the loop? For example, many videos
have a description which indicates what camera was used to shoot the video. As a
result, some of these videos might get classified as videos about “photography.” If a
channel has such a misclassified video, it might be classified as a “photography” chan‐
nel, making it even more likely for future videos on this channel to be wrongly classi‐
fied as “photography.” This could even lead to runaway virus-like classifications! One
way to break this feedback loop is to classify videos with and without the channel sig‐
nal. Then when classifying the channels, you can only use the classes obtained without
the channel signal. This way, the feedback loop is broken."|Géron; Mueller report; Russia Today and Mueller report; Russia Today possibly gaming
"timating the constraints might mean that you fail to consider and react to important
issues.
The best thing to do is to keep an open mind. If you remain open to the possibility
that deep learning might solve part of your problem with less data or complexity than
you expect, you can design a process through which you can find the specific capabil‐
ities and constraints related to your particular problem. This doesn’t mean making
any risky bets—we will show you how you can gradually roll out models so that they
don’t create significant risks, and can even backtest them prior to putting them in
production.
<header><largefont><b>Starting</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Project</b></largefont></header>
So where should you start your deep learning journey? The most important thing is
to ensure that you have a project to work on—it is only through working on your
own projects that you will get real experience building and using models. When
selecting a project, the most important consideration is data availability.
Regardless of whether you are doing a project just for your own learning or for prac‐
tical application in your organization, you want to be able to start quickly. We have
seen many students, researchers, and industry practitioners waste months or years
while they attempt to find their perfect dataset. The goal is not to find the “perfect”
dataset or project, but just to get started and iterate from there. If you take this
approach, you will be on your third iteration of learning and improving while the
perfectionists are still in the planning stages!
We also suggest that you iterate from end to end in your project; don’t spend months
fine-tuning your model, or polishing the perfect GUI, or labeling the perfect dataset.
…Instead, complete every step as well as you can in a reasonable amount of time, all
the way to the end. For instance, if your final goal is an application that runs on a
mobile phone, that should be what you have after each iteration. But perhaps in the
early iterations you take shortcuts; for instance, by doing all of the processing on a
remote server and using a simple responsive web application. By completing the
project end to end, you will see where the trickiest bits are, and which bits make the
biggest difference to the final result.
As you work through this book, we suggest that you complete lots of small experi‐
ments, by running and adjusting the notebooks we provide, at the same time that you
gradually develop your own projects. That way, you will be getting experience with all
of the tools and techniques that we’re explaining as we discuss them."|experiments lead to projects; data availability; process end-to-end; iterate development end to end; iterate end to end
"At the end of 2015, the Rossmann sales competition ran on Kaggle. Competitors were
given a wide range of information about various stores in Germany, and were tasked
with trying to predict sales on a number of days. The goal was to help the company
manage stock properly and be able to satisfy demand without holding unnecessary
inventory. The official training set provided a lot of information about the stores. It
was also permitted for competitors to use additional data, as long as that data was
made public and available to all participants.
One of the gold medalists used deep learning, in one of the earliest known examples
of a state-of-the-art deep learning tabular model. Their method involved far less fea‐
ture engineering, based on domain knowledge, than those of the other gold medalists.
The paper “Entity Embeddings of Categorical Variables” describes their approach. In
an online-only chapter on the book’s website, we show how to replicate it from
scratch and attain the same accuracy shown in the paper. In the abstract of the paper,
the authors (Cheng Guo and Felix Bekhahn) say:
Entity embedding not only reduces memory usage and speeds up neural networks
compared with one-hot encoding, but more importantly by mapping similar values
close to each other in the embedding space it reveals the intrinsic properties of the cat‐
egorical variables…[It] is especially useful for datasets with lots of high cardinality fea‐
tures, where other methods tend to overfit…As entity embedding defines a distance
measure for categorical variables, it can be used for visualizing categorical data and for
data clustering.
We have already noticed all of these points when we built our collaborative filtering
model. We can clearly see that these insights go far beyond just collaborative filtering,
however.
The paper also points out that (as we discussed in the preceding chapter) an embed‐
ding layer is exactly equivalent to placing an ordinary linear layer after every one-hot-
encoded input layer. The authors used the diagram in Figure 9-1 to show this
equivalence. Note that “dense layer” is a term with the same meaning as “linear layer,”
and the one-hot encoding layers represent inputs.
The insight is important because we already know how to train linear layers, so this
shows that from the point of view of the architecture and our training algorithm, the
embedding layer is just another layer. We also saw this in practice in the preceding
chapter, when we built a collaborative filtering neural network that looks exactly like
this diagram.
Just as we analyzed the embedding weights for movie reviews, the authors of the
entity embeddings paper analyzed the embedding weights for their sales prediction
model. What they found was quite amazing, and illustrates their second key insight:
the embedding transforms the categorical variables into inputs that are both continu‐
ous and meaningful."|Berkhahn; entity embedding and; categorical variables; continuous variables from; embedding transforming categorical into; predicting sales from stores; categorical variables transformed into continuous; entity embedding; Guo; predicting sales from stores competition; entity embedding reducing; entity embedding contrasted; sales from stores; tabular data for models; variables; predicting sales from stores paper
"<i>Figure</i> <i>12-9.</i> <i>Architecture</i> <i>of</i> <i>an</i> <i>LSTM</i>
In this picture, our input <i>x</i> enters on the left with the previous hidden state (h )
<i>t</i> <i>t−1</i>
and cell state (c ). The four orange boxes represent four layers (our neural nets),
<i>t−1</i>
with the activation being either sigmoid (σ) or tanh. tanh is just a sigmoid function
rescaled to the range –1 to 1. Its mathematical expression can be written like this:
<i>x</i> −x
<i>e</i> + <i>e</i>
tanh <i>x</i> = = 2σ 2x − 1
<i>x</i> −x
<i>e</i> − <i>e</i>
where <i>σ</i> is the sigmoid function. The green circles in the figure are elementwise oper‐
ations. What goes out on the right is the new hidden state (h ) and new cell state (c ),
<i>t</i> <i>t</i>
ready for our next input. The new hidden state is also used as output, which is why
the arrow splits to go up.
Let’s go over the four neural nets (called <i>gates)</i> one by one and explain the diagram—
but before this, notice how very little the cell state (at the top) is changed. It doesn’t
even go directly through a neural net! This is exactly why it will carry on a longer-
term state.
First, the arrows for input and old hidden state are joined together. In the RNN we
wrote earlier in this chapter, we were adding them together. In the LSTM, we stack
them in one big tensor. This means the dimension of our embeddings (which is the
dimension of <i>x</i> ) can be different from the dimension of our hidden state. If we call
<i>t</i>
those n_in and n_hid, the arrow at the bottom is of size n_in + n_hid; thus all the
neural nets (orange boxes) are linear layers with n_in + n_hid inputs and n_hid
outputs."|building from scratch; LSTM language model
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 2.309385 2.302744 0.113500 00:08
Let’s see what the penultimate layer looks like:
learn.activation_stats.plot_layer_stats(-2)
Again, we’ve got most of our activations near zero. Let’s see what else we can do to
improve training stability.
<header><largefont><b>1cycle</b></largefont> <largefont><b>Training</b></largefont></header>
Our initial weights are not well suited to the task we’re trying to solve. Therefore, it is
dangerous to begin training with a high learning rate: we may very well make the
training diverge instantly, as we’ve seen. We probably don’t want to end training with
a high learning rate either, so that we don’t skip over a minimum. But we want to
train at a high learning rate for the rest of the training period, because we’ll be able to
train more quickly that way. Therefore, we should change the learning rate during
training, from low, to high, and then back to low again.
Leslie Smith (yes, the same guy who invented the learning rate finder!) developed this
idea in his article “Super-Convergence: Very Fast Training of Neural Networks Using
Large Learning Rates”. He designed a schedule for learning rate separated into two
phases: one where the learning rate grows from the minimum value to the maximum
value (warmup), and one where it decreases back to the minimum value (annealing).
Smith called this combination of approaches <i>1cycle</i> <i>training.</i>
1cycle training allows us to use a much higher maximum learning rate than other
types of training, which gives two benefits:
• By training with higher learning rates, we train faster—a phenomenon Smith
calls <i>super-convergence.</i>"|annealing learning rate; convolutional neural network (CNN); 1cycle training; building a CNN; changing during training; training with large learning rates; training more stable; training on all digits; Smith; neural networks and learning rate; warmup learning rate
"and we often use it even when there are just two categories, just to make things a bit
more consistent. We could create other functions that have the properties that all acti‐
vations are between 0 and 1, and sum to 1; however, no other function has the same
relationship to the sigmoid function, which we’ve seen is smooth and symmetric.
Also, we’ll see shortly that the softmax function works well hand in hand with the loss
function we will look at in the next section.
If we have three output activations, such as in our bear classifier, calculating softmax
for a single bear image would then look like something like Figure 5-3.
<i>Figure</i> <i>5-3.</i> <i>Example</i> <i>of</i> <i>softmax</i> <i>on</i> <i>the</i> <i>bear</i> <i>classifier</i>
What does this function do in practice? Taking the exponential ensures all our num‐
bers are positive, and then dividing by the sum ensures we are going to have a bunch
of numbers that add up to 1. The exponential also has a nice property: if one of the
numbers in our activations x is slightly bigger than the others, the exponential will
amplify this (since it grows, well…exponentially), which means that in the softmax,
that number will be closer to 1.
Intuitively, the softmax function <i>really</i> wants to pick one class among the others, so
it’s ideal for training a classifier when we know each picture has a definite label. (Note
that it may be less ideal during inference, as you might want your model to some‐
times tell you it doesn’t recognize any of the classes that it has seen during training,
and not pick a class because it has a slightly bigger activation score. In this case, it
might be better to train a model using multiple binary output columns, each using a
sigmoid activation.)
Softmax is the first part of the cross-entropy loss—the second part is log likelihood.
<header><largefont><b>Log</b></largefont> <largefont><b>Likelihood</b></largefont></header>
When we calculated the loss for our MNIST example in the preceding chapter, we
used this:
<b>def</b> mnist_loss(inputs, targets):
inputs = inputs.sigmoid()
<b>return</b> torch.where(targets==1, 1-inputs, inputs).mean()"|cross-entropy loss; loss; MNIST loss function; pet breeds image classifier; binary to multiple categories
"As you see, we had to tell fastai which columns are <i>categorical</i> (contain values that are
one of a discrete set of choices, such as occupation) versus <i>continuous</i> (contain a
number that represents a quantity, such as age ).
There is no pretrained model available for this task (in general, pretrained models are
not widely available for any tabular modeling tasks, although some organizations
have created them for internal use), so we don’t use fine_tune in this case. Instead,
we use fit_one_cycle, the most commonly used method for training fastai models
<i>from</i> <i>scratch</i> (i.e., without transfer learning):
learn.fit_one_cycle(3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.359960 0.357917 0.831388 00:11
1 0.353458 0.349657 0.837991 00:10
2 0.338368 0.346997 0.843213 00:10
This model is using the <i>Adult</i> dataset from the paper “Scaling Up the Accuracy of
Naive-Bayes Classifiers: a Decision-Tree Hybrid” by Ron Kohavi, which contains
some demographic data about individuals (like their education, marital status, race,
sex and whether they have an annual income greater than $50k). The model is over
80% accurate and took around 30 seconds to train.
Let’s look at one more. Recommendation systems are important, particularly in
ecommerce. Companies like Amazon and Netflix try hard to recommend products or
movies that users might like. Here’s how to train a model that will predict movies
people might like based on their previous viewing habits, using the MovieLens
dataset:
<b>from</b> <b>fastai.collab</b> <b>import</b> *
path = untar_data(URLs.ML_SAMPLE)
dls = CollabDataLoaders.from_csv(path/'ratings.csv')
learn = collab_learner(dls, y_range=(0.5,5.5))
learn.fine_tune(10)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 1.554056 1.428071 00:01
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 1.393103 1.361342 00:01
1 1.297930 1.159169 00:00
2 1.052705 0.827934 00:01
3 0.810124 0.668735 00:01
4 0.711552 0.627836 00:01"|demographics; MovieLens; demographics dataset; Kohavi; movie recommendation system; MovieLens sample model; MovieLens dataset; tabular model rarity; movies based on viewing habits; pretrained model rarity; research papers
"It would be nice to not lose those two pixels on each axis. The way we do that is to
add <i>padding,</i> which is simply additional pixels added around the outside of our
image. Most commonly, pixels of zeros are added.
<header><largefont><b>Strides</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Padding</b></largefont></header>
With appropriate padding, we can ensure that the output activation map is the same
size as the original image, which can make things a lot simpler when we construct our
architectures. Figure 13-4 shows how adding padding allows us to apply the kernel in
the image corners.
<i>Figure</i> <i>13-4.</i> <i>A</i> <i>convolution</i> <i>with</i> <i>padding</i>
With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activa‐
tion map, as we can see in Figure 13-5.
<i>Figure</i> <i>13-5.</i> <i>A</i> <i>4×4</i> <i>kernel</i> <i>with</i> <i>5×5</i> <i>input</i> <i>and</i> <i>2</i> <i>pixels</i> <i>of</i> <i>padding</i> <i>(courtesy</i> <i>of</i> <i>Vincent</i>
<i>Dumoulin</i> <i>and</i> <i>Francesco</i> <i>Visin)</i>
If we add a kernel of size ks by ks (with ks an odd number), the necessary padding
on each side to keep the same shape is ks//2. An even number for ks would require a
different amount of padding on the top/bottom and left/right, but in practice we
almost never use an even filter size.
So far, when we have applied the kernel to the grid, we have moved it one pixel over
at a time. But we can jump further; for instance, we could move over two pixels after
each kernel application, as in Figure 13-6. This is known as a <i>stride-2</i> convolution.
The most common kernel size in practice is 3×3, and the most common padding is 1.
As you’ll see, stride-2 convolutions are useful for decreasing the size of our outputs,"|convolutional neural network (CNN); padding a convolution; stride-2 convolutions
"This makes sense, since when coordinates are used as the dependent variable, most of
the time we’re likely to be trying to predict something as close as possible; that’s basi‐
cally what MSELoss (mean squared error loss) does. If you want to use a different loss
function, you can pass it to cnn_learner by using the loss_func parameter.
Note also that we didn’t specify any metrics. That’s because the MSE is already a use‐
ful metric for this task (although it’s probably more interpretable after we take the
square root).
We can pick a good learning rate with the learning rate finder:
learn.lr_find()
We’ll try an LR of 2e-2:
lr = 2e-2
learn.fit_one_cycle(5, lr)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 0.045840 0.012957 00:36
1 0.006369 0.001853 00:36
2 0.003000 0.000496 00:37
3 0.001963 0.000360 00:37
4 0.001584 0.000116 00:36
Generally, when we run this, we get a loss of around 0.0001, which corresponds to
this average coordinate prediction error:
math.sqrt(0.0001)
0.01
This sounds very accurate! But it’s important to take a look at our results with
Learner.show_results . The left side has the actual (ground <i>truth)</i> coordinates and
the right side has our model’s predictions:"|loss function parameter; Learner; MSELoss; passing to learner
"possible. Let’s see what the impact of removing one of these model columns has on
the random forest:
xs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)
valid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)
m2 = rf(xs_filt2, y_filt)
m_rmse(m, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)
(0.176706, 0.230642)
There’s minimal impact, so we will remove it as a predictor for our neural network:
cat_nn.remove('fiModelDescriptor')
We can create our TabularPandas object in the same way as when we created our
random forest, with one very important addition: normalization. A random forest
does not need any normalization—the tree building procedure cares only about the
order of values in a variable, not at all about how they are scaled. But as we have seen,
a neural network definitely does care about this. Therefore, we add the Normalize
processor when we build our TabularPandas object:
procs_nn = [Categorify, FillMissing, Normalize]
to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,
splits=splits, y_names=dep_var)
Tabular models and data don’t generally require much GPU RAM, so we can use
larger batch sizes:
dls = to_nn.dataloaders(1024)
As we’ve discussed, it’s a good idea to set y_range for regression models, so let’s find
the min and max of our dependent variable:
y = to_nn.train.y
y.min(),y.max()
(8.465899897028686, 11.863582336583399)
We can now create the Learner to create this tabular model. As usual, we use the
application-specific learner function, to take advantage of its application-customized
defaults. We set the loss function to MSE, since that’s what this competition uses.
By default, for tabular data fastai creates a neural network with two hidden layers,
with 200 and 100 activations, respectively. This works quite well for small datasets,
but here we’ve got quite a large dataset, so we increase the layer sizes to 500 and 250:
<b>from</b> <b>fastai.tabular.all</b> <b>import</b> *
learn = tabular_learner(dls, y_range=(8,12), layers=[500,250],
n_out=1, loss_func=F.mse_loss)
learn.lr_find()
(0.005754399299621582, 0.0002754228771664202)"|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"However, the underlying items are all numeric:
to.items.head(3)
<b>state</b> <b>ProductGroup</b> <b>Drive_System</b> <b>Enclosure</b>
<b>0</b> 1 6 0 3
<b>1</b> 33 6 0 3
<b>2</b> 32 3 0 6
The conversion of categorical columns to numbers is done by simply replacing each
unique level with a number. The numbers associated with the levels are chosen con‐
secutively as they are seen in a column, so there’s no particular meaning to the num‐
bers in categorical columns after conversion. The exception is if you first convert a
column to a Pandas ordered category (as we did for ProductSize earlier), in which
case the ordering you chose is used. We can see the mapping by looking at the
classes attribute:
to.classes['ProductSize']
(#7) ['#na#','Large','Large / Medium','Medium','Small','Mini','Compact']
Since it takes a minute or so to process the data to get to this point, we should save it
—that way, in the future, we can continue our work from here without rerunning the
previous steps. fastai provides a save method that uses Python’s <i>pickle</i> system to save
nearly any Python object:
(path/'to.pkl').save(to)
To read this back later, you would type this:
to = (path/'to.pkl').load()
Now that all this preprocessing is done, we are ready to create a decision tree.
<header><largefont><b>Creating</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Decision</b></largefont> <largefont><b>Tree</b></largefont></header>
To begin, we define our independent and dependent variables:
xs,y = to.train.xs,to.train.y
valid_xs,valid_y = to.valid.xs,to.valid.y
Now that our data is all numeric, and there are no missing values, we can create a
decision tree:
m = DecisionTreeRegressor(max_leaf_nodes=4)
m.fit(xs, y);
To keep it simple, we’ve told sklearn to create just four <i>leaf</i> <i>nodes.</i> To see what it’s
learned, we can display the tree:
draw_tree(m, xs, size=7, leaves_parallel=True, precision=2)"|tabular dataset prep; save method; decision trees; creating decision tree; pickle system for save method; tabular data for models; training
"This shows a classic picture of “bad training.” We start with nearly all activations at
zero—that’s what we see at the far left, with all the dark blue. The bright yellow at the
bottom represents the near-zero activations. Then, over the first few batches, we see
the number of nonzero activations exponentially increasing. But it goes too far and
collapses! We see the dark blue return, and the bottom becomes bright yellow again.
It almost looks like training restarts from scratch. Then we see the activations
increase again and collapse again. After repeating this a few times, eventually we see a
spread of activations throughout the range.
It’s much better if training can be smooth from the start. The cycles of exponential
increase and then collapse tend to result in a lot of near-zero activations, resulting in
slow training and poor final results. One way to solve this problem is to use batch
normalization.
<header><largefont><b>Batch</b></largefont> <largefont><b>Normalization</b></largefont></header>
To fix the slow training and poor final results we ended up with in the previous sec‐
tion, we need to fix the initial large percentage of near-zero activations, and then try
to maintain a good distribution of activations throughout training.
Sergey Ioffe and Christian Szegedy presented a solution to this problem in the 2015
paper “Batch Normalization: Accelerating Deep Network Training by Reducing Inter‐
nal Covariate Shift”. In the abstract, they describe just the problem that we’ve seen:
Training Deep Neural Networks is complicated by the fact that the distribution of each
layer’s inputs changes during training, as the parameters of the previous layers change.
This slows down the training by requiring lower learning rates and careful parameter
initialization…We refer to this phenomenon as internal covariate shift, and address the
problem by normalizing layer inputs.
Their solution, they say is as follows:
Making normalization a part of the model architecture and performing the normaliza‐
tion for each training mini-batch. Batch Normalization allows us to use much higher
learning rates and be less careful about initialization."|batch normalization; convolutional neural network (CNN); building a CNN; Ioffe; training more stable; training on all digits; research papers; Szegedy
"• A <i>propagation</i> <i>rule</i> for propagating patterns of activities through the network of
connectivities
• An <i>activation</i> <i>rule</i> for combining the inputs impinging on a unit with the current
state of that unit to produce an output for the unit
• A <i>learning</i> <i>rule</i> whereby patterns of connectivity are modified by experience
• An <i>environment</i> within which the system must operate
We will see in this book that modern neural networks handle each of these
requirements.
In the 1980s, most models were built with a second layer of neurons, thus avoiding
the problem that had been identified by Minsky and Papert (this was their “pattern of
connectivity among units,” to use the preceding framework). And indeed, neural net‐
works were widely used during the ’80s and ’90s for real, practical projects. However,
again a misunderstanding of the theoretical issues held back the field. In theory,
adding just one extra layer of neurons was enough to allow any mathematical func‐
tion to be approximated with these neural networks, but in practice such networks
were often too big and too slow to be useful.
Although researchers showed 30 years ago that to get practical, good performance
you need to use even more layers of neurons, it is only in the last decade that this
principle has been more widely appreciated and applied. Neural networks are now
finally living up to their potential, thanks to the use of more layers, coupled with the
capacity to do so because of improvements in computer hardware, increases in data
availability, and algorithmic tweaks that allow neural networks to be trained faster
and more easily. We now have what Rosenblatt promised: “a machine capable of per‐
ceiving, recognizing, and identifying its surroundings without any human training or
control.”
This is what you will learn how to build in this book. But first, since we are going to
be spending a lot of time together, let’s get to know each other a bit…
<header><largefont><b>Who</b></largefont> <largefont><b>We</b></largefont> <largefont><b>Are</b></largefont></header>
We are Sylvain and Jeremy, your guides on this journey. We hope that you will find us
well suited for this position.
Jeremy has been using and teaching machine learning for around 30 years. He started
using neural networks 25 years ago. During this time, he has led many companies and
projects that have machine learning at their core, including founding the first com‐
pany to focus on deep learning and medicine, Enlitic, and taking on the role of presi‐
dent and chief scientist at the world’s largest machine learning community, Kaggle.
He is the cofounder, along with Dr. Rachel Thomas, of fast.ai, the organization that
built the course this book is based on."|Enlitic company malignant tumor identification; fast.ai ML courses; Kaggle machine learning community; medicine; neural networks; tumor identification
"decision that separated high-value from low-value auction results. Asking only about
coupler_system predicts an average value of 9.21 versus 10.1.
Returning back to the top node after the first decision point, we can see that a second
binary decision split has been made, based on asking whether YearMade is less than or
equal to 1991.5. For the group where this is true (remember, this is now following
two binary decisions, based on coupler_system and YearMade), the average value is
9.97, and there are 155,724 auction records in this group. For the group of auctions
where this decision is false, the average value is 10.4, and there are 205,123 records.
So again, we can see that the decision tree algorithm has successfully split our more
expensive auction records into two more groups that differ in value significantly.
We can show the same information using Terence Parr’s powerful dtreeviz library:
samp_idx = np.random.permutation(len(y))[:500]
dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,
fontname='DejaVu Sans', scale=1.6, label_fontsize=10,
orientation='LR')
This shows a chart of the distribution of the data for each split point. We can clearly
see that there’s a problem with our YearMade data: there are bulldozers made in the
year 1000, apparently! Presumably, this is just a missing value code (a value that
doesn’t otherwise appear in the data and that is used as a placeholder in cases where a
value is missing). For modeling purposes, 1000 is fine, but as you can see, this outlier
makes visualizing the values we are interested in more difficult. So, let’s replace it with
1950:
xs.loc[xs['YearMade']<1900, 'YearMade'] = 1950
valid_xs.loc[valid_xs['YearMade']<1900, 'YearMade'] = 1950"|decision trees; Parr; tabular data for models; training; decision tree viewer
"Let’s see if this trains:
learn = get_learner(opt_func=opt_func)
learn.fit(3, 0.03)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 2.730918 2.009971 0.332739 00:09
1 2.204893 1.747202 0.441529 00:09
2 1.875621 1.684515 0.445350 00:09
It’s working! So that’s how we create SGD from scratch in fastai. Now let’s see what
this “momentum” is.
<header><largefont><b>Momentum</b></largefont></header>
As described in Chapter 4, SGD can be thought of as standing at the top of a moun‐
tain and working your way down by taking a step in the direction of the steepest
slope at each point in time. But what if we have a ball rolling down the mountain? It
won’t, at each given point, exactly follow the direction of the gradient, as it will have
<i>momentum.</i> A ball with more momentum (for instance, a heavier ball) will skip over
little bumps and holes, and be more likely to get to the bottom of a bumpy mountain.
A ping pong ball, on the other hand, will get stuck in every little crevice.
So how can we bring this idea over to SGD? We can use a moving average, instead of
only the current gradient, to make our step:
weight.avg = beta * weight.avg + (1-beta) * weight.grad
new_weight = weight - lr * weight.avg
Here beta is some number we choose that defines how much momentum to use. If
beta is 0, the first equation becomes weight.avg = weight.grad , so we end up with
plain SGD. But if it’s a number close to 1, the main direction chosen is an average of
the previous steps. (If you have done a bit of statistics, you may recognize in the first
equation an <i>exponentially</i> <i>weighted</i> <i>moving</i> <i>average,</i> which is often used to denoise
data and get the underlying tendency.)"|momentum in SGD; SGD class; stochastic gradient descent (SGD); training
"<i>Figure</i> <i>3-10.</i> <i>Error</i> <i>rate</i> <i>per</i> <i>gender</i> <i>and</i> <i>race</i> <i>for</i> <i>various</i> <i>facial</i> <i>recognition</i> <i>systems</i>
IBM’s system, for instance, had a 34.7% error rate for darker females, versus 0.3% for
lighter males—over 100 times more errors! Some people incorrectly reacted to these
experiments by claiming that the difference was simply because darker skin is harder
for computers to recognize. However, what happened was that, after the negative
publicity that this result created, all of the companies in question dramatically
improved their models for darker skin, such that one year later, they were nearly as
good as for lighter skin. So what this showed is that the developers failed to utilize
datasets containing enough darker faces, or test their product with darker faces.
One of the MIT researchers, Joy Buolamwini, warned: “We have entered the age of
automation overconfident yet underprepared. If we fail to make ethical and inclusive
artificial intelligence, we risk losing gains made in civil rights and gender equity
under the guise of machine neutrality.”"|bias; Buolamwini; ethics; facial recognition across races; racial balance of; historical bias; racial bias; training
"Note that the input of the model is a tensor of shape batch_size x 2 , where the first
column (x[:, 0]) contains the user IDs, and the second column (x[:, 1]) contains
the movie IDs. As explained before, we use the <i>embedding</i> layers to represent our
matrices of user and movie latent factors:
x,y = dls.one_batch()
x.shape
torch.Size([64, 2])
Now that we have defined our architecture and created our parameter matrices, we
need to create a Learner to optimize our model. In the past, we have used special
functions, such as cnn_learner , which set up everything for us for a particular appli‐
cation. Since we are doing things from scratch here, we will use the plain Learner
class:
model = DotProduct(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
We are now ready to fit our model:
learn.fit_one_cycle(5, 5e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>time</b>
0 1.326261 1.295701 00:12
1 1.091352 1.091475 00:11
2 0.961574 0.977690 00:11
3 0.829995 0.893122 00:11
4 0.781661 0.876511 00:12
The first thing we can do to make this model a little bit better is to force those predic‐
tions to be between 0 and 5. For this, we just need to use sigmoid_range, as in Chap‐
ter 6. One thing we discovered empirically is that it’s better to have the range go a
little bit over 5, so we use (0, 5.5):
<b>class</b> <b>DotProduct(Module):</b>
<b>def</b> <b>__init__(self,</b> n_users, n_movies, n_factors, y_range=(0,5.5)):
self.user_factors = Embedding(n_users, n_factors)
self.movie_factors = Embedding(n_movies, n_factors)
self.y_range = y_range
<b>def</b> forward(self, x):
users = self.user_factors(x[:,0])
movies = self.movie_factors(x[:,1])
<b>return</b> sigmoid_range((users * movies).sum(dim=1), *self.y_range)
model = DotProduct(n_users, n_movies, 50)
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3)"|categorical variables; collaborative filtering; embedding; collaborative filtering system; Learner from scratch; embedding categorical variables; sigmoid_range
"Using what we just saw, let’s build a custom model for this task and train it. How? We
will use a pretrained architecture and pass our two images through it. Then we can
concatenate the results and send them to a custom head that will return two predic‐
tions. In terms of modules, this looks like this:
<b>class</b> <b>SiameseModel(Module):</b>
<b>def</b> <b>__init__(self,</b> encoder, head):
self.encoder,self.head = encoder,head
<b>def</b> forward(self, x1, x2):
ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1)
<b>return</b> self.head(ftrs)
To create our encoder, we just need to take a pretrained model and cut it, as we
explained before. The function create_body does that for us; we just have to pass it
the place where we want to cut. As we saw earlier, per the dictionary of metadata for
pretrained models, the cut value for a ResNet is –2:
encoder = create_body(resnet34, cut=-2)
Then we can create our head. A look at the encoder tells us the last layer has 512 fea‐
tures, so this head will need to receive 512*4. Why 4? First we have to multiply by 2
because we have two images. Then we need a second multiplication by 2 because of
our concat-pool trick. So we create the head as follows:
head = create_head(512*4, 2, ps=0.5)
With our encoder and head, we can now build our model:
model = SiameseModel(encoder, head)
Before using Learner , we have two more things to define. First, we must define the
loss function we want to use. It’s regular cross entropy, but since our targets are Boo‐
leans, we need to convert them to integers or PyTorch will throw an error:
<b>def</b> loss_func(out, targ):
<b>return</b> nn.CrossEntropyLoss()(out, targ.long())
More importantly, to take full advantage of transfer learning, we have to define a cus‐
tom <i>splitter.</i> A splitter is a function that tells the fastai library how to split the model
into parameter groups. These are used behind the scenes to train only the head of a
model when we do transfer learning.
Here we want two parameter groups: one for the encoder and one for the head. We
can thus define the following splitter (params is just a function that returns all param‐
eters of a given module):
<b>def</b> siamese_splitter(model):
<b>return</b> [params(model.encoder), params(model.head)]"|architecture of model; Siamese model with custom head
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.902943 2.447006 0.401419 00:30
1 1.315203 1.572992 0.525765 00:30
2 1.001199 0.767886 0.759149 00:30
3 0.765864 0.665562 0.797984 00:30
Then you can replace the DataLoaders inside the Learner , and fine-tune:
learn.dls = get_dls(64, 224)
learn.fine_tune(5, 1e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.985213 1.654063 0.565721 01:06
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.706869 0.689622 0.784541 01:07
1 0.739217 0.928541 0.712472 01:07
2 0.629462 0.788906 0.764003 01:07
3 0.491912 0.502622 0.836445 01:06
4 0.414880 0.431332 0.863331 01:06
As you can see, we’re getting much better performance, and the initial training on
small images was much faster on each epoch.
You can repeat the process of increasing size and training more epochs as many times
as you like, for as big an image as you wish—but of course, you will not get any bene‐
fit by using an image size larger than the size of your images on disk.
Note that for transfer learning, progressive resizing may actually hurt performance.
This is most likely to happen if your pretrained model was quite similar to your
transfer learning task and the dataset and was trained on similar-sized images, so the
weights don’t need to be changed much. In that case, training on smaller images may
damage the pretrained weights.
On the other hand, if the transfer learning task is going to use images that are of dif‐
ferent sizes, shapes, or styles than those used in the pretraining task, progressive
resizing will probably help. As always, the answer to “Will it help?” is “Try it!”
Another thing we could try is applying data augmentation to the validation set. Up
until now, we have applied it only on the training set; the validation set always gets
the same images. But maybe we could try to make predictions for a few augmented
versions of the validation set and average them. We’ll consider this approach next."|transfer learning performance hurt; progressive resizing hurting performance
"To implement this process in fastai, you use Resize as an item transform with a large
size, and RandomResizedCrop as a batch transform with a smaller size.
RandomResizedCrop will be added for you if you include the min_scale parameter in
your aug_transforms function, as was done in the DataBlock call in the previous sec‐
tion. Alternatively, you can use pad or squish instead of crop (the default) for the ini‐
tial Resize .
Figure 5-2 shows the difference between an image that has been zoomed, interpola‐
ted, rotated, and then interpolated again (which is the approach used by all other
deep learning libraries), shown here on the right, and an image that has been zoomed
and rotated as one operation and then interpolated once (the fastai approach), shown
here on the left.
<i>Figure</i> <i>5-2.</i> <i>A</i> <i>comparison</i> <i>of</i> <i>fastai’s</i> <i>data</i> <i>augmentation</i> <i>strategy</i> <i>(left)</i> <i>and</i> <i>the</i> <i>traditional</i>
<i>approach</i> <i>(right)</i>
You can see that the image on the right is less well defined and has reflection padding
artifacts in the bottom-left corner; also, the grass at the top left has disappeared
entirely. We find that, in practice, using presizing significantly improves the accuracy
of models and often results in speedups too.
The fastai library also provides simple ways to check how your data looks right before
training your model, which is an extremely important step. We’ll look at those next.
<header><largefont><b>Checking</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Debugging</b></largefont> <largefont><b>a</b></largefont> <largefont><b>DataBlock</b></largefont></header>
We can never just assume that our code is working perfectly. Writing a DataBlock is
like writing a blueprint. You will get an error message if you have a syntax error
somewhere in your code, but you have no guarantee that your template is going to
work on your data source as you intend. So, before training a model, you should
always check your data."|labels
"• Responding to apartment rental ads on Craigslist with a Black name elicited
fewer responses than with a white name.
• An all-white jury was 16 percentage points more likely to convict a Black defend‐
ant than a white one, but when a jury had one Black member, it convicted both at
the same rate.
The COMPAS algorithm, widely used for sentencing and bail decisions in the US, is
an example of an important algorithm that, when tested by ProPublica, showed clear
racial bias in practice (Figure 3-7).
<i>Figure</i> <i>3-7.</i> <i>Results</i> <i>of</i> <i>the</i> <i>COMPAS</i> <i>algorithm</i>
Any dataset involving humans can have this kind of bias: medical data, sales data,
housing data, political data, and so on. Because underlying bias is so pervasive, bias in
datasets is very pervasive. Racial bias even turns up in computer vision, as shown in
the example of autocategorized photos shared on Twitter by a Google Photos user
shown in Figure 3-8.
<i>Figure</i> <i>3-8.</i> <i>One</i> <i>of</i> <i>these</i> <i>labels</i> <i>is</i> <i>very</i> <i>wrong…</i>"|bias; COMPAS algorithm; ethics; Google; historical bias; Google Photos label racial bias; labels; sentencing and bail algorithm bias; Google Photos label; sentencing and bail algorithm
"<i>Character-based</i>
Split a sentence into its individual characters.
We’ll look at word and subword tokenization here, and we’ll leave character-based
tokenization for you to implement in the questionnaire at the end of this chapter.
<b>Jargon:Token</b>
One element of a list created by the tokenization process. It could
be a word, part of a word (a <i>subword),</i> or a single character.
<header><largefont><b>Word</b></largefont> <largefont><b>Tokenization</b></largefont> <largefont><b>with</b></largefont> <largefont><b>fastai</b></largefont></header>
Rather than providing its own tokenizers, fastai provides a consistent interface to a
range of tokenizers in external libraries. Tokenization is an active field of research,
and new and improved tokenizers are coming out all the time, so the defaults that
fastai uses change too. However, the API and options shouldn’t change too much,
since fastai tries to maintain a consistent API even as the underlying technology
changes.
Let’s try it out with the IMDb dataset that we used in Chapter 1:
<b>from</b> <b>fastai.text.all</b> <b>import</b> *
path = untar_data(URLs.IMDB)
We’ll need to grab the text files in order to try out a tokenizer. Just as
get_image_files (which we’ve used many times already), gets all the image files in a
path, get_text_files gets all the text files in a path. We can also optionally pass
folders to restrict the search to a particular list of subfolders:
files = get_text_files(path, folders = ['train', 'test', 'unsup'])
Here’s a review that we’ll tokenize (we’ll print just the start of it here to save space):
txt = files[0].open().read(); txt[:75]
'This movie, which I just discovered at the video store, has apparently sit '
As we write this book, the default English word tokenizer for fastai uses a library
called <i>spaCy.</i> It has a sophisticated rules engine with special rules for URLs, individ‐
ual special English words, and much more. Rather than directly using SpacyToken
izer, however, we’ll use WordTokenizer, since that will always point to fastai’s current
default word tokenizer (which may not necessarily be spaCy, depending when you’re
reading this).
Let’s try it out. We’ll use fastai’s coll_repr(collection,n) function to display the
results. This displays the first <i>n</i> items of <i>collection</i> , along with the full size—it’s"|natural language processing (NLP); tokenization; word tokenization
"As we can see, using TTA gives us good a boost in performance, with no additional
training required. However, it does make inference slower—if you’re averaging five
images for TTA, inference will be five times slower.
We’ve seen a few examples of how data augmentation helps train better models. Let’s
now focus on a new data augmentation technique called <i>Mixup.</i>
<header><largefont><b>Mixup</b></largefont></header>
Mixup, introduced in the 2017 paper ""mixup: Beyond Empirical Risk Minimization”
by Hongyi Zhang et al., is a powerful data augmentation technique that can provide
dramatically higher accuracy, especially when you don’t have much data and don’t
have a pretrained model that was trained on data similar to your dataset. The paper
explains: “While data augmentation consistently leads to improved generalization, the
procedure is dataset-dependent, and thus requires the use of expert knowledge.” For
instance, it’s common to flip images as part of data augmentation, but should you flip
only horizontally or also vertically? The answer is that it depends on your dataset. In
addition, if flipping (for instance) doesn’t provide enough data augmentation for you,
you can’t “flip more.” It’s helpful to have data augmentation techniques that “dial up”
or “dial down” the amount of change, to see what works best for you.
Mixup works as follows, for each image:
1. Select another image from your dataset at random.
2. Pick a weight at random.
3. Take a weighted average (using the weight from step 2) of the selected image with
your image; this will be your independent variable.
4. Take a weighted average (with the same weight) of this image’s labels with your
image’s labels; this will be your dependent variable.
In pseudocode, we’re doing this (where t is the weight for our weighted average):
image2,target2 = dataset[randint(0,len(dataset)]
t = random_float(0.5,1.0)
new_image = t * image1 + (1-t) * image2
new_target = t * target1 + (1-t) * target2
For this to work, our targets need to be one-hot encoded. The paper describes this
using the equations in Figure 7-1 (where <i>λ</i> is the same as t in our pseudocode)."|Mixup augmentation improving; data augmentation; image classifier model training; Mixup augmentation technique; research papers; Zhang
"A window will pop up with this:
Signature: verify_images(fns)
Source:
def verify_images(fns):
""Find images in `fns` that can't be opened""
return L(fns[i] for i,o in
enumerate(parallel(verify_image, fns)) if not o)
File: ~/git/fastai/fastai/vision/utils.py
Type: function
This tells us what argument the function accepts ( fns ), and then shows us the source
code and the file it comes from. Looking at that source code, we can see it applies the
function verify_image in parallel and keeps only the image files for which the result
of that function is False, which is consistent with the doc string: it finds the images in
fns that can’t be opened.
Here are some other features that are very useful in Jupyter notebooks:
• At any point, if you don’t remember the exact spelling of a function or argument
name, you can press Tab to get autocompletion suggestions.
• When inside the parentheses of a function, pressing Shift and Tab simultaneously
will display a window with the signature of the function and a short description.
Pressing these keys twice will expand the documentation, and pressing them
three times will open a full window with the same information at the bottom of
your screen.
? <i>func_name</i>
• In a cell, typing and executing will open a window with the signa‐
ture of the function and a short description.
• In a cell, typing ??func_name and executing will open a window with the signa‐
ture of the function, a short description, and the source code.
• If you are using the fastai library, we added a doc function for you: executing
doc(func_name) in a cell will open a window with the signature of the function,
a short description, and links to the source code on GitHub and the full docu‐
mentation of the function in the library docs.
• Unrelated to the documentation but still very useful: to get help at any point if
%debug
you get an error, type in the next cell and execute to open the Python
debugger, which will let you inspect the content of every variable.
One thing to be aware of in this process: as we discussed in Chapter 1, models can
reflect only the data used to train them. And the world is full of biased data, which
ends up reflected in, for example, Bing Image Search (which we used to create our
dataset). For instance, let’s say you were interested in creating an app that could help
users figure out whether they had healthy skin, so you trained a model on the results"|autocompletion in notebooks; bias; gathering data; help with errors; doc for method documentation; error debugging; process end-to-end; methods; notebooks; Python; signature of function; training; variables; web resources
"<i>Figure</i> <i>1-20.</i> <i>A</i> <i>poor</i> <i>training</i> <i>subset</i>
Instead, use the earlier data as your training set (and the later data for the validation
set), as shown in Figure 1-21.
<i>Figure</i> <i>1-21.</i> <i>A</i> <i>good</i> <i>training</i> <i>subset</i>"|validation set; testing models; training
"environmental sound detection model using a dataset of 8,732 urban sounds. fastai’s
show_batch clearly shows how each sound has a quite distinctive spectrogram, as you
can see in Figure 1-14.
<i>Figure</i> <i>1-14.</i> <i>show_batch</i> <i>with</i> <i>spectrograms</i> <i>of</i> <i>sounds</i>
A time series can easily be converted into an image by simply plotting the time series
on a graph. However, it is often a good idea to try to represent your data in a way that
makes it as easy as possible to pull out the most important components. In a time ser‐
ies, things like seasonality and anomalies are most likely to be of interest.
Various transformations are available for time series data. For instance, fast.ai student
Ignacio Oguiza created images from a time series dataset for olive oil classification,
using a technique called Gramian Angular Difference Field (GADF); you can see the
result in Figure 1-15. He then fed those images to an image classification model just
like the one you see in this chapter. His results, despite having only 30 training set
images, were well over 90% accurate, and close to the state of the art."|non-image tasks; Gramian Angular Difference Field (GADF); converting to image
"<header><largefont><b>Removing</b></largefont> <largefont><b>Low-Importance</b></largefont> <largefont><b>Variables</b></largefont></header>
It seems likely that we could use a subset of the columns by removing the variables of
low importance and still get good results. Let’s try keeping just those with a feature
importance greater than 0.005:
to_keep = fi[fi.imp>0.005].cols
len(to_keep)
21
We can retrain our model using just this subset of the columns:
xs_imp = xs[to_keep]
valid_xs_imp = valid_xs[to_keep]
m = rf(xs_imp, y)
And here’s the result:
m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y)
(0.181208, 0.232323)
Our accuracy is about the same, but we have far fewer columns to study:
len(xs.columns), len(xs_imp.columns)
(78, 21)
We’ve found that generally the first step to improving a model is simplifying it—78
columns was too many for us to study them all in depth! Furthermore, in practice,
often a simpler, more interpretable model is easier to roll out and maintain.
This also makes our feature importance plot easier to interpret. Let’s look at it again:
plot_fi(rf_feat_importance(m, xs_imp));"|bagging; decision trees; removing low-importance variables; machine learning (ML); predictions; random forests; tabular data for models; training
"Let’s check that we get the same results using this refactoring:
learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy,
metrics=accuracy)
learn.fit_one_cycle(4, 1e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.816274 1.964143 0.460185 00:02
1 1.423805 1.739964 0.473259 00:02
2 1.430327 1.685172 0.485382 00:02
3 1.388390 1.657033 0.470406 00:02
We can also refactor our pictorial representation in exactly the same way, as shown in
Figure 12-4 (we’re also removing the details of activation sizes here, and using the
same arrow colors as in Figure 12-3).
<i>Figure</i> <i>12-4.</i> <i>Basic</i> <i>recurrent</i> <i>neural</i> <i>network</i>
You will see that a set of activations is being updated each time through the loop,
h—this
stored in the variable is called the <i>hidden</i> <i>state.</i>
<b>Jargon:HiddenState</b>
The activations that are updated at each step of a recurrent neural
network.
A neural network that is defined using a loop like this is called a <i>recurrent</i> <i>neural</i> <i>net‐</i>
<i>work</i> (RNN). It is important to realize that an RNN is not a complicated new architec‐
ture, but simply a refactoring of a multilayer neural network using a for loop."|hidden state; language model; hidden state activations; natural language processing (NLP); neural networks; recurrent neural networks (RNNs)
"One change compared to the preceding chapter is the metric we use: because this is a
multilabel problem, we can’t use the accuracy function. Why is that? Well, accuracy
was comparing our outputs to our targets like so:
<b>def</b> accuracy(inp, targ, axis=-1):
""Compute accuracy with `targ` when `pred` is bs * n_classes""
pred = inp.argmax(dim=axis)
<b>return</b> (pred == targ).float().mean()
The class predicted was the one with the highest activation (this is what argmax does).
Here it doesn’t work because we could have more than one prediction on a single
image. After applying the sigmoid to our activations (to make them between 0 and 1),
we need to decide which ones are 0s and which ones are 1s by picking a <i>threshold.</i>
Each value above the threshold will be considered as a 1, and each value lower than
the threshold will be considered a 0:
<b>def</b> accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):
""Compute accuracy when `inp` and `targ` are the same size.""
<b>if</b> sigmoid: inp = inp.sigmoid()
<b>return</b> ((inp>thresh)==targ.bool()).float().mean()
If we pass accuracy_multi directly as a metric, it will use the default value for
threshold, which is 0.5. We might want to adjust that default and create a new ver‐
sion of accuracy_multi that has a different default. To help with this, there is a func‐
tion in Python called partial. It allows us to <i>bind</i> a function with some arguments or
keyword arguments, making a new version of that function that, whenever it is called,
always includes those arguments. For instance, here is a simple function taking two
arguments:
<b>def</b> say_hello(name, say_what=""Hello""): <b>return</b> f""{say_what} {name}.""
say_hello('Jeremy'),say_hello('Jeremy', 'Ahoy!')
('Hello Jeremy.', 'Ahoy! Jeremy.')
We can switch to a French version of that function by using partial:
f = partial(say_hello, say_what=""Bonjour"")
f(""Jeremy""),f(""Sylvain"")
('Bonjour Jeremy.', 'Bonjour Sylvain.')"|argument binding with partial function; binary cross entropy loss function; labels; 0s and 1s threshold; binary cross entropy; partial function to bind arguments
"We’ll create a ResNet-34 without pretraining and pass along any arguments received:
<b>def</b> get_learner(**kwargs):
<b>return</b> cnn_learner(dls, resnet34, pretrained=False,
metrics=accuracy, **kwargs).to_fp16()
Here’s the default fastai optimizer, with the usual 3e-3 learning rate:
learn = get_learner()
learn.fit_one_cycle(3, 0.003)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 2.571932 2.685040 0.322548 00:11
1 1.904674 1.852589 0.437452 00:11
2 1.586909 1.374908 0.594904 00:11
Now let’s try plain SGD. We can pass opt_func (optimization function) to
cnn_learner to get fastai to use any optimizer:
learn = get_learner(opt_func=SGD)
The first thing to look at is lr_find :
learn.lr_find()
(0.017378008365631102, 3.019951861915615e-07)
It looks like we’ll need to use a higher learning rate than we normally use:
learn.fit_one_cycle(3, 0.03, moms=(0,0,0))
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 2.969412 2.214596 0.242038 00:09
1 2.442730 1.845950 0.362548 00:09
2 2.157159 1.741143 0.408917 00:09"|SGD class; training
"<b>Overfitting</b> <b>is</b> <b>the</b> <b>single</b> <b>most</b> <b>important</b> <b>and</b> <b>challenging</b> <b>issue</b> when training for all
machine learning practitioners, and all algorithms. As you will see, it is easy to create
a model that does a great job at making predictions on the exact data it has been
trained on, but it is much harder to make accurate predictions on data the model has
never seen before. And of course, this is the data that will matter in practice. For
instance, if you create a handwritten digit classifier (as we will soon!) and use it to
recognize numbers written on checks, then you are never going to see any of the
numbers that the model was trained on—every check will have slightly different var‐
iations of writing to deal with.
You will learn many methods to avoid overfitting in this book. However, you should
use those methods only after you have confirmed that overfitting is occurring (i.e.,
if you have observed the validation accuracy getting worse during training). We often
see practitioners using overfitting avoidance techniques even when they have enough
data that they didn’t need to do so, ending up with a model that may be less accurate
than what they could have achieved.
<b>ValidationSet</b>
When you train a model, you must <i>always</i> have both a training set
and a validation set, and you must measure the accuracy of your
model only on the validation set. If you train for too long, with not
enough data, you will see the accuracy of your model start to get
worse; this is called <i>overfitting.</i> fastai defaults valid_pct to 0.2 , so
even if you forget, fastai will create a validation set for you!
The fifth line of the code training our image recognizer tells fastai to create a <i>convolu‐</i>
<i>tional</i> <i>neural</i> <i>network</i> (CNN) and specifies what <i>architecture</i> to use (i.e., what kind of
model to create), what data we want to train it on, and what <i>metric</i> to use:
learn = cnn_learner(dls, resnet34, metrics=error_rate)
Why a CNN? It’s the current state-of-the-art approach to creating computer vision
models. We’ll be learning all about how CNNs work in this book. Their structure is
inspired by how the human vision system works.
There are many architectures in fastai, which we will introduce in this book (as well
as discussing how to create your own). Most of the time, however, picking an archi‐
tecture isn’t a very important part of the deep learning process. It’s something that
academics love to talk about, but in practice it is unlikely to be something you need to
spend much time on. There are some standard architectures that work most of the
time, and in this case we’re using one called <i>ResNet</i> that we’ll be talking a lot about in
the book; it is both fast and accurate for many datasets and problems. The 34 in
resnet34 refers to the number of layers in this variant of the architecture (other
options are 18, 50, 101, and 152). Models using architectures with more layers take"|validation set; picking not so important; ResNet; beginning; cnn_learner; computer vision models; convolutional neural network (CNN); architecture not so important; epochs; fastai software library; convolutional neural network; layers; ResNet architecture; Learner; first model declaration; notebooks; overfitting; avoid only when occurring; predictions; training
"Notice that the bias term, <i>b,</i> is the same for each section of the image. You can con‐
sider the bias as part of the filter, just as the weights (α, β, γ, δ) are part of the filter.
Here’s an interesting insight—a convolution can be represented as a special kind of
matrix multiplication, as illustrated in Figure 13-9. The weight matrix is just like the
ones from traditional neural networks. However, this weight matrix has two special
properties:
1. The zeros shown in gray are untrainable. This means that they’ll stay zero
throughout the optimization process.
2. Some of the weights are equal, and while they are trainable (i.e., changeable),
they must remain equal. These are called <i>shared</i> <i>weights.</i>
The zeros correspond to the pixels that the filter can’t touch. Each row of the weight
matrix corresponds to one application of the filter.
<i>Figure</i> <i>13-9.</i> <i>Convolution</i> <i>as</i> <i>matrix</i> <i>multiplication</i>
Now that we understand what convolutions are, let’s use them to build a neural net.
<header><largefont><b>Our</b></largefont> <largefont><b>First</b></largefont> <largefont><b>Convolutional</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Network</b></largefont></header>
There is no reason to believe that some particular edge filters are the most useful ker‐
nels for image recognition. Furthermore, we’ve seen that in later layers, convolutional
kernels become complex transformations of features from lower levels, but we don’t
have a good idea of how to manually construct these.
Instead, it would be best to learn the values of the kernels. We already know how to
do this—SGD! In effect, the model will learn the features that are useful for classifica‐
tion. When we use convolutions instead of (or in addition to) regular linear layers, we
create a <i>convolutional</i> <i>neural</i> <i>network</i> (CNN)."|convolutional neural network (CNN); convolution as matrix multiplication
"To tell Module that we want to treat a tensor as a parameter, we have to wrap it in the
nn.Parameter class. This class doesn’t add any functionality (other than automatically
calling requires_grad_ for us). It’s used only as a “marker” to show what to include
in parameters :
<b>class</b> <b>T(Module):</b>
<b>def</b> <b>__init__(self):</b> self.a = nn.Parameter(torch.ones(3))
L(T().parameters())
(#1) [Parameter containing:
tensor([1., 1., 1.], requires_grad=True)]
All PyTorch modules use nn.Parameter for any trainable parameters, which is why
we haven’t needed to explicitly use this wrapper until now:
<b>class</b> <b>T(Module):</b>
<b>def</b> <b>__init__(self):</b> self.a = nn.Linear(1, 3, bias=False)
t = T()
L(t.parameters())
(#1) [Parameter containing:
tensor([[-0.9595],
[-0.8490],
[ 0.8159]], requires_grad=True)]
type(t.a.weight)
torch.nn.parameter.Parameter
We can create a tensor as a parameter, with random initialization, like so:
<b>def</b> create_params(size):
<b>return</b> nn.Parameter(torch.zeros(*size).normal_(0, 0.01))
Let’s use this to create DotProductBias again, but without Embedding:
<b>class</b> <b>DotProductBias</b> (Module):
<b>def</b> <b>__init__(self,</b> n_users, n_movies, n_factors, y_range=(0,5.5)):
self.user_factors = create_params([n_users, n_factors])
self.user_bias = create_params([n_users])
self.movie_factors = create_params([n_movies, n_factors])
self.movie_bias = create_params([n_movies])
self.y_range = y_range
<b>def</b> forward(self, x):
users = self.user_factors[x[:,0]]
movies = self.movie_factors[x[:,1]]
res = (users*movies).sum(dim=1)
res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]]
<b>return</b> sigmoid_range(res, *self.y_range)
Then let’s train it again to check we get around the same results we saw in the previ‐
ous section:"|built from scratch; Module class; embedding from scratch; parameters
"<header><largefont><b>Random</b></largefont> <largefont><b>Forests</b></largefont></header>
In 1994, Berkeley professor Leo Breiman, one year after his retirement, published a
small technical report called “Bagging Predictors”, which turned out to be one of the
most influential ideas in modern machine learning. The report began:
Bagging predictors is a method for generating multiple versions of a predictor and
using these to get an aggregated predictor. The aggregation averages over the ver‐
sions…The multiple versions are formed by making bootstrap replicates of the learn‐
ing set and using these as new learning sets. Tests…show that bagging can give
substantial gains in accuracy. The vital element is the instability of the prediction
method. If perturbing the learning set can cause significant changes in the predictor
constructed, then bagging can improve accuracy.
Here is the procedure that Breiman is proposing:
1. Randomly choose a subset of the rows of your data (i.e., “bootstrap replicates of
your learning set”).
2. Train a model using this subset.
3. Save that model, and then return to step 1 a few times.
4. This will give you multiple trained models. To make a prediction, predict using
all of the models, and then take the average of each of those model’s predictions.
This procedure is known as <i>bagging.</i> It is based on a deep and important insight:
although each of the models trained on a subset of data will make more errors than a
model trained on the full dataset, those errors will not be correlated with each other.
Different models will make different errors. The average of those errors, therefore, is
zero! So if we take the average of all of the models’ predictions, we should end up
with a prediction that gets closer and closer to the correct answer, the more models
we have. This is an extraordinary result—it means that we can improve the accuracy
of nearly any kind of machine learning algorithm by training it multiple times, each
time on a different random subset of the data, and averaging its predictions.
In 2001, Breiman went on to demonstrate that this approach to building models,
when applied to decision tree building algorithms, was particularly powerful. He
went even further than just randomly choosing rows for each model’s training, but
also randomly selected from a subset of columns when choosing each split in each
decision tree. He called this method the <i>random</i> <i>forest.</i> Today it is, perhaps, the most
widely used and practically important machine learning method.
In essence, a random forest is a model that averages the predictions of a large number
of decision trees, which are generated by randomly varying various parameters that
specify what data is used to train the tree and other tree parameters. Bagging is a
particular approach to <i>ensembling,</i> or combining the results of multiple models"|bagging; Breiman; decision trees; machine learning (ML); predictions; random forests; research papers; tabular data for models; training
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>10</b></largefont></header>
<header><largefont><b>NLP</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Dive:</b></largefont> <largefont><b>RNNs</b></largefont></header>
In Chapter 1, we saw that deep learning can be used to get great results with natural
language datasets. Our example relied on using a pretrained language model and
fine-tuning it to classify reviews. That example highlighted a difference between
transfer learning in NLP and computer vision: in general, in NLP the pretrained
model is trained on a different task.
What we call a <i>language</i> <i>model</i> is a model that has been trained to guess the next word
in a text (having read the ones before). This kind of task is called <i>self-supervised</i> <i>learn‐</i>
<i>ing:</i> we do not need to give labels to our model, just feed it lots and lots of texts. It has
a process to automatically get labels from the data, and this task isn’t trivial: to prop‐
erly guess the next word in a sentence, the model will have to develop an understand‐
ing of the English (or other) language. Self-supervised learning can also be used in
other domains; for instance, see “Self-Supervised Learning and Computer Vision” for
an introduction to vision applications. Self-supervised learning is not usually used for
the model that is trained directly, but instead is used for pretraining a model used for
transfer learning.
<b>Jargon:Self-SupervisedLearning</b>
Training a model using labels that are embedded in the independ‐
ent variable, rather than requiring external labels. For instance,
training a model to predict the next word in a text.
The language model we used in Chapter 1 to classify IMDb reviews was pretrained
on Wikipedia. We got great results by directly fine-tuning this language model to a
movie review classifier, but with one extra step, we can do even better. The Wikipedia
English is slightly different from the IMDb English, so instead of jumping directly to"|natural language processing; natural language models; pretraining NLP on; language model; movie review sentiment model; natural language processing (NLP); pretrained models; self-supervised learning; Wikipedia for pretraining NLP
"The basic idea is that by using more linear layers, we can have our model do more
computation, and therefore model more complex functions. But there’s no point in
just putting one linear layout directly after another one, because when we multiply
things together and then add them up multiple times, that could be replaced by mul‐
tiplying different things together and adding them up just once! That is to say, a series
of any number of linear layers in a row can be replaced with a single linear layer with
a different set of parameters.
But if we put a nonlinear function between them, such as max, this is no longer true.
Now each linear layer is somewhat decoupled from the other ones and can do its own
useful work. The max function is particularly interesting, because it operates as a sim‐
ple if statement.
<b>SylvainSays</b>
Mathematically, we say the composition of two linear functions is
another linear function. So, we can stack as many linear classifiers
as we want on top of each other, and without nonlinear functions
between them, it will just be the same as one linear classifier.
Amazingly enough, it can be mathematically proven that this little function can solve
any computable problem to an arbitrarily high level of accuracy, if you can find the
right parameters for w1 and w2 and if you make these matrices big enough. For any
arbitrarily wiggly function, we can approximate it as a bunch of lines joined together;
to make it closer to the wiggly function, we just have to use shorter lines. This is
known as the <i>universal</i> <i>approximation</i> <i>theorem.</i> The three lines of code that we have
here are known as <i>layers.</i> The first and third are known as <i>linear</i> <i>layers,</i> and the sec‐
ond line of code is known variously as a <i>nonlinearity,</i> or <i>activation</i> <i>function.</i>
Just as in the previous section, we can replace this code with something a bit simpler
by taking advantage of PyTorch:
simple_net = nn.Sequential(
nn.Linear(28*28,30),
nn.ReLU(),
nn.Linear(30,1)
)
nn.Sequential creates a module that will call each of the listed layers or functions in
turn.
nn.ReLU is a PyTorch module that does exactly the same thing as the F.relu function.
Most functions that can appear in a model also have identical forms that are modules.
Generally, it’s just a case of replacing F with nn and changing the capitalization. When
using nn.Sequential, PyTorch requires us to use the module version. Since modules"|nonlinear layer; more linear layers; nonlinear function between linears; numerical digit classifier; linear and nonlinear layers; PyTorch; nonlinear and linear layers; creating an optimizer; Sequential class; stochastic gradient descent (SGD); universal approximation theorem
"To see why so much computation occurs in the early layers, consider the very first
convolution on a 128-pixel input image. If it is a stride-1 convolution, it will apply the
kernel to every one of the 128×128 pixels. That’s a lot of work! In the later layers,
however, the grid size could be as small as 4×4 or even 2×2, so there are far fewer
kernel applications to do.
On the other hand, the first-layer convolution has only 3 input features and 32 output
features. Since it is a 3×3 kernel, this is 3×32×3×3 = 864 parameters in the weights.
But the last convolution will have 256 input features and 512 output features, result‐
ing in 1,179,648 weights! So the first layers contain the vast majority of the computa‐
tion, but the last layers contain the vast majority of the parameters.
A ResNet block takes more computation than a plain convolutional block, since (in
the stride-2 case) a ResNet block has three convolutions and a pooling layer. That’s
why we want to have plain convolutions to start off our ResNet.
We’re now ready to show the implementation of a modern ResNet, with the “bag of
tricks.” It uses the four groups of ResNet blocks, with 64, 128, 256, then 512 filters.
Each group starts with a stride-2 block, except for the first one, since it’s just after a
MaxPooling layer:
<b>class</b> <b>ResNet(nn.Sequential):</b>
<b>def</b> <b>__init__(self,</b> n_out, layers, expansion=1):
stem = _resnet_stem(3,32,32,64)
self.block_szs = [64, 64, 128, 256, 512]
<b>for</b> i <b>in</b> range(1,5): self.block_szs[i] *= expansion
blocks = [self._make_layer(*o) <b>for</b> o <b>in</b> enumerate(layers)]
super().__init__(*stem, *blocks,
nn.AdaptiveAvgPool2d(1), Flatten(),
nn.Linear(self.block_szs[-1], n_out))
<b>def</b> _make_layer(self, idx, n_layers):
stride = 1 <b>if</b> idx==0 <b>else</b> 2
ch_in,ch_out = self.block_szs[idx:idx+2]
<b>return</b> nn.Sequential(*[
ResBlock(ch_in <b>if</b> i==0 <b>else</b> ch_out, ch_out, stride <b>if</b> i==0 <b>else</b> 1)
<b>for</b> i <b>in</b> range(n_layers)
])
The _make_layer function is just there to create a series of n_layers blocks. The first
one is going from ch_in to ch_out with the indicated stride, and all the others are
blocks of stride 1 with ch_out to ch_out tensors. Once the blocks are defined, our
model is purely sequential, which is why we define it as a subclass of nn.Sequential .
(Ignore the expansion parameter for now; we’ll discuss it in the next section. For
now, it’ll be 1, so it doesn’t do anything.)"|building state-of-the-art ResNet
"value of the parameter. When we know how our function will change, we know what
we need to do to make it smaller. This is the key to machine learning: having a way to
change the parameters of a function to make it smaller. Calculus provides us with a
computational shortcut, the derivative, which lets us directly calculate the gradients
of our functions.
One important thing to be aware of is that our function has lots of weights that we
need to adjust, so when we calculate the derivative, we won’t get back one number,
but lots of them—a gradient for every weight. But there is nothing mathematically
tricky here; you can calculate the derivative with respect to one weight and treat all
the other ones as constant, and then repeat that for each other weight. This is how all
of the gradients are calculated, for every weight.
We mentioned just now that you won’t have to calculate any gradients yourself. How
can that be? Amazingly enough, PyTorch is able to automatically compute the deriva‐
tive of nearly any function! What’s more, it does it very fast. Most of the time, it will
be at least as fast as any derivative function that you can create by hand. Let’s see an
example.
First, let’s pick a tensor value at which we want gradients:
xt = tensor(3.).requires_grad_()
Notice the special method requires_grad_? That’s the magical incantation we use to
tell PyTorch that we want to calculate gradients with respect to that variable at that
value. It is essentially tagging the variable, so PyTorch will remember to keep track of
how to compute gradients of the other direct calculations on it that you will ask for.
<b>AlexisSays</b>
This API might throw you off if you’re coming from math or phys‐
ics. In those contexts, the “gradient” of a function is just another
function (i.e., its derivative), so you might expect gradient-related
APIs to give you a new function. But in deep learning, “gradient”
usually means the <i>value</i> of a function’s derivative at a particular
argument value. The PyTorch API also puts the focus on the argu‐
ment, not the function you’re actually computing the gradients of.
It may feel backward at first, but it’s just a different perspective.
Now we calculate our function with that value. Notice how PyTorch prints not just
the value calculated, but also a note that it has a gradient function it’ll be using to cal‐
culate our gradients when needed:
yt = f(xt)
yt
tensor(9., grad_fn=<PowBackward0>)"|gradients; key to ML via derivatives; stochastic gradient descent (SGD); training; weights
"of searches for (say) “healthy skin.” Figure 2-3 shows you kind of the results you
would get.
<i>Figure</i> <i>2-3.</i> <i>Data</i> <i>for</i> <i>a</i> <i>healthy</i> <i>skin</i> <i>detector?</i>
With this as your training data, you would end up not with a healthy skin detector,
but a <i>young</i> <i>white</i> <i>woman</i> <i>touching</i> <i>her</i> <i>face</i> detector! Be sure to think carefully about
the types of data that you might expect to see in practice in your application, and
check carefully to ensure that all these types are reflected in your model’s source data.
(Thanks to Deb Raji, who came up with the healthy skin example. See her paper
“Actionable Auditing: Investigating the Impact of Publicly Naming Biased Perfor‐
mance Results of Commercial AI Products” for more fascinating insights into model
bias.)
Now that we have downloaded some data, we need to assemble it in a format suitable
for model training. In fastai, that means creating an object called DataLoaders."|Raji; research papers
"standard approach is to treat a small loss as good and a large loss as bad, although
this is just a convention).
<i>Step</i>
A simple way to figure out whether a weight should be increased a bit or
decreased a bit would be just to try it: increase the weight by a small amount, and
see if the loss goes up or down. Once you find the correct direction, you could
then change that amount by a bit more, or a bit less, until you find an amount
that works well. However, this is slow! As we will see, the magic of calculus allows
us to directly figure out in which direction, and by roughly how much, to change
each weight, without having to try all these small changes. The way to do this is
by calculating <i>gradients.</i> This is just a performance optimization; we would get
exactly the same results by using the slower manual process as well.
<i>Stop</i>
Once we’ve decided how many epochs to train the model for (a few suggestions
for this were given in the earlier list), we apply that decision. For our digit classi‐
fier, we would keep training until the accuracy of the model started getting worse,
or we ran out of time.
Before applying these steps to our image classification problem, let’s illustrate what
they look like in a simpler case. First we will define a very simple function, the quad‐
ratic—let’s pretend that this is our loss function, and x is a weight parameter of the
function:
<b>def</b> f(x): <b>return</b> x**2
Here is a graph of that function:
plot_function(f, 'x', 'x**2')"|stochastic gradient descent; stochastic gradient descent (SGD)
"We just completed various mathematical operations on PyTorch tensors. If you’ve
done numeric programming in PyTorch before, you may recognize these as being
similar to NumPy arrays. Let’s have a look at those two important data structures.
<header><largefont><b>NumPy</b></largefont> <largefont><b>Arrays</b></largefont> <largefont><b>and</b></largefont> <largefont><b>PyTorch</b></largefont> <largefont><b>Tensors</b></largefont></header>
NumPy is the most widely used library for scientific and numeric programming in
Python. It provides similar functionality and a similar API to that provided by
PyTorch; however, it does not support using the GPU or calculating gradients, which
are both critical for deep learning. Therefore, in this book, we will generally use
PyTorch tensors instead of NumPy arrays, where possible.
(Note that fastai adds some features to NumPy and PyTorch to make them a bit more
similar to each other. If any code in this book doesn’t work on your computer, it’s pos‐
sible that you forgot to include a line like this at the start of your notebook: from
fastai.vision.all import *.)
But what are arrays and tensors, and why should you care?
Python is slow compared to many languages. Anything fast in Python, NumPy, or
PyTorch is likely to be a wrapper for a compiled object written (and optimized) in
another language—specifically, C. In fact, <i>NumPy</i> <i>arrays</i> <i>and</i> <i>PyTorch</i> <i>tensors</i> <i>can</i> <i>fin‐</i>
<i>ish</i> <i>computations</i> <i>many</i> <i>thousands</i> <i>of</i> <i>times</i> <i>faster</i> <i>than</i> <i>using</i> <i>pure</i> <i>Python.</i>
A NumPy array is a multidimensional table of data, with all items of the same type.
Since that can be any type at all, they can even be arrays of arrays, with the innermost
arrays potentially being different sizes—this is called a <i>jagged</i> <i>array.</i> By “multidimen‐
sional table,” we mean, for instance, a list (dimension of one), a table or matrix
(dimension of two), a table of tables or cube (dimension of three), and so forth. If the
items are all of simple type such as integer or float, NumPy will store them as a com‐
pact C data structure in memory. This is where NumPy shines. NumPy has a wide
variety of operators and methods that can run computations on these compact struc‐
tures at the same speed as optimized C, because they are written in optimized C.
A PyTorch tensor is nearly the same thing as a NumPy array, but with an additional
restriction that unlocks additional capabilities. It’s the same in that it, too, is a multi‐
dimensional table of data, with all items of the same type. However, the restriction is
that a tensor cannot use just any old type—it has to use a single basic numeric type
for all components. As a result, a tensor is not as flexible as a genuine array of arrays.
For example, a PyTorch tensor cannot be jagged. It is always a regularly shaped multi‐
dimensional rectangular structure.
The vast majority of methods and operators supported by NumPy on these structures
are also supported by PyTorch, but PyTorch tensors have additional capabilities. One
major capability is that these structures can live on the GPU, in which case their com‐
putation will be optimized for the GPU and can run much faster (given lots of values"|arrays; arrays within arrays; GPU deep learning servers; jagged arrays; PyTorch; NumPy; tensors
"gradients of the loss with respect to <i>l</i> , which will need the gradients of the loss with
2
respect to <i>out.</i>
So to compute all the gradients we need for the update, we need to begin from the
output of the model and work our way <i>backward,</i> one layer after the other—which is
why this step is known as <i>backpropagation.</i> We can automate it by having each func‐
tion we implemented (relu, mse, lin) provide its backward step: that is, how to
derive the gradients of the loss with respect to the input(s) from the gradients of the
loss with respect to the output.
Here we populate those gradients in an attribute of each tensor, a bit like PyTorch
does with .grad .
The first are the gradients of the loss with respect to the output of our model (which
squeeze mse,
is the input of the loss function). We undo the we did in and then we
use the formula that gives us the derivative of <i>x</i> 2 : 2x. The derivative of the mean is just
1/n, where <i>n</i> is the number of elements in our input:
<b>def</b> mse_grad(inp, targ):
<i>#</i> <i>grad</i> <i>of</i> <i>loss</i> <i>with</i> <i>respect</i> <i>to</i> <i>output</i> <i>of</i> <i>previous</i> <i>layer</i>
inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]
For the gradients of the ReLU and our linear layer, we use the gradients of the loss
with respect to the output (in out.g) and apply the chain rule to compute the gradi‐
ents of the loss with respect to the output (in inp.g ). The chain rule tells us that
inp.g = relu'(inp) * out.g. The derivative of relu is either 0 (when inputs are
negative) or 1 (when inputs are positive), so this gives us the following:
<b>def</b> relu_grad(inp, out):
<i>#</i> <i>grad</i> <i>of</i> <i>relu</i> <i>with</i> <i>respect</i> <i>to</i> <i>input</i> <i>activations</i>
inp.g = (inp>0).float() * out.g
The scheme is the same to compute the gradients of the loss with respect to the
inputs, weights, and bias in the linear layer:
<b>def</b> lin_grad(inp, out, w, b):
<i>#</i> <i>grad</i> <i>of</i> <i>matmul</i> <i>with</i> <i>respect</i> <i>to</i> <i>input</i>
inp.g = out.g @ w.t()
w.g = inp.t() @ out.g
b.g = out.g.sum(0)
We won’t linger on the mathematical formulas that define them since they’re not
important for our purposes, but do check out Khan Academy’s excellent calculus les‐
sons if you’re interested in this topic."|backward pass and; gradients and backward pass
"when he was an undergraduate. Even at Tesla, where they’re trying to solve the
extremely tough challenge of making a self-driving car, CEO Elon Musk says:
A PhD is definitely not required. All that matters is a deep understanding of AI & abil‐
ity to implement NNs in a way that is actually useful (latter point is what’s truly hard).
Don’t care if you even graduated high school.
What you will need to do to succeed, however, is to apply what you learn in this book
to a personal project, and always persevere.
<header><largefont><b>Your</b></largefont> <largefont><b>Projects</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Mindset</b></largefont></header>
Whether you’re excited to identify if plants are diseased from pictures of their leaves,
autogenerate knitting patterns, diagnose TB from X-rays, or determine when a rac‐
coon is using your cat door, we will get you using deep learning on your own prob‐
lems (via pretrained models from others) as quickly as possible, and then will
progressively drill into more details. You’ll learn how to use deep learning to solve
your own problems at state-of-the-art accuracy within the first 30 minutes of the next
chapter! (And feel free to skip straight there now if you’re dying to get coding right
away.) There is a pernicious myth out there that you need to have computing resour‐
ces and datasets the size of those at Google to be able to do deep learning, but it’s not
true.
So, what sorts of tasks make for good test cases? You could train your model to distin‐
guish between Picasso and Monet paintings or to pick out pictures of your daughter
instead of pictures of your son. It helps to focus on your hobbies and passions—set‐
ting yourself four or five little projects rather than striving to solve a big, grand prob‐
lem tends to work better when you’re getting started. Since it is easy to get stuck,
trying to be too ambitious too early can often backfire. Then, once you’ve got the
basics mastered, aim to complete something you’re really proud of!
<b>JeremySays</b>
Deep learning can be set to work on almost any problem. For
instance, my first startup was a company called FastMail, which
provided enhanced email services when it launched in 1999 (and
still does to this day). In 2002, I set it up to use a primitive form of
deep learning, single-layer neural networks, to help categorize
emails and stop customers from receiving spam.
Common character traits in the people who do well at deep learning include playful‐
ness and curiosity. The late physicist Richard Feynman is an example of someone we’d
expect to be great at deep learning: his development of an understanding of the
movement of subatomic particles came from his amusement at how plates wobble
when they spin in the air."|how to learn; Feynman
"To see if this is any good, let’s check what a very simple model would give us. In this
case, we could always predict the most common token, so let’s find out which token is
most often the target in our validation set:
n,counts = 0,torch.zeros(len(vocab))
<b>for</b> x,y <b>in</b> dls.valid:
n += y.shape[0]
<b>for</b> i <b>in</b> range_of(vocab): counts[i] += (y==i).long().sum()
idx = torch.argmax(counts)
idx, vocab[idx.item()], counts[idx].item()/n
(tensor(29), 'thousand', 0.15165200855716662)
The most common token has the index 29, which corresponds to the token thousand.
Always predicting this token would give us an accuracy of roughly 15%, so we are
faring way better!
<b>AlexisSays</b>
My first guess was that the separator would be the most common
token, since there is one for every number. But looking at tokens
reminded me that large numbers are written with many words, so
on the way to 10,000 you write “thousand” a lot: five thousand, five
thousand and one, five thousand and two, etc. Oops! Looking at
your data is great for noticing subtle features as well as embarrass‐
ingly obvious ones.
This is a nice first baseline. Let’s see how we can refactor it with a loop.
<header><largefont><b>Our</b></largefont> <largefont><b>First</b></largefont> <largefont><b>Recurrent</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Network</b></largefont></header>
Looking at the code for our module, we could simplify it by replacing the duplicated
code that calls the layers with a for loop. In addition to making our code simpler, this
will have the benefit that we will be able to apply our module equally well to token
sequences of different lengths—we won’t be restricted to token lists of length three:
<b>class</b> <b>LMModel2(Module):</b>
<b>def</b> <b>__init__(self,</b> vocab_sz, n_hidden):
self.i_h = nn.Embedding(vocab_sz, n_hidden)
self.h_h = nn.Linear(n_hidden, n_hidden)
self.h_o = nn.Linear(n_hidden,vocab_sz)
<b>def</b> forward(self, x):
h = 0
<b>for</b> i <b>in</b> range(3):
h = h + self.i_h(x[:,i])
h = F.relu(self.h_h(h))
<b>return</b> self.h_o(h)"|examining data importance; language model; natural language processing (NLP); first RNN; most common token prediction; NLP most common token
"<b>Broadcastingwithascalar</b>
Broadcasting with a scalar is the easiest type of broadcasting. When we have a tensor
a and a scalar, we just imagine a tensor of the same shape as a filled with that scalar
and perform the operation:
a = tensor([10., 6, -4])
a > 0
tensor([ True, True, False])
How are we able to do this comparison? 0 is being <i>broadcast</i> to have the same dimen‐
sions as a. Note that this is done without creating a tensor full of zeros in memory
(that would be inefficient).
This is useful if you want to normalize your dataset by subtracting the mean (a scalar)
from the entire dataset (a matrix) and dividing by the standard deviation (another
scalar):
m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])
(m - 5) / 2.73
tensor([[-1.4652, -1.0989, -0.7326],
[-0.3663, 0.0000, 0.3663],
[ 0.7326, 1.0989, 1.4652]])
What if you have different means for each row of the matrix? In that case, you will
need to broadcast a vector to a matrix.
<b>Broadcastingavectortoamatrix</b>
We can broadcast a vector to a matrix as follows:
c = tensor([10.,20,30])
m = tensor([[1., 2, 3], [4,5,6], [7,8,9]])
m.shape,c.shape
(torch.Size([3, 3]), torch.Size([3]))
m + c
tensor([[11., 22., 33.],
[14., 25., 36.],
[17., 28., 39.]])
Here the elements of c are expanded to make three rows that match, making the
operation possible. Again, PyTorch doesn’t actually create three copies of c in mem‐
ory. This is done by the expand_as method behind the scenes:
c.expand_as(m)
tensor([[10., 20., 30.],
[10., 20., 30.],
[10., 20., 30.]])"|broadcasting with a scalar; vector to matrix; building layer from scratch; broadcasting vector to matrix
"The <i>independent</i> <i>variable</i> is the thing we are using to make predictions from, and the
<i>dependent</i> <i>variable</i> is our target. In this case, our independent variable is a set of
images, and our dependent variables are the categories (type of bear) for each image.
We will see many other types of block in the rest of this book.
For this DataLoaders, our underlying items will be file paths. We have to tell fastai
how to get a list of those files. The get_image_files function takes a path, and
returns a list of all of the images in that path (recursively, by default):
get_items=get_image_files
Often, datasets that you download will already have a validation set defined. Some‐
times this is done by placing the images for the training and validation sets into dif‐
ferent folders. Sometimes it is done by providing a CSV file in which each filename is
listed along with which dataset it should be in. There are many ways that this can be
done, and fastai provides a general approach that allows you to use one of its prede‐
fined classes for this or to write your own.
In this case, we want to split our training and validation sets randomly. However, we
would like to have the same training/validation split each time we run this notebook,
so we fix the random seed (computers don’t really know how to create random num‐
bers at all, but simply create lists of numbers that look random; if you provide the
same starting point for that list each time—called the <i>seed—then</i> you will get the
exact same list each time).
splitter=RandomSplitter(valid_pct=0.2, seed=42)
The independent variable is often referred to as x, and the dependent variable is often
referred to as y. Here, we are telling fastai what function to call to create the labels in
our dataset:
get_y=parent_label
parent_label is a function provided by fastai that simply gets the name of the folder
a file is in. Because we put each of our bear images into folders based on the type of
bear, this is going to give us the labels that we need.
Our images are all different sizes, and this is a problem for deep learning: we don’t
feed the model one image at a time but several of them (what we call a <i>mini-batch).</i> To
group them in a big array (usually called a <i>tensor)</i> that is going to go through our
model, they all need to be of the same size. So, we need to add a transform that will
resize these images to the same size. <i>Item</i> <i>transforms</i> are pieces of code that run on
each individual item, whether it be an image, category, or so forth. fastai includes
many predefined transforms; we use the Resize transform here and specify a size of
128 pixels:
item_tfms=Resize(128)"|mini-batch; DataLoaders; dependent variable; process end-to-end; independent variable; item transforms; image sizes same; predictions; random seed for validation set selection; seed for validation set selection; Transforms; splitting from training set
"and then looking at the first row of the independent variable, which should be the
start of the first text:
' '.join(num.vocab[o] <b>for</b> o <b>in</b> x[0][:20])
'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently
> sit around for a'
The dependent variable is the same thing offset by one token:
' '.join(num.vocab[o] <b>for</b> o <b>in</b> y[0][:20])
'xxmaj this movie , which i just xxunk at the video store , has apparently sit
> around for a couple'
This concludes all the preprocessing steps we need to apply to our data. We are now
ready to train our text classifier.
<header><largefont><b>Training</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Text</b></largefont> <largefont><b>Classifier</b></largefont></header>
As we saw at the beginning of this chapter, there are two steps to training a state-of-
the-art text classifier using transfer learning: first we need to fine-tune our language
model pretrained on Wikipedia to the corpus of IMDb reviews, and then we can use
that model to train a classifier.
As usual, let’s start with assembling our data.
<header><largefont><b>Language</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>Using</b></largefont> <largefont><b>DataBlock</b></largefont></header>
fastai handles tokenization and numericalization automatically when TextBlock is
passed to DataBlock. All of the arguments that can be passed to Tokenizer and
Numericalize can also be passed to TextBlock . In the next chapter, we’ll discuss the
easiest ways to run each of these steps separately, to ease debugging, but you can
always just debug by running them manually on a subset of your data as shown in the
previous sections. And don’t forget about DataBlock’s handy summary method, which
is very useful for debugging data issues.
Here’s how we use TextBlock to create a language model, using fastai’s defaults:
get_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])
dls_lm = DataBlock(
blocks=TextBlock.from_folder(path, is_lm=True),
get_items=get_imdb, splitter=RandomSplitter(0.1)
).dataloaders(path, path=path, bs=128, seq_len=80)
One thing that’s different from previous types we’ve used in DataBlock is that we’re
not just using the class directly (i.e., TextBlock(...) , but instead are calling a <i>class</i>
<i>method.</i> A class method is a Python method that, as the name suggests, belongs to a
<i>class</i> rather than an <i>object.</i> (Be sure to search online for more information about class"|batch operations; class methods; DataBlock; language model using; debugging; language model using DataBlock; methods; natural language processing (NLP); training text classifier; Python; debugging text dataset; TextBlock; tokenization; training
"Jupyter notebooks can be in one of two modes: edit mode or command mode. In edit
mode, typing on your keyboard enters the letters into the cell in the usual way. How‐
ever, in command mode, you will not see any flashing cursor, and each key on your
keyboard will have a special function.
Before continuing, press the Escape key on your keyboard to switch to command
mode (if you are already in command mode, this does nothing, so press it now just in
case). To see a complete list of all the functions available, press H; press Escape to
remove this help screen. Notice that in command mode, unlike in most programs,
commands do not require you to hold down Control, Alt, or similar—you simply
press the required letter key.
You can make a copy of a cell by pressing C (the cell needs to be selected first, indica‐
ted with an outline around it; if it is not already selected, click it once). Then press V
to paste a copy of it.
Click the cell that begins with the line “# CLICK ME” to select it. The first character
in that line indicates that what follows is a comment in Python, so it is ignored when
executing the cell. The rest of the cell is, believe it or not, a complete system for creat‐
ing and training a state-of-the-art model for recognizing cats versus dogs. So, let’s
train it now! To do so, just press Shift-Enter on your keyboard, or click the Play but‐
ton on the toolbar. Then wait a few minutes while the following things happen:
1. A dataset called the Oxford-IIIT Pet Dataset that contains 7,349 images of cats
and dogs from 37 breeds will be downloaded from the fast.ai datasets collection
to the GPU server you are using, and will then be extracted.
2. A <i>pretrained</i> <i>model</i> that has already been trained on 1.3 million images using a
competition-winning model will be downloaded from the internet.
3. The pretrained model will be <i>fine-tuned</i> using the latest advances in transfer
learning to create a model that is specially customized for recognizing dogs and
cats.
The first two steps need to be run only once on your GPU server. If you run the cell
again, it will use the dataset and model that have already been downloaded, rather
than downloading them again. Let’s take a look at the contents of the cell and the
results (Table 1-2):
<i>#</i> <i>CLICK</i> <i>ME</i>
<b>from</b> <b>fastai.vision.all</b> <b>import</b> *
path = untar_data(URLs.PETS)/'images'
<b>def</b> is_cat(x): <b>return</b> x[0].isupper()
dls = ImageDataLoaders.from_name_func(
path, get_image_files(path), valid_pct=0.2, seed=42,
label_func=is_cat, item_tfms=Resize(224))"|beginning; cats and dogs first model; cells in notebooks; first cell CLICK ME; table output by; pet images; dogs and cats first model; escape key for command/edit mode; fine-tuning models; first model; fine-tuning; H for help; help by pressing H; notebooks; outputs; pet images dataset; pretrained models; fine-tuning first model
"Let’s create a mini-batch of size 4 for testing:
batch = train_x[:4]
batch.shape
torch.Size([4, 784])
preds = linear1(batch)
preds
tensor([[-11.1002],
[ 5.9263],
[ 9.9627],
[ -8.1484]], grad_fn=<AddBackward0>)
loss = mnist_loss(preds, train_y[:4])
loss
tensor(0.5006, grad_fn=<MeanBackward0>)
Now we can calculate the gradients:
loss.backward()
weights.grad.shape,weights.grad.mean(),bias.grad
(torch.Size([784, 1]), tensor(-0.0001), tensor([-0.0008]))
Let’s put that all in a function:
<b>def</b> calc_grad(xb, yb, model):
preds = model(xb)
loss = mnist_loss(preds, yb)
loss.backward()
And test it:
calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
(tensor(-0.0002), tensor([-0.0015]))
But look what happens if we call it twice:
calc_grad(batch, train_y[:4], linear1)
weights.grad.mean(),bias.grad
(tensor(-0.0003), tensor([-0.0023]))
The gradients have changed! The reason for this is that loss.backward <i>adds</i> the gra‐
dients of loss to any gradients that are currently stored. So, we have to set the current
gradients to 0 first:
weights.grad.zero_()
bias.grad.zero_();"|numerical digit classifier; stochastic gradient descent (SGD)
"above the point where you are. We have found this to be useful when developing the
fastai library.
If you ever have any questions about a fastai method, you should use the function
doc , passing it the method name:
doc(learn.predict)
A window pops up containing a brief one-line explanation. The “Show in docs” link
takes you to the full documentation, where you’ll find all the details and lots of exam‐
ples. Also, most of fastai’s methods are just a handful of lines, so you can click the
“source” link to see exactly what’s going on behind the scenes.
Let’s move on to something much less sexy, but perhaps significantly more widely
commercially useful: building models from plain <i>tabular</i> data.
<b>Jargon:Tabular</b>
Data that is in the form of a table, such as from a spreadsheet, data‐
base, or a comma-separated values (CSV) file. A tabular model is a
model that tries to predict one column of a table based on informa‐
tion in other columns of the table.
It turns out that looks very similar too. Here is the code necessary to train a model
that will predict whether a person is a high-income earner, based on their socioeco‐
nomic background:
<b>from</b> <b>fastai.tabular.all</b> <b>import</b> *
path = untar_data(URLs.ADULT_SAMPLE)
dls = TabularDataLoaders.from_csv(path/'adult.csv', path=path, y_names=""salary"",
cat_names = ['workclass', 'education', 'marital-status', 'occupation',
'relationship', 'race'],
cont_names = ['age', 'fnlwgt', 'education-num'],
procs = [Categorify, FillMissing, Normalize])
learn = tabular_learner(dls, metrics=accuracy)"|database data for models; doc for method documentation; fastai software library; methods; models; spreadsheet data for models; tabular data for models; web resources
"if we name our inputs <i>x</i> ,⋯,x , our weights <i>w</i> ,⋯,w , and our bias <i>b.</i> In code
1 <i>n</i> 1 <i>n</i>
this translates into the following:
output = sum([x*w <b>for</b> x,w <b>in</b> zip(inputs,weights)]) + bias
This output is then fed into a nonlinear function called an <i>activation</i> <i>function</i> before
being sent to another neuron. In deep learning, the most common of these is the <i>rec‐</i>
<i>tified</i> <i>linear</i> <i>unit,</i> or <i>ReLU,</i> which, as we’ve seen, is a fancy way of saying this:
<b>def</b> relu(x): <b>return</b> x <b>if</b> x >= 0 <b>else</b> 0
A deep learning model is then built by stacking a lot of those neurons in successive
layers. We create a first layer with a certain number of neurons (known as the <i>hidden</i>
<i>size)</i> and link all the inputs to each of those neurons. Such a layer is often called a
<i>fully</i> <i>connected</i> <i>layer</i> or a <i>dense</i> <i>layer</i> (for densely connected), or a <i>linear</i> <i>layer.</i>
input weight,
It requires you to compute, for each and each neuron with a given the
dot product:
sum([x*w <b>for</b> x,w <b>in</b> zip(input,weight)])
If you have done a little bit of linear algebra, you may remember that having a lot of
those dot products happens when you do a <i>matrix</i> <i>multiplication.</i> More precisely, if
our inputs are in a matrix x with a size of batch_size by n_inputs, and if we have
grouped the weights of our neurons in a matrix w of size n_neurons by n_inputs
(each neuron must have the same number of weights as it has inputs) as well as all the
biases in a vector b of size n_neurons , then the output of this fully connected layer is
y = x @ w.t() + b
where @ represents the matrix product and w.t() is the transpose matrix of w. The
output y is then of size batch_size by n_neurons, and in position (i,j) we have this
(for the mathy folks out there):
<i>n</i>
<i>y</i> = <largefont>∑</largefont> <i>x</i> <i>w</i> + <i>b</i>
<i>i,</i> <i>j</i> <i>i,k</i> <i>k,</i> <i>j</i> <i>j</i>
<i>k</i> = 1
Or in code:
y[i,j] = sum([a * b <b>for</b> a,b <b>in</b> zip(x[i,:],w[j,:])]) + b[j]
The transpose is necessary because in the mathematical definition of the matrix prod‐
uct m @ n , the coefficient (i,j) is as follows:
sum([a * b <b>for</b> a,b <b>in</b> zip(m[i,:],n[:,j])])
So the very basic operation we need is a matrix multiplication, as it’s what is hidden in
the core of a neural net."|building layer from scratch
"Now you know what those pictures in Chapter 1 of “what a neural net learns” from
the Zeiler and Fergus paper mean! As a reminder, this is their picture of some of the
layer 1 weights:
This is taking the three slices of the convolutional kernel, for each output feature, and
displaying them as images. We can see that even though the creators of the neural net
never explicitly created kernels to find edges, for instance, the neural net automati‐
cally discovered these features using SGD.
Now let’s see how we can train these CNNs, and show you all the techniques fastai
uses under the hood for efficient training.
<header><largefont><b>Improving</b></largefont> <largefont><b>Training</b></largefont> <largefont><b>Stability</b></largefont></header>
Since we are so good at recognizing 3s from 7s, let’s move on to something harder—
recognizing all 10 digits. That means we’ll need to use MNIST instead of
MNIST_SAMPLE:
path = untar_data(URLs.MNIST)
path.ls()
(#2) [Path('testing'),Path('training')]
The data is in two folders named <i>training</i> and <i>testing,</i> so we have to tell
GrandparentSplitter train valid).
about that (it defaults to and We do that in the
get_dls function, which we define to make it easy to change our batch size later:
<b>def</b> get_dls(bs=64):
<b>return</b> DataBlock(
blocks=(ImageBlock(cls=PILImageBW), CategoryBlock),
get_items=get_image_files,
splitter=GrandparentSplitter('training','testing'),
get_y=parent_label,
batch_tfms=Normalize()
).dataloaders(path, bs=bs)
dls = get_dls()"|convolutional neural network (CNN); building a CNN; handwritten digits; MNIST handwritten digits dataset; handwritten digits dataset
"<i>Figure</i> <i>7-1.</i> <i>An</i> <i>excerpt</i> <i>from</i> <i>the</i> <i>Mixup</i> <i>paper</i>
<header><largefont><b>Papers</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Math</b></largefont></header>
We’re going to be looking at more and more research papers from here on in the
book. Now that you have the basic jargon, you might be surprised to discover how
much of them you can understand, with a little practice! One issue you’ll notice is
that Greek letters, such as <i>λ,</i> appear in most papers. It’s a good idea to learn the names
of all the Greek letters, since otherwise it’s hard to read the papers to yourself and
remember them (or to read code based on them, since code often uses the names of
the Greek letters spelled out, such as lambda).
The bigger issue with papers is that they use math, instead of code, to explain what’s
going on. If you don’t have much of a math background, this will likely be intimidat‐
ing and confusing at first. But remember: what is being shown in the math is some‐
thing that will be implemented in code. It’s just another way of talking about the same
thing! After reading a few papers, you’ll pick up more and more of the notation. If
you don’t know what a symbol is, try looking it up in Wikipedia’s list of mathematical
symbols or drawing it in Detexify, which (using machine learning!) will find the
name of your hand-drawn symbol. Then you can search online for that name to find
out what it’s for.
Figure 7-2 shows what it looks like when we take a <i>linear</i> <i>combination</i> of images, as
done in Mixup.
<i>Figure</i> <i>7-2.</i> <i>Mixing</i> <i>a</i> <i>church</i> <i>and</i> <i>a</i> <i>gas</i> <i>station</i>"|Greek letters; research papers; web resources
"df_dom = pd.concat([xs_final, valid_xs_final])
is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final))
m = rf(df_dom, is_valid)
rf_feat_importance(m, df_dom)[:6]
<b>cols</b> <b>imp</b>
<b>5</b> saleElapsed 0.859446
<b>9</b>
SalesID 0.119325
<b>13</b> MachineID 0.014259
<b>0</b> YearMade 0.001793
<b>8</b> fiModelDesc 0.001740
<b>11</b> Enclosure 0.000657
This shows that three columns differ significantly between the training and validation
sets: saleElapsed, SalesID, and MachineID. It’s fairly obvious why this is the case for
saleElapsed:
it’s the number of days between the start of the dataset and each row, so
it directly encodes the date. The difference in SalesID suggests that identifiers for
auction sales might increment over time. MachineID suggests something similar
might be happening for individual items sold in those auctions.
Let’s get a baseline of the original random forest model’s RMSE, and then determine
the effect of removing each of these columns in turn:
m = rf(xs_final, y)
<b>print('orig',</b> m_rmse(m, valid_xs_final, valid_y))
<b>for</b> c <b>in</b> ('SalesID','saleElapsed','MachineID'):
m = rf(xs_final.drop(c,axis=1), y)
<b>print(c,</b> m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))
orig 0.232795
SalesID 0.23109
saleElapsed 0.236221
MachineID 0.233492
It looks like we should be able to remove SalesID and MachineID without losing any
accuracy. Let’s check:
time_vars = ['SalesID','MachineID']
xs_final_time = xs_final.drop(time_vars, axis=1)
valid_xs_time = valid_xs_final.drop(time_vars, axis=1)
m = rf(xs_final_time, y)
m_rmse(m, valid_xs_time, valid_y)
0.231307"|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"The various versions of the models (ResNet-18, -34, -50, etc.) just change the number
of blocks in each of those groups. This is the definition of a ResNet-18:
rn = ResNet(dls.c, [2,2,2,2])
Let’s train it for a little bit and see how it fares compared to the previous model:
learn = get_learner(rn)
learn.fit_one_cycle(5, 3e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 1.673882 1.828394 0.413758 00:13
1 1.331675 1.572685 0.518217 00:13
2 1.087224 1.086102 0.650701 00:13
3 0.900428 0.968219 0.684331 00:12
4 0.760280 0.782558 0.757197 00:12
Even though we have more channels (and our model is therefore even more accu‐
rate), our training is just as fast as before thanks to our optimized stem.
To make our model deeper without taking too much compute or memory, we can use
another kind of layer introduced by the ResNet paper for ResNets with a depth of 50
or more: the bottleneck layer.
<header><largefont><b>Bottleneck</b></largefont> <largefont><b>Layers</b></largefont></header>
Instead of stacking two convolutions with a kernel size of 3, bottleneck layers use
three convolutions: two 1×1 (at the beginning and the end) and one 3×3, as shown on
the right in Figure 14-4.
<i>Figure</i> <i>14-4.</i> <i>Comparison</i> <i>of</i> <i>regular</i> <i>and</i> <i>bottleneck</i> <i>ResNet</i> <i>blocks</i> <i>(courtesy</i> <i>of</i> <i>Kaiming</i>
<i>He</i> <i>et</i> <i>al.)</i>"|ResNet architecture; building state-of-the-art ResNet; ResNet-18
"strategy for dealing with this is by being careful about initialization, which is a topic
we’ll investigate in Chapter 17.
For RNNs, two types of layers are frequently used to avoid exploding activations:
<i>gated</i> <i>recurrent</i> <i>units</i> (GRUs) and <i>long</i> <i>short-term</i> <i>memory</i> (LSTM) layers. Both of
these are available in PyTorch and are drop-in replacements for the RNN layer. We
will cover only LSTMs in this book; plenty of good tutorials online explain GRUs,
which are a minor variant on the LSTM design.
<header><largefont><b>LSTM</b></largefont></header>
LSTM is an architecture that was introduced back in 1997 by Jürgen Schmidhuber
and Sepp Hochreiter. In this architecture, there are not one, but two, hidden states. In
our base RNN, the hidden state is the output of the RNN at the previous time step.
That hidden state is then responsible for two things:
• Having the right information for the output layer to predict the correct next
token
• Retaining memory of everything that happened in the sentence
Consider, for example, the sentences “Henry has a dog and he likes his dog very
much” and “Sophie has a dog and she likes her dog very much.” It’s very clear that the
RNN needs to remember the name at the beginning of the sentence to be able to pre‐
dict <i>he/she</i> or <i>his/her.</i>
In practice, RNNs are really bad at retaining memory of what happened much earlier
in the sentence, which is the motivation to have another hidden state (called <i>cell</i> <i>state)</i>
in the LSTM. The cell state will be responsible for keeping <i>long</i> <i>short-term</i> <i>memory,</i>
while the hidden state will focus on the next token to predict. Let’s take a closer look
at how this is achieved and build an LSTM from scratch.
<header><largefont><b>Building</b></largefont> <largefont><b>an</b></largefont> <largefont><b>LSTM</b></largefont> <largefont><b>from</b></largefont> <largefont><b>Scratch</b></largefont></header>
In order to build an LSTM, we first have to understand its architecture. Figure 12-9
shows its inner structure."|building from scratch; multilayer RNNs; LSTM language model
"<i>probabilistic</i> <i>matrix</i> <i>factorization</i> (PMF). Another approach, which generally works
similarly well given the same data, is deep learning.
<header><largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Collaborative</b></largefont> <largefont><b>Filtering</b></largefont></header>
To turn our architecture into a deep learning model, the first step is to take the results
of the embedding lookup and concatenate those activations together. This gives us a
matrix that we can then pass through linear layers and nonlinearities in the usual way.
Since we’ll be concatenating the embedding matrices, rather than taking their dot
product, the two embedding matrices can have different sizes (different numbers of
latent factors). fastai has a function get_emb_sz that returns recommended sizes for
embedding matrices for your data, based on a heuristic that fast.ai has found tends to
work well in practice:
embs = get_emb_sz(dls)
embs
[(944, 74), (1635, 101)]
Let’s implement this class:
<b>class</b> <b>CollabNN(Module):</b>
<b>def</b> <b>__init__(self,</b> user_sz, item_sz, y_range=(0,5.5), n_act=100):
self.user_factors = Embedding(*user_sz)
self.item_factors = Embedding(*item_sz)
self.layers = nn.Sequential(
nn.Linear(user_sz[1]+item_sz[1], n_act),
nn.ReLU(),
nn.Linear(n_act, 1))
self.y_range = y_range
<b>def</b> forward(self, x):
embs = self.user_factors(x[:,0]),self.item_factors(x[:,1])
x = self.layers(torch.cat(embs, dim=1))
<b>return</b> sigmoid_range(x, *self.y_range)
And use it to create a model:
model = CollabNN(*embs)
CollabNN creates our Embedding layers in the same way as previous classes in this
chapter, except that we now use the embs sizes. self.layers is identical to the mini-
forward,
neural net we created in Chapter 4 for MNIST. Then, in we apply the
embeddings, concatenate the results, and pass this through the mini-neural net.
Finally, we apply sigmoid_range as we have in previous models.
Let’s see if it trains:
learn = Learner(dls, model, loss_func=MSELossFlat())
learn.fit_one_cycle(5, 5e-3, wd=0.01)"|deep learning model; built from scratch; embedding from scratch
"learn.show_results(ds_idx=1, max_n=3, figsize=(6,8))
It’s quite amazing that with just a few minutes of computation, we’ve created such an
accurate key points model, and without any special domain-specific application. This
is the power of building on flexible APIs and using transfer learning! It’s particularly
striking that we’ve been able to use transfer learning so effectively, even between
totally different tasks; our pretrained model was trained to do image classification,
and we fine-tuned for image regression.
<header><largefont><b>Conclusion</b></largefont></header>
In problems that are at first glance completely different (single-label classification,
multi-label classification, and regression), we end up using the same model with just
different numbers of outputs. The loss function is the one thing that changes, which
is why it’s important to double-check that you are using the right loss function for
your problem.
fastai will automatically try to pick the right one from the data you built, but if you
are using pure PyTorch to build your DataLoaders, make sure you think hard about
your choice of loss function, and remember that you most probably want the
following:
• nn.CrossEntropyLoss for single-label classification
• nn.BCEWithLogitsLoss for multi-label classification
nn.MSELoss
• for regression"|BCEWithLogitsLoss; loss function selected by; loss; fastai selecting function; MSELoss
"All our training up until now has been done at size 224. We could have begun train‐
ing at a smaller size before going to that. This is called <i>progressive</i> <i>resizing.</i>
<header><largefont><b>Progressive</b></largefont> <largefont><b>Resizing</b></largefont></header>
When fast.ai and its team of students won the DAWNBench competition in 2018, one
of the most important innovations was something very simple: start training using
small images, and end training using large images. Spending most of the epochs
training with small images helps training complete much faster. Completing training
using large images makes the final accuracy much higher. We call this approach <i>pro‐</i>
<i>gressive</i> <i>resizing.</i>
<b>Jargon:ProgressiveResizing</b>
Gradually using larger and larger images as you train.
As we have seen, the kinds of features that are learned by convolutional neural net‐
works are not in any way specific to the size of the image—early layers find things like
edges and gradients, and later layers may find things like noses and sunsets. So, when
we change image size in the middle of training, it doesn’t mean that we have to find
totally different parameters for our model.
But clearly there are some differences between small images and big ones, so we
shouldn’t expect our model to continue working exactly as well, with no changes at
all. Does this remind you of something? When we developed this idea, it reminded us
of transfer learning! We are trying to get our model to learn to do something a little
bit different from what it has learned to do before. Therefore, we should be able to
use the fine_tune method after we resize our images.
Progressive resizing has an additional benefit: it is another form of data augmenta‐
tion. Therefore, you should expect to see better generalization of your models that are
trained with progressive resizing.
To implement progressive resizing, it is most convenient if you first create a get_dls
function that takes an image size and a batch size, as we did in the previous section,
and returns your DataLoaders.
Now you can create your DataLoaders with a small size and use and fit_one_cycle
in the usual way, training for fewer epochs than you might otherwise do:
dls = get_dls(128, 128)
learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(),
metrics=accuracy)
learn.fit_one_cycle(4, 3e-3)"|progressive resizing as; images sized progressively; progressive resizing
"rm_useless_spaces
Removes all repetitions of the space character
replace_all_caps
Lowercases a word written in all caps and adds a special token for all caps
(xxcap) in front of it
replace_maj
Lowercases a capitalized word and adds a special token for capitalized ( xxmaj ) in
front of it
lowercase
Lowercases all text and adds a special token at the beginning ( xxbos ) and/or the
end (xxeos)
Let’s take a look at a few of them in action:
coll_repr(tkn('&copy; Fast.ai www.fast.ai/INDEX'), 31)
""(#11) ['xxbos','©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','ind
> ex'...]""
Now let’s take a look at how subword tokenization would work.
<header><largefont><b>Subword</b></largefont> <largefont><b>Tokenization</b></largefont></header>
In addition to the <i>word</i> <i>tokenization</i> approach seen in the preceding section, another
popular tokenization method is <i>subword</i> <i>tokenization.</i> Word tokenization relies on an
assumption that spaces provide a useful separation of components of meaning in a
sentence. However, this assumption is not always appropriate. For instance, consider
this sentence: (“My name is Jeremy Howard” in Chinese). That’s
我的名字是郝杰瑞
not going to work very well with a word tokenizer, because there are no spaces in it!
Languages like Chinese and Japanese don’t use spaces, and in fact they don’t even
have a well-defined concept of a “word.” Other languages, like Turkish and Hungar‐
ian, can add many subwords together without spaces, creating very long words that
include a lot of separate pieces of information.
To handle these cases, it’s generally best to use subword tokenization. This proceeds
in two steps:
1. Analyze a corpus of documents to find the most commonly occurring groups of
letters. These become the vocab.
2. Tokenize the corpus using this vocab of <i>subword</i> <i>units.</i>
Let’s look at an example. For our corpus, we’ll use the first 2,000 movie reviews:
txts = L(o.open().read() <b>for</b> o <b>in</b> files[:2000])"|lowercase rule; natural language processing (NLP); replace_all_caps; replace_maj; rm_useless_spaces; subword tokenization; tokenization; word tokenization
"The following attributes are added by TrainEvalCallback and should be available
unless you went out of your way to remove that callback:
train_iter
The number of training iterations done since the beginning of this training
pct_train
The percentage of training iterations completed (from 0 to 1)
training
A flag to indicate whether we’re in training mode
The following attribute is added by Recorder and should be available unless you went
out of your way to remove that callback:
smooth_loss
An exponentially averaged version of the training loss
Callbacks can also interrupt any part of the training loop by using a system of
exceptions.
<header><largefont><b>Callback</b></largefont> <largefont><b>Ordering</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Exceptions</b></largefont></header>
Sometimes callbacks need to be able to tell fastai to skip over a batch or an epoch, or
stop training altogether. For instance, consider TerminateOnNaNCallback. This handy
callback will automatically stop training anytime the loss becomes infinite or NaN (not
<i>a</i> <i>number).</i> Here’s the fastai source for this callback:
<b>class</b> <b>TerminateOnNaNCallback(Callback):</b>
run_before=Recorder
<b>def</b> after_batch(self):
<b>if</b> torch.isinf(self.loss) <b>or</b> torch.isnan(self.loss):
<b>raise</b> CancelFitException
The line raise CancelFitException tells the training loop to interrupt training at
this point. The training loop catches this exception and does not run any further
training or validation. The callback control flow exceptions available are as follows:
CancelFitException
Skip the rest of this batch and go to after_batch .
CancelEpochException
after_train.
Skip the rest of the training part of the epoch and go to
CancelTrainException
Skip the rest of the validation part of the epoch and go to after_validate."|callbacks; training
"Although this is an easy enough algorithm to implement yourself (and it is a good
exercise to do so), we can save some time by using the implementation built into
sklearn.
First, however, we need to do a little data preparation.
<b>AlexisSays</b>
Here’s a productive question to ponder. If you consider that the
procedure for defining a decision tree essentially chooses one
<i>sequence</i> <i>of</i> <i>splitting</i> <i>questions</i> <i>about</i> <i>variables,</i> you might ask your‐
self, how do we know this procedure chooses the <i>correct</i> <i>sequence?</i>
The rule is to choose the splitting question that produces the best
split (i.e., that most accurately separates the items into two distinct
categories), and then to apply the same rule to the groups that split
produces, and so on. This is known in computer science as a
“greedy” approach. Can you imagine a scenario in which asking a
“less powerful” splitting question would enable a better split down
the road (or should I say down the trunk!) and lead to a better
result overall?
<header><largefont><b>Handling</b></largefont> <largefont><b>Dates</b></largefont></header>
The first piece of data preparation we need to do is to enrich our representation of
dates. The fundamental basis of the decision tree that we just described is <i>bisection—</i>
dividing a group into two. We look at the ordinal variables and divide the dataset
based on whether the variable’s value is greater (or lower) than a threshold, and we
look at the categorical variables and divide the dataset based on whether the variable’s
level is a particular level. So this algorithm has a way of dividing the dataset based on
both ordinal and categorical data.
But how does this apply to a common data type, the date? You might want to treat a
date as an ordinal value, because it is meaningful to say that one date is greater than
another. However, dates are a bit different from most ordinal values in that some
dates are qualitatively different from others in a way that that is often relevant to the
systems we are modeling.
To help our algorithm handle dates intelligently, we’d like our model to know more
than whether a date is more recent or less recent than another. We might want our
model to make decisions based on that date’s day of the week, on whether a day is a
holiday, on what month it is in, and so forth. To do this, we replace every date column
with a set of date metadata columns, such as holiday, day of week, and month. These
columns provide categorical data that we suspect will be useful."|tabular dataset prep; date handling; date handling in tabular data; decision trees; tabular data for models; training
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>19</b></largefont></header>
<header><largefont><b>A</b></largefont> <largefont><b>fastai</b></largefont> <largefont><b>Learner</b></largefont> <largefont><b>from</b></largefont> <largefont><b>Scratch</b></largefont></header>
This final chapter (other than the conclusion and the online chapters) is going to look
a bit different. It contains far more code and far less prose than the previous chapters.
We will introduce new Python keywords and libraries without discussing them. This
chapter is meant to be the start of a significant research project for you. You see, we
are going to implement many of the key pieces of the fastai and PyTorch APIs from
scratch, building on nothing other than the components that we developed in Chap‐
Learner
ter 17! The key goal here is to end up with your own class and some call‐
backs—enough to be able to train a model on Imagenette, including examples of each
of the key techniques we’ve studied. On the way to building Learner , we will create
Module, Parameter DataLoader
our own versions of and a parallel so you’ll have a
very good idea of what those PyTorch classes do.
The end-of-chapter questionnaire is particularly important for this chapter. This is
where we will be pointing you in the many interesting directions that you could take,
using this chapter as your starting point. We suggest that you follow along with this
chapter on your computer, and do lots of experiments, web searches, and whatever
else you need to understand what’s going on. You’ve built up the skills and expertise
to do this in the rest of this book, so we think you are going to do great!
Let’s begin by gathering (manually) some data.
<header><largefont><b>Data</b></largefont></header>
Have a look at the source to untar_data to see how it works. We’ll use it here to
access the 160-pixel version of Imagenette for use in this chapter:
path = untar_data(URLs.IMAGENETTE_160)"|Learner; untar_data
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>17</b></largefont></header>
<header><largefont><b>A</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Net</b></largefont> <largefont><b>from</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Foundations</b></largefont></header>
This chapter begins a journey where we will dig deep into the internals of the models
we used in the previous chapters. We will be covering many of the same things we’ve
seen before, but this time around we’ll be looking much more closely at the imple‐
mentation details, and much less closely at the practical issues of how and why things
are as they are.
We will build everything from scratch, using only basic indexing into a tensor. We’ll
write a neural net from the ground up, and then implement backpropagation man‐
ually so we know exactly what’s happening in PyTorch when we call loss.backward.
We’ll also see how to extend PyTorch with custom <i>autograd</i> functions that allow us to
specify our own forward and backward computations.
<header><largefont><b>Building</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Net</b></largefont> <largefont><b>Layer</b></largefont> <largefont><b>from</b></largefont> <largefont><b>Scratch</b></largefont></header>
Let’s start by refreshing our understanding of how matrix multiplication is used in a
basic neural network. Since we’re building everything up from scratch, we’ll use noth‐
ing but plain Python initially (except for indexing into PyTorch tensors), and then
replace the plain Python with PyTorch functionality after we’ve seen how to create it.
<header><largefont><b>Modeling</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Neuron</b></largefont></header>
A neuron receives a given number of inputs and has an internal weight for each of
them. It sums those weighted inputs to produce an output and adds an inner bias. In
math, this can be written as
<i>n</i>
<largefont>∑</largefont>
<i>out</i> = <i>x</i> <i>w</i> + <i>b</i>
<i>i</i> <i>i</i>
<i>i</i> = 1"|building layer from scratch; modeling a neuron
"<header><largefont><b>Saving</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Loading</b></largefont> <largefont><b>Models</b></largefont></header>
You can easily save the state of your model like so:
learn.save('1epoch')
This will create a file in <i>learn.path/models/</i> named <i>1epoch.pth.</i> If you want to load
your model in another machine after creating your Learner the same way, or resume
training later, you can load the content of this file as follows:
learn = learn.load('1epoch')
Once the initial training has completed, we can continue fine-tuning the model after
unfreezing:
learn.unfreeze()
learn.fit_one_cycle(10, 2e-3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>perplexity</b> <b>time</b>
0 3.893486 3.772820 0.317104 43.502548 12:37
1 3.820479 3.717197 0.323790 41.148880 12:30
2 3.735622 3.659760 0.330321 38.851997 12:09
3 3.677086 3.624794 0.333960 37.516987 12:12
4 3.636646 3.601300 0.337017 36.645859 12:05
5 3.553636 3.584241 0.339355 36.026001 12:04
6 3.507634 3.571892 0.341353 35.583862 12:08
7 3.444101 3.565988 0.342194 35.374371 12:08
8 3.398597 3.566283 0.342647 35.384815 12:11
9 3.375563 3.568166 0.342528 35.451500 12:05
Once this is done, we save all of our model except the final layer that converts activa‐
tions to probabilities of picking each token in our vocabulary. The model not includ‐
ing the final layer is called the <i>encoder.</i> We can save it with save_encoder:
learn.save_encoder('finetuned')
<b>Jargon:Encoder</b>
The model not including the task-specific final layer(s). This term
means much the same thing as “body” when applied to vision
CNNs, but “encoder” tends to be more used for NLP and genera‐
tive models.
This completes the second stage of the text classification process: fine-tuning the lan‐
guage model. We can now use it to fine-tune a classifier using the IMDb sentiment"|save method; encoder; fine-tuning models; Learner; load method; models; pretrained language model; fine-tuning pretrained language model; fine-tuning language model
"There’s a top edge at cell 5,7. Let’s repeat our calculation there:
(im3_t[4:7,6:9] * top_edge).sum()
tensor(762.)
There’s a right edge at cell 8,18. What does that give us?
(im3_t[7:10,17:20] * top_edge).sum()
tensor(-29.)
As you can see, this little calculation is returning a high number where the 3×3-pixel
square represents a top edge (i.e., where there are low values at the top of the square
and high values immediately underneath). That’s because the -1 values in our kernel
have little impact in that case, but the 1 values have a lot.
Let’s look a tiny bit at the math. The filter will take any window of size 3×3 in our
images, and if we name the pixel values like this
<i>a1</i> <i>a2</i> <i>a3</i>
<i>a4</i> <i>a5</i> <i>a6</i>
<i>a7</i> <i>a8</i> <i>a9</i>
it will return <i>a1</i> + <i>a2</i> + <i>a3</i> − <i>a7</i> − <i>a8</i> − <i>a9.</i> If we are in a part of the image where <i>a1,</i>
<i>a2,</i> and <i>a3</i> add up to the same as <i>a7,</i> <i>a8,</i> and <i>a9,</i> then the terms will cancel each other
out and we will get 0. However, if <i>a1</i> is greater than <i>a7,</i> <i>a2</i> is greater than <i>a8,</i> and <i>a3</i> is
greater than <i>a9,</i> we will get a bigger number as a result. So this filter detects horizon‐
tal edges—more precisely, edges where we go from bright parts of the image at the
top to darker parts at the bottom.
Changing our filter to have the row of 1s at the top and the –1s at the bottom would
1s –1s
detect horizontal edges that go from dark to light. Putting the and in columns
versus rows would give us filters that detect vertical edges. Each set of weights will
produce a different kind of outcome.
Let’s create a function to do this for one location, and check that it matches our result
from before:
<b>def</b> apply_kernel(row, col, kernel):
<b>return</b> (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()
apply_kernel(5,7,top_edge)
tensor(762.)
But note that we can’t apply it to the corner (e.g., location 0,0), since there isn’t a com‐
plete 3×3 square there."|convolutional neural network (CNN); kernel of convolution
"called <i>R2,</i> although the details aren’t important for this explanation.) We don’t need it
to be very accurate—we’re just going to use it to compare different models, based on
removing some of the possibly redundant columns:
<b>def</b> get_oob(df):
m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,
max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)
m.fit(df, y)
<b>return</b> m.oob_score_
Here’s our baseline:
get_oob(xs_imp)
0.8771039618198545
Now we try removing each of our potentially redundant variables, one at a time:
{c:get_oob(xs_imp.drop(c, axis=1)) <b>for</b> c <b>in</b> (
'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',
'fiModelDesc', 'fiBaseModel',
'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}
{'saleYear': 0.8759666979317242,
'saleElapsed': 0.8728423449081594,
'ProductGroupDesc': 0.877877012281002,
'ProductGroup': 0.8772503407182847,
'fiModelDesc': 0.8756415073829513,
'fiBaseModel': 0.8765165299438019,
'Hydraulics_Flow': 0.8778545895742573,
'Grouser_Tracks': 0.8773718142788077,
'Coupler_System': 0.8778016988955392}
Now let’s try dropping multiple variables. We’ll drop one from each of the tightly
aligned pairs we noticed earlier. Let’s see what that does:
to_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']
get_oob(xs_imp.drop(to_drop, axis=1))
0.8739605718147015
Looking good! This is really not much worse than the model with all the fields. Let’s
create DataFrames without these columns, and save them:
xs_final = xs_imp.drop(to_drop, axis=1)
valid_xs_final = valid_xs_imp.drop(to_drop, axis=1)
(path/'xs_final.pkl').save(xs_final)
(path/'valid_xs_final.pkl').save(valid_xs_final)"|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"We can now train our model. Let’s try setting the accuracy threshold to 0.2 for our
metric:
learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))
learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy_multi</b> <b>time</b>
0 0.903610 0.659728 0.263068 00:07
1 0.724266 0.346332 0.525458 00:07
2 0.415597 0.125662 0.937590 00:07
3 0.254987 0.116880 0.945418 00:07
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy_multi</b> <b>time</b>
0 0.123872 0.132634 0.940179 00:08
1 0.112387 0.113758 0.949343 00:08
2 0.092151 0.104368 0.951195 00:08
Picking a threshold is important. If you pick a threshold that’s too low, you’ll often be
failing to select correctly labeled objects. We can see this by changing our metric and
then calling validate , which returns the validation loss and metrics:
learn.metrics = partial(accuracy_multi, thresh=0.1)
learn.validate()
(#2) [0.10436797887086868,0.93057781457901]
If you pick a threshold that’s too high, you’ll be selecting only the objects about which
the model is very confident:
learn.metrics = partial(accuracy_multi, thresh=0.99)
learn.validate()
(#2) [0.10436797887086868,0.9416930675506592]
We can find the best threshold by trying a few levels and seeing what works best. This
is much faster if we grab the predictions just once:
preds,targs = learn.get_preds()
Then we can call the metric directly. Note that by default get_preds applies the out‐
put activation function (sigmoid, in this case) for us, so we’ll need to tell
accuracy_multi to not apply it:
accuracy_multi(preds, targs, thresh=0.9, sigmoid=False)
TensorMultiCategory(0.9554)"|binary cross entropy loss function; 0s and 1s threshold; binary cross entropy
"w1 = init_params((28*28,30))
b1 = init_params(30)
w2 = init_params((30,1))
b2 = init_params(1)
The key point is that w1 has 30 output activations (which means that w2 must have 30
input activations, so they match). That means that the first layer can construct 30 dif‐
ferent features, each representing a different mix of pixels. You can change that 30 to
anything you like, to make the model more or less complex.
That little function res.max(tensor(0.0)) is called a <i>rectified</i> <i>linear</i> <i>unit,</i> also known
as <i>ReLU.</i> We think we can all agree that <i>rectified</i> <i>linear</i> <i>unit</i> sounds pretty fancy and
complicated…But actually, there’s nothing more to it than res.max(tensor(0.0))—
in other words, replace every negative number with a zero. This tiny function is also
available in PyTorch as F.relu :
plot_function(F.relu)
<b>JeremySays</b>
There is an enormous amount of jargon in deep learning, including
terms like <i>rectified</i> <i>linear</i> <i>unit.</i> The vast majority of this jargon is no
more complicated than can be implemented in a short line of code,
as we saw in this example. The reality is that for academics to get
their papers published, they need to make them sound as impres‐
sive and sophisticated as possible. One way that they do that is to
introduce jargon. Unfortunately, this results in the field becoming
far more intimidating and difficult to get into than it should be.
You do have to learn the jargon, because otherwise papers and
tutorials are not going to mean much to you. But that doesn’t mean
you have to find the jargon intimidating. Just remember, when you
come across a word or phrase that you haven’t seen before, it will
almost certainly turn out to be referring to a very simple concept."|jargon; numerical digit classifier; PyTorch; creating an optimizer; rectified linear unit (ReLU); stochastic gradient descent (SGD)
"widely studied by researchers and used to compare algorithmic changes. Some of
these become household names (at least, among households that train models!), such
as MNIST, CIFAR-10, and ImageNet.
The datasets used in this book have been selected because they provide great exam‐
ples of the kinds of data that you are likely to encounter, and the academic literature
has many examples of model results using these datasets to which you can compare
your work.
Most datasets used in this book took the creators a lot of work to build. For instance,
later in the book we’ll be showing you how to create a model that can translate
between French and English. The key input to this is a French/English parallel text
corpus prepared in 2009 by Professor Chris Callison-Burch of the University of Penn‐
sylvania. This dataset contains over 20 million sentence pairs in French and English.
He built the dataset in a really clever way: by crawling millions of Canadian web pages
(which are often multilingual) and then using a set of simple heuristics to transform
URLs of French content to URLs pointing to the same content in English.
As you look at datasets throughout this book, think about where they might have
come from and how they might have been curated. Then think about what kinds of
interesting datasets you could create for your own projects. (We’ll even take you step
by step through the process of creating your own image dataset soon.)
fast.ai has spent a lot of time creating cut-down versions of popular datasets that are
specially designed to support rapid prototyping and experimentation, and to be easier
to learn with. In this book, we will often start by using one of the cut-down versions
and later scale up to the full-size version (just as we’re doing in this chapter!). This is
how the world’s top practitioners do their modeling in practice; they do most of their
experimentation and prototyping with subsets of their data, and use the full dataset
only when they have a good understanding of what they have to do.
Each of the models we trained showed a training and validation loss. A good valida‐
tion set is one of the most important pieces of the training process. Let’s see why and
learn how to create one.
<header><largefont><b>Validation</b></largefont> <largefont><b>Sets</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Test</b></largefont> <largefont><b>Sets</b></largefont></header>
As we’ve discussed, the goal of a model is to make predictions about data. But the
model training process is fundamentally dumb. If we trained a model with all our
data and then evaluated the model using that same data, we would not be able to tell
how well our model can perform on data it hasn’t seen. Without this very valuable
piece of information to guide us in training our model, there is a very good chance it
would become good at making predictions about that data but would perform poorly
on new data."|Callison-Burch; cut-down versions of popular; datasets cut down; French/English parallel text data
"The third image is built by adding 0.3 times the first one and 0.7 times the second. In
this example, should the model predict “church” or “gas station”? The right answer is
30% church and 70% gas station, since that’s what we’ll get if we take the linear com‐
bination of the one-hot-encoded targets. For instance, suppose we have 10 classes,
and “church” is represented by the index 2 and “gas station” by the index 7. The one-
hot-encoded representations are as follows:
[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
So here is our final target:
[0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]
This all done for us inside fastai by adding a <i>callback</i> to our Learner. Callbacks are
what is used inside fastai to inject custom behavior in the training loop (like a learn‐
ing rate schedule, or training in mixed precision). You’ll be learning all about call‐
backs, including how to make your own, in Chapter 16. For now, all you need to
know is that you use the cbs parameter to Learner to pass callbacks.
Here is how we train a model with Mixup:
model = xresnet50()
learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(),
metrics=accuracy, cbs=Mixup)
learn.fit_one_cycle(5, 3e-3)
What happens when we train a model with data that’s “mixed up” in this way? Clearly,
it’s going to be harder to train, because it’s harder to see what’s in each image. And the
model has to predict two labels per image, rather than just one, as well as figuring out
how much each one is weighted. Overfitting seems less likely to be a problem, how‐
ever, because we’re not showing the same image in each epoch, but are instead show‐
ing a random combination of two images.
Mixup requires far more epochs to train to get better accuracy, compared to other
augmentation approaches we’ve seen. You can try training Imagenette with and
without Mixup by using the <i>examples/train_imagenette.py</i> script in the fastai repo. At
the time of writing, the leaderboard in the Imagenette repo is showing that Mixup is
used for all leading results for trainings of >80 epochs, and for fewer epochs Mixup is
not being used. This is in line with our experience of using Mixup too.
One of the reasons that Mixup is so exciting is that it can be applied to types of data
other than photos. In fact, some people have even shown good results by using
Mixup on activations <i>inside</i> their models, not just on inputs—this allows Mixup to be
used for NLP and other data types too.
There’s another subtle issue that Mixup deals with for us, which is that it’s not actually
possible with the models we’ve seen before for our loss to ever be perfect. The prob‐
lem is that our labels are 1s and 0s, but the outputs of softmax and sigmoid can never
equal 1 or 0. This means training our model pushes our activations ever closer to"|callbacks; image classifier model training; Learner; Mixup data augmentation
"However, using a deeper model is going to require more GPU RAM, so you may need
to lower the size of your batches to avoid an <i>out-of-memory</i> <i>error.</i> This happens when
you try to fit too much inside your GPU and looks like this:
Cuda runtime error: out of memory
You may have to restart your notebook when this happens. The way to solve it is to
use a smaller batch size, which means passing smaller groups of images at any given
time through your model. You can pass the batch size you want to the call by creating
your DataLoaders with bs= .
The other downside of deeper architectures is that they take quite a bit longer to
train. One technique that can speed things up a lot is <i>mixed-precision</i> <i>training.</i> This
refers to using less-precise numbers (half-precision <i>floating</i> <i>point,</i> also called <i>fp16)</i>
where possible during training. As we are writing these words in early 2020, nearly all
current NVIDIA GPUs support a special feature called <i>tensor</i> <i>cores</i> that can dramati‐
cally speed up neural network training, by 2–3×. They also require a lot less GPU
memory. To enable this feature in fastai, just add to_fp16() after your Learner cre‐
ation (you also need to import the module).
You can’t really know the best architecture for your particular problem ahead of time
—you need to try training some. So let’s try a ResNet-50 now with mixed precision:
<b>from</b> <b>fastai2.callback.fp16</b> <b>import</b> *
learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()
learn.fine_tune(6, freeze_epochs=3)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 1.427505 0.310554 0.098782 00:21
1 0.606785 0.302325 0.094723 00:22
2 0.409267 0.294803 0.091340 00:21
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.261121 0.274507 0.083897 00:26
1 0.296653 0.318649 0.084574 00:26
2 0.242356 0.253677 0.069012 00:26
3 0.150684 0.251438 0.065629 00:26
4 0.094997 0.239772 0.064276 00:26
5 0.061144 0.228082 0.054804 00:26
You’ll see here we’ve gone back to using fine_tune, since it’s so handy! We can pass
freeze_epochs
to tell fastai how many epochs to train for while frozen. It will auto‐
matically change learning rates appropriately for most datasets."|out-of-memory error; half-precision floating point (fp16); tensor core support; batch operations out-of-memory error; mixed-precision training; number precision and training; precision of numbers and training; tensor core support by GPUs; training
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>11</b></largefont></header>
<header><largefont><b>Data</b></largefont> <largefont><b>Munging</b></largefont> <largefont><b>with</b></largefont> <largefont><b>fastai’s</b></largefont> <largefont><b>Mid-Level</b></largefont> <largefont><b>API</b></largefont></header>
We have seen what Tokenizer and Numericalize do to a collection of texts, and how
they’re used inside the data block API, which handles those transforms for us directly
using the TextBlock. But what if we want to apply only one of those transforms,
either to see intermediate results or because we have already tokenized texts? More
generally, what can we do when the data block API is not flexible enough to accom‐
modate our particular use case? For this, we need to use fastai’s <i>mid-level</i> <i>API</i> for pro‐
cessing data. The data block API is built on top of that layer, so it will allow you to do
everything the data block API does, and much much more.
<header><largefont><b>Going</b></largefont> <largefont><b>Deeper</b></largefont> <largefont><b>into</b></largefont> <largefont><b>fastai’s</b></largefont> <largefont><b>Layered</b></largefont> <largefont><b>API</b></largefont></header>
The fastai library is built on a <i>layered</i> <i>API.</i> In the very top layer are <i>applications</i> that
allow us to train a model in five lines of code, as we saw in Chapter 1. In the case of
creating DataLoaders for a text classifier, for instance, we used this line:
<b>from</b> <b>fastai.text.all</b> <b>import</b> *
dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')
The factory method TextDataLoaders.from_folder is very convenient when your
data is arranged the exact same way as the IMDb dataset, but in practice, that often
won’t be the case. The data block API offers more flexibility. As we saw in the preced‐
ing chapter, we can get the same result with the following:
path = untar_data(URLs.IMDB)
dls = DataBlock(
blocks=(TextBlock.from_folder(path),CategoryBlock),
get_y = parent_label,
get_items=partial(get_text_files, folders=['train', 'test']),
splitter=GrandparentSplitter(valid_name='test')
).dataloaders(path)"|applications; mid-level API; mid-level API foundation; layered API; fastai layered API; TextDataLoaders.from_folder
"<header><largefont><b>Categorical</b></largefont> <largefont><b>Variables</b></largefont></header>
In the previous chapter, when working with deep learning networks, we dealt with
categorical variables by one-hot encoding them and feeding them to an embedding
layer. The embedding layer helped the model to discover the meaning of the different
levels of these variables (the levels of a categorical variable do not have an intrinsic
meaning, unless we manually specify an ordering using Pandas). In a decision tree,
we don’t have embedding layers—so how can these untreated categorical variables do
anything useful in a decision tree? For instance, how could something like a product
code be used?
The short answer is: it just works! Think about a situation in which one product code
is far more expensive at auction than any other one. In that case, any binary split will
result in that one product code being in some group, and that group will be more
expensive than the other group. Therefore, our simple decision tree building algo‐
rithm will choose that split. Later, during training, the algorithm will be able to fur‐
ther split the subgroup that contains the expensive product code, and over time, the
tree will home in on that one expensive product.
It is also possible to use one-hot encoding to replace a single categorical variable with
multiple one-hot-encoded columns, where each column represents a possible level of
the variable. Pandas has a get_dummies method that does just that.
However, there is not really any evidence that such an approach improves the end
result. So, we generally avoid it where possible, because it does end up making your
dataset harder to work with. In 2019, this issue was explored in the paper “Splitting
on Categorical Predictors in Random Forests” by Marvin Wright and Inke König:
The standard approach for nominal predictors is to consider all 2 <i>k−1</i> − 1 2-partitions
of the <i>k</i> predictor categories. However, this exponential relationship produces a large
number of potential splits to be evaluated, increasing computational complexity and
restricting the possible number of categories in most implementations. For binary clas‐
sification and regression, it was shown that ordering the predictor categories in each
split leads to exactly the same splits as the standard approach. This reduces computa‐
tional complexity because only <i>k</i> − 1 splits have to be considered for a nominal predic‐
tor with <i>k</i> categories.
Now that you understand how decision trees work, it’s time for that best-of-both-
worlds solution: random forests."|categorical variables; get_dummies for categorical variables; König; one-hot encoding; embedding categorical variables; Pandas library; Wright
"<b>Addressingdifferenttypesofbias</b>
Different types of bias require different approaches for mitigation. While gathering a
more diverse dataset can address representation bias, this would not help with histor‐
ical bias or measurement bias. All datasets contain bias. There is no such thing as a
completely debiased dataset. Many researchers in the field have been converging on a
set of proposals to enable better documentation of the decisions, context, and
specifics about how and why a particular dataset was created, what scenarios it is
appropriate to use in, and what the limitations are. This way, those using a particular
dataset will not be caught off guard by its biases and limitations.
We often hear the question, “Humans are biased, so does algorithmic bias even mat‐
ter?” This comes up so often, there must be some reasoning that makes sense to the
people who ask it, but it doesn’t seem very logically sound to us! Independently of
whether this is logically sound, it’s important to realize that algorithms (particularly
machine learning algorithms!) and people are different. Consider these points about
machine learning algorithms:
<i>Machine</i> <i>learning</i> <i>can</i> <i>create</i> <i>feedback</i> <i>loops</i>
Small amounts of bias can rapidly increase exponentially because of feedback
loops.
<i>Machine</i> <i>learning</i> <i>can</i> <i>amplify</i> <i>bias</i>
Human bias can lead to larger amounts of machine learning bias.
<i>Algorithms</i> <i>and</i> <i>humans</i> <i>are</i> <i>used</i> <i>differently</i>
Human decision makers and algorithmic decision makers are not used in a plug-
and-play interchangeable way in practice. These examples are given in the list on
the next page.
<i>Technology</i> <i>is</i> <i>power</i>
And with that comes responsibility.
As the Arkansas healthcare example showed, machine learning is often implemented
in practice not because it leads to better outcomes, but because it is cheaper and more
efficient. Cathy O’Neill, in her book <i>Weapons</i> <i>of</i> <i>Math</i> <i>Destruction</i> (Crown), described
a pattern in which the privileged are processed by people, whereas the poor are pro‐
cessed by algorithms. This is just one of a number of ways that algorithms are used
differently than human decision makers. Others include the following:
• People are more likely to assume algorithms are objective or error-free (even if
they’re given the option of a human override).
• Algorithms are more likely to be implemented with no appeals process in place.
• Algorithms are often used at scale."|algorithm buggy; Arkansas healthcare buggy algorithm (ethics); bias; buggy algorithm ethics; ethics; healthcare benefits buggy algorithm (ethics); O’Neill; socioeconomic bias; Weapons of Math Destruction book (O’Neill)
"<header><largefont><b>The</b></largefont> <largefont><b>State</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont></header>
Let’s start by considering whether deep learning can be any good at the problem you
are looking to work on. This section provides a summary of the state of deep learning
at the start of 2020. However, things move very fast, and by the time you read this,
some of these constraints may no longer exist. We will try to keep the book’s website
up-to-date; in addition, a Google search for “what can AI do now” is likely to provide
current information.
<b>Computervision</b>
There are many domains in which deep learning has not been used to analyze images
yet, but those where it has been tried have nearly universally shown that computers
can recognize items in an image at least as well as people can—even specially trained
people, such as radiologists. This is known as <i>object</i> <i>recognition.</i> Deep learning is also
good at recognizing where objects in an image are, and can highlight their locations
and name each found object. This is known as <i>object</i> <i>detection</i> (in a variant of this
that we saw in Chapter 1, every pixel is categorized based on the kind of object it is
part of—this is called <i>segmentation).</i>
Deep learning algorithms are generally not good at recognizing images that are sig‐
nificantly different in structure or style from those used to train the model. For
instance, if there were no black-and-white images in the training data, the model may
do poorly on black-and-white images. Similarly, if the training data did not contain
hand-drawn images, the model will probably do poorly on hand-drawn images.
There is no general way to check which types of images are missing in your training
set, but we will show in this chapter some ways to try to recognize when unexpected
image types arise in the data when the model is being used in production (this is
known as checking for <i>out-of-domain</i> data).
One major challenge for object detection systems is that image labeling can be slow
and expensive. There is a lot of work at the moment going into tools to try to make
this labeling faster and easier, and to require fewer handcrafted labels to train accu‐
rate object detection models. One approach that is particularly helpful is to syntheti‐
cally generate variations of input images, such as by rotating them or changing their
brightness and contrast; this is called <i>data</i> <i>augmentation</i> and also works well for text
and other types of models. We will be discussing it in detail in this chapter.
Another point to consider is that although your problem might not look like a com‐
puter vision problem, it might be possible with a little imagination to turn it into one.
For instance, if what you are trying to classify are sounds, you might try converting
the sounds into images of their acoustic waveforms and then training a model on
those images."|deep learning applicability to problem; data augmentation; data augmentation definition; deep learning; process end-to-end; labels; machine learning (ML); object detection; object recognition; segmentation; black-and-white or hand-drawn images
"Deep learning has power, flexibility, and simplicity. That’s why we believe it should be
applied across many disciplines. These include the social and physical sciences, the
arts, medicine, finance, scientific research, and many more. To give a personal exam‐
ple, despite having no background in medicine, Jeremy started Enlitic, a company
that uses deep learning algorithms to diagnose illness and disease. Within months of
starting the company, it was announced that its algorithm could identify malignant
tumors more accurately than radiologists.
Here’s a list of some of the thousands of tasks in different areas for which deep learn‐
ing, or methods heavily using deep learning, is now the best in the world:
<i>Natural</i> <i>language</i> <i>processing</i> <i>(NLP)</i>
Answering questions; speech recognition; summarizing documents; classifying
documents; finding names, dates, etc. in documents; searching for articles men‐
tioning a concept
<i>Computer</i> <i>vision</i>
Satellite and drone imagery interpretation (e.g., for disaster resilience), face rec‐
ognition, image captioning, reading traffic signs, locating pedestrians and vehi‐
cles in autonomous vehicles
<i>Medicine</i>
Finding anomalies in radiology images, including CT, MRI, and X-ray images;
counting features in pathology slides; measuring features in ultrasounds; diag‐
nosing diabetic retinopathy
<i>Biology</i>
Folding proteins; classifying proteins; many genomics tasks, such as tumor-
normal sequencing and classifying clinically actionable genetic mutations; cell
classification; analyzing protein/protein interactions
<i>Image</i> <i>generation</i>
Colorizing images, increasing image resolution, removing noise from images,
converting images to art in the style of famous artists
<i>Recommendation</i> <i>systems</i>
Web search, product recommendations, home page layout
<i>Playing</i> <i>games</i>
Chess, Go, most Atari video games, and many real-time strategy games
<i>Robotics</i>
Handling objects that are challenging to locate (e.g., transparent, shiny, lacking
texture) or hard to pick up
<i>Other</i> <i>applications</i>
Financial and logistical forecasting, text to speech, and much, much more…"|deep learning; Enlitic company malignant tumor identification; medicine; tumor identification
"In Chapter 19, we will start from such a model and see how to build a training loop
from scratch and refactor it to what we’ve been using in previous chapters.
<header><largefont><b>Conclusion</b></largefont></header>
In this chapter, we explored the foundations of deep learning, beginning with matrix
multiplication and moving on to implementing the forward and backward passes of a
neural net from scratch. We then refactored our code to show how PyTorch works
beneath the hood.
Here are a few things to remember:
• A neural net is basically a bunch of matrix multiplications with nonlinearities in
between.
• Python is slow, so to write fast code, we have to vectorize it and take advantage of
techniques such as elementwise arithmetic and broadcasting.
• Two tensors are broadcastable if the dimensions starting from the end and going
backward match (if they are the same, or one of them is 1). To make tensors
unsqueeze
broadcastable, we may need to add dimensions of size 1 with or a
None index.
• Properly initializing a neural net is crucial to get training started. Kaiming initial‐
ization should be used when we have ReLU nonlinearities.
• The backward pass is the chain rule applied multiple times, computing the gradi‐
ents from the output of our model and going back, one layer at a time.
• When subclassing nn.Module (if not using fastai’s Module ), we have to call the
__init__ __init__
superclass method in our method and we have to define a
forward function that takes an input and returns the desired result.
<header><largefont><b>Questionnaire</b></largefont></header>
1. Write the Python code to implement a single neuron.
2. Write the Python code to implement ReLU.
3. Write the Python code for a dense layer in terms of matrix multiplication.
4. Write the Python code for a dense layer in plain Python (that is, with list compre‐
hensions and functionality built into Python).
5. What is the “hidden size” of a layer?
6. What does the t method do in PyTorch?
7. Why is matrix multiplication written in plain Python very slow?
8. In matmul, why is ac==br?"|PyTorch
"training data, so in this case you’d want your validation set to also include boats that
are not in the training set.
Sometimes it may not be clear how your validation data will differ. For instance, for a
problem using satellite imagery, you’d need to gather more information on whether
the training set contained just certain geographic locations or came from geographi‐
cally scattered data.
Now that you have gotten a taste of how to build a model, you can decide what you
want to dig into next.
<header><largefont><b>Choose</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Own</b></largefont> <largefont><b>Adventure</b></largefont></header>
<header><largefont><b>A</b></largefont> <largefont><b>Moment</b></largefont></header>
If you would like to learn more about how to use deep learning models in practice,
including how to identify and fix errors, create a real working web application, and
avoid your model causing unexpected harm to your organization or society more
generally, then keep reading the next two chapters. If you would like to start learning
the foundations of how deep learning works under the hood, skip to Chapter 4. (Did
you ever read <i>Choose</i> <i>Your</i> <i>Own</i> <i>Adventure</i> books as a kid? Well, this is kind of like
that…except with more deep learning than that book series contained.)
You will need to read all these chapters to progress further in the book, but the order
in which you read them is totally up to you. They don’t depend on each other. If you
skip ahead to Chapter 4, we will remind you at the end to come back and read the
chapters you skipped over before you go any further.
<header><largefont><b>Questionnaire</b></largefont></header>
After reading pages and pages of prose, it can be hard to know which key things you
really need to focus on and remember. So, we’ve prepared a list of questions and sug‐
gested steps to complete at the end of each chapter. All the answers are in the text of
the chapter, so if you’re not sure about anything here, reread that part of the text and
make sure you understand it. Answers to all these questions are also available on the
book’s website. You can also visit the forums if you get stuck to get help from other
folks studying this material.
1. Do you need these for deep learning?
• Lots of math T/F
• Lots of data T/F
• Lots of expensive computers T/F
• A PhD T/F
2. Name five areas where deep learning is now the best tool in the world."|validation set; testing models; training
"In this section, we will give an overview of some of the most important issues to con‐
sider; for a more detailed discussion of deployment issues, we refer you to the excel‐
lent <i>Building</i> <i>Machine</i> <i>Learning</i> <i>Powered</i> <i>Applications</i> by Emmanuel Ameisin
(O’Reilly).
One of the biggest issues to consider is that understanding and testing the behavior of
a deep learning model is much more difficult than with most other code you write.
With normal software development, you can analyze the exact steps that the software
is taking, and carefully study which of these steps match the desired behavior that you
are trying to create. But with a neural network, the behavior emerges from the mod‐
el’s attempt to match the training data, rather than being exactly defined.
This can result in disaster! For instance, let’s say we really were rolling out a bear
detection system that will be attached to video cameras around campsites in national
parks and will warn campers of incoming bears. If we used a model trained with the
dataset we downloaded, there would be all kinds of problems in practice, such as
these:
• Working with video data instead of images
• Handling nighttime images, which may not appear in this dataset
• Dealing with low-resolution camera images
• Ensuring results are returned fast enough to be useful in practice
• Recognizing bears in positions that are rarely seen in photos that people post
online (for example from behind, partially covered by bushes, or a long way away
from the camera)
A big part of the issue is that the kinds of photos that people are most likely to upload
to the internet are the kinds of photos that do a good job of clearly and artistically
displaying their subject matter—which isn’t the kind of input this system is going to
be getting. So, we may need to do a lot of our own data collection and labeling to cre‐
ate a useful system.
This is just one example of the more general problem of <i>out-of-domain</i> data. That is
to say, there may be data that our model sees in production that is very different from
what it saw during training. There isn’t a complete technical solution to this problem;
instead, we have to be careful about our approach to rolling out the technology.
There are other reasons we need to be careful too. One very common problem is
<i>domain</i> <i>shift,</i> whereby the type of data that our model sees changes over time. For
instance, an insurance company may use a deep learning model as part of its pricing
and risk algorithm, but over time the types of customers the company attracts and the
types of risks it represents may change so much that the original training data is no
longer relevant."|Ameisin; domain shift; web resource discussing; data seen changing over time; neural networks; web resource discussing issues; production; testing models; production complexity and; deployment issue discussion
"Here’s an example of one row from the dependent variable:
yb[0]
tensor([[0.0111, 0.1810]], device='cuda:5')
As you can see, we haven’t had to use a separate <i>image</i> <i>regression</i> application; all we’ve
had to do is label the data and tell fastai what kinds of data the independent and
dependent variables represent.
It’s the same for creating our Learner. We will use the same function as before, with
one new parameter, and we will be ready to train our model.
<header><largefont><b>Training</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Model</b></largefont></header>
cnn_learner Learner.
As usual, we can use to create our Remember way back in
Chapter 1 how we used y_range to tell fastai the range of our targets? We’ll do the
same here (coordinates in fastai and PyTorch are always rescaled between –1 and +1):
learn = cnn_learner(dls, resnet18, y_range=(-1,1))
y_range is implemented in fastai using sigmoid_range, which is defined as follows:
<b>def</b> sigmoid_range(x, lo, hi): <b>return</b> torch.sigmoid(x) * (hi-lo) + lo
This is set as the final layer of the model, if y_range is defined. Take a moment to
think about what this function does, and why it forces the model to output activa‐
tions in the range (lo,hi) .
Here’s what it looks like:
plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)
We didn’t specify a loss function, which means we’re getting whatever fastai chooses
as the default. Let’s see what it picked for us:
dls.loss_func
FlattenedLoss of MSELoss()"|loss function selected by; fastai selecting function; sigmoid_range; y_range
"Another example comes from the paper “Malware Classification with Deep Convolu‐
tional Neural Networks” by Mahmoud Kalash et al., which explains that “the malware
binary file is divided into 8-bit sequences which are then converted to equivalent dec‐
imal values. This decimal vector is reshaped and [a] gray-scale image is generated that
represent[s] the malware sample,” in Figure 1-17.
<i>Figure</i> <i>1-17.</i> <i>Malware</i> <i>classification</i> <i>process</i>
The authors then show “pictures” generated through this process of malware in dif‐
ferent categories, as shown in Figure 1-18.
<i>Figure</i> <i>1-18.</i> <i>Malware</i> <i>examples</i>
As you can see, the different types of malware look very distinctive to the human eye.
The model the researchers trained based on this image representation was more
accurate at malware classification than any previous approach shown in the academic
literature. This suggests a good rule of thumb for converting a dataset into an image
representation: if the human eye can recognize categories from the images, then a
deep learning model should be able to do so too.
In general, you’ll find that a small number of general approaches in deep learning can
go a long way, if you’re a bit creative in how you represent your data! You shouldn’t
think of approaches like the ones described here as “hacky workarounds,” because"|dataset image representation rule; non-image tasks; image representation rule of thumb; image representation rule; Kalash; malware classification; research papers; web resources
"Let’s see what happens to the RMSE as we add more and more trees. As you can see,
the improvement levels off quite a bit after around 30 trees:
plt.plot([r_mse(preds[:i+1].mean(0), valid_y) <b>for</b> i <b>in</b> range(40)]);
The performance on our validation set is worse than on our training set. But is that
because we’re overfitting, or because the validation set covers a different time period,
or a bit of both? With the existing information we’ve seen, we can’t tell. However, ran‐
dom forests have a very clever trick called <i>out-of-bag</i> (OOB) error that can help us
with this (and more!).
<header><largefont><b>Out-of-Bag</b></largefont> <largefont><b>Error</b></largefont></header>
Recall that in a random forest, each tree is trained on a different subset of the training
data. The OOB error is a way of measuring prediction error in the training dataset by
including in the calculation of a row’s error trees only where that row was <i>not</i>
included in training. This allows us to see whether the model is overfitting, without
needing a separate validation set.
<b>AlexisSays</b>
My intuition for this is that, since every tree was trained with a dif‐
ferent randomly selected subset of rows, out-of-bag error is a little
like imagining that every tree therefore also has its own validation
set. That validation set is simply the rows that were not selected for
that tree’s training."|bagging; decision trees; out-of-bag error; machine learning (ML); root mean squared log error; predictions; random forests; root mean squared log error as metric; tabular data for models; training and validation sets; training
"These are then all added together to produce a single number for each grid location
for each output feature, as shown in Figure 13-13.
<i>Figure</i> <i>13-13.</i> <i>Adding</i> <i>the</i> <i>RGB</i> <i>filters</i>
Then we have ch_out filters like this, so in the end, the result of our convolutional
layer will be a batch of images with ch_out channels and a height and width given by
the formula outlined earlier. This give us ch_out tensors of size ch_in x ks x ks
that we represent in one big tensor of four dimensions. In PyTorch, the order of the
dimensions for those weights is ch_out x ch_in x ks x ks.
Additionally, we may want to have a bias for each filter. In the preceding example, the
final result for our convolutional layer would be <i>y</i> + <i>y</i> + <i>y</i> + <i>b</i> in that case. As in a
<i>R</i> <i>G</i> <i>B</i>
linear layer, there are as many biases as we have kernels, so the bias is a vector of size
ch_out.
No special mechanisms are required when setting up a CNN for training with color
images. Just make sure your first layer has three inputs.
There are lots of ways of processing color images. For instance, you can change them
to black and white, change from RGB to HSV (hue, saturation, and value) color
space, and so forth. In general, it turns out experimentally that changing the encoding
of colors won’t make any difference to your model results, as long as you don’t lose
information in the transformation. So, transforming to black and white is a bad idea,
since it removes the color information entirely (and this can be critical; for instance, a
pet breed may have a distinctive color); but converting to HSV generally won’t make
any difference."|convolutional neural network (CNN); building a CNN
"These are not just algorithm questions. They are data product design questions. But
the product managers, executives, judges, journalists, doctors—whoever ends up
developing and using the system of which your model is a part—will not be well-
placed to understand the decisions that you made, let alone change them.
For instance, two studies found that Amazon’s facial recognition software produced
inaccurate and racially biased results. Amazon claimed that the researchers should
have changed the default parameters, without explaining how this would have
changed the biased results. Furthermore, it turned out that Amazon was not instruct‐
ing police departments that used its software to do this either. There was, presumably,
a big distance between the researchers who developed these algorithms and the Ama‐
zon documentation staff who wrote the guidelines provided to the police.
A lack of tight integration led to serious problems for society at large, the police, and
Amazon. It turned out that its system erroneously matched 28 members of Congress
to criminal mugshots! (And the Congresspeople wrongly matched to criminal mug‐
shots were disproportionately people of color, as seen in Figure 3-4.)
<i>Figure</i> <i>3-4.</i> <i>Congresspeople</i> <i>matched</i> <i>to</i> <i>criminal</i> <i>mugshots</i> <i>by</i> <i>Amazon</i> <i>software</i>
Data scientists need to be part of a cross-disciplinary team. And researchers need to
work closely with the kinds of people who will end up using their research. Better
still, domain experts themselves could learn enough to be able to train and debug
some models themselves—hopefully, a few of you are reading this book right now!
The modern workplace is a very specialized place. Everybody tends to have well-
defined jobs to perform. Especially in large companies, it can be hard to know all the
pieces of the puzzle. Sometimes companies even intentionally obscure the overall"|facial recognition bias; bias; ethics; racial bias
"The first gate (looking from left to right) is called the <i>forget</i> <i>gate.</i> Since it’s a linear
layer followed by a sigmoid, its output will consist of scalars between 0 and 1. We
multiply this result by the cell state to determine which information to keep and
which to throw away: values closer to 0 are discarded, and values closer to 1 are kept.
This gives the LSTM the ability to forget things about its long-term state. For
instance, when crossing a period or an xxbos token, we would expect it to (have
learned to) reset its cell state.
The second gate is called the <i>input</i> <i>gate.</i> It works with the third gate (which doesn’t
really have a name but is sometimes called the <i>cell</i> <i>gate)</i> to update the cell state. For
instance, we may see a new gender pronoun, in which case we’ll need to replace the
information about gender that the forget gate removed. Similar to the forget gate, the
input gate decides which elements of the cell state to update (values close to 1) or not
(values close to 0). The third gate determines what those updated values are, in the
range of –1 to 1 (thanks to the tanh function). The result is added to the cell state.
The last gate is the <i>output</i> <i>gate.</i> It determines which information from the cell state to
use to generate the output. The cell state goes through a tanh before being combined
with the sigmoid output from the output gate, and the result is the new hidden state.
In terms of code, we can write the same steps like this:
<b>class</b> <b>LSTMCell(Module):</b>
<b>def</b> <b>__init__(self,</b> ni, nh):
self.forget_gate = nn.Linear(ni + nh, nh)
self.input_gate = nn.Linear(ni + nh, nh)
self.cell_gate = nn.Linear(ni + nh, nh)
self.output_gate = nn.Linear(ni + nh, nh)
<b>def</b> forward(self, input, state):
h,c = state
h = torch.stack([h, input], dim=1)
forget = torch.sigmoid(self.forget_gate(h))
c = c * forget
inp = torch.sigmoid(self.input_gate(h))
cell = torch.tanh(self.cell_gate(h))
c = c + inp * cell
out = torch.sigmoid(self.output_gate(h))
h = outgate * torch.tanh(c)
<b>return</b> h, (h,c)
In practice, we can then refactor the code. Also, in terms of performance, it’s better to
do one big matrix multiplication than four smaller ones (that’s because we launch the
special fast kernel on the GPU only once, and it gives the GPU more work to do in
parallel). The stacking takes a bit of time (since we have to move one of the tensors
around on the GPU to have it all in a contiguous array), so we use two separate layers
for the input and the hidden state. The optimized and refactored code then looks like
this:"|building from scratch; LSTM language model
"And for our weights, we’ll use the right scale, which is known as <i>Xavier</i> <i>initialization</i>
(or <i>Glorot</i> <i>initialization):</i>
<b>from</b> <b>math</b> <b>import</b> sqrt
w1 = torch.randn(100,50) / sqrt(100)
b1 = torch.zeros(50)
w2 = torch.randn(50,1) / sqrt(50)
b2 = torch.zeros(1)
Now if we compute the result of the first layer, we can check that the mean and stan‐
dard deviation are under control:
l1 = lin(x, w1, b1)
l1.mean(),l1.std()
(tensor(-0.0050), tensor(1.0000))
Very good. Now we need to go through a ReLU, so let’s define one. A ReLU removes
the negatives and replaces them with zeros, which is another way of saying it clamps
our tensor at zero:
<b>def</b> relu(x): <b>return</b> x.clamp_min(0.)
We pass our activations through this:
l2 = relu(l1)
l2.mean(),l2.std()
(tensor(0.3961), tensor(0.5783))
And we’re back to square one: the mean of our activations has gone to 0.4 (which is
understandable since we removed the negatives), and the std went down to 0.58. So
like before, after a few layers we will probably wind up with zeros:
x = torch.randn(200, 100)
<b>for</b> i <b>in</b> range(50): x = relu(x @ (torch.randn(100,100) * 0.1))
x[0:5,0:5]
tensor([[0.0000e+00, 1.9689e-08, 4.2820e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.6701e-08, 4.3501e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.0976e-08, 3.0411e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.8457e-08, 4.9469e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.9949e-08, 4.1643e-08, 0.0000e+00, 0.0000e+00]])
This means our initialization wasn’t right. Why? At the time Glorot and Bengio wrote
their article, the most popular activation in a neural net was the hyperbolic tangent
(tanh, which is the one they used), and that initialization doesn’t account for our
ReLU. Fortunately, someone else has done the math for us and computed the right
scale for us to use. In “Delving Deep into Rectifiers: Surpassing Human-Level Perfor‐
mance” (which we’ve seen before—it’s the article that introduced the ResNet),
Kaiming He et al. show that we should use the following scale instead: 2/n , where
<i>in</i>
<i>n</i> is the number of inputs of our model. Let’s see what this gives us:
<i>in</i>"|defining and initializing a layer; rectifier deep dive
"<b>SylvainSays</b>
To make the most of this book, take the time to experiment
between each chapter, whether on your own project or by explor‐
ing the notebooks we provide. Then try rewriting those notebooks
from scratch on a new dataset. It’s only by practicing (and failing) a
lot that you will develop intuition of how to train a model.
By using the end-to-end iteration approach, you will also get a better understanding
of how much data you really need. For instance, you may find you can easily get only
200 labeled data items, and you can’t really know until you try whether that’s enough
to get the performance you need for your application to work well in practice.
In an organizational context, you will be able to show your colleagues that your idea
can work by showing them a real working prototype. We have repeatedly observed
that this is the secret to getting good organizational buy-in for a project.
Since it is easiest to get started on a project for which you already have data available,
that means it’s probably easiest to get started on a project related to something you
are already doing, because you already have data about things that you are doing. For
instance, if you work in the music business, you may have access to many recordings.
If you work as a radiologist, you probably have access to lots of medical images. If
you are interested in wildlife preservation, you may have access to lots of images of
wildlife.
Sometimes you have to get a bit creative. Maybe you can find a previous machine
learning project, such as a Kaggle competition, that is related to your field of interest.
Sometimes you have to compromise. Maybe you can’t find the exact data you need for
the precise project you have in mind; but you might be able to find something from a
similar domain, or measured in a different way, tackling a slightly different problem.
Working on these kinds of similar projects will still give you a good understanding of
the overall process, and may help you identify other shortcuts, data sources, and so
forth.
Especially when you are just starting out with deep learning, it’s not a good idea to
branch out into very different areas, to places that deep learning has not been applied
to before. That’s because if your model does not work at first, you will not know
whether it is because you have made a mistake, or if the very problem you are trying
to solve is simply not solvable with deep learning. And you won’t know where to look
to get help. Therefore, it is best at first to start by finding an example online of some‐
thing that somebody has had good results with and that is at least somewhat similar
to what you are trying to achieve, by converting your data into a format similar to
what someone else has used before (such as creating an image from your data). Let’s
have a look at the state of deep learning, just so you know what kinds of things deep
learning is good at right now."|beginning; process end-to-end; project buy-in
"consecutive activations on the sequence length axis (the dimension in the middle).
With this, TAR can be expressed as follows:
loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean()
alpha and beta are then two hyperparameters to tune. To make this work, we need
our model with dropout to return three things: the proper output, the activations of
the LSTM pre-dropout, and the activations of the LSTM post-dropout. AR is often
applied on the dropped-out activations (to not penalize the activations we turned into
zeros afterward), while TAR is applied on the non-dropped-out activations (because
those zeros create big differences between two consecutive time steps). A callback
called RNNRegularizer will then apply this regularization for us.
<header><largefont><b>Training</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Weight-Tied</b></largefont> <largefont><b>Regularized</b></largefont> <largefont><b>LSTM</b></largefont></header>
We can combine dropout (applied before we go into our output layer) with AR and
TAR to train our previous LSTM. We just need to return three things instead of one:
the normal output of our LSTM, the dropped-out activations, and the activations
RNNRegularization
from our LSTMs. The last two will be picked up by the callback
for the contributions it has to make to the loss.
Another useful trick we can add from the AWD-LSTM paper is <i>weight</i> <i>tying.</i> In a lan‐
guage model, the input embeddings represent a mapping from English words to acti‐
vations, and the output hidden layer represents a mapping from activations to
English words. We might expect, intuitively, that these mappings could be the same.
We can represent this in PyTorch by assigning the same weight matrix to each of
these layers:
self.h_o.weight = self.i_h.weight
In LMMModel7, we include these final tweaks:
<b>class</b> <b>LMModel7(Module):</b>
<b>def</b> <b>__init__(self,</b> vocab_sz, n_hidden, n_layers, p):
self.i_h = nn.Embedding(vocab_sz, n_hidden)
self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True)
self.drop = nn.Dropout(p)
self.h_o = nn.Linear(n_hidden, vocab_sz)
self.h_o.weight = self.i_h.weight
self.h = [torch.zeros(n_layers, bs, n_hidden) <b>for</b> _ <b>in</b> range(2)]
<b>def</b> forward(self, x):
raw,h = self.rnn(self.i_h(x), self.h)
out = self.drop(raw)
self.h = [h_.detach() <b>for</b> h_ <b>in</b> h]
<b>return</b> self.h_o(out),raw,out
<b>def</b> reset(self):
<b>for</b> h <b>in</b> self.h: h.zero_()"|training weight-tied regularized LSTM; language model; LSTM model; LSTM training; training a regularized LSTM; natural language processing (NLP); recurrent neural networks (RNNs); weights
"This is particularly beneficial in cases where we have only a small amount of training
data, as it allows us to see whether our model generalizes without removing items to
create a validation set. The OOB predictions are available in the oob_prediction_
attribute. Note that we compare them to the training labels, since this is being calcu‐
lated on trees using the training set:
r_mse(m.oob_prediction_, y)
0.210686
We can see that our OOB error is much lower than our validation set error. This
means that something else is causing that error, in <i>addition</i> to normal generalization
error. We’ll discuss the reasons for this later in this chapter.
This is one way to interpret our model’s predictions—let’s focus on more of those
now.
<header><largefont><b>Model</b></largefont> <largefont><b>Interpretation</b></largefont></header>
For tabular data, model interpretation is particularly important. For a given model,
we are most likely to be interested in are the following:
• How confident are we in our predictions using a particular row of data?
• For predicting with a particular row of data, what were the most important fac‐
tors, and how did they influence that prediction?
• Which columns are the strongest predictors, which can we ignore?
• Which columns are effectively redundant with each other, for purposes of pre‐
diction?
• How do predictions vary as we vary these columns?
As we will see, random forests are particularly well suited to answering these ques‐
tions. Let’s start with the first one!
<header><largefont><b>Tree</b></largefont> <largefont><b>Variance</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Prediction</b></largefont> <largefont><b>Confidence</b></largefont></header>
We saw how the model averages the individual tree’s predictions to get an overall pre‐
diction—that is, an estimate of the value. But how can we know the confidence of the
estimate? One simple way is to use the standard deviation of predictions across the
trees, instead of just the mean. This tells us the <i>relative</i> confidence of predictions. In
general, we would want to be more cautious of using the results for rows where trees
give very different results (higher standard deviations), compared to cases where they
are more consistent (lower standard deviations)."|bagging; decision trees; machine learning (ML); predictions; random forest confidence; random forests; tabular data for models; training
"<i>Figure</i> <i>9-1.</i> <i>Entity</i> <i>embeddings</i> <i>in</i> <i>a</i> <i>neural</i> <i>network</i> <i>(courtesy</i> <i>of</i> <i>Cheng</i> <i>Guo</i> <i>and</i> <i>Felix</i>
<i>Berkhahn)</i>
The images in Figure 9-2 illustrate these ideas. They are based on the approaches
used in the paper, along with some analysis we have added.
<i>Figure</i> <i>9-2.</i> <i>State</i> <i>embeddings</i> <i>and</i> <i>map</i> <i>(courtesy</i> <i>of</i> <i>Cheng</i> <i>Guo</i> <i>and</i> <i>Felix</i> <i>Berkhahn)</i>
On the left is a plot of the embedding matrix for the possible values of the State
category. For a categorical variable, we call the possible values of the variable its “lev‐
els” (or “categories” or “classes”), so here one level is “Berlin,” another is “Hamburg,”"|categorical variables
"<b>FullandStrippedNotebooks</b>
There are two folders containing different versions of the note‐
books. The <i>full</i> folder contains the exact notebooks used to create
the book you’re reading now, with all the prose and outputs. The
<i>stripped</i> version has the same headings and code cells, but all out‐
puts and prose have been removed. After reading a section of the
book, we recommend working through the stripped notebooks,
with the book closed, and seeing if you can figure out what each
cell will show before you execute it. Also try to recall what the code
is demonstrating.
To open a notebook, just click it. The notebook will open, and it will look something
like Figure 1-3 (note that there may be slight differences in details across different
platforms; you can ignore those differences).
<i>Figure</i> <i>1-3.</i> <i>A</i> <i>Jupyter</i> <i>notebook</i>
A notebook consists of <i>cells.</i> There are two main types of cell:
• Cells containing formatted text, images, and so forth. These use a format called
<i>Markdown,</i> which you will learn about soon.
• Cells containing code that can be executed, and outputs will appear immediately
underneath (which could be plain text, tables, images, animations, sounds, or
even interactive applications)."|beginning; cats and dogs first model; cells in notebooks; dogs and cats first model; first model; Markdown in notebook cells; notebooks; full versus stripped; cells containing executable code
"But then we end up with two separate objects for our inputs and targets, which is not
what we want. This is where Datasets comes to the rescue.
<header><largefont><b>Datasets</b></largefont></header>
Datasets will apply two (or more) pipelines in parallel to the same raw object and
build a tuple with the result. Like TfmdLists , it will automatically do the setup for us,
and when we index into a Datasets, it will return us a tuple with the results of each
pipeline:
x_tfms = [Tokenizer.from_folder(path), Numericalize]
y_tfms = [parent_label, Categorize()]
dsets = Datasets(files, [x_tfms, y_tfms])
x,y = dsets[0]
x[:20],y
Like a TfmdLists , we can pass along splits to a Datasets to split our data between
training and validation sets:
x_tfms = [Tokenizer.from_folder(path), Numericalize]
y_tfms = [parent_label, Categorize()]
dsets = Datasets(files, [x_tfms, y_tfms], splits=splits)
x,y = dsets.valid[0]
x[:20],y
(tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379,
> 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509]),
TensorCategory(0))
It can also decode any processed tuple or show it directly:
t = dsets.valid[0]
dsets.decode(t)
('xxbos xxmaj this movie had horrible lighting and terrible camera movements .
> xxmaj this movie is a jumpy horror flick with no meaning at all . xxmaj the
> slashes are totally fake looking . xxmaj it looks like some 17 year - old
> idiot wrote this movie and a 10 year old kid shot it . xxmaj with the worst
> acting you can ever find . xxmaj people are tired of knives . xxmaj at least
> move on to guns or fire . xxmaj it has almost exact lines from "" when a xxmaj
> stranger xxmaj calls "" . xxmaj with gruesome killings , only crazy people
> would enjoy this movie . xxmaj it is obvious the writer does n\'t have kids
> or even care for them . i mean at show some mercy . xxmaj just to sum it up ,
> this movie is a "" b "" movie and it sucked . xxmaj just for your own sake , do
> n\'t even think about wasting your time watching this crappy movie .',
'neg')
The last step is to convert our Datasets object to a DataLoaders, which can be done
with the dataloaders method. Here we need to pass along a special argument to take
care of the padding problem (as we saw in the preceding chapter). This needs to hap‐
pen just before we batch the elements, so we pass it to before_batch :"|mid-level API; TfmdLists; Transforms
"their models will be used, and consider how to best ensure that they are used as posi‐
tively as possible. There are things you can do. And if you don’t do them, things can
go pretty badly.
One particularly hideous example of what happens when technologists focus on tech‐
nology at all costs is the story of IBM and Nazi Germany. In 2001, a Swiss judge ruled
that it was not unreasonable “to deduce that IBM’s technical assistance facilitated the
tasks of the Nazis in the commission of their crimes against humanity, acts also
involving accountancy and classification by IBM machines and utilized in the con‐
centration camps themselves.”
IBM, you see, supplied the Nazis with data tabulation products necessary to track the
extermination of Jews and other groups on a massive scale. This was driven from the
top of the company, with marketing to Hitler and his leadership team. Company
President Thomas Watson personally approved the 1939 release of special IBM alpha‐
betizing machines to help organize the deportation of Polish Jews. Pictured in
Figure 3-2 is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (second
from left), shortly before Hitler awarded Watson a special “Service to the Reich”
medal in 1937.
<i>Figure</i> <i>3-2.</i> <i>IBM</i> <i>CEO</i> <i>Tom</i> <i>Watson</i> <i>Sr.</i> <i>meeting</i> <i>with</i> <i>Adolf</i> <i>Hitler</i>
But this was not an isolated incident—the organization’s involvement was extensive.
IBM and its subsidiaries provided regular training and maintenance onsite at the
concentration camps: printing off cards, configuring machines, and repairing them as"|IBM and Nazi Germany; Hitler; Nazi Germany and IBM; Watson
"underlying mapping as <i>H(x),</i> we let the stacked nonlinear layers fit another mapping of
<i>F(x)</i> := H(x)−x. The original mapping is recast into <i>F(x)+x.</i> We hypothesize that it is
easier to optimize the residual mapping than to optimize the original, unreferenced
mapping. To the extreme, if an identity mapping were optimal, it would be easier to
push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.
Again, this is rather inaccessible prose—so let’s try to restate it in plain English! If the
x
outcome of a given layer is and we’re using a ResNet block that returns
y = x + block(x) , we’re not asking the block to predict y ; we are asking it to predict
the difference between y and x. So the job of those blocks isn’t to predict certain fea‐
tures, but to minimize the error between x and the desired y . A ResNet is, therefore,
good at learning about slight differences between doing nothing and passing through
a block of two convolutional layers (with trainable weights). This is how these models
got their name: they’re predicting residuals (reminder: “residual” is prediction minus
target).
One key concept that both of these two ways of thinking about ResNets share is the
idea of ease of learning. This is an important theme. Recall the universal approxima‐
tion theorem, which states that a sufficiently large network can learn anything. This is
still true, but there turns out to be a very important difference between what a net‐
work <i>can</i> <i>learn</i> in principle, and what it is <i>easy</i> <i>for</i> <i>it</i> <i>to</i> <i>learn</i> with realistic data and
training regimes. Many of the advances in neural networks over the last decade have
been like the ResNet block: the result of realizing how to make something that was
always possible actually feasible.
<b>TrueIdentityPath</b>
The original paper didn’t actually do the trick of using zero for the
initial value of gamma in the last batchnorm layer of each block; that
came a couple of years later. So, the original version of ResNet
didn’t quite begin training with a true identity path through the
ResNet blocks, but nonetheless having the ability to “navigate
through” the skip connections did make it train better. Adding the
batchnorm gamma init trick made the models train at even higher
learning rates.
gamma
Here’s the definition of a simple ResNet block (fastai initializes the weights of
the last batchnorm layer to zero because of norm_type=NormType.BatchZero):
<b>class</b> <b>ResBlock(Module):</b>
<b>def</b> <b>__init__(self,</b> ni, nf):
self.convs = nn.Sequential(
ConvLayer(ni,nf),
ConvLayer(nf,nf, norm_type=NormType.BatchZero))
<b>def</b> forward(self, x): <b>return</b> x + self.convs(x)"|building ResNet CNN; ResNet architecture; skip connections; universal approximation theorem
"would want to find out <i>why</i> it’s missing so often and what that <i>means.</i> Missing values
can sometimes be useful predictors—it entirely depends on what causes them to be
missing. Sometimes, however, they can indicate <i>data</i> <i>leakage.</i>
<header><largefont><b>Data</b></largefont> <largefont><b>Leakage</b></largefont></header>
In the paper “Leakage in Data Mining: Formulation, Detection, and Avoidance”, Sha‐
char Kaufman et al. describe leakage as follows:
The introduction of information about the target of a data mining problem, which
should not be legitimately available to mine from. A trivial example of leakage would
be a model that uses the target itself as an input, thus concluding for example that “it
rains on rainy days.” In practice, the introduction of this illegitimate information is
unintentional, and facilitated by the data collection, aggregation, and preparation
process.
They give as an example:
A real-life business intelligence project at IBM where potential customers for certain
products were identified, among other things, based on keywords found on their web‐
sites. This turned out to be leakage since the website content used for training had
been sampled at the point in time where the potential customer has already become a
customer, and where the website contained traces of the IBM products purchased, such
as the word “Websphere” (e.g., in a press release about the purchase or a specific prod‐
uct feature the client uses).
Data leakage is subtle and can take many forms. In particular, missing values often
represent data leakage.
For instance, Jeremy competed in a Kaggle competition designed to predict which
researchers would end up receiving research grants. The information was provided
by a university and included thousands of examples of research projects, along with
information about the researchers involved and data on whether or not each grant
was eventually accepted. The university hoped to be able to use the models developed
in this competition to rank which grant applications were most likely to succeed, so it
could prioritize its processing.
Jeremy used a random forest to model the data, and then used feature importance to
find out which features were most predictive. He noticed three surprising things:
• The model was able to correctly predict who would receive grants over 95% of
the time.
• Apparently meaningless identifier columns were the most important predictors.
• The day of week and day of year columns were also highly predictive; for
instance, the vast majority of grant applications dated on a Sunday were accepted,
and many accepted grant applications were dated on January 1."|bagging; data leakage of illegitimate information; missing values as; missing values as data leakage; decision trees; machine learning (ML); predictions; random forests; research papers; tabular data for models; training
"Table 4-1 summarizes the key concepts related to SGD.
<i>Table</i> <i>4-1.</i> <i>Deep</i> <i>learning</i> <i>vocabulary</i>
<b>Term</b> <b>Meaning</b>
ReLU Functionthatreturns0fornegativenumbersanddoesn’tchangepositivenumbers.
Mini-batch Asmallgroupofinputsandlabelsgatheredtogetherintwoarrays.Agradientdescentstepisupdatedon
thisbatch(ratherthanawholeepoch).
Forwardpass Applyingthemodeltosomeinputandcomputingthepredictions.
Loss Avaluethatrepresentshowwell(orbadly)ourmodelisdoing.
Gradient Thederivativeofthelosswithrespecttosomeparameterofthemodel.
Backwardpass Computingthegradientsofthelosswithrespecttoallmodelparameters.
Gradient Takingastepinthedirectionoppositetothegradientstomakethemodelparametersalittlebitbetter.
descent
Learningrate ThesizeofthestepwetakewhenapplyingSGDtoupdatetheparametersofthemodel.
<b>ChooseYourOwnAdventureReminder</b>
Did you choose to skip over Chapters 2 and 3, in your excitement
to peek under the hood? Well, here’s your reminder to head back to
Chapter 2 now, because you’ll be needing to know that stuff soon!
<header><largefont><b>Questionnaire</b></largefont></header>
1. How is a grayscale image represented on a computer? How about a color image?
2. How are the files and folders in the MNIST_SAMPLE dataset structured? Why?
3. Explain how the “pixel similarity” approach to classifying digits works.
4. What is a list comprehension? Create one now that selects odd numbers from a
list and doubles them.
5. What is a rank-3 tensor?
6. What is the difference between tensor rank and shape? How do you get the rank
from the shape?
7. What are RMSE and L1 norm?
8. How can you apply a calculation on thousands of numbers at once, many thou‐
sands of times faster than a Python loop?
9. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it.
Select the bottom-right four numbers.
10. What is broadcasting?
11. Are metrics generally calculated using the training set or the validation set? Why?"|forward pass; backward pass; mini-batch; gradient descent; gradients; learning rate (LR); loss; optimization; rectified linear unit (ReLU); training
"<b>AlexisSays</b>
My true opinion: if they were called “looping neural networks,” or
LNNs, they would seem 50% less daunting!
Now that we know what an RNN is, let’s try to make it a little bit better.
<header><largefont><b>Improving</b></largefont> <largefont><b>the</b></largefont> <largefont><b>RNN</b></largefont></header>
Looking at the code for our RNN, one thing that seems problematic is that we are
initializing our hidden state to zero for every new input sequence. Why is that a prob‐
lem? We made our sample sequences short so they would fit easily into batches. But if
we order those samples correctly, the sample sequences will be read in order by the
model, exposing the model to long stretches of the original sequence.
Another thing we can look at is having more signal: why predict only the fourth word
when we could use the intermediate predictions to also predict the second and third
words? Let’s see how we can implement those changes, starting with adding some
state.
<header><largefont><b>Maintaining</b></largefont> <largefont><b>the</b></largefont> <largefont><b>State</b></largefont> <largefont><b>of</b></largefont> <largefont><b>an</b></largefont> <largefont><b>RNN</b></largefont></header>
Because we initialize the model’s hidden state to zero for each new sample, we are
throwing away all the information we have about the sentences we have seen so far,
which means that our model doesn’t actually know where we are up to in the overall
counting sequence. This is easily fixed; we can simply move the initialization of the
hidden state to __init__.
But this fix will create its own subtle, but important, problem. It effectively makes our
neural network as deep as the entire number of tokens in our document. For instance,
if there were 10,000 tokens in our dataset, we would be creating a 10,000-layer neural
network.
To see why this is the case, consider the original pictorial representation of our recur‐
rent neural network in Figure 12-3, before refactoring it with a for loop. You can see
each layer corresponds with one token input. When we talk about the representation
of a recurrent neural network before refactoring with the for loop, we call this the
<i>unrolled</i> <i>representation.</i> It is often helpful to consider the unrolled representation
when trying to understand an RNN.
The problem with a 10,000-layer neural network is that if and when you get to the
10,000th word of the dataset, you will still need to calculate the derivatives all the way
back to the first layer. This is going to be slow indeed, and memory-intensive. It is
unlikely that you’ll be able to store even one mini-batch on your GPU."|recurrent neural networks (RNNs); improved RNN
"<b>class</b> <b>Normalize:</b>
<b>def</b> <b>__init__(self,</b> stats): self.stats=stats
<b>def</b> <b>__call__(self,</b> x):
<b>if</b> x.device != self.stats[0].device:
self.stats = to_device(self.stats, x.device)
<b>return</b> (x-self.stats[0])/self.stats[1]
We always like to test everything we build in a notebook, as soon as we build it:
norm = Normalize(stats)
<b>def</b> tfm_x(x): <b>return</b> norm(x).permute((0,3,1,2))
t = tfm_x(x)
t.mean((0,2,3)),t.std((0,2,3))
(tensor([0.3732, 0.4907, 0.5633]), tensor([1.0212, 1.0311, 1.0131]))
Here tfm_x isn’t just applying Normalize, but is also permuting the axis order from
NHWC to NCHW (see Chapter 13 if you need a reminder of what these acronyms refer to).
PIL uses HWC axis order, which we can’t use with PyTorch, hence the need for this
permute.
That’s all we need for the data for our model. So now we need the model itself!
<header><largefont><b>Module</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Parameter</b></largefont></header>
To create a model, we’ll need Module . To create Module , we’ll need Parameter , so let’s
start there. Recall that in Chapter 8 we said that the Parameter class “doesn’t add any
functionality (other than automatically calling requires_grad_ for us). It’s used only
parameters.”
as a ‘marker’ to show what to include in Here’s a definition that does
exactly that:
<b>class</b> <b>Parameter(Tensor):</b>
<b>def</b> <b>__new__(self,</b> x): <b>return</b> Tensor._make_subclass(Parameter, x, True)
<b>def</b> <b>__init__(self,</b> *args, **kwargs): self.requires_grad_()
The implementation here is a bit awkward: we have to define the special __new__
Python method and use the internal PyTorch method _make_subclass because, at the
time of writing, PyTorch doesn’t otherwise work correctly with this kind of subclass‐
ing or provide an officially supported API to do this. This may have been fixed by the
time you read this, so look on the book’s website to see if there are updated details.
Our Parameter now behaves just like a tensor, as we wanted:
Parameter(tensor(3.))
tensor(3., requires_grad=True)"|Learner; testing models
"To be more specific, here are the steps required to turn this function into a machine
learning classifier:
1. <i>Initialize</i> the weights.
2. For each image, use these weights to <i>predict</i> whether it appears to be a 3 or a 7.
3. Based on these predictions, calculate how good the model is (its <i>loss).</i>
4. Calculate the <i>gradient,</i> which measures for each weight how changing that weight
would change the loss.
5. <i>Step</i> (that is, change) all the weights based on that calculation.
6. Go back to step 2 and <i>repeat</i> the process.
7. Iterate until you decide to <i>stop</i> the training process (for instance, because the
model is good enough or you don’t want to wait any longer).
These seven steps, illustrated in Figure 4-1, are the key to the training of all deep
learning models. That deep learning turns out to rely entirely on these steps is
extremely surprising and counterintuitive. It’s amazing that this process can solve
such complex problems. But, as you’ll see, it really does!
<i>Figure</i> <i>4-1.</i> <i>The</i> <i>gradient</i> <i>descent</i> <i>process</i>
There are many ways to do each of these seven steps, and we will be learning about
them throughout the rest of this book. These are the details that make a big difference
for deep learning practitioners, but it turns out that the general approach to each one
follows some basic principles. Here are a few guidelines:
<i>Initialize</i>
We initialize the parameters to random values. This may sound surprising. There
are certainly other choices we could make, such as initializing them to the per‐
centage of times that pixel is activated for that category—but since we already
know that we have a routine to improve these weights, it turns out that just start‐
ing with random weights works perfectly well.
<i>Loss</i>
This is what Samuel referred to when he spoke of <i>testing</i> <i>the</i> <i>effectiveness</i> <i>of</i> <i>any</i>
<i>current</i> <i>weight</i> <i>assignment</i> <i>in</i> <i>terms</i> <i>of</i> <i>actual</i> <i>performance.</i> We need a function that
will return a number that is small if the performance of the model is good (the"|stochastic gradient descent; stochastic gradient descent (SGD)
"We reached 94.3% accuracy, which was state-of-the-art performance just three years
ago. By training another model on all the texts read backward and averaging the pre‐
dictions of those two models, we can even get to 95.1% accuracy, which was the state
of the art introduced by the ULMFiT paper. It was beaten only a few months ago, by
fine-tuning a much bigger model and using expensive data augmentation techniques
(translating sentences in another language and back, using another model for
translation).
Using a pretrained model let us build a fine-tuned language model that is pretty pow‐
erful, to either generate fake reviews or help classify them. This is exciting stuff, but
it’s good to remember that this technology can also be used for malign purposes.
<header><largefont><b>Disinformation</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Language</b></largefont> <largefont><b>Models</b></largefont></header>
Even simple algorithms based on rules, before the days of widely available deep learn‐
ing language models, could be used to create fraudulent accounts and try to influence
policymakers. Jeff Kao, now a computational journalist at ProPublica, analyzed the
comments that were sent to the US Federal Communications Commission (FCC)
regarding a 2017 proposal to repeal net neutrality. In his article “More than a Million
Pro-Repeal Net Neutrality Comments Were Likely Faked”, he reports how he discov‐
ered a large cluster of comments opposing net neutrality that seemed to have been
generated by some sort of Mad Libs–style mail merge. In Figure 10-2, the fake com‐
ments have been helpfully color-coded by Kao to highlight their formulaic nature.
<i>Figure</i> <i>10-2.</i> <i>Comments</i> <i>received</i> <i>by</i> <i>the</i> <i>FCC</i> <i>during</i> <i>the</i> <i>net</i> <i>neutrality</i> <i>debate</i>"|disinformation; Kao; natural language processing (NLP); net neutrality disinformation; text generation
"As you can see, Learner is really just using our standard training loop, except that it’s
also calling callbacks at appropriate times. So let’s define some callbacks!
<header><largefont><b>Callbacks</b></largefont></header>
In Learner.__init__ we have
<b>for</b> cb <b>in</b> cbs: cb.learner = self
In other words, every callback knows what learner it is used in. This is critical, since
otherwise a callback can’t get information from the learner, or change things in the
learner. Because getting information from the learner is so common, we make that
easier by defining Callback as a subclass of GetAttr , with a default attribute of
learner:
<b>class</b> <b>Callback(GetAttr):</b> _default='learner'
GetAttr __getattr__ __dir__
is a fastai class that implements Python’s standard and
methods for you, so that anytime you try to access an attribute that doesn’t exist, it
passes the request along to whatever you have defined as _default.
For instance, we want to move all model parameters to the GPU automatically at the
start of fit . We could do this by defining before_fit as self.learner.model.cuda ;
however, because learner is the default attribute, and we have SetupLearnerCB
Callback GetAttr), .learner
inherit from (which inherits from we can remove the
and just call self.model.cuda :
<b>class</b> <b>SetupLearnerCB(Callback):</b>
<b>def</b> before_batch(self):
xb,yb = to_device(self.batch)
self.learner.batch = tfm_x(xb),yb
<b>def</b> before_fit(self): self.model.cuda()
In SetupLearnerCB , we also move each mini-batch to the GPU, by calling
to_device(self.batch) (we could also have used the longer
to_device(self.learner.batch).
Note, however, that in the line
self.learner.batch = tfm_x(xb),yb, we can’t remove .learner, because here
we’re <i>setting</i> the attribute, not getting it.
Learner,
Before we try our let’s create a callback to track and print progress. Other‐
wise, we won’t really know if it’s working properly:
<b>class</b> <b>TrackResults(Callback):</b>
<b>def</b> before_epoch(self): self.accs,self.losses,self.ns = [],[],[]
<b>def</b> after_epoch(self):
n = sum(self.ns)
<b>print(self.epoch,</b> self.model.training,
sum(self.losses).item()/n, sum(self.accs).item()/n)"|Learner
"In the mid-level API for data collection, we have two objects that can help us apply
transforms on a set of items: TfmdLists and Datasets. If you remember what we
have just seen, one applies a Pipeline of transforms and the other applies several
Pipelines of transforms in parallel, to build tuples. Here, our main transform already
builds the tuples, so we use TfmdLists:
tls = TfmdLists(files, tfm, splits=splits)
show_at(tls.valid, 0);
And we can finally get our data in DataLoaders by calling the dataloaders method.
item_tfms
One thing to be careful of here is that this method does not take and
batch_tfms like a DataBlock . The fastai DataLoader has several hooks that are
named after events; here what we apply on the items after they are grabbed is called
after_item, after_batch:
and what we apply on the batch once it’s built is called
dls = tls.dataloaders(after_item=[Resize(224), ToTensor],
after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)])
Note that we need to pass more transforms than usual—that’s because the data block
API usually adds them automatically:
ToTensor
• is the one that converts images to tensors (again, it’s applied on every
part of the tuple).
IntToFloatTensor
• converts the tensor of images containing integers from 0 to
255 to a tensor of floats, and divides by 255 to make the values between 0 and 1.
We can now train a model using this DataLoaders . It will need a bit more customiza‐
tion than the usual model provided by cnn_learner since it has to take two images
instead of one, but we will see how to create such a model and train it in Chapter 15."|Siamese model image comparison
"<i>Figure</i> <i>9-4.</i> <i>Date</i> <i>embeddings</i> <i>(courtesy</i> <i>of</i> <i>Cheng</i> <i>Guo</i> <i>and</i> <i>Felix</i> <i>Berkhahn)</i>
In addition, it is valuable in its own right that embeddings are continuous, because
models are better at understanding continuous variables. This is unsurprising consid‐
ering models are built of many continuous parameter weights and continuous activa‐
tion values, which are updated via gradient descent (a learning algorithm for finding
the minimums of continuous functions).
Another benefit is that we can combine our continuous embedding values with truly
continuous input data in a straightforward manner: we just concatenate the variables
and feed the concatenation into our first dense layer. In other words, the raw catego‐
rical data is transformed by an embedding layer before it interacts with the raw con‐
tinuous input data. This is how fastai and Guo and Berkhahn handle tabular models
containing continuous and categorical variables.
An example using this concatenation approach is how Google does its recommenda‐
tions on Google Play, as explained in the paper “Wide & Deep Learning for Recom‐
mender Systems”. Figure 9-5 illustrates this.
Interestingly, the Google team combined both approaches we saw in the previous
chapter: the dot product (which they call <i>cross</i> <i>product)</i> and neural network
approaches."|categorical variables; concatenating categorical and continuous variables; Google Play; embedded categorical combined with; dot product of vectors; Play concatenation approach; Google Play concatenation approach; vector dot product
"<b>Text(naturallanguageprocessing)</b>
Computers are good at classifying both short and long documents based on cate‐
gories such as spam or not spam, sentiment (e.g., is the review positive or negative),
author, source website, and so forth. We are not aware of any rigorous work done in
this area to compare computers to humans, but anecdotally it seems to us that deep
learning performance is similar to human performance on these tasks.
Deep learning is also good at generating context-appropriate text, such as replies to
social media posts, and imitating a particular author’s style. It’s good at making this
content compelling to humans too—in fact, even more compelling than human-
generated text. However, deep learning is not good at generating <i>correct</i> responses!
We don’t have a reliable way to, for instance, combine a knowledge base of medical
information with a deep learning model for generating medically correct natural lan‐
guage responses. This is dangerous, because it is so easy to create content that appears
to a layman to be compelling, but actually is entirely incorrect.
Another concern is that context-appropriate, highly compelling responses on social
media could be used at massive scale—thousands of times greater than any troll farm
previously seen—to spread disinformation, create unrest, and encourage conflict. As
a rule of thumb, text generation models will always be technologically a bit ahead of
models for recognizing automatically generated text. For instance, it is possible to use
a model that can recognize artificially generated content to actually improve the gen‐
erator that creates that content, until the classification model is no longer able to
complete its task.
Despite these issues, deep learning has many applications in NLP: it can be used to
translate text from one language to another, summarize long documents into some‐
thing that can be digested more quickly, find all mentions of a concept of interest, and
more. Unfortunately, the translation or summary could well include completely
incorrect information! However, the performance is already good enough that many
people are using these systems—for instance, Google’s online translation system (and
every other online service we are aware of) is based on deep learning.
<b>Combiningtextandimages</b>
The ability of deep learning to combine text and images into a single model is, gener‐
ally, far better than most people intuitively expect. For example, a deep learning
model can be trained on input images with output captions written in English, and
can learn to generate surprisingly appropriate captions automatically for new images!
But again, we have the same warning that we discussed in the previous section: there
is no guarantee that these captions will be correct.
Because of this serious issue, we generally recommend that deep learning be used not
as an entirely automated process, but as part of a process in which the model and a
human user interact closely. This can potentially make humans orders of magnitude"|ethics; model and human interaction; disinformation; medicine and text generation; images combined with text; correct responses not ensured; natural language processing (NLP); correct response not ensured; text combined with images; text generation; translation of languages
"Do you recognize that name? You saw it in Chapter 2, when we were talking about
the Turing Award winners who established the foundations of deep learning today!
Jeremy also asked on Twitter for help checking that our description of label smooth‐
ing in Chapter 7 was accurate, and got a response again directly from Christian Szeg‐
edy (label smoothing was originally introduced in the Inception paper):
Many of the top people in deep learning today are Twitter regulars, and are very open
about interacting with the wider community. One good way to get started is to look at
a list of Jeremy’s recent Twitter likes, or Sylvain’s. That way, you can see a list of Twit‐
ter users whom we think have interesting and useful things to say.
Twitter is the main way we both stay up to date with interesting papers, software
releases, and other deep learning news. For making connections with the deep learn‐
ing community, we recommend getting involved both in the fast.ai forums and on
Twitter.
That said, let’s get back to the meat of this chapter. Up until now, we have shown you
examples of pictures in only black and white, with one value per pixel. In practice,"|building a CNN
"<header><largefont><b>CHAPTER</b></largefont> <largefont><b>2</b></largefont></header>
<header><largefont><b>From</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>to</b></largefont> <largefont><b>Production</b></largefont></header>
The six lines of code we saw in Chapter 1 are just one small part of the process of
using deep learning in practice. In this chapter, we’re going to use a computer vision
example to look at the end-to-end process of creating a deep learning application.
More specifically, we’re going to build a bear classifier! In the process, we’ll discuss
the capabilities and constraints of deep learning, explore how to create datasets, look
at possible gotchas when using deep learning in practice, and more. Many of the key
points will apply equally well to other deep learning problems, such as those in Chap‐
ter 1. If you work through a problem similar in key respects to our example problems,
we expect you to get excellent results with little code, quickly.
Let’s start with how you should frame your problem.
<header><largefont><b>The</b></largefont> <largefont><b>Practice</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont></header>
We’ve seen that deep learning can solve a lot of challenging problems quickly and
with little code. As a beginner, there’s a sweet spot of problems that are similar
enough to our example problems that you can very quickly get extremely useful
results. However, deep learning isn’t magic! The same six lines of code won’t work for
every problem anyone can think of today.
Underestimating the constraints and overestimating the capabilities of deep learning
may lead to frustratingly poor results, at least until you gain some experience and can
solve the problems that arise. Conversely, overestimating the constraints and under‐
estimating the capabilities of deep learning may mean you do not attempt a solvable
problem because you talk yourself out of it.
We often talk to people who underestimate both the constraints and the capabilities
of deep learning. Both of these can be problems: underestimating the capabilities
means that you might not even try things that could be very beneficial, and underes‐"|capabilities and constraints; process end-to-end
"For this initial test, we’ll use the same simple model that we used in Chapter 1:
learn = cnn_learner(dls, resnet34, metrics=error_rate)
learn.fine_tune(2)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 1.491732 0.337355 0.108254 00:18
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
0 0.503154 0.293404 0.096076 00:23
1 0.314759 0.225316 0.066306 00:23
As we’ve briefly discussed before, the table shown when we fit a model shows us the
results after each epoch of training. Remember, an epoch is one complete pass
through all of the images in the data. The columns shown are the average loss over
the items of the training set, the loss on the validation set, and any metrics that we
requested—in this case, the error rate.
Remember that <i>loss</i> is whatever function we’ve decided to use to optimize the param‐
eters of our model. But we haven’t actually told fastai what loss function we want to
use. So what is it doing? fastai will generally try to select an appropriate loss function
based on the kind of data and model you are using. In this case, we have image data
and a categorical outcome, so fastai will default to using <i>cross-entropy</i> <i>loss.</i>
<header><largefont><b>Cross-Entropy</b></largefont> <largefont><b>Loss</b></largefont></header>
<i>Cross-entropy</i> <i>loss</i> is a loss function that is similar to the one we used in the previous
chapter, but (as we’ll see) has two benefits:
• It works even when our dependent variable has more than two categories.
• It results in faster and more reliable training.
To understand how cross-entropy loss works for dependent variables with more than
two categories, we first have to understand what the actual data and activations that
are seen by the loss function look like.
<header><largefont><b>Viewing</b></largefont> <largefont><b>Activations</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Labels</b></largefont></header>
Let’s take a look at the activations of our model. To get a batch of real data from our
DataLoaders, we can use the one_batch method:
x,y = dls.one_batch()"|categorical outcome cross-entropy loss; cross-entropy loss; image data and categorical outcome; epochs; loss function selected by; fitting models; image classifier model training; cross-entropy; fastai selecting function; pet breeds image classifier; loss function selected by fastai
"you are happy to wait for. Then look at the training and validation loss plots, as
shown previously, and in particular your metrics. If you see that they are still getting
better even in your final epochs, you know that you have not trained for too long.
On the other hand, you may well see that the metrics you have chosen are really get‐
ting worse at the end of training. Remember, it’s not just that we’re looking for the
validation loss to get worse, but the actual metrics. Your validation loss will first get
worse during training because the model gets overconfident, and only later will get
worse because it is incorrectly memorizing the data. We care in practice about only
the latter issue. Remember, our loss function is something that we use to allow our
optimizer to have something it can differentiate and optimize; it’s not the thing we
care about in practice.
Before the days of 1cycle training, it was common to save the model at the end of
each epoch, and then select whichever model had the best accuracy out of all of the
models saved in each epoch. This is known as <i>early</i> <i>stopping.</i> However, this is unlikely
to give you the best answer, because those epochs in the middle occur before the
learning rate has had a chance to reach the small values, where it can really find the
best result. Therefore, if you find that you have overfit, what you should do is retrain
your model from scratch, and this time select a total number of epochs based on
where your previous best results were found.
If you have the time to train for more epochs, you may want to instead use that time
to train more parameters—that is, use a deeper architecture.
<header><largefont><b>Deeper</b></largefont> <largefont><b>Architectures</b></largefont></header>
In general, a model with more parameters can model your data more accurately.
(There are lots and lots of caveats to this generalization, and it depends on the
specifics of the architectures you are using, but it is a reasonable rule of thumb for
now.) For most of the architectures that we will be seeing in this book, you can create
larger versions of them by simply adding more layers. However, since we want to use
pretrained models, we need to make sure that we choose a number of layers that have
already been pretrained for us.
This is why, in practice, architectures tend to come in a small number of variants. For
instance, the ResNet architecture that we are using in this chapter comes in variants
with 18, 34, 50, 101, and 152 layers, pretrained on ImageNet. A larger (more layers
and parameters; sometimes described as the <i>capacity</i> of a model) version of a ResNet
will always be able to give us a better training loss, but it can suffer more from overfit‐
ting, because it has more parameters to overfit with.
In general, a bigger model has the ability to better capture the real underlying rela‐
tionships in your data, as well as to capture and memorize the specific details of your
individual images."|architecture of model; ResNet; capacity of a model; deeper models having more layers; early stopping; epochs; metrics and validation loss; ResNet architecture; models; retrain from scratch; more accuracy from more parameters; ResNet-18; training
"<header><largefont><b>Loss</b></largefont></header>
We’ve already seen how to define “negative log likelihood”:
<b>def</b> nll(input, target): <b>return</b> -input[range(target.shape[0]), target].mean()
Well actually, there’s no log here, since we’re using the same definition as PyTorch.
That means we need to put the log together with softmax:
<b>def</b> log_softmax(x): <b>return</b> (x.exp()/(x.exp().sum(-1,keepdim=True))).log()
sm = log_softmax(r); sm[0][0]
tensor(-1.2790, grad_fn=<SelectBackward>)
Combining these gives us our cross-entropy loss:
loss = nll(sm, yb)
loss
tensor(2.5666, grad_fn=<NegBackward>)
Note that the formula
<i>a</i>
log = log <i>a</i> − log <i>b</i>
<i>b</i>
gives a simplification when we compute the log softmax, which was previously
defined as (x.exp()/(x.exp().sum(-1))).log() :
<b>def</b> log_softmax(x): <b>return</b> x - x.exp().sum(-1,keepdim=True).log()
sm = log_softmax(r); sm[0][0]
tensor(-1.2790, grad_fn=<SelectBackward>)
Then, there is a more stable way to compute the log of the sum of exponentials, called
the <i>LogSumExp</i> trick. The idea is to use the following formula
<i>n</i> <i>x</i> <i>n</i> <i>x</i> −a <i>n</i> <i>x</i> −a
<largefont>∑</largefont> <i>j</i> <i>a</i> <largefont>∑</largefont> <i>j</i> <largefont>∑</largefont> <i>j</i>
log <i>e</i> = log <i>e</i> <i>e</i> = <i>a</i> + log <i>e</i>
<i>j</i> = 1 <i>j</i> = 1 <i>j</i> = 1
where <i>a</i> is the maximum of <i>x</i> .
<i>j</i>
Here’s the same thing in code:
x = torch.rand(5)
a = x.max()
x.exp().sum().log() == a + (x-a).exp().sum().log()
tensor(True)"|Learner
"This has improved our model a bit, but there’s more we can do. The deepest layers of
our pretrained model might not need as high a learning rate as the last ones, so we
should probably use different learning rates for those—this is known as using <i>dis‐</i>
<i>criminative</i> learning rates.
<header><largefont><b>Discriminative</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Rates</b></largefont></header>
Even after we unfreeze, we still care a lot about the quality of those pretrained
weights. We would not expect that the best learning rate for those pretrained parame‐
ters would be as high as for the randomly added parameters, even after we have tuned
those randomly added parameters for a few epochs. Remember, the pretrained
weights have been trained for hundreds of epochs, on millions of images.
In addition, do you remember the images we saw in Chapter 1, showing what each
layer learns? The first layer learns very simple foundations, like edge and gradient
detectors; these are likely to be just as useful for nearly any task. The later layers learn
much more complex concepts, like “eye” and “sunset,” which might not be useful in
your task at all (maybe you’re classifying car models, for instance). So it makes sense
to let the later layers fine-tune more quickly than earlier layers.
Therefore, fastai’s default approach is to use discriminative learning rates. This tech‐
nique was originally developed in the ULMFiT approach to NLP transfer learning
that we will introduce in Chapter 10. Like many good ideas in deep learning, it is
extremely simple: use a lower learning rate for the early layers of the neural network,
and a higher learning rate for the later layers (and especially the randomly added lay‐
ers). The idea is based on insights developed by Jason Yosinski et al., who showed in
2014 that with transfer learning, different layers of a neural network should train at
different speeds, as seen in Figure 5-4."|discriminative learning rates; layers
"opportunity for you to rectify that! We find that regular expressions are one of the
most useful tools in our programming toolkit, and many of our students tell us that
this is one of the things they are most excited to learn about. So head over to Google
and search for “regular expressions tutorial” now, and then come back here after
you’ve had a good look around. The book’s website also provides a list of our
favorites.
<b>AlexisSays</b>
Not only are regular expressions dead handy, but they also have
interesting roots. They are “regular” because they were originally
examples of a “regular” language, the lowest rung within the
Chomsky hierarchy. This is a grammar classification developed by
linguist Noam Chomsky, who also wrote <i>Syntactic</i> <i>Structures,</i> the
pioneering work searching for the formal grammar underlying
human language. This is one of the charms of computing: the ham‐
mer you reach for every day may have, in fact, come from a
spaceship.
When you are writing a regular expression, the best way to start is to try it against
one example at first. Let’s use the findall method to try a regular expression against
the filename of the fname object:
re.findall(r'(.+)_\d+.jpg$', fname.name)
['great_pyrenees']
This regular expression plucks out all the characters leading up to the last underscore
character, as long as the subsequent characters are numerical digits and then the
JPEG file extension.
Now that we confirmed the regular expression works for the example, let’s use it to
label the whole dataset. fastai comes with many classes to help with labeling. For
labeling with regular expressions, we can use the RegexLabeller class. In this exam‐
ple, we use the data block API that we saw in Chapter 2 (in fact, we nearly always use
the data block API—it’s so much more flexible than the simple factory methods we
saw in Chapter 1):
pets = DataBlock(blocks = (ImageBlock, CategoryBlock),
get_items=get_image_files,
splitter=RandomSplitter(seed=42),
get_y=using_attr(RegexLabeller(r'(.+)_\d+.jpg$'), 'name'),
item_tfms=Resize(460),
batch_tfms=aug_transforms(size=224, min_scale=0.75))
dls = pets.dataloaders(path/""images"")"|CategoryBlock; Chomsky; filename extraction; pet breeds dataset; Chomsky’s syntax book; Syntactic Structures book (Chomsky); regular expression tutorials
"Let’s consider another example: recommendation systems. The <i>objective</i> of a recom‐
mendation engine is to drive additional sales by surprising and delighting the cus‐
tomer with recommendations of items they would not have purchased without the
recommendation. The <i>lever</i> is the ranking of the recommendations. New <i>data</i> must
be collected to generate recommendations that will <i>cause</i> <i>new</i> <i>sales.</i> This will require
conducting many randomized experiments in order to collect data about a wide
range of recommendations for a wide range of customers. This is a step that few
organizations take; but without it, you don’t have the information you need to opti‐
mize recommendations based on your true objective (more sales!).
Finally, you could build two <i>models</i> for purchase probabilities, conditional on seeing
or not seeing a recommendation. The difference between these two probabilities is a
utility function for a given recommendation to a customer. It will be low in cases
where the algorithm recommends a familiar book that the customer has already
rejected (both components are small) or a book that they would have bought even
without the recommendation (both components are large and cancel each other out).
As you can see, in practice often the practical implementation of your models will
require a lot more than just training a model! You’ll often need to run experiments to
collect more data, and consider how to incorporate your models into the overall sys‐
tem you’re developing. Speaking of data, let’s now focus on how to find data for your
project.
<header><largefont><b>Gathering</b></largefont> <largefont><b>Data</b></largefont></header>
For many types of projects, you may be able to find all the data you need online. The
project we’ll be completing in this chapter is a <i>bear</i> <i>detector.</i> It will discriminate
between three types of bear: grizzly, black, and teddy bears. There are many images
on the internet of each type of bear that we can use. We just need a way to find them
and download them.
We’ve provided a tool you can use for this purpose, so you can follow along with this
chapter and create your own image recognition application for whatever kinds of
objects you’re interested in. In the fast.ai course, thousands of students have presen‐
ted their work in the course forums, displaying everything from hummingbird variet‐
ies in Trinidad to bus types in Panama—one student even created an application that
would help his fiancée recognize his 16 cousins during Christmas vacation!
At the time of writing, Bing Image Search is the best option we know of for finding
and downloading images. It’s free for up to 1,000 queries per month, and each query
can download up to 150 images. However, something better might have come along
between when we wrote this and when you’re reading the book, so be sure to check
out this book’s website for our current recommendation."|Bing Image Search for gathering data; datasets; gathering data; image recognition applications; process end-to-end; recommendation systems
"For example, Kaggle had a competition to predict the sales in a chain of Ecuadorian
grocery stores. Kaggle’s training data ran from Jan 1, 2013 to Aug 15, 2017, and the
test data spanned from Aug 16, 2017 to Aug 31, 2017. That way, the competition
organizer ensured that entrants were making predictions for a time period that was <i>in</i>
<i>the</i> <i>future,</i> from the perspective of their model. This is similar to the way quantitative
hedge fund traders do <i>backtesting</i> to check whether their models are predictive of
future periods, based on past data.
A second common case occurs when you can easily anticipate ways the data you will
be making predictions for in production may be <i>qualitatively</i> <i>different</i> from the data
you have to train your model with.
In the Kaggle distracted driver competition, the independent variables are pictures of
drivers at the wheel of a car, and the dependent variables are categories such as text‐
ing, eating, or safely looking ahead. Lots of pictures are of the same drivers in differ‐
ent positions, as we can see in Figure 1-22. If you were an insurance company
building a model from this data, note that you would be most interested in how the
model performs on drivers it hasn’t seen before (since you would likely have training
data for only a small group of people). In recognition of this, the test data for the
competition consists of images of people that don’t appear in the training set.
<i>Figure</i> <i>1-22.</i> <i>Two</i> <i>pictures</i> <i>from</i> <i>the</i> <i>training</i> <i>data</i>
If you put one of the images in Figure 1-22 in your training set and one in the valida‐
tion set, your model will have an easy time making a prediction for the one in the
validation set, so it will seem to be performing better than it would on new people.
Another perspective is that if you used all the people in training your model, your
model might be overfitting to particularities of those specific people and not just
learning the states (texting, eating, etc.).
A similar dynamic was at work in the Kaggle fisheries competition to identify the spe‐
cies of fish caught by fishing boats in order to reduce illegal fishing of endangered
populations. The test set consisted of images from boats that didn’t appear in the"|validation set; distracted driver model; fisheries monitoring model competition; distracted driver model competition; time series analysis model competition; testing models; training
"<b>DoYourOwnExperiments</b>
In previous chapters of the book, we’d be adding a code example for
bernoulli_ here, so you can see exactly how it works. But now that
you know enough to do this yourself, we’re going to be doing fewer
and fewer examples for you, and instead expecting you to do your
own experiments to see how things work. In this case, you’ll see in
the end-of-chapter questionnaire that we’re asking you to experi‐
bernoulli_—but
ment with don’t wait for us to ask you to experi‐
ment to develop your understanding of the code we’re studying; go
ahead and do it anyway!
Using dropout before passing the output of our LSTM to the final layer will help
reduce overfitting. Dropout is also used in many other models, including the default
CNN head used in fastai.vision, and is available in fastai.tabular by passing the
ps parameter (where each “p” is passed to each added Dropout layer), as we’ll see in
Chapter 15.
Dropout has different behavior in training and validation mode, which we specified
using the training attribute in Dropout. Calling the train method on a Module sets
training to True (both for the module you call the method on and for every module
it recursively contains), and eval sets it to False. This is done automatically when
calling the methods of Learner , but if you are not using that class, remember to
switch from one to the other as needed.
<header><largefont><b>Activation</b></largefont> <largefont><b>Regularization</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Temporal</b></largefont> <largefont><b>Activation</b></largefont> <largefont><b>Regularization</b></largefont></header>
<i>Activation</i> <i>regularization</i> (AR) and <i>temporal</i> <i>activation</i> <i>regularization</i> (TAR) are two
regularization methods very similar to weight decay, discussed in Chapter 8. When
applying weight decay, we add a small penalty to the loss that aims at making the
weights as small as possible. For activation regularization, it’s the final activations
produced by the LSTM that we will try to make as small as possible, instead of the
weights.
To regularize the final activations, we have to store those somewhere, then add the
means of the squares of them to the loss (along with a multiplier alpha, which is just
like wd for weight decay):
loss += alpha * activations.pow(2).mean()
Temporal activation regularization is linked to the fact we are predicting tokens in a
sentence. That means it’s likely that the outputs of our LSTMs should somewhat make
sense when we read them in order. TAR is there to encourage that behavior by adding
a penalty to the loss to make the difference between two consecutive activations as
small as possible: our activations tensor has a shape bs x sl x n_hid , and we read"|activation regularization; temporal activation regularization; LSTM model; recurrent neural networks (RNNs)
"low, but it is not that common to find people who like science-fiction without action).
It would probably be better to pick a particular user to represent <i>average</i> <i>taste.</i>
Better still is to use a tabular model based on user metadata to construct your initial
embedding vector. When a user signs up, think about what questions you could ask
to help you understand their tastes. Then you can create a model in which the depen‐
dent variable is a user’s embedding vector, and the independent variables are the
results of the questions that you ask them, along with their signup metadata. We will
see in the next section how to create these kinds of tabular models. (You may have
noticed that when you sign up for services such as Pandora and Netflix, they tend to
ask you a few questions about what genres of movie or music you like; this is how
they come up with your initial collaborative filtering recommendations.)
One thing to be careful of is that a small number of extremely enthusiastic users may
end up effectively setting the recommendations for your whole user base. This is a
very common problem, for instance, in movie recommendation systems. People who
watch anime tend to watch a whole lot of it, and don’t watch very much else, and
spend a lot of time putting their ratings on websites. As a result, anime tends to be
heavily overrepresented in a lot of <i>best</i> <i>ever</i> <i>movies</i> lists. In this particular case, it can
be fairly obvious that you have a problem of representation bias, but if the bias is
occurring in the latent factors, it may not be obvious at all.
Such a problem can change the entire makeup of your user base, and the behavior of
your system. This is particularly true because of positive feedback loops. If a small
number of your users tend to set the direction of your recommendation system, they
are naturally going to end up attracting more people like them to your system. And
that will, of course, amplify the original representation bias. This type of bias is a nat‐
ural tendency to be amplified exponentially. You may have seen examples of company
executives expressing surprise at how their online platforms rapidly deteriorated in
such a way that they expressed values at odds with the values of the founders. In the
presence of these kinds of feedback loops, it is easy to see how such a divergence can
happen both quickly and in a way that is hidden until it is too late.
In a self-reinforcing system like this, we should probably expect these kinds of feed‐
back loops to be the norm, not the exception. Therefore, you should assume that you
will see them, plan for that, and identify up front how you will deal with these issues.
Try to think about all of the ways in which feedback loops may be represented in your
system, and how you might be able to identify them in your data. In the end, this is
coming back to our original advice about how to avoid disaster when rolling out any
kind of machine learning system. It’s all about ensuring that there are humans in the
loop; that there is careful monitoring, and a gradual and thoughtful rollout.
Our dot product model works quite well, and it is the basis of many successful real-
world recommendation systems. This approach to collaborative filtering is known as"|bias; collaborative filtering; built from scratch; probabilistic matrix factorization; feedback loops; latent factors; embedding from scratch; recommendation systems; representation bias
"fastai comes with a function that will do this for us—we just have to pass a column
name that contains dates:
df = add_datepart(df, 'saledate')
Let’s do the same for the test set while we’re there:
df_test = pd.read_csv(path/'Test.csv', low_memory=False)
df_test = add_datepart(df_test, 'saledate')
We can see that there are now lots of new columns in our DataFrame:
' '.join(o <b>for</b> o <b>in</b> df.columns <b>if</b> o.startswith('sale'))
'saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear
> saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start
> saleIs_year_end saleIs_year_start saleElapsed'
This is a good first step, but we will need to do a bit more cleaning. For this, we will
use fastai objects called TabularPandas and TabularProc.
<header><largefont><b>Using</b></largefont> <largefont><b>TabularPandas</b></largefont> <largefont><b>and</b></largefont> <largefont><b>TabularProc</b></largefont></header>
A second piece of preparatory processing is to be sure we can handle strings and
missing data. Out of the box, sklearn cannot do either. Instead we will use fastai’s class
TabularPandas , which wraps a Pandas DataFrame and provides a few conveniences.
To populate a TabularPandas, we will use two TabularProcs, Categorify and
FillMissing . A TabularProc is like a regular Transform , except for the following:
• It returns the exact same object that’s passed to it, after modifying the object in
place.
• It runs the transform once, when data is first passed in, rather than lazily as the
data is accessed.
Categorify is a TabularProc that replaces a column with a numeric categorical col‐
umn. FillMissing is a TabularProc that replaces missing values with the median of
the column, and creates a new Boolean column that is set to True for any row where
the value was missing. These two transforms are needed for nearly every tabular data‐
set you will use, so this is a good starting point for your data processing:
procs = [Categorify, FillMissing]
TabularPandas will also handle splitting the dataset into training and validation sets
for us. However, we need to be very careful about our validation set. We want to
design it so that it is like the <i>test</i> <i>set</i> Kaggle will use to judge the contest.
Recall the distinction between a validation set and a test set, as discussed in Chap‐
ter 1. A <i>validation</i> <i>set</i> is data we hold back from training in order to ensure that the
training process does not overfit on the training data. A <i>test</i> <i>set</i> is data that is held"|validation set; tabular dataset prep; time series dataset splitting; TabularPandas class; decision trees; fastai TabularPandas class; tabular data for models; TabularProc; TabularPandas splitting data; training
"<b>JeremySays</b>
Don’t worry; neither SGD nor neural nets are mathematically com‐
plex. Both nearly entirely rely on addition and multiplication to do
their work (but they do a <i>lot</i> of addition and multiplication!). The
main reaction we hear from students when they see the details is:
“Is that all it is?”
In other words, to recap, a neural network is a particular kind of machine learning
model, which fits right in to Samuel’s original conception. Neural networks are spe‐
cial because they are highly flexible, which means they can solve an unusually wide
range of problems just by finding the right weights. This is powerful, because stochas‐
tic gradient descent provides us a way to find those weight values automatically.
Having zoomed out, let’s now zoom back in and revisit our image classification prob‐
lem using Samuel’s framework.
Our inputs are the images. Our weights are the weights in the neural net. Our model
is a neural net. Our results are the values that are calculated by the neural net, like
“dog” or “cat.”
What about the next piece, an <i>automatic</i> <i>means</i> <i>of</i> <i>testing</i> <i>the</i> <i>effectiveness</i> <i>of</i> <i>any</i> <i>cur‐</i>
<i>rent</i> <i>weight</i> <i>assignment</i> <i>in</i> <i>terms</i> <i>of</i> <i>actual</i> <i>performance?</i> Determining “actual perfor‐
mance” is easy enough: we can simply define our model’s performance as its accuracy
at predicting the correct answers.
Putting this all together, and assuming that SGD is our mechanism for updating the
weight assignments, we can see how our image classifier is a machine learning model,
much like Samuel envisioned.
<header><largefont><b>A</b></largefont> <largefont><b>Bit</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Deep</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Jargon</b></largefont></header>
Samuel was working in the 1960s, and since then terminology has changed. Here is
the modern deep learning terminology for all the pieces we have discussed:
• The functional form of the <i>model</i> is called its <i>architecture</i> (but be careful—some‐
times people use <i>model</i> as a synonym of <i>architecture,</i> so this can get confusing).
• The <i>weights</i> are called <i>parameters.</i>
• The <i>predictions</i> are calculated from the <i>independent</i> <i>variable,</i> which is the <i>data</i>
not including the <i>labels.</i>
• The <i>results</i> of the model are called <i>predictions.</i>
• The measure of <i>performance</i> is called the <i>loss.</i>"|architecture of model; independent variable definition; deep learning; as neural net; independent variable; image classification explanation; labels; loss; first model as neural net; neural networks; first model as; performance of model as loss; predictions; terminology for deep learning; weights
"<header><largefont><b>TfmdLists</b></largefont></header>
Here is the short way of doing the transformation we saw in the previous section:
tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize])
At initialization, the TfmdLists will automatically call the setup method of each
Transform in order, providing each not with the raw items but the items transformed
by all the previous Transforms, in order. We can get the result of our Pipeline on any
raw element just by indexing into the TfmdLists:
t = tls[0]; t[:20]
tensor([ 2, 8, 91, 11, 22, 5793, 22, 37, 4910, 34,
> 11, 8, 13042, 23, 107, 30, 11, 25, 44, 14])
And the TfmdLists knows how to decode for show purposes:
tls.decode(t)[:100]
'xxbos xxmaj well , "" cube "" ( 1997 ) , xxmaj vincenzo \'s first movie , was one
> of the most interesti'
In fact, it even has a show method:
tls.show(t)
xxbos xxmaj well , "" cube "" ( 1997 ) , xxmaj vincenzo 's first movie , was one
> of the most interesting and tricky ideas that xxmaj i 've ever seen when
> talking about movies . xxmaj they had just one scenery , a bunch of actors
> and a plot . xxmaj so , what made it so special were all the effective
> direction , great dialogs and a bizarre condition that characters had to deal
> like rats in a labyrinth . xxmaj his second movie , "" cypher "" ( 2002 ) , was
> all about its story , but it was n't so good as "" cube "" but here are the
> characters being tested like rats again .
"" nothing "" is something very interesting and gets xxmaj vincenzo coming back
> to his ' cube days ' , locking the characters once again in a very different
> space with no time once more playing with the characters like playing with
> rats in an experience room . xxmaj but instead of a thriller sci - fi ( even
> some of the promotional teasers and trailers erroneous seemed like that ) , ""
> nothing "" is a loose and light comedy that for sure can be called a modern
> satire about our society and also about the intolerant world we 're living .
> xxmaj once again xxmaj xxunk amaze us with a great idea into a so small kind
> of thing . 2 actors and a blinding white scenario , that 's all you got most
> part of time and you do n't need more than that . xxmaj while "" cube "" is a
> claustrophobic experience and "" cypher "" confusing , "" nothing "" is
> completely the opposite but at the same time also desperate .
xxmaj this movie proves once again that a smart idea means much more than just
> a millionaire budget . xxmaj of course that the movie fails sometimes , but
> its prime idea means a lot and offsets any flaws . xxmaj there 's nothing
> more to be said about this movie because everything is a brilliant surprise
> and a totally different experience that i had in movies since "" cube "" ."|TfmdLists
"learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,
loss_func=mnist_loss, metrics=batch_accuracy)
Now we can call fit:
learn.fit(10, lr=lr)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>batch_accuracy</b> <b>time</b>
0 0.636857 0.503549 0.495584 00:00
1 0.545725 0.170281 0.866045 00:00
2 0.199223 0.184893 0.831207 00:00
3 0.086580 0.107836 0.911187 00:00
4 0.045185 0.078481 0.932777 00:00
5 0.029108 0.062792 0.946516 00:00
6 0.022560 0.053017 0.955348 00:00
7 0.019687 0.046500 0.962218 00:00
8 0.018252 0.041929 0.965162 00:00
9 0.017402 0.038573 0.967615 00:00
As you can see, there’s nothing magic about the PyTorch and fastai classes. They are
just convenient prepackaged pieces that make your life a bit easier! (They also pro‐
vide a lot of extra functionality we’ll be using in future chapters.)
With these classes, we can now replace our linear model with a neural network.
<header><largefont><b>Adding</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Nonlinearity</b></largefont></header>
So far, we have a general procedure for optimizing the parameters of a function, and
we have tried it out on a boring function: a simple linear classifier. A linear classifier
is constrained in terms of what it can do. To make it a bit more complex (and able to
handle more tasks), we need to add something nonlinear (i.e., different from ax+b)
between two linear classifiers—this is what gives us a neural network.
Here is the entire definition of a basic neural network:
<b>def</b> simple_net(xb):
res = xb@w1 + b1
res = res.max(tensor(0.0))
res = res@w2 + b2
<b>return</b> res
That’s it! All we have in simple_net is two linear classifiers with a max function
between them.
Here, w1 and w2 are weight tensors, and b1 and b2 are bias tensors; that is, parameters
that are initially randomly initialized, just as we did in the previous section:"|numerical digit classifier; linear and nonlinear layers; neural networks; PyTorch; nonlinear and linear layers; optimization; creating an optimizer; stochastic gradient descent (SGD)
"The clearest way to display the contributions is with a <i>waterfall</i> <i>plot.</i> This shows how
the positive and negative contributions from all the independent variables sum up to
create the final prediction, which is the righthand column labeled “net” here:
waterfall(valid_xs_final.columns, contributions[0], threshold=0.08,
rotation_value=45,formatting='{:,.3f}');
This kind of information is most useful in production, rather than during model
development. You can use it to provide useful information to users of your data prod‐
uct about the underlying reasoning behind the predictions.
Now that we covered some classic machine learning techniques to solve this problem,
let’s see how deep learning can help!
<header><largefont><b>Extrapolation</b></largefont> <largefont><b>and</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Networks</b></largefont></header>
A problem with random forests, like all machine learning or deep learning algo‐
rithms, is that they don’t always generalize well to new data. We’ll see in which situa‐
tions neural networks generalize better, but first, let’s look at the extrapolation
problem that random forests have and how they can help identify out-of-domain
data."|bagging; decision trees; generalization by models; machine learning (ML); predictions; random forests; tabular data for models; training
"In practice, we have not seen many examples of people training multi-label classifiers
for this purpose—but we often see both users and developers complaining about this
problem. It appears that this simple solution is not at all widely understood or appre‐
ciated! Because in practice it is probably more common to have some images with
zero matches or more than one match, we should probably expect in practice that
multi-label classifiers are more widely applicable than single-label classifiers.
First let’s see what a multi-label dataset looks like; then we’ll explain how to get it
ready for our model. You’ll see that the architecture of the model does not change
from the preceding chapter; only the loss function does. Let’s start with the data.
<header><largefont><b>The</b></largefont> <largefont><b>Data</b></largefont></header>
For our example, we are going to use the PASCAL dataset, which can have more than
one kind of classified object per image.
We begin by downloading and extracting the dataset as per usual:
<b>from</b> <b>fastai.vision.all</b> <b>import</b> *
path = untar_data(URLs.PASCAL_2007)
This dataset is different from the ones we have seen before, in that it is not structured
by filename or folder but instead comes with a CSV file telling us what labels to use
for each image. We can inspect the CSV file by reading it into a Pandas DataFrame:
df = pd.read_csv(path/'train.csv')
df.head()
<b>fname</b> <b>labels</b> <b>is_valid</b>
<b>0</b> 000005.jpg chair True
<b>1</b> 000007.jpg car True
<b>2</b> 000009.jpg horseperson True
<b>3</b> 000012.jpg car False
<b>4</b>
000016.jpg bicycle True
As you can see, the list of categories in each image is shown as a space-delimited
string.
<header><largefont><b>Pandas</b></largefont> <largefont><b>and</b></largefont> <largefont><b>DataFrames</b></largefont></header>
No, it’s not actually a panda! <i>Pandas</i> is a Python library that is used to manipulate and
analyze tabular and time series data. The main class is DataFrame, which represents a
table of rows and columns."|multi-label classification; multi-label CSV file; PASCAL multi-label dataset; labels
"the models. Most interestingly, the difference was observed not only in the validation
set, but also in the training set; so it wasn’t just a generalization issue, but a training
issue. As the paper explains:
Unexpectedly, such degradation is not caused by overfitting, and adding more layers to
a suitably deep model leads to higher training error, as [previously reported] and thor‐
oughly verified by our experiments.
This phenomenon was illustrated by the graph in Figure 14-1, with training error on
the left and test error on the right.
<i>Figure</i> <i>14-1.</i> <i>Training</i> <i>of</i> <i>networks</i> <i>of</i> <i>different</i> <i>depth</i> <i>(courtesy</i> <i>of</i> <i>Kaiming</i> <i>He</i> <i>et</i> <i>al.)</i>
As the authors mention here, they are not the first people to have noticed this curious
fact. But they were the first to make a very important leap:
Let us consider a shallower architecture and its deeper counterpart that adds more lay‐
ers onto it. There exists a solution by construction to the deeper model: the added lay‐
ers are identity mapping, and the other layers are copied from the learned shallower
model.
As this is an academic paper, this process is described in a rather inaccessible way, but
the concept is actually very simple: start with a 20-layer neural network that is trained
well, and add another 36 layers that do nothing at all (for instance, they could be lin‐
ear layers with a single weight equal to 1, and bias equal to 0). The result will be a 56-
layer network that does exactly the same thing as the 20-layer network, proving that
there are always deep networks that should be <i>at</i> <i>least</i> <i>as</i> <i>good</i> as any shallow network.
But for some reason, SGD does not seem able to find them.
<b>Jargon:IdentityMapping</b>
Returning the input without changing it at all. This process is per‐
formed by an <i>identity</i> <i>function.</i>"|building ResNet CNN; identity function; identity mapping; ResNet architecture; skip connections
"what L uses by default. Note that fastai’s tokenizers take a collection of documents to
tokenize, so we have to wrap txt in a list:
spacy = WordTokenizer()
toks = first(spacy([txt]))
<b>print(coll_repr(toks,</b> 30))
(#201) ['This','movie',',','which','I','just','discovered','at','the','video','s
> tore',',','has','apparently','sit','around','for','a','couple','of','years','
> without','a','distributor','.','It',""'s"",'easy','to','see'...]
As you see, spaCy has mainly just separated out the words and punctuation. But it
does something else here too: it has split “it’s” into “it” and “’s”. That makes intuitive
sense; these are separate words, really. Tokenization is a surprisingly subtle task, when
you think about all the little details that have to be handled. Fortunately, spaCy han‐
dles these pretty well for us—for instance, here we see that “.” is separated when it ter‐
minates a sentence, but not in an acronym or number:
first(spacy(['The U.S. dollar $1 is $1.00.']))
(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']
fastai then adds some additional functionality to the tokenization process with the
Tokenizer class:
tkn = Tokenizer(spacy)
<b>print(coll_repr(tkn(txt),</b> 31))
(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',
> 'the','video','store',',','has','apparently','sit','around','for','a','couple
> ','of','years','without','a','distributor','.','xxmaj','it',""'s"",'easy'...]
Notice that there are now some tokens that start with the characters “xx”, which is not
a common word prefix in English. These are <i>special</i> <i>tokens.</i>
For example, the first item in the list, xxbos , is a special token that indicates the start
of a new text (“BOS” is a standard NLP acronym that means “beginning of stream”).
By recognizing this start token, the model will be able to learn it needs to “forget”
what was said previously and focus on upcoming words.
These special tokens don’t come from spaCy directly. They are there because fastai
adds them by default, by applying a number of rules when processing text. These
rules are designed to make it easier for a model to recognize the important parts of a
sentence. In a sense, we are translating the original English language sequence into a
simplified tokenized language—a language that is designed to be easy for a model to
learn.
For instance, the rules will replace a sequence of four exclamation points with a single
exclamation point, followed by a special <i>repeated</i> <i>character</i> token and then the num‐
ber four. In this way, the model’s embedding matrix can encode information about
general concepts such as repeated punctuation rather than requiring a separate token"|natural language processing (NLP); special tokens; tokenization
"This is important because it means that whatever ideas we have in our heads, we can
implement them. We need never dig into the source code of PyTorch or fastai and
hack together a one-off system to try out our ideas. And when we do implement our
own callbacks to develop our own ideas, we know that they will work together with
all of the other functionality provided by fastai—so we will get progress bars, mixed-
precision training, hyperparameter annealing, and so forth.
Another advantage is that it makes it easy to gradually remove or add functionality
and perform ablation studies. You just need to adjust the list of callbacks you pass
along to your fit function.
As an example, here is the fastai source code that is run for each batch of the training
loop:
<b>try:</b>
self._split(b); self('begin_batch')
self.pred = self.model(*self.xb); self('after_pred')
self.loss = self.loss_func(self.pred, *self.yb); self('after_loss')
<b>if</b> <b>not</b> self.training: <b>return</b>
self.loss.backward(); self('after_backward')
self.opt.step(); self('after_step')
self.opt.zero_grad()
<b>except</b> CancelBatchException: self('after_cancel_batch')
<b>finally:</b> self('after_batch')
The calls of the form self('...') are where the callbacks are called. As you see, this
happens after every step. The callback will receive the entire state of training and can
also modify it. For instance, the input data and target labels are in self.xb and
self.yb , respectively; a callback can modify these to modify the data the training
loop sees. It can also modify self.loss or even the gradients.
Let’s see how this works in practice by writing a callback.
<header><largefont><b>Creating</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Callback</b></largefont></header>
When you want to write your own callback, the full list of available events is as
follows:
begin_fit
Called before doing anything; ideal for initial setup.
begin_epoch
Called at the beginning of each epoch; useful for any behavior you need to reset
at each epoch.
begin_train
Called at the beginning of the training part of an epoch."|callbacks; training
"We can see, however, that if we were to just zero those activations without doing any‐
thing else, our model would have problems training: if we go from the sum of five
activations (that are all positive numbers since we apply a ReLU) to just two, this
won’t have the same scale. Therefore, if we apply dropout with a probability p , we
rescale all activations by dividing them by 1-p (on average p will be zeroed, so it
leaves 1-p), as shown in Figure 12-11.
<i>Figure</i> <i>12-11.</i> <i>Why</i> <i>we</i> <i>scale</i> <i>the</i> <i>activations</i> <i>when</i> <i>applying</i> <i>dropout</i> <i>(courtesy</i> <i>of</i> <i>Nitish</i>
<i>Srivastava</i> <i>et</i> <i>al.)</i>
This is a full implementation of the dropout layer in PyTorch (although PyTorch’s
native layer is actually written in C, not Python):
<b>class</b> <b>Dropout(Module):</b>
<b>def</b> <b>__init__(self,</b> p): self.p = p
<b>def</b> forward(self, x):
<b>if</b> <b>not</b> self.training: <b>return</b> x
mask = x.new(*x.shape).bernoulli_(1-p)
<b>return</b> x * mask.div_(1-p)
bernoulli_ p)
The method is creating a tensor of random zeros (with probability and
ones (with probability 1-p), which is then multiplied with our input before dividing
by 1-p . Note the use of the training attribute, which is available in any PyTorch
nn.Module, and tells us if we are doing training or inference."|LSTM model; recurrent neural networks (RNNs)
"That change makes the split much clearer in the tree visualization, even although it
doesn’t change the result of the model in any significant way. This is a great example
of how resilient decision trees are to data issues!
m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y)
dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,
fontname='DejaVu Sans', scale=1.6, label_fontsize=10,
orientation='LR')
Let’s now have the decision tree algorithm build a bigger tree. Here, we are not pass‐
ing in any stopping criteria such as max_leaf_nodes:
m = DecisionTreeRegressor()
m.fit(xs, y);
We’ll create a little function to check the root mean squared error of our model
(m_rmse), since that’s how the competition was judged:
<b>def</b> r_mse(pred,y): <b>return</b> round(math.sqrt(((pred-y)**2).mean()), 6)
<b>def</b> m_rmse(m, xs, y): <b>return</b> r_mse(m.predict(xs), y)
m_rmse(m, xs, y)
0.0
So, our model is perfect, right? Not so fast…remember, we really need to check the
validation set, to ensure we’re not overfitting:
m_rmse(m, valid_xs, valid_y)
0.337727
Oops—it looks like we might be overfitting pretty badly. Here’s why:
m.get_n_leaves(), len(xs)
(340909, 404710)"|decision trees; root mean squared log error; root mean squared log error as metric; tabular data for models; training
"We are already starting to see examples of machine learning being used to generate
identities. For example, Figure 10-4 shows a LinkedIn profile for Katie Jones.
<i>Figure</i> <i>10-4.</i> <i>Katie</i> <i>Jones’s</i> <i>LinkedIn</i> <i>profile</i>
Katie Jones was connected on LinkedIn to several members of mainstream Washing‐
ton think tanks. But she didn’t exist. That image you see was autogenerated by a gen‐
erative adversarial network, and somebody named Katie Jones has not, in fact,
graduated from the Center for Strategic and International Studies.
Many people assume or hope that algorithms will come to our defense here—that we
will develop classification algorithms that can automatically recognize autogenerated
content. The problem, however, is that this will always be an arms race, in which bet‐
ter classification (or discriminator) algorithms can be used to create better generation
algorithms.
<header><largefont><b>Conclusion</b></largefont></header>
In this chapter, we explored the last application covered out of the box by the fastai
library: text. We saw two types of models: language models that can generate texts,
and a classifier that determines whether a review is positive or negative. To build a
state-of-the art classifier, we used a pretrained language model, fine-tuned it to the
corpus of our task, then used its body (the encoder) with a new head to do the
classification.
Before we end this part of the book, we’ll take a look at how the fastai library can help
you assemble your data for your specific problems."|LinkedIn ML-generated profile; profile identity generated by ML
"<i>Figure</i> <i>4-6.</i> <i>Matrix</i> <i>multiplication</i>
This image shows two matrices, A and B, being multiplied together. Each item of the
AB, A
result, which we’ll call contains each item of its corresponding row of multiplied
by each item of its corresponding column of B, added together. For instance, row 1,
column 2 (the yellow dot with a red border) is calculated as <i>a</i> *b + <i>a</i> *b . If
1,1 1,2 1,2 2,2
you need a refresher on matrix multiplication, we suggest you take a look at the
“Intro to Matrix Multiplication” on Khan Academy, since this is the most important
mathematical operation in deep learning.
In Python, matrix multiplication is represented with the @ operator. Let’s try it:
<b>def</b> linear1(xb): <b>return</b> xb@weights + bias
preds = linear1(train_x)
preds
tensor([[20.2336],
[17.0644],
[15.2384],
...,
[18.3804],
[23.8567],
[28.6816]], grad_fn=<AddBackward0>)
The first element is the same as we calculated before, as we’d expect. This equation,
batch @ weights + bias, is one of the two fundamental equations of any neural net‐
work (the other one is the <i>activation</i> <i>function,</i> which we’ll see in a moment).
Let’s check our accuracy. To decide if an output represents a 3 or a 7, we can just
check whether it’s greater than 0, so our accuracy for each item can be calculated
(using broadcasting, so no loops!) as follows:"|MNIST loss function; numerical digit image classifier; fundamental weights and bias equation
"You can combine these with Python slice syntax ( [start:end] , with <i>end</i> being exclu‐
ded) to select part of a row or column:
tns[1,1:3]
tensor([5, 6])
And you can use the standard operators, such as +, -, *, and /:
tns+1
tensor([[2, 3, 4],
[5, 6, 7]])
Tensors have a type:
tns.type()
'torch.LongTensor'
And will automatically change that type as needed; for example, from int to float:
tns*1.5
tensor([[1.5000, 3.0000, 4.5000],
[6.0000, 7.5000, 9.0000]])
So, is our baseline model any good? To quantify this, we must define a metric.
<header><largefont><b>Computing</b></largefont> <largefont><b>Metrics</b></largefont> <largefont><b>Using</b></largefont> <largefont><b>Broadcasting</b></largefont></header>
Recall that a <i>metric</i> is a number that is calculated based on the predictions of our
model and the correct labels in our dataset, in order to tell us how good our model is.
For instance, we could use either of the functions we saw in the previous section,
mean squared error or mean absolute error, and take the average of them over the
whole dataset. However, neither of these are numbers that are very understandable to
most people; in practice, we normally use <i>accuracy</i> as the metric for classification
models.
As we’ve discussed, we want to calculate our metric over a <i>validation</i> <i>set.</i> This is so
that we don’t inadvertently overfit—that is, train a model to work well only on our
training data. This is not really a risk with the pixel similarity model we’re using here
as a first try, since it has no trained components, but we’ll use a validation set anyway
to follow normal practices and to be ready for our second try later.
To get a validation set, we need to remove some of the data from training entirely, so
it is not seen by the model at all. As it turns out, the creators of the MNIST dataset
have already done this for us. Do you remember how there was a whole separate
directory called <i>valid?</i> That’s what this directory is for!"|classification models; arrays; slicing row or column; accuracy as metric; metrics; tensors; numeric digit classifier
"<b>SylvainSays</b>
In mathematical terms, accuracy is a function that is constant
almost everywhere (except at the threshold, 0.5), so its derivative is
nil almost everywhere (and infinity at the threshold). This then
gives gradients that are 0 or infinite, which are useless for updating
the model.
Instead, we need a loss function that, when our weights result in slightly better pre‐
dictions, gives us a slightly better loss. So what does a “slightly better prediction” look
like, exactly? Well, in this case, it means that if the correct answer is a 3, the score is a
little higher, or if the correct answer is a 7, the score is a little lower.
Let’s write such a function now. What form does it take?
The loss function receives not the images themselves, but the predictions from the
model. So let’s make one argument, prds, of values between 0 and 1, where each value
is the prediction that an image is a 3. It is a vector (i.e., a rank-1 tensor) indexed over
the images.
The purpose of the loss function is to measure the difference between predicted val‐
ues and the true values—that is, the targets (aka labels). Let’s therefore make another
argument, trgts , with values of 0 or 1 that tells whether an image actually is a 3 or
not. It is also a vector (i.e., another rank-1 tensor) indexed over the images.
For instance, suppose we had three images that we knew were a 3, a 7, and a 3. And
suppose our model predicted with high confidence (0.9) that the first was a 3, with
slight confidence (0.4) that the second was a 7, and with fair confidence (0.2), but
incorrectly, that the last was a 7. This would mean our loss function would receive
these values as its inputs:
trgts = tensor([1,0,1])
prds = tensor([0.9, 0.4, 0.2])
Here’s a first try at a loss function that measures the distance between predictions
and targets:
<b>def</b> mnist_loss(predictions, targets):
<b>return</b> torch.where(targets==1, 1-predictions, predictions).mean()
We’re using a new function, torch.where(a,b,c). This is the same as running the list
comprehension [b[i] if a[i] else c[i] for i in range(len(a))] , except it
works on tensors, at C/CUDA speed. In plain English, this function will measure how
distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it
should be 0, and then it will take the mean of all those distances."|MNIST loss function; numerical digit image classifier
"Note that the output of the final Conv2d layer is 64x2x1x1 . We need to remove those
extra 1x1 axes; that’s what Flatten does. It’s basically the same as PyTorch’s squeeze
method, but as a module.
Let’s see if this trains! Since this is a deeper network than we’ve built from scratch
before, we’ll use a lower learning rate and more epochs:
learn.fit_one_cycle(2, 0.01)
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.072684 0.045110 0.990186 00:05
1 0.022580 0.030775 0.990186 00:05
Success! It’s getting closer to the resnet18 result we had, although it’s not quite there
yet, and it’s taking more epochs, and we’re needing to use a lower learning rate. We
still have a few more tricks to learn, but we’re getting closer and closer to being able to
create a modern CNN from scratch.
<header><largefont><b>Understanding</b></largefont> <largefont><b>Convolution</b></largefont> <largefont><b>Arithmetic</b></largefont></header>
We can see from the summary that we have an input of size 64x1x28x28 . The axes are
batch,channel,height,width. This is often represented as NCHW (where N refers to
batch size). TensorFlow, on the other hand, uses NHWC axis order. Here is the first
layer:
m = learn.model[0]
m
Sequential(
(0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
(1): ReLU()
)
So we have 1 input channel, 4 output channels, and a 3×3 kernel. Let’s check the
weights of the first convolution:
m[0].weight.shape
torch.Size([4, 1, 3, 3])
The summary shows we have 40 parameters, and 4*1*3*3 is 36. What are the other
four parameters? Let’s see what the bias contains:
m[0].bias.shape
torch.Size([4])
We can now use this information to clarify our statement in the previous section:
“When we use a stride-2 convolution, we often increase the number of features"|convolutional neural network (CNN); building a CNN
"Yes, that is showing what you think it is: Google Photos classified a Black user’s photo
with their friend as “gorillas”! This algorithmic misstep got a lot of attention in the
media. “We’re appalled and genuinely sorry that this happened,” a company spokes‐
woman said. “There is still clearly a lot of work to do with automatic image labeling,
and we’re looking at how we can prevent these types of mistakes from happening in
the future.”
Unfortunately, fixing problems in machine learning systems when the input data has
problems is hard. Google’s first attempt didn’t inspire confidence, as coverage by <i>The</i>
<i>Guardian</i> suggested (Figure 3-9).
<i>Figure</i> <i>3-9.</i> <i>Google’s</i> <i>first</i> <i>response</i> <i>to</i> <i>the</i> <i>problem</i>
These kinds of problems are certainly not limited to Google. MIT researchers studied
the most popular online computer vision APIs to see how accurate they were. But
they didn’t just calculate a single accuracy number—instead, they looked at the accu‐
racy across four groups, as illustrated in Figure 3-10."|facial recognition bias; bias; ethics; facial recognition accuracy; historical bias; racial bias
"• By training with higher learning rates, we overfit less because we skip over the
sharp local minima to end up in a smoother (and therefore more generalizable)
part of the loss.
The second point is an interesting and subtle one; it is based on the observation that a
model that generalizes well is one whose loss would not change very much if you
changed the input by a small amount. If a model trains at a large learning rate for
quite a while, and can find a good loss when doing so, it must have found an area that
also generalizes well, because it is jumping around a lot from batch to batch (that is
basically the definition of a high learning rate). The problem is that, as we have dis‐
cussed, just jumping to a high learning rate is more likely to result in diverging losses,
rather than seeing your losses improve. So we don’t jump straight to a high learning
rate. Instead, we start at a low learning rate, where our losses do not diverge, and we
allow the optimizer to gradually find smoother and smoother areas of our parameters
by gradually going to higher and higher learning rates.
Then, once we have found a nice smooth area for our parameters, we want to find the
very best part of that area, which means we have to bring our learning rates down
again. This is why 1cycle training has a gradual learning rate warmup, and a gradual
learning rate cooldown. Many researchers have found that in practice this approach
leads to more accurate models and trains more quickly. That is why it is the approach
fine_tune
that is used by default for in fastai.
In Chapter 16, we’ll learn all about <i>momentum</i> in SGD. Briefly, momentum is a tech‐
nique whereby the optimizer takes a step not only in the direction of the gradients,
but also that continues in the direction of previous steps. Leslie Smith introduced the
idea of <i>cyclical</i> <i>momentum</i> in “A Disciplined Approach to Neural Network Hyper-
Parameters: Part 1”. It suggests that the momentum varies in the opposite direction of
the learning rate: when we are at high learning rates, we use less momentum, and we
use more again in the annealing phase.
We can use 1cycle training in fastai by calling fit_one_cycle:
<b>def</b> fit(epochs=1, lr=0.06):
learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,
metrics=accuracy, cbs=ActivationStats(with_hist=True))
learn.fit_one_cycle(epochs, lr)
<b>return</b> learn
learn = fit()
<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>accuracy</b> <b>time</b>
0 0.210838 0.084827 0.974300 00:08
We’re finally making some progress! It’s giving us a reasonable accuracy now."|convolutional neural network (CNN); building a CNN; cyclical momentum; training more stable; training on all digits; research papers; Smith; stochastic gradient descent (SGD); training
"“Latanya Sweeney, Arrested?” even though she is the only known Latanya Sweeney
and has never been arrested. However, when she Googled other names, such as
“Kirsten Lindquist,” she got more neutral ads, even though Kirsten Lindquist has
been arrested three times.
<i>Figure</i> <i>3-1.</i> <i>Google</i> <i>search</i> <i>showing</i> <i>ads</i> <i>about</i> <i>Professor</i> <i>Latanya</i> <i>Sweeney’s</i> <i>(nonexistent)</i>
<i>arrest</i> <i>record</i>
Being a computer scientist, she studied this systematically and looked at over 2,000
names. She found a clear pattern: historically Black names received advertisements
suggesting that the person had a criminal record, whereas traditionally white names
had more neutral advertisements.
This is an example of bias. It can make a big difference to people’s lives—for instance,
if a job applicant is Googled, it may appear that they have a criminal record when
they do not.
<header><largefont><b>Why</b></largefont> <largefont><b>Does</b></largefont> <largefont><b>This</b></largefont> <largefont><b>Matter?</b></largefont></header>
One very natural reaction to considering these issues is: “So what? What’s that got to
do with me? I’m a data scientist, not a politician. I’m not one of the senior executives
at my company who make the decisions about what we do. I’m just trying to build the
most predictive model I can.”
These are very reasonable questions. But we’re going to try to convince you that the
answer is that everybody who is training models absolutely needs to consider how"|ethics; training
"To do this, we are going to rearrange our dataset. First we divide the samples into
m = len(dset) // bs groups (this is the equivalent of splitting the whole concaten‐
ated dataset into, for example, 64 equally sized pieces, since we’re using bs=64 here). m
is the length of each of these pieces. For instance, if we’re using our whole dataset
(although we’ll actually split it into train versus valid in a moment), we have this:
m = len(seqs)//bs
m,bs,len(seqs)
(328, 64, 21031)
The first batch will be composed of the samples
(0, m, 2*m, ..., (bs-1)*m)
the second batch of the samples
(1, m+1, 2*m+1, ..., (bs-1)*m+1)
and so forth. This way, at each epoch, the model will see a chunk of contiguous text of
size 3*m (since each text is of size 3) on each line of the batch.
The following function does that reindexing:
<b>def</b> group_chunks(ds, bs):
m = len(ds) // bs
new_ds = L()
<b>for</b> i <b>in</b> range(m): new_ds += L(ds[i + m*j] <b>for</b> j <b>in</b> range(bs))
<b>return</b> new_ds
Then we just pass drop_last=True when building our DataLoaders to drop the last
batch that does not have a shape of bs. We also pass shuffle=False to make sure the
texts are read in order:
cut = int(len(seqs) * 0.8)
dls = DataLoaders.from_dsets(
group_chunks(seqs[:cut], bs),
group_chunks(seqs[cut:], bs),
bs=bs, drop_last=True, shuffle=False)
The last thing we add is a little tweak of the training loop via a Callback . We will talk
more about callbacks in Chapter 16; this one will call the reset method of our model
at the beginning of each epoch and before each validation phase. Since we imple‐
mented that method to set the hidden state of the model to zero, this will make sure
we start with a clean state before reading those continuous chunks of text. We can
also start training a bit longer:
learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy,
metrics=accuracy, cbs=ModelResetter)
learn.fit_one_cycle(10, 3e-3)"|callbacks; language model; natural language processing (NLP); improved RNN
"<header><largefont><b>Test</b></largefont> <largefont><b>Time</b></largefont> <largefont><b>Augmentation</b></largefont></header>
We have been using random cropping as a way to get some useful data augmentation,
which leads to better generalization, and results in a need for less training data. When
we use random cropping, fastai will automatically use center-cropping for the valida‐
tion set—that is, it will select the largest square area it can in the center of the image,
without going past the image’s edges.
This can often be problematic. For instance, in a multi-label dataset, sometimes there
are small objects toward the edges of an image; these could be entirely cropped out by
center cropping. Even for problems such as our pet breed classification example, it’s
possible that a critical feature necessary for identifying the correct breed, such as the
color of the nose, could be cropped out.
One solution to this problem is to avoid random cropping entirely. Instead, we could
simply squish or stretch the rectangular images to fit into a square space. But then we
miss out on a very useful data augmentation, and we also make the image recognition
more difficult for our model, because it has to learn how to recognize squished and
squeezed images, rather than just correctly proportioned images.
Another solution is to not center crop for validation, but instead to select a number of
areas to crop from the original rectangular image, pass each of them through our
model, and take the maximum or average of the predictions. In fact, we could do this
not just for different crops, but for different values across all of our test time augmen‐
tation parameters. This is known as <i>test</i> <i>time</i> <i>augmentation</i> (TTA).
<b>Jargon:TestTimeAugmentation(TTA)</b>
During inference or validation, creating multiple versions of each
image using data augmentation, and then taking the average or
maximum of the predictions for each augmented version of the
image.
Depending on the dataset, test time augmentation can result in dramatic improve‐
ments in accuracy. It does not change the time required to train at all, but will
increase the amount of time required for validation or inference by the number of
test-time-augmented images requested. By default, fastai will use the unaugmented
center crop image plus four randomly augmented images.
You can pass any DataLoader to fastai’s tta method; by default, it will use your vali‐
dation set:
preds,targs = learn.tta()
accuracy(preds, targs).item()
0.8737863898277283"|test time augmentation; test time augmentation instead; test time augmentation (TTA)
"array(im3)[4:10,4:10]
array([[ 0, 0, 0, 0, 0, 0],
[ 0, 0, 0, 0, 0, 29],
[ 0, 0, 0, 48, 166, 224],
[ 0, 93, 244, 249, 253, 187],
[ 0, 107, 253, 253, 230, 48],
[ 0, 3, 20, 20, 15, 0]], dtype=uint8)
The 4:10 indicates we requested the rows from index 4 (inclusive) to 10 (noninclu‐
sive), and the same for the columns. NumPy indexes from top to bottom and from
left to right, so this section is located near the top-left corner of the image. Here’s the
same thing as a PyTorch tensor:
tensor(im3)[4:10,4:10]
tensor([[ 0, 0, 0, 0, 0, 0],
[ 0, 0, 0, 0, 0, 29],
[ 0, 0, 0, 48, 166, 224],
[ 0, 93, 244, 249, 253, 187],
[ 0, 107, 253, 253, 230, 48],
[ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8)
We can slice the array to pick just the part with the top of the digit in it, and then use
a Pandas DataFrame to color-code the values using a gradient, which shows us clearly
how the image is created from the pixel values:
im3_t = tensor(im3)
df = pd.DataFrame(im3_t[4:15,4:22])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')
You can see that the background white pixels are stored as the number 0, black is the
number 255, and shades of gray are between the two. The entire image contains 28
pixels across and 28 pixels down, for a total of 768 pixels. (This is much smaller than
an image that you would get from a phone camera, which has millions of pixels, but
is a convenient size for our initial learning and experiments. We will build up to big‐
ger, full-color images soon.)"|pixels as foundation; color-code image values; PyTorch; color-code array or tensor; pixels; tensors
"<b>def</b> after_batch(self):
xb,yb = self.batch
acc = (self.preds.argmax(dim=1)==yb).float().sum()
self.accs.append(acc)
n = len(xb)
self.losses.append(self.loss*n)
self.ns.append(n)
Now we’re ready to use our Learner for the first time!
cbs = [SetupLearnerCB(),TrackResults()]
learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs)
learn.fit(1)
0 True 2.1275552130636814 0.2314922378287042
0 False 1.9942575636942674 0.2991082802547771
It’s quite amazing to realize that we can implement all the key ideas from fastai’s
Learner in so little code! Let’s now add some learning rate scheduling.
<header><largefont><b>Scheduling</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Learning</b></largefont> <largefont><b>Rate</b></largefont></header>
If we’re going to get good results, we’ll want an LR finder and 1cycle training. These
are both <i>annealing</i> callbacks—that is, they are gradually changing hyperparameters as
we train. Here’s LRFinder :
<b>class</b> <b>LRFinder(Callback):</b>
<b>def</b> before_fit(self):
self.losses,self.lrs = [],[]
self.learner.lr = 1e-6
<b>def</b> before_batch(self):
<b>if</b> <b>not</b> self.model.training: <b>return</b>
self.opt.lr *= 1.2
<b>def</b> after_batch(self):
<b>if</b> <b>not</b> self.model.training: <b>return</b>
<b>if</b> self.opt.lr>10 <b>or</b> torch.isnan(self.loss): <b>raise</b> CancelFitException
self.losses.append(self.loss.item())
self.lrs.append(self.opt.lr)
This shows how we’re using CancelFitException , which is itself an empty class, used
only to signify the type of exception. You can see in Learner that this exception is
caught. (You should add and test CancelBatchException, CancelEpochException,
etc. yourself.) Let’s try it out, by adding it to our list of callbacks:
lrfind = LRFinder()
learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind])
learn.fit(2)"|callbacks; learning rate scheduling
"This looks identical to our original diagram in Figure 1-4, just with the word <i>program</i>
replaced with <i>model.</i> This is an important insight: <i>a</i> <i>trained</i> <i>model</i> <i>can</i> <i>be</i> <i>treated</i> <i>just</i>
<i>like</i> <i>a</i> <i>regular</i> <i>computer</i> <i>program.</i>
<b>Jargon:MachineLearning</b>
The training of programs developed by allowing a computer to
learn from its experience, rather than through manually coding the
individual steps.
<header><largefont><b>What</b></largefont> <largefont><b>Is</b></largefont> <largefont><b>a</b></largefont> <largefont><b>Neural</b></largefont> <largefont><b>Network?</b></largefont></header>
It’s not too hard to imagine what the model might look like for a checkers program.
There might be a range of checkers strategies encoded, and some kind of search
mechanism, and then the weights could vary how strategies are selected, what parts of
the board are focused on during a search, and so forth. But it’s not at all obvious what
the model might look like for an image recognition program, or for understanding
text, or for many other interesting problems we might imagine.
What we would like is some kind of function that is so flexible that it could be used to
solve any given problem, just by varying its weights. Amazingly enough, this function
actually exists! It’s the neural network, which we already discussed. That is, if you
regard a neural network as a mathematical function, it turns out to be a function that
is extremely flexible depending on its weights. A mathematical proof called the <i>uni‐</i>
<i>versal</i> <i>approximation</i> <i>theorem</i> shows that this function can solve any problem to any
level of accuracy, in theory. The fact that neural networks are so flexible means that,
in practice, they are often a suitable kind of model, and you can focus your effort on
the process of training them—that is, of finding good weight assignments.
But what about that process? One could imagine that you might need to find a new
“mechanism” for automatically updating weight for every problem. This would be
laborious. What we’d like here as well is a completely general way to update the
weights of a neural network, to make it improve at any given task. Conveniently, this
also exists!
This is called <i>stochastic</i> <i>gradient</i> <i>descent</i> (SGD). We’ll see how neural networks and
SGD work in detail in Chapter 4, as well as explaining the universal approximation
theorem. For now, however, we will instead use Samuel’s own words: <i>We</i> <i>need</i> <i>not</i> <i>go</i>
<i>into</i> <i>the</i> <i>details</i> <i>of</i> <i>such</i> <i>a</i> <i>procedure</i> <i>to</i> <i>see</i> <i>that</i> <i>it</i> <i>could</i> <i>be</i> <i>made</i> <i>entirely</i> <i>automatic</i> <i>and</i> <i>to</i>
<i>see</i> <i>that</i> <i>a</i> <i>machine</i> <i>so</i> <i>programmed</i> <i>would</i> <i>“learn”</i> <i>from</i> <i>its</i> <i>experience.</i>"|machine learning (ML); via neural networks; models; neural networks; machine learning concepts; programs versus models; stochastic gradient descent (SGD); trained model is program; universal approximation theorem; weights
"<b>epoch</b> <b>train_loss</b> <b>valid_loss</b> <b>error_rate</b> <b>time</b>
5 0.169985 0.187885 0.056157 00:25
6 0.153205 0.186145 0.058863 00:25
7 0.141480 0.185316 0.053451 00:25
8 0.128564 0.180999 0.051421 00:25
9 0.126941 0.186288 0.054127 00:25
10 0.130064 0.181764 0.054127 00:25
11 0.124281 0.181855 0.054127 00:25
Now the fine-tuning is working great!
fastai can show us a graph of the training and validation loss:
learn.recorder.plot_loss()
As you can see, the training loss keeps getting better and better. But notice that even‐
tually the validation loss improvement slows and sometimes even gets worse! This is
the point at which the model is starting to overfit. In particular, the model is becom‐
ing overconfident of its predictions. But this does <i>not</i> mean that it is getting less accu‐
rate, necessarily. Take a look at the table of training results per epoch, and you will
often see that the accuracy continues improving, even as the validation loss gets
worse. In the end, what matters is your accuracy, or more generally your chosen met‐
rics, not the loss. The loss is just the function we’ve given the computer to help us to
optimize.
Another decision you have to make when training the model is how long to train for.
We’ll consider that next.
<header><largefont><b>Selecting</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Number</b></largefont> <largefont><b>of</b></largefont> <largefont><b>Epochs</b></largefont></header>
Often you will find that you are limited by time, rather than generalization and accu‐
racy, when choosing how many epochs to train for. So your first approach to training
should be to simply pick a number of epochs that will train in the amount of time that"|improving while validation loss worse; epochs; image classifier model training; validation loss improvement slowing; training versus validation loss; predictions
"To move images for which we’ve selected a different category, we would run this:
<b>for</b> idx,cat <b>in</b> cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)
<b>SylvainSays</b>
Cleaning the data and getting it ready for your model are two of
the biggest challenges for data scientists; they say it takes 90% of
their time. The fastai library aims to provide tools that make it as
easy as possible.
We’ll be seeing more examples of model-driven data cleaning throughout this book.
Once we’ve cleaned up our data, we can retrain our model. Try it yourself, and see if
your accuracy improves!
<b>NoNeedforBigData</b>
After cleaning the dataset using these steps, we generally are seeing
100% accuracy on this task. We even see that result when we down‐
load a lot fewer images than the 150 per class we’re using here. As
you can see, the common complaint that <i>you</i> <i>need</i> <i>massive</i> <i>amounts</i>
<i>of</i> <i>data</i> <i>to</i> <i>do</i> <i>deep</i> <i>learning</i> can be a very long way from the truth!
Now that we have trained our model, let’s see how we can deploy it to be used in
practice.
<header><largefont><b>Turning</b></largefont> <largefont><b>Your</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>into</b></largefont> <largefont><b>an</b></largefont> <largefont><b>Online</b></largefont> <largefont><b>Application</b></largefont></header>
We are now going to look at what it takes to turn this model into a working online
application. We will just go as far as creating a basic working prototype; we do not
have the scope in this book to teach you all the details of web application develop‐
ment generally.
<header><largefont><b>Using</b></largefont> <largefont><b>the</b></largefont> <largefont><b>Model</b></largefont> <largefont><b>for</b></largefont> <largefont><b>Inference</b></largefont></header>
Once you’ve got a model you’re happy with, you need to save it so you can then copy
it over to a server where you’ll use it in production. Remember that a model consists
of two parts: the <i>architecture</i> and the trained <i>parameters.</i> The easiest way to save a
model is to save both of these, because that way, when you load the model, you can be
sure that you have the matching architecture and parameters. To save both parts, use
the export method."|architecture of model; datasets; before versus after training; deployment; export method; process end-to-end; web application from model; models; web application from; parameters; data cleanup before versus after; web applications
"• Algorithmic systems are cheap.
Even in the absence of bias, algorithms (and deep learning especially, since it is such
an effective and scalable algorithm) can lead to negative societal problems, such as
when used for <i>disinformation.</i>
<header><largefont><b>Disinformation</b></largefont></header>
<i>Disinformation</i> has a history stretching back hundreds or even thousands of years. It
is not necessarily about getting someone to believe something false, but rather often
used to sow disharmony and uncertainty, and to get people to give up on seeking the
truth. Receiving conflicting accounts can lead people to assume that they can never
know whom or what to trust.
Some people think disinformation is primarily about false information or <i>fake</i> <i>news,</i>
but in reality, disinformation can often contain seeds of truth, or half-truths taken out
of context. Ladislav Bittman was an intelligence officer in the USSR who later defec‐
ted to the US and wrote some books in the 1970s and 1980s on the role of disinfor‐
mation in Soviet propaganda operations. In <i>The</i> <i>KGB</i> <i>and</i> <i>Soviet</i> <i>Disinformation</i>
(Pergamon), he wrote “Most campaigns are a carefully designed mixture of facts,
half-truths, exaggerations, and deliberate lies.”
In the US, this has hit close to home in recent years, with the FBI detailing a massive
disinformation campaign linked to Russia in the 2016 election. Understanding the
disinformation that was used in this campaign is very educational. For instance, the
FBI found that the Russian disinformation campaign often organized two separate
fake “grass roots” protests, one for each side of an issue, and got them to protest at the
same time! The Houston Chronicle reported on one of these odd events
(Figure 3-15):
A group that called itself the “Heart of Texas” had organized it on social media—a pro‐
test, they said, against the “Islamization” of Texas. On one side of Travis Street, I found
about 10 protesters. On the other side, I found around 50 counterprotesters. But I
couldn’t find the rally organizers. No “Heart of Texas.” I thought that was odd, and
mentioned it in the article: What kind of group is a no-show at its own event? Now I
know why. Apparently, the rally’s organizers were in Saint Petersburg, Russia, at the
time. “Heart of Texas” is one of the internet troll groups cited in Special Prosecutor
Robert Mueller’s recent indictment of Russians attempting to tamper with the US pres‐
idential election."|Bittman; ethics; disinformation; (Bittman); Mueller report; Russia and 2016 election
"We can load them back later:
xs_final = (path/'xs_final.pkl').load()
valid_xs_final = (path/'valid_xs_final.pkl').load()
Now we can check our RMSE again, to confirm that the accuracy hasn’t substantially
changed:
m = rf(xs_final, y)
m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)
(0.183263, 0.233846)
By focusing on the most important variables and removing some redundant ones,
we’ve greatly simplified our model. Now, let’s see how those variables affect our pre‐
dictions using partial dependence plots.
<header><largefont><b>Partial</b></largefont> <largefont><b>Dependence</b></largefont></header>
ProductSize YearMade.
As we’ve seen, the two most important predictors are and
We’d like to understand the relationship between these predictors and sale price. It’s a
good idea to first check the count of values per category (provided by the Pandas
value_counts method), to see how common each category is:
p = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()
c = to.classes['ProductSize']
plt.yticks(range(len(c)), c);
The largest group is #na#, which is the label fastai applies to missing values."|bagging; decision trees; machine learning (ML); predictions; random forests; tabular data for models; training
"edges at the middle, top, and bottom, and so forth. So what if we could extract infor‐
mation about where the edges occur in each image, and then use that information as
our features, instead of raw pixels?
It turns out that finding the edges in an image is a very common task in computer
vision and is surprisingly straightforward. To do it, we use something called a <i>convo‐</i>
<i>lution.</i> A convolution requires nothing more than multiplication and addition—two
operations that are responsible for the vast majority of work that we will see in every
single deep learning model in this book!
A convolution applies a <i>kernel</i> across an image. A kernel is a little matrix, such as the
3×3 matrix in the top right of Figure 13-1.
<i>Figure</i> <i>13-1.</i> <i>Applying</i> <i>a</i> <i>kernel</i> <i>to</i> <i>one</i> <i>location</i>
The 7×7 grid to the left is the <i>image</i> we’re going to apply the kernel to. The convolu‐
tion operation multiplies each element of the kernel by each element of a 3×3 block of
the image. The results of these multiplications are then added together. The diagram
in Figure 13-1 shows an example of applying a kernel to a single location in the
image, the 3×3 block around cell 18.
Let’s do this with code. First, we create a little 3×3 matrix like so:
top_edge = tensor([[-1,-1,-1],
[ 0, 0, 0],
[ 1, 1, 1]]).float()
We’re going to call this our kernel (because that’s what fancy computer vision
researchers call these). And we’ll need an image, of course:"|finding edges via convolution; convolutional neural network (CNN); kernel of convolution
"<b>class</b> <b>Lin(LayerFunction):</b>
<b>def</b> <b>__init__(self,</b> w, b): self.w,self.b = w,b
<b>def</b> forward(self, inp): <b>return</b> inp@self.w + self.b
<b>def</b> bwd(self, out, inp):
inp.g = out.g @ self.w.t()
self.w.g = self.inp.t() @ self.out.g
self.b.g = out.g.sum(0)
<b>class</b> <b>Mse(LayerFunction):</b>
<b>def</b> forward (self, inp, targ): <b>return</b> (inp.squeeze() - targ).pow(2).mean()
<b>def</b> bwd(self, out, inp, targ):
inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]
The rest of our model can be the same as before. This is getting closer and closer to
what PyTorch does. Each basic function we need to differentiate is written as a
torch.autograd.Function object that has a forward and a backward method.
PyTorch will then keep track of any computation we do to be able to properly run the
backward pass, unless we set the requires_grad attribute of our tensors to False .
Writing one of these is (almost) as easy as writing our original classes. The difference
is that we choose what to save and what to put in a context variable (so that we make
sure we don’t save anything we don’t need), and we return the gradients in the
backward pass. It’s rare to have to write your own Function, but if you ever need
something exotic or want to mess with the gradients of a regular function, here is
how to write one:
<b>from</b> <b>torch.autograd</b> <b>import</b> Function
<b>class</b> <b>MyRelu(Function):</b>
@staticmethod
<b>def</b> forward(ctx, i):
result = i.clamp_min(0.)
ctx.save_for_backward(i)
<b>return</b> result
@staticmethod
<b>def</b> backward(ctx, grad_output):
i, = ctx.saved_tensors
<b>return</b> grad_output * (i>0).float()
The structure used to build a more complex model that takes advantage of those Func
tions is a torch.nn.Module. This is the base structure for all models, and all the neu‐
ral nets you have seen up until now were from that class. It mostly helps to register all
the trainable parameters, which as we’ve seen can be used in the training loop.
To implement an nn.Module you just need to do the following:
__init__
1. Make sure the superclass is called first when you initialize it."|PyTorch
"<i>Figure</i> <i>3-12.</i> <i>Object</i> <i>detection</i> <i>in</i> <i>action</i>
In this example, we can see that the lower-income soap example is a very long way
away from being accurate, with every commercial image recognition service predict‐
ing “food” as the most likely answer!"|bias; ethics; historical bias
