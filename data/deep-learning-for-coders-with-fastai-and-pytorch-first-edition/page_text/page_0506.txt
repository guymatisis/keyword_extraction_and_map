And for our weights, we’ll use the right scale, which is known as <i>Xavier</i> <i>initialization</i>
(or <i>Glorot</i> <i>initialization):</i>
<b>from</b> <b>math</b> <b>import</b> sqrt
w1 = torch.randn(100,50) / sqrt(100)
b1 = torch.zeros(50)
w2 = torch.randn(50,1) / sqrt(50)
b2 = torch.zeros(1)
Now if we compute the result of the first layer, we can check that the mean and stan‐
dard deviation are under control:
l1 = lin(x, w1, b1)
l1.mean(),l1.std()
(tensor(-0.0050), tensor(1.0000))
Very good. Now we need to go through a ReLU, so let’s define one. A ReLU removes
the negatives and replaces them with zeros, which is another way of saying it clamps
our tensor at zero:
<b>def</b> relu(x): <b>return</b> x.clamp_min(0.)
We pass our activations through this:
l2 = relu(l1)
l2.mean(),l2.std()
(tensor(0.3961), tensor(0.5783))
And we’re back to square one: the mean of our activations has gone to 0.4 (which is
understandable since we removed the negatives), and the std went down to 0.58. So
like before, after a few layers we will probably wind up with zeros:
x = torch.randn(200, 100)
<b>for</b> i <b>in</b> range(50): x = relu(x @ (torch.randn(100,100) * 0.1))
x[0:5,0:5]
tensor([[0.0000e+00, 1.9689e-08, 4.2820e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.6701e-08, 4.3501e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.0976e-08, 3.0411e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.8457e-08, 4.9469e-08, 0.0000e+00, 0.0000e+00],
[0.0000e+00, 1.9949e-08, 4.1643e-08, 0.0000e+00, 0.0000e+00]])
This means our initialization wasn’t right. Why? At the time Glorot and Bengio wrote
their article, the most popular activation in a neural net was the hyperbolic tangent
(tanh, which is the one they used), and that initialization doesn’t account for our
ReLU. Fortunately, someone else has done the math for us and computed the right
scale for us to use. In “Delving Deep into Rectifiers: Surpassing Human-Level Perfor‐
mance” (which we’ve seen before—it’s the article that introduced the ResNet),
Kaiming He et al. show that we should use the following scale instead: 2/n , where
<i>in</i>
<i>n</i> is the number of inputs of our model. Let’s see what this gives us:
<i>in</i>